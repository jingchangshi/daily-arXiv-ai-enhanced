{"id": "2602.06057", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.06057", "abs": "https://arxiv.org/abs/2602.06057", "authors": ["Satyam Kumar", "Saurabh Jha"], "title": "Quantifying Energy-Efficient Edge Intelligence: Inference-time Scaling Laws for Heterogeneous Computing", "comment": null, "summary": "Large language model inference on resource constrained edge devices remains a major challenge for low latency intelligent systems, as existing solutions depend heavily on cloud or datacenter infrastructure. This work introduces QEIL, Quantifying Edge Intelligence via Inference time Scaling Laws, a unified framework for efficient local LLM inference using principled scaling laws and heterogeneous orchestration across CPU, GPU, and NPU accelerators. We derive five architecture agnostic theorems that characterize how inference efficiency scales with model size, sample budget, and device level constraints. QEIL integrates three optimization dimensions. First, inference time scaling laws show that heterogeneous workload distribution achieves superlinear efficiency gains that are not observed in homogeneous execution. Second, hardware aware routing is enabled through analytical cost models that account for compute throughput, memory bandwidth, power consumption, and thermal limits. Third, performance energy trade offs are quantified using novel metrics including Intelligence Per Watt, Energy Coverage Efficiency, and Price Power Performance. A unified orchestrator combines these components through progressive sample multiplexing to improve coverage. Extensive evaluation across five model families from 125M to 2.6B parameters demonstrates consistent gains, including 7 to 10.5 percentage point improvement in pass at k coverage, 35.6 to 78.2 percent energy reduction, 68 percent average power reduction enabling edge thermal budgets, 15.8 percent latency improvement, and zero accuracy loss. Results confirm that inference time scaling laws are universal and architecture agnostic, establishing heterogeneous edge orchestration as the optimal strategy for energy constrained intelligent systems.", "AI": {"tldr": "QEIL\u6846\u67b6\u901a\u8fc7\u63a8\u7406\u65f6\u95f4\u7f29\u653e\u5b9a\u5f8b\u548c\u5f02\u6784\u52a0\u901f\u5668\u7f16\u6392\uff0c\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5b9e\u73b0\u9ad8\u6548\u672c\u5730LLM\u63a8\u7406\uff0c\u663e\u8457\u63d0\u5347\u80fd\u6548\u548c\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\u8fdb\u884c\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u9762\u4e34\u91cd\u5927\u6311\u6218\uff0c\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u4e25\u91cd\u4f9d\u8d56\u4e91\u6216\u6570\u636e\u4e2d\u5fc3\u57fa\u7840\u8bbe\u65bd\uff0c\u65e0\u6cd5\u6ee1\u8db3\u4f4e\u5ef6\u8fdf\u667a\u80fd\u7cfb\u7edf\u7684\u9700\u6c42\u3002", "method": "\u63d0\u51faQEIL\u7edf\u4e00\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u4f18\u5316\u7ef4\u5ea6\uff1a1) \u63a8\u7406\u65f6\u95f4\u7f29\u653e\u5b9a\u5f8b\u663e\u793a\u5f02\u6784\u5de5\u4f5c\u8d1f\u8f7d\u5206\u914d\u80fd\u83b7\u5f97\u8d85\u7ebf\u6027\u6548\u7387\u589e\u76ca\uff1b2) \u901a\u8fc7\u5206\u6790\u6210\u672c\u6a21\u578b\u5b9e\u73b0\u786c\u4ef6\u611f\u77e5\u8def\u7531\uff1b3) \u4f7f\u7528\u65b0\u6307\u6807\u91cf\u5316\u6027\u80fd-\u80fd\u8017\u6743\u8861\u3002\u901a\u8fc7\u6e10\u8fdb\u5f0f\u6837\u672c\u590d\u7528\u7edf\u4e00\u7f16\u6392\u5668\u6574\u5408\u8fd9\u4e9b\u7ec4\u4ef6\u3002", "result": "\u57285\u4e2a\u6a21\u578b\u5bb6\u65cf\uff08125M\u52302.6B\u53c2\u6570\uff09\u4e0a\u7684\u5e7f\u6cdb\u8bc4\u4f30\u663e\u793a\uff1apass@k\u8986\u76d6\u7387\u63d0\u53477-10.5\u4e2a\u767e\u5206\u70b9\uff0c\u80fd\u8017\u964d\u4f4e35.6-78.2%\uff0c\u5e73\u5747\u529f\u8017\u964d\u4f4e68%\u6ee1\u8db3\u8fb9\u7f18\u70ed\u9884\u7b97\uff0c\u5ef6\u8fdf\u6539\u558415.8%\uff0c\u4e14\u65e0\u7cbe\u5ea6\u635f\u5931\u3002", "conclusion": "\u63a8\u7406\u65f6\u95f4\u7f29\u653e\u5b9a\u5f8b\u662f\u901a\u7528\u4e14\u67b6\u6784\u65e0\u5173\u7684\uff0c\u5f02\u6784\u8fb9\u7f18\u7f16\u6392\u662f\u80fd\u8017\u53d7\u9650\u667a\u80fd\u7cfb\u7edf\u7684\u6700\u4f18\u7b56\u7565\u3002"}}
{"id": "2602.06063", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.06063", "abs": "https://arxiv.org/abs/2602.06063", "authors": ["Shouyu Du", "Miaoxiang Yu", "Zhiheng Ni", "Jillian Cai", "Qing Yang", "Tao Wei", "Zhenyu Xu"], "title": "Mapping Gemma3 onto an Edge Dataflow Architecture", "comment": "Original Version, data shall be updated", "summary": "We present the first end-to-end deployment of the Gemma3 family of large language and vision models on a tiled edge dataflow architecture (AMD Ryzen AI NPU). Our work introduces a set of hardware-aware techniques. For prefill, we introduce an efficient dequantization engine, optimize tiled matrix multiplication kernels, and propose FlowQKV, a chunked, pipelined attention mechanism. For decoding, we introduce FusedDQP, which fuses dequantization and projection into a single kernel, and FlowKV, which re-structures attention to sustain high memory bandwidth utilization. Together with a compact Q4NX 4-bit quantization format, these methods yield up to $5.2\\times$ faster prefill and $4.8\\times$ faster decoding versus the iGPU, and $33.5\\times$ and $2.2\\times$ over the CPU, respectively. Power efficiency improves by as much as $67.2\\times$ and $222.9\\times$ compared to the iGPU and CPU. The proposed approach demonstrates that modern NPUs can deliver practical, low-power LLM and VLM inference at the edge, and provides a generalizable blueprint for mapping transformer-based models onto tiled dataflow accelerators.", "AI": {"tldr": "\u672c\u6587\u5c55\u793a\u4e86\u5728AMD Ryzen AI NPU\u4e0a\u9996\u6b21\u7aef\u5230\u7aef\u90e8\u7f72Gemma3\u7cfb\u5217\u5927\u8bed\u8a00\u548c\u89c6\u89c9\u6a21\u578b\uff0c\u901a\u8fc7\u786c\u4ef6\u611f\u77e5\u4f18\u5316\u6280\u672f\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u548c\u80fd\u6548\u6539\u8fdb\u3002", "motivation": "\u73b0\u4ee3NPU\u80fd\u5426\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5b9e\u73b0\u5b9e\u7528\u3001\u4f4e\u529f\u8017\u7684LLM\u548cVLM\u63a8\u7406\uff0c\u4ee5\u53ca\u5982\u4f55\u5c06\u57fa\u4e8etransformer\u7684\u6a21\u578b\u6620\u5c04\u5230\u5206\u5757\u6570\u636e\u6d41\u52a0\u901f\u5668\u4e0a\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u7cfb\u5217\u786c\u4ef6\u611f\u77e5\u4f18\u5316\u6280\u672f\uff1a\u9884\u586b\u5145\u9636\u6bb5\u91c7\u7528\u9ad8\u6548\u53cd\u91cf\u5316\u5f15\u64ce\u3001\u4f18\u5316\u5206\u5757\u77e9\u9635\u4e58\u6cd5\u5185\u6838\u548cFlowQKV\uff08\u5206\u5757\u6d41\u6c34\u7ebf\u6ce8\u610f\u529b\u673a\u5236\uff09\uff1b\u89e3\u7801\u9636\u6bb5\u91c7\u7528FusedDQP\uff08\u878d\u5408\u53cd\u91cf\u5316\u548c\u6295\u5f71\uff09\u548cFlowKV\uff08\u91cd\u6784\u6ce8\u610f\u529b\u4ee5\u7ef4\u6301\u9ad8\u5185\u5b58\u5e26\u5bbd\u5229\u7528\u7387\uff09\uff0c\u914d\u5408\u7d27\u51d1\u7684Q4NX 4\u4f4d\u91cf\u5316\u683c\u5f0f\u3002", "result": "\u76f8\u6bd4iGPU\uff0c\u9884\u586b\u5145\u901f\u5ea6\u63d0\u53475.2\u500d\uff0c\u89e3\u7801\u901f\u5ea6\u63d0\u53474.8\u500d\uff1b\u76f8\u6bd4CPU\uff0c\u9884\u586b\u5145\u63d0\u534733.5\u500d\uff0c\u89e3\u7801\u63d0\u53472.2\u500d\u3002\u80fd\u6548\u76f8\u6bd4iGPU\u63d0\u534767.2\u500d\uff0c\u76f8\u6bd4CPU\u63d0\u5347222.9\u500d\u3002", "conclusion": "\u73b0\u4ee3NPU\u80fd\u591f\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5b9e\u73b0\u5b9e\u7528\u3001\u4f4e\u529f\u8017\u7684LLM\u548cVLM\u63a8\u7406\uff0c\u4e3a\u5c06\u57fa\u4e8etransformer\u7684\u6a21\u578b\u6620\u5c04\u5230\u5206\u5757\u6570\u636e\u6d41\u52a0\u901f\u5668\u63d0\u4f9b\u4e86\u53ef\u63a8\u5e7f\u7684\u84dd\u56fe\u3002"}}
{"id": "2602.06064", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2602.06064", "abs": "https://arxiv.org/abs/2602.06064", "authors": ["Yi-Xiang Hu", "Yuke Wang", "Feng Wu", "Zirui Huang", "Shuli Zeng", "Xiang-Yang Li"], "title": "iScheduler: Reinforcement Learning-Driven Continual Optimization for Large-Scale Resource Investment Problems", "comment": "13 pages, 7 figures,", "summary": "Scheduling precedence-constrained tasks under shared renewable resources is central to modern computing platforms. The Resource Investment Problem (RIP) models this setting by minimizing the cost of provisioned renewable resources under precedence and timing constraints. Exact mixed-integer programming and constraint programming become impractically slow on large instances, and dynamic updates require schedule revisions under tight latency budgets. We present iScheduler, a reinforcement-learning-driven iterative scheduling framework that formulates RIP solving as a Markov decision process over decomposed subproblems and constructs schedules through sequential process selection. The framework accelerates optimization and supports reconfiguration by reusing unchanged process schedules and rescheduling only affected processes. We also release L-RIPLIB, an industrial-scale benchmark derived from cloud-platform workloads with 1,000 instances of 2,500-10,000 tasks. Experiments show that iScheduler attains competitive resource costs while reducing time to feasibility by up to 43$\\times$ against strong commercial baselines.", "AI": {"tldr": "iScheduler\u662f\u4e00\u4e2a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u8fed\u4ee3\u8c03\u5ea6\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u8d44\u6e90\u6295\u8d44\u95ee\u9898\uff0c\u901a\u8fc7\u5c06\u95ee\u9898\u5206\u89e3\u4e3a\u5b50\u95ee\u9898\u5e76\u987a\u5e8f\u9009\u62e9\u8fdb\u7a0b\u6765\u52a0\u901f\u4f18\u5316\uff0c\u652f\u6301\u52a8\u6001\u91cd\u65b0\u914d\u7f6e\u3002", "motivation": "\u4f20\u7edf\u6df7\u5408\u6574\u6570\u89c4\u5212\u548c\u7ea6\u675f\u89c4\u5212\u65b9\u6cd5\u5728\u5927\u89c4\u6a21\u5b9e\u4f8b\u4e0a\u901f\u5ea6\u8fc7\u6162\uff0c\u4e14\u52a8\u6001\u66f4\u65b0\u9700\u8981\u5728\u4e25\u683c\u5ef6\u8fdf\u9884\u7b97\u4e0b\u91cd\u65b0\u8c03\u5ea6\uff0c\u9700\u8981\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5c06\u8d44\u6e90\u6295\u8d44\u95ee\u9898\u5efa\u6a21\u4e3a\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff0c\u5206\u89e3\u4e3a\u5b50\u95ee\u9898\uff0c\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u9a71\u52a8\u7684\u987a\u5e8f\u8fdb\u7a0b\u9009\u62e9\u6784\u5efa\u8c03\u5ea6\u65b9\u6848\uff0c\u652f\u6301\u91cd\u7528\u672a\u66f4\u6539\u7684\u8fdb\u7a0b\u8c03\u5ea6\u3002", "result": "iScheduler\u5728\u4fdd\u6301\u7ade\u4e89\u529b\u7684\u8d44\u6e90\u6210\u672c\u7684\u540c\u65f6\uff0c\u5c06\u53ef\u884c\u6027\u65f6\u95f4\u51cf\u5c11\u4e86\u9ad8\u8fbe43\u500d\uff0c\u76f8\u6bd4\u5546\u4e1a\u57fa\u7ebf\u663e\u8457\u52a0\u901f\u3002", "conclusion": "iScheduler\u4e3a\u5927\u89c4\u6a21\u8d44\u6e90\u6295\u8d44\u95ee\u9898\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u5f3a\u5316\u5b66\u4e60\u89e3\u51b3\u65b9\u6848\uff0c\u652f\u6301\u5feb\u901f\u4f18\u5316\u548c\u52a8\u6001\u91cd\u65b0\u914d\u7f6e\uff0c\u5e76\u53d1\u5e03\u4e86\u5de5\u4e1a\u7ea7\u57fa\u51c6\u6570\u636e\u96c6L-RIPLIB\u3002"}}
{"id": "2602.06069", "categories": ["cs.DC", "cs.AI", "cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2602.06069", "abs": "https://arxiv.org/abs/2602.06069", "authors": ["Dinesh Gopalan", "Ratul Ali"], "title": "HQP: Sensitivity-Aware Hybrid Quantization and Pruning for Ultra-Low-Latency Edge AI Inference", "comment": "7 pages, 3 figures, 2 tables", "summary": "The escalating demand for high-fidelity, real-time inference in distributed edge-cloud environments necessitates aggressive model optimization to counteract severe latency and energy constraints. This paper introduces the Hybrid Quantization and Pruning (HQP) framework, a novel, integrated methodology designed to achieve synergistic model acceleration while adhering to strict quality guarantees. We detail a sensitivity-aware structural pruning algorithm that employs a dynamic weight sensitivity metric, derived from a highly efficient approximation of the Fisher Information Matrix (FIM), to guide the iterative removal of redundant filters. This pruning is strictly conditional, enforcing an adherence to a maximum permissible accuracy drop (Delta ax) before the model proceeds to 8-bit post-training quantization. This rigorous coordination is critical, as it ensures the resultant sparse model structure is maximally robust to quantization error and hardware-specific kernel optimization. Exhaustive evaluation across heterogeneous NVIDIA Jetson edge platforms, utilizing resource-efficient architectures like MobileNetV3 and ResNet-18, demonstrates that the HQP framework achieves a peak performance gain of 3.12 times inference speedup and a 55 percent model size reduction, while rigorously containing the accuracy drop below the 1.5 percent constraint. A comprehensive comparative analysis against conventional single-objective compression techniques validates the HQP framework as a superior, hardware-agnostic solution for deploying ultra-low-latency AI in resource-limited edge infrastructures.", "AI": {"tldr": "HQP\u6846\u67b6\u901a\u8fc7\u654f\u611f\u5ea6\u611f\u77e5\u7684\u7ed3\u6784\u526a\u679d\u548c8\u4f4d\u540e\u8bad\u7ec3\u91cf\u5316\u7684\u534f\u540c\u4f18\u5316\uff0c\u5728\u4fdd\u8bc1\u7cbe\u5ea6\u4e0b\u964d\u4e0d\u8d85\u8fc71.5%\u7684\u524d\u63d0\u4e0b\uff0c\u5b9e\u73b03.12\u500d\u63a8\u7406\u52a0\u901f\u548c55%\u6a21\u578b\u538b\u7f29\uff0c\u9002\u7528\u4e8e\u8fb9\u7f18\u8bbe\u5907\u90e8\u7f72\u3002", "motivation": "\u8fb9\u7f18\u4e91\u73af\u5883\u4e2d\u5bf9\u9ad8\u4fdd\u771f\u5b9e\u65f6\u63a8\u7406\u7684\u9700\u6c42\u65e5\u76ca\u589e\u957f\uff0c\u4f46\u9762\u4e34\u4e25\u91cd\u7684\u5ef6\u8fdf\u548c\u80fd\u8017\u9650\u5236\uff0c\u9700\u8981\u6fc0\u8fdb\u7684\u6a21\u578b\u4f18\u5316\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u6df7\u5408\u91cf\u5316\u4e0e\u526a\u679d\u6846\u67b6\uff1a1\uff09\u57fa\u4e8eFisher\u4fe1\u606f\u77e9\u9635\u9ad8\u6548\u8fd1\u4f3c\u7684\u52a8\u6001\u6743\u91cd\u654f\u611f\u5ea6\u6307\u6807\u6307\u5bfc\u8fed\u4ee3\u526a\u679d\uff1b2\uff09\u5728\u6ee1\u8db3\u6700\u5927\u5141\u8bb8\u7cbe\u5ea6\u4e0b\u964d\u540e\u624d\u8fdb\u884c8\u4f4d\u540e\u8bad\u7ec3\u91cf\u5316\uff1b3\uff09\u786e\u4fdd\u7a00\u758f\u6a21\u578b\u7ed3\u6784\u5bf9\u91cf\u5316\u8bef\u5dee\u548c\u786c\u4ef6\u4f18\u5316\u5177\u6709\u6700\u5927\u9c81\u68d2\u6027\u3002", "result": "\u5728NVIDIA Jetson\u8fb9\u7f18\u5e73\u53f0\u4e0a\uff0c\u4f7f\u7528MobileNetV3\u548cResNet-18\u67b6\u6784\uff0c\u5b9e\u73b0\u5cf0\u503c3.12\u500d\u63a8\u7406\u52a0\u901f\u548c55%\u6a21\u578b\u5927\u5c0f\u7f29\u51cf\uff0c\u540c\u65f6\u7cbe\u5ea6\u4e0b\u964d\u4e25\u683c\u63a7\u5236\u57281.5%\u4ee5\u5185\u3002", "conclusion": "HQP\u6846\u67b6\u76f8\u6bd4\u4f20\u7edf\u5355\u76ee\u6807\u538b\u7f29\u6280\u672f\u5177\u6709\u4f18\u8d8a\u6027\uff0c\u662f\u8d44\u6e90\u53d7\u9650\u8fb9\u7f18\u57fa\u7840\u8bbe\u65bd\u4e2d\u90e8\u7f72\u8d85\u4f4e\u5ef6\u8fdfAI\u7684\u786c\u4ef6\u65e0\u5173\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.06252", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2602.06252", "abs": "https://arxiv.org/abs/2602.06252", "authors": ["Ahmed J. Abdelmaksoud", "Cristian Sestito", "Shiwei Wang", "Themis Prodromakis"], "title": "D-Legion: A Scalable Many-Core Architecture for Accelerating Matrix Multiplication in Quantized LLMs", "comment": null, "summary": "The performance gains obtained by large language models (LLMs) are closely linked to their substantial computational and memory requirements. Quantized LLMs offer significant advantages with extremely quantized models, motivating the development of specialized architectures to accelerate their workloads. This paper proposes D-Legion, a novel scalable many-core architecture, designed using many adaptive-precision systolic array cores, to accelerate matrix multiplication in quantized LLMs. The proposed architecture consists of a set of Legions where each Legion has a group of adaptive-precision systolic arrays. D-Legion supports multiple computation modes, including quantized sparse and dense matrix multiplications. The block structured sparsity is exploited within a fully-sparse, or partially-sparse windows. In addition, memory accesses of partial summations (psums) are spatially reduced through parallel accumulators. Furthermore, data reuse is maximized through optimized scheduling techniques by multicasting matrix tiles across the Legions. A comprehensive design space exploration is performed in terms of Legion/core granularity to determine the optimal Legion configuration. Moreover, D-Legion is evaluated on attention workloads from two BitNet models, delivering up to 8.2$\\times$ lower latency, up to 3.8$\\times$ higher memory savings, and up to 3$\\times$ higher psum memory savings compared to state-of-the-art work. D-Legion, with eight Legions and 64 total cores, achieves a peak throughput of 135,68 TOPS at a frequency of 1 GHz. A scaled version of D-Legion, with 32 Legions, is compared to Google TPUv4i, achieving up to 2.5$\\times$ lower total latency, up to 2.3$\\times$ higher total throughput, and up to 2.7$\\times$ higher total memory savings.", "AI": {"tldr": "D-Legion\u662f\u4e00\u79cd\u65b0\u578b\u53ef\u6269\u5c55\u591a\u6838\u67b6\u6784\uff0c\u4f7f\u7528\u81ea\u9002\u5e94\u7cbe\u5ea6\u8109\u52a8\u9635\u5217\u6838\u5fc3\u52a0\u901f\u91cf\u5316LLM\u4e2d\u7684\u77e9\u9635\u4e58\u6cd5\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6848\u5728\u5ef6\u8fdf\u3001\u5185\u5b58\u548c\u541e\u5410\u91cf\u65b9\u9762\u6709\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u6027\u80fd\u63d0\u5347\u4f34\u968f\u7740\u5de8\u5927\u7684\u8ba1\u7b97\u548c\u5185\u5b58\u9700\u6c42\uff0c\u91cf\u5316LLM\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u4f46\u9700\u8981\u4e13\u95e8\u7684\u67b6\u6784\u6765\u52a0\u901f\u5176\u5de5\u4f5c\u8d1f\u8f7d\u3002", "method": "\u63d0\u51faD-Legion\u67b6\u6784\uff0c\u5305\u542b\u591a\u4e2aLegion\uff0c\u6bcf\u4e2aLegion\u6709\u4e00\u7ec4\u81ea\u9002\u5e94\u7cbe\u5ea6\u8109\u52a8\u9635\u5217\u3002\u652f\u6301\u91cf\u5316\u7a00\u758f\u548c\u7a20\u5bc6\u77e9\u9635\u4e58\u6cd5\uff0c\u5229\u7528\u5757\u7ed3\u6784\u5316\u7a00\u758f\u6027\uff0c\u901a\u8fc7\u5e76\u884c\u7d2f\u52a0\u5668\u51cf\u5c11\u90e8\u5206\u548c\u7684\u5185\u5b58\u8bbf\u95ee\uff0c\u901a\u8fc7\u591a\u64ad\u4f18\u5316\u6570\u636e\u91cd\u7528\u3002", "result": "\u5728BitNet\u6a21\u578b\u7684\u6ce8\u610f\u529b\u5de5\u4f5c\u8d1f\u8f7d\u4e0a\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u65b9\u6848\uff0c\u5ef6\u8fdf\u964d\u4f4e8.2\u500d\uff0c\u5185\u5b58\u8282\u77013.8\u500d\uff0c\u90e8\u5206\u548c\u5185\u5b58\u8282\u77013\u500d\u30028\u4e2aLegion\u300164\u6838\u5fc3\u7248\u672c\u5cf0\u503c\u541e\u5410\u91cf135.68 TOPS\u300232\u4e2aLegion\u7248\u672c\u76f8\u6bd4Google TPUv4i\uff0c\u5ef6\u8fdf\u964d\u4f4e2.5\u500d\uff0c\u541e\u5410\u91cf\u63d0\u53472.3\u500d\uff0c\u5185\u5b58\u8282\u77012.7\u500d\u3002", "conclusion": "D-Legion\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u53ef\u6269\u5c55\u67b6\u6784\uff0c\u80fd\u591f\u663e\u8457\u52a0\u901f\u91cf\u5316LLM\u7684\u77e9\u9635\u4e58\u6cd5\u8fd0\u7b97\uff0c\u5728\u6027\u80fd\u3001\u5185\u5b58\u6548\u7387\u548c\u541e\u5410\u91cf\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.06142", "categories": ["cs.PL", "cs.AI", "cs.CL", "cs.LG", "cs.PF"], "pdf": "https://arxiv.org/pdf/2602.06142", "abs": "https://arxiv.org/abs/2602.06142", "authors": ["Amir H. Ashouri", "Shayan Shirahmad Gale Bagi", "Kavin Satheeskumar", "Tejas Srikanth", "Jonathan Zhao", "Ibrahim Saidoun", "Ziwen Wang", "Bryan Chan", "Tomasz S. Czajkowski"], "title": "Protean Compiler: An Agile Framework to Drive Fine-grain Phase Ordering", "comment": "Version 1- Submitted for a possible publication in 2026", "summary": "The phase ordering problem has been a long-standing challenge since the late 1970s, yet it remains an open problem due to having a vast optimization space and an unbounded nature, making it an open-ended problem without a finite solution, one can limit the scope by reducing the number and the length of optimizations. Traditionally, such locally optimized decisions are made by hand-coded algorithms tuned for a small number of benchmarks, often requiring significant effort to be retuned when the benchmark suite changes. In the past 20 years, Machine Learning has been employed to construct performance models to improve the selection and ordering of compiler optimizations, however, the approaches are not baked into the compiler seamlessly and never materialized to be leveraged at a fine-grained scope of code segments. This paper presents Protean Compiler: An agile framework to enable LLVM with built-in phase-ordering capabilities at a fine-grained scope. The framework also comprises a complete library of more than 140 handcrafted static feature collection methods at varying scopes, and the experimental results showcase speedup gains of up to 4.1% on average and up to 15.7% on select Cbench applications wrt LLVM's O3 by just incurring a few extra seconds of build time on Cbench. Additionally, Protean compiler allows for an easy integration with third-party ML frameworks and other Large Language Models, and this two-step optimization shows a gain of 10.1% and 8.5% speedup wrt O3 on Cbench's Susan and Jpeg applications. Protean compiler is seamlessly integrated into LLVM and can be used as a new, enhanced, full-fledged compiler. We plan to release the project to the open-source community in the near future.", "AI": {"tldr": "Protean Compiler\u662f\u4e00\u4e2a\u96c6\u6210\u5230LLVM\u4e2d\u7684\u654f\u6377\u6846\u67b6\uff0c\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u4ee3\u7801\u6bb5\u7684\u7f16\u8bd1\u5668\u4f18\u5316\u9636\u6bb5\u6392\u5e8f\uff0c\u76f8\u6bd4O3\u4f18\u5316\u5e73\u5747\u63d0\u53474.1%\uff0c\u7279\u5b9a\u5e94\u7528\u6700\u9ad8\u63d0\u534715.7%\u3002", "motivation": "\u7f16\u8bd1\u5668\u4f18\u5316\u9636\u6bb5\u6392\u5e8f\u95ee\u9898\u81ea1970\u5e74\u4ee3\u4ee5\u6765\u4e00\u76f4\u662f\u4e2a\u6311\u6218\uff0c\u4f20\u7edf\u624b\u5de5\u8c03\u4f18\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u4eba\u529b\u4e14\u96be\u4ee5\u9002\u5e94\u4e0d\u540c\u57fa\u51c6\u6d4b\u8bd5\u96c6\u7684\u53d8\u5316\u3002\u73b0\u6709\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u672a\u80fd\u65e0\u7f1d\u96c6\u6210\u5230\u7f16\u8bd1\u5668\u4e2d\uff0c\u4e5f\u65e0\u6cd5\u5728\u7ec6\u7c92\u5ea6\u4ee3\u7801\u6bb5\u7ea7\u522b\u5e94\u7528\u3002", "method": "\u63d0\u51faProtean Compiler\u6846\u67b6\uff0c\u65e0\u7f1d\u96c6\u6210\u5230LLVM\u4e2d\uff0c\u652f\u6301\u7ec6\u7c92\u5ea6\u4ee3\u7801\u6bb5\u7684\u4f18\u5316\u9636\u6bb5\u6392\u5e8f\u3002\u5305\u542b140\u591a\u79cd\u624b\u5de5\u8bbe\u8ba1\u7684\u9759\u6001\u7279\u5f81\u6536\u96c6\u65b9\u6cd5\uff0c\u652f\u6301\u4e0e\u7b2c\u4e09\u65b9\u673a\u5668\u5b66\u4e60\u6846\u67b6\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u96c6\u6210\u3002", "result": "\u5728Cbench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u76f8\u6bd4LLVM O3\u4f18\u5316\uff0c\u5e73\u5747\u83b7\u5f974.1%\u7684\u6027\u80fd\u63d0\u5347\uff0c\u7279\u5b9a\u5e94\u7528\u6700\u9ad8\u63d0\u534715.7%\uff0c\u4ec5\u589e\u52a0\u51e0\u79d2\u6784\u5efa\u65f6\u95f4\u3002\u4e0e\u7b2c\u4e09\u65b9ML\u96c6\u6210\u540e\uff0cSusan\u548cJpeg\u5e94\u7528\u5206\u522b\u83b7\u5f9710.1%\u548c8.5%\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "Protean Compiler\u6210\u529f\u89e3\u51b3\u4e86\u7f16\u8bd1\u5668\u4f18\u5316\u9636\u6bb5\u6392\u5e8f\u7684\u957f\u671f\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u7ec6\u7c92\u5ea6\u3001\u53ef\u6269\u5c55\u7684\u4f18\u5316\u51b3\u7b56\uff0c\u53ef\u4f5c\u4e3a\u589e\u5f3a\u7248\u7f16\u8bd1\u5668\u4f7f\u7528\uff0c\u5e76\u8ba1\u5212\u5f00\u6e90\u3002"}}
{"id": "2602.06070", "categories": ["cs.DC", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.06070", "abs": "https://arxiv.org/abs/2602.06070", "authors": ["Nikola Stankovic"], "title": "Computationally Efficient Laplacian CL-colME", "comment": "4 pages, 1 figure", "summary": "Decentralized collaborative mean estimation (colME) is a fundamental task in heterogeneous networks. Its graph-based variants B-colME and C-colME achieve high scalability of the problem. This paper evaluates the consensus-based C-colME framework, which relies on doubly stochastic averaging matrices to ensure convergence to the oracle solution. We propose CL-colME, a novel variant utilizing Laplacian-based consensus to avoid the computationally expensive normalization processes. Simulation results show that the proposed CL-colME maintains the convergence behavior and accuracy of C-colME while improving computational efficiency.", "AI": {"tldr": "\u63d0\u51faCL-colME\uff0c\u4e00\u79cd\u57fa\u4e8e\u62c9\u666e\u62c9\u65af\u5171\u8bc6\u7684\u5206\u6563\u5f0f\u534f\u4f5c\u5747\u503c\u4f30\u8ba1\u65b0\u65b9\u6cd5\uff0c\u76f8\u6bd4\u73b0\u6709C-colME\u65b9\u6cd5\u907f\u514d\u4e86\u8ba1\u7b97\u6602\u8d35\u7684\u5f52\u4e00\u5316\u8fc7\u7a0b\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6536\u655b\u6027\u548c\u7cbe\u5ea6\u3002", "motivation": "\u5206\u6563\u5f0f\u534f\u4f5c\u5747\u503c\u4f30\u8ba1\uff08colME\uff09\u662f\u5f02\u6784\u7f51\u7edc\u4e2d\u7684\u57fa\u7840\u4efb\u52a1\u3002\u73b0\u6709\u7684\u57fa\u4e8e\u56fe\u7684B-colME\u548cC-colME\u65b9\u6cd5\u867d\u7136\u5b9e\u73b0\u4e86\u9ad8\u53ef\u6269\u5c55\u6027\uff0c\u4f46C-colME\u4f9d\u8d56\u53cc\u968f\u673a\u5e73\u5747\u77e9\u9635\uff0c\u9700\u8981\u8fdb\u884c\u8ba1\u7b97\u6602\u8d35\u7684\u5f52\u4e00\u5316\u8fc7\u7a0b\uff0c\u5f71\u54cd\u4e86\u8ba1\u7b97\u6548\u7387\u3002", "method": "\u63d0\u51faCL-colME\uff08\u57fa\u4e8e\u62c9\u666e\u62c9\u65af\u5171\u8bc6\u7684\u534f\u4f5c\u5747\u503c\u4f30\u8ba1\uff09\uff0c\u91c7\u7528\u62c9\u666e\u62c9\u65af\u57fa\u5171\u8bc6\u673a\u5236\u66ff\u4ee3\u53cc\u968f\u673a\u5e73\u5747\u77e9\u9635\uff0c\u907f\u514d\u4e86\u590d\u6742\u7684\u5f52\u4e00\u5316\u8ba1\u7b97\u8fc7\u7a0b\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6536\u655b\u5230\u6700\u4f18\u89e3\u7684\u80fd\u529b\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0cCL-colME\u5728\u4fdd\u6301C-colME\u6536\u655b\u884c\u4e3a\u548c\u7cbe\u5ea6\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8ba1\u7b97\u6548\u7387\u3002\u65b0\u65b9\u6cd5\u80fd\u591f\u5728\u4e0d\u727a\u7272\u6027\u80fd\u7684\u60c5\u51b5\u4e0b\u51cf\u5c11\u8ba1\u7b97\u5f00\u9500\u3002", "conclusion": "CL-colME\u662f\u4e00\u79cd\u6709\u6548\u7684\u5206\u6563\u5f0f\u534f\u4f5c\u5747\u503c\u4f30\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u62c9\u666e\u62c9\u65af\u5171\u8bc6\u673a\u5236\u907f\u514d\u4e86\u6602\u8d35\u7684\u5f52\u4e00\u5316\u8fc7\u7a0b\uff0c\u5728\u4fdd\u6301\u6536\u655b\u6027\u548c\u7cbe\u5ea6\u7684\u540c\u65f6\u63d0\u9ad8\u4e86\u8ba1\u7b97\u6548\u7387\uff0c\u4e3a\u5927\u89c4\u6a21\u5f02\u6784\u7f51\u7edc\u4e2d\u7684\u5747\u503c\u4f30\u8ba1\u95ee\u9898\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.06386", "categories": ["cs.PL", "cs.LO"], "pdf": "https://arxiv.org/pdf/2602.06386", "abs": "https://arxiv.org/abs/2602.06386", "authors": ["Liam O'Connor", "Pilar Selene Linares Arevalo", "Christine Rizkallah"], "title": "Uniqueness is Separation", "comment": null, "summary": "Value independence is enormously beneficial for reasoning about software systems at scale. These benefits carry over into the world of formal verification. Reasoning about programs algebraically is a simple affair in a proof assistant, whereas programs with unconstrained mutation necessitate much more complex techniques, such as Separation Logic, where invariants about memory safety, aliasing, and state changes must be established by manual proof. Uniqueness type systems allow programs to be compiled to code that uses mutation for efficiency, while retaining a semantics that enjoys value independence for reasoning. The restrictions of these type systems, however, are often too onerous for realistic software. Thus, most uniqueness type systems include some \"escape hatch\" where the benefits of value independence for reasoning are lost, but the restrictions of uniqueness types are lifted. To formally verify a system with such mixed guarantees, the value independence guarantees from uniqueness types must be expressed in terms of imperative, mutable semantics. In other words, we ought to express value independence as an assertion in Separation Logic.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u5c06\u503c\u72ec\u7acb\u6027\u4f5c\u4e3a\u5206\u79bb\u903b\u8f91\u4e2d\u7684\u65ad\u8a00\uff0c\u4ee5\u5728\u5f62\u5f0f\u9a8c\u8bc1\u4e2d\u7ed3\u5408\u552f\u4e00\u6027\u7c7b\u578b\u7cfb\u7edf\u7684\u63a8\u7406\u4f18\u52bf\u4e0e\u53ef\u53d8\u8bed\u4e49\u7684\u5b9e\u9645\u9700\u6c42\u3002", "motivation": "\u552f\u4e00\u6027\u7c7b\u578b\u7cfb\u7edf\u867d\u7136\u80fd\u901a\u8fc7\u503c\u72ec\u7acb\u6027\u7b80\u5316\u7a0b\u5e8f\u63a8\u7406\uff0c\u4f46\u5176\u9650\u5236\u8fc7\u4e8e\u4e25\u683c\uff0c\u5b9e\u9645\u8f6f\u4ef6\u5e38\u9700\u8981\"\u9003\u751f\u8231\u53e3\"\u6765\u653e\u5bbd\u9650\u5236\uff0c\u8fd9\u5bfc\u81f4\u63a8\u7406\u4f18\u52bf\u4e27\u5931\u3002\u9700\u8981\u5728\u53ef\u53d8\u8bed\u4e49\u4e2d\u8868\u8fbe\u503c\u72ec\u7acb\u6027\u4ee5\u4fdd\u8bc1\u5f62\u5f0f\u9a8c\u8bc1\u3002", "method": "\u63d0\u51fa\u5c06\u503c\u72ec\u7acb\u6027\u4f5c\u4e3a\u5206\u79bb\u903b\u8f91\u4e2d\u7684\u65ad\u8a00\uff0c\u5c06\u552f\u4e00\u6027\u7c7b\u578b\u7cfb\u7edf\u7684\u63a8\u7406\u4f18\u52bf\u8f6c\u5316\u4e3a\u53ef\u5728\u53ef\u53d8\u3001\u547d\u4ee4\u5f0f\u8bed\u4e49\u4e2d\u4f7f\u7528\u7684\u5f62\u5f0f\u5316\u65ad\u8a00\u3002", "result": "\u8be5\u65b9\u6cd5\u5141\u8bb8\u5728\u4fdd\u6301\u7a0b\u5e8f\u6548\u7387\uff08\u4f7f\u7528\u53ef\u53d8\u64cd\u4f5c\uff09\u7684\u540c\u65f6\uff0c\u5728\u5206\u79bb\u903b\u8f91\u6846\u67b6\u4e2d\u8868\u8fbe\u503c\u72ec\u7acb\u6027\uff0c\u4ece\u800c\u652f\u6301\u5bf9\u6df7\u5408\u4fdd\u8bc1\u7cfb\u7edf\u7684\u5f62\u5f0f\u9a8c\u8bc1\u3002", "conclusion": "\u901a\u8fc7\u5c06\u503c\u72ec\u7acb\u6027\u8868\u8fbe\u4e3a\u5206\u79bb\u903b\u8f91\u65ad\u8a00\uff0c\u53ef\u4ee5\u5728\u5b9e\u9645\u8f6f\u4ef6\u7684\u5f62\u5f0f\u9a8c\u8bc1\u4e2d\u7ed3\u5408\u552f\u4e00\u6027\u7c7b\u578b\u7cfb\u7edf\u7684\u63a8\u7406\u4f18\u52bf\u4e0e\u53ef\u53d8\u8bed\u4e49\u7684\u7075\u6d3b\u6027\u3002"}}
{"id": "2602.06071", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.06071", "abs": "https://arxiv.org/abs/2602.06071", "authors": ["Rajat Vadiraj Dwaraknath", "Sungyoon Kim", "Mert Pilanci"], "title": "FlashSketch: Sketch-Kernel Co-Design for Fast Sparse Sketching on GPUs", "comment": null, "summary": "Sparse sketches such as the sparse Johnson-Lindenstrauss transform are a core primitive in randomized numerical linear algebra because they leverage random sparsity to reduce the arithmetic cost of sketching, while still offering strong approximation guarantees. Their random sparsity, however, is at odds with efficient implementations on modern GPUs, since it leads to irregular memory access patterns that degrade memory bandwidth utilization. Motivated by this tension, we pursue a sketch-kernel co-design approach: we design a new family of sparse sketches, BlockPerm-SJLT, whose sparsity structure is chosen to enable FlashSketch, a corresponding optimized CUDA kernel that implements these sketches efficiently. The design of BlockPerm-SJLT introduces a tunable parameter that explicitly trades off the tension between GPU-efficiency and sketching robustness. We provide theoretical guarantees for BlockPerm-SJLT under the oblivious subspace embedding (OSE) framework, and also analyze the effect of the tunable parameter on sketching quality. We empirically evaluate FlashSketch on standard RandNLA benchmarks, as well as an end-to-end ML data attribution pipeline called GraSS. FlashSketch pushes the Pareto frontier of sketching quality versus speed, across a range of regimes and tasks, and achieves a global geomean speedup of roughly 1.7x over the prior state-of-the-art GPU sketches.", "AI": {"tldr": "\u63d0\u51faBlockPerm-SJLT\u7a00\u758f\u8349\u56fe\u4e0eFlashSketch CUDA\u5185\u6838\u7684\u534f\u540c\u8bbe\u8ba1\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u7a00\u758f\u6a21\u5f0f\u63d0\u5347GPU\u6548\u7387\uff0c\u5728\u4fdd\u6301\u8349\u56fe\u8d28\u91cf\u7684\u540c\u65f6\u83b7\u5f971.7\u500d\u52a0\u901f\u3002", "motivation": "\u4f20\u7edf\u7a00\u758f\u968f\u673a\u8349\u56fe\u867d\u7136\u80fd\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\uff0c\u4f46\u5176\u968f\u673a\u7a00\u758f\u6a21\u5f0f\u5bfc\u81f4GPU\u5185\u5b58\u8bbf\u95ee\u4e0d\u89c4\u5219\uff0c\u4e25\u91cd\u5f71\u54cd\u5185\u5b58\u5e26\u5bbd\u5229\u7528\u7387\uff0c\u9700\u8981\u5728GPU\u6548\u7387\u548c\u8349\u56fe\u9c81\u68d2\u6027\u4e4b\u95f4\u627e\u5230\u5e73\u8861\u3002", "method": "\u91c7\u7528\u8349\u56fe-\u5185\u6838\u534f\u540c\u8bbe\u8ba1\u65b9\u6cd5\uff1a\u8bbe\u8ba1BlockPerm-SJLT\u7a00\u758f\u8349\u56fe\u5bb6\u65cf\uff0c\u5176\u7a00\u758f\u7ed3\u6784\u4e13\u95e8\u4e3aGPU\u4f18\u5316\uff1b\u5f00\u53d1\u5bf9\u5e94\u7684FlashSketch CUDA\u5185\u6838\u5b9e\u73b0\u9ad8\u6548\u8ba1\u7b97\uff1b\u5f15\u5165\u53ef\u8c03\u53c2\u6570\u5e73\u8861GPU\u6548\u7387\u4e0e\u8349\u56fe\u8d28\u91cf\u3002", "result": "\u7406\u8bba\u8bc1\u660eBlockPerm-SJLT\u6ee1\u8db3\u65e0\u610f\u8bc6\u5b50\u7a7a\u95f4\u5d4c\u5165(OSE)\u4fdd\u8bc1\uff1b\u5b9e\u9a8c\u663e\u793aFlashSketch\u5728RandNLA\u57fa\u51c6\u6d4b\u8bd5\u548cGraSS\u6570\u636e\u5f52\u56e0\u7ba1\u9053\u4e2d\u5747\u8868\u73b0\u4f18\u5f02\uff0c\u76f8\u6bd4\u73b0\u6709GPU\u8349\u56fe\u83b7\u5f97\u7ea61.7\u500d\u51e0\u4f55\u5e73\u5747\u52a0\u901f\u3002", "conclusion": "\u901a\u8fc7\u8349\u56fe\u7ed3\u6784\u4e0e\u8ba1\u7b97\u5185\u6838\u7684\u534f\u540c\u8bbe\u8ba1\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u7a00\u758f\u8349\u56fe\u5728GPU\u4e0a\u7684\u6548\u7387\u74f6\u9888\uff0c\u5728\u8349\u56fe\u8d28\u91cf\u4e0e\u901f\u5ea6\u7684\u5e15\u7d2f\u6258\u524d\u6cbf\u4e0a\u53d6\u5f97\u663e\u8457\u8fdb\u6b65\uff0c\u4e3aGPU\u52a0\u901f\u7684\u968f\u673a\u6570\u503c\u7ebf\u6027\u4ee3\u6570\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.06466", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2602.06466", "abs": "https://arxiv.org/abs/2602.06466", "authors": ["Lydia Zoghbi", "David Thien", "Ranjit Jhala", "Deian Stefan", "Caleb Stanford"], "title": "Auditing Rust Crates Effectively", "comment": null, "summary": "We introduce Cargo Scan, the first interactive program analysis tool designed to help developers audit third-party Rust code. Real systems written in Rust rely on thousands of transitive dependencies. These dependencies are as dangerous in Rust as they are in other languages (e.g., C or JavaScript) -- and auditing these dependencies today means manually inspecting every line of code. Unlike for most industrial languages, though, we can take advantage of Rust's type and module system to minimize the amount of code that developers need to inspect to the code that is potentially dangerous. Cargo Scan models such potentially dangerous code as effects and performs a side-effects analysis, tailored to Rust, to identify effects and track them across crate and module boundaries. In most cases (69.2%) developers can inspect flagged effects and decide whether the code is potentially dangerous locally. In some cases, however, the safety of an effect depends on the calling context -- how a function is called, potentially by a crate the developer imports later. Hence, Cargo Scan tracks context-dependent information using a call-graph, and collects audit results into composable and reusable audit files. In this paper, we describe our experience auditing Rust crates with Cargo Scan. In particular, we audit the popular client and server HTTP crate, hyper, and all of its dependencies; our experience shows that Cargo Scan can reduce the auditing burden of potentially dangerous code to a median of 0.2% of lines of code when compared to auditing whole crates. Looking at the Rust ecosystem more broadly, we find that Cargo Scan can automatically classify ~3.5K of the top 10K crates on crates.io as safe; of the crates that do require manual inspection, we find that most of the potentially dangerous side-effects are concentrated in roughly 3% of these crates.", "AI": {"tldr": "Cargo Scan\u662f\u9996\u4e2a\u7528\u4e8e\u5ba1\u8ba1\u7b2c\u4e09\u65b9Rust\u4ee3\u7801\u7684\u4ea4\u4e92\u5f0f\u7a0b\u5e8f\u5206\u6790\u5de5\u5177\uff0c\u5229\u7528Rust\u7c7b\u578b\u7cfb\u7edf\u8bc6\u522b\u6f5c\u5728\u5371\u9669\u4ee3\u7801\uff0c\u53ef\u5c06\u5ba1\u8ba1\u5de5\u4f5c\u91cf\u51cf\u5c11\u5230\u4ee3\u7801\u91cf\u76840.2%", "motivation": "Rust\u7cfb\u7edf\u4f9d\u8d56\u5927\u91cf\u7b2c\u4e09\u65b9\u5e93\uff0c\u8fd9\u4e9b\u4f9d\u8d56\u4e0e\u5176\u4ed6\u8bed\u8a00\u4e00\u6837\u5371\u9669\uff0c\u4f46\u5f53\u524d\u5ba1\u8ba1\u9700\u8981\u4eba\u5de5\u9010\u884c\u68c0\u67e5\u6240\u6709\u4ee3\u7801\uff0c\u5de5\u4f5c\u91cf\u5de8\u5927", "method": "\u5c06\u6f5c\u5728\u5371\u9669\u4ee3\u7801\u5efa\u6a21\u4e3a\"\u6548\u679c\"\uff0c\u8fdb\u884c\u9488\u5bf9Rust\u7684\u526f\u4f5c\u7528\u5206\u6790\uff0c\u8ddf\u8e2a\u6548\u679c\u8de8\u8d8acrate\u548c\u6a21\u5757\u8fb9\u754c\uff0c\u4f7f\u7528\u8c03\u7528\u56fe\u8ddf\u8e2a\u4e0a\u4e0b\u6587\u4f9d\u8d56\u4fe1\u606f\uff0c\u751f\u6210\u53ef\u7ec4\u5408\u91cd\u7528\u7684\u5ba1\u8ba1\u6587\u4ef6", "result": "69.2%\u60c5\u51b5\u4e0b\u5f00\u53d1\u8005\u53ef\u672c\u5730\u68c0\u67e5\u6807\u8bb0\u7684\u6548\u679c\uff1b\u5ba1\u8ba1hyper\u53ca\u5176\u4f9d\u8d56\u65f6\uff0c\u5c06\u6f5c\u5728\u5371\u9669\u4ee3\u7801\u5ba1\u8ba1\u91cf\u51cf\u5c11\u5230\u4e2d\u4f4d\u65700.2%\u7684\u4ee3\u7801\u884c\u6570\uff1b\u53ef\u81ea\u52a8\u5c06\u7ea63.5K\u4e2a\u9876\u7ea7crate\u5206\u7c7b\u4e3a\u5b89\u5168", "conclusion": "Cargo Scan\u80fd\u663e\u8457\u51cf\u5c11Rust\u7b2c\u4e09\u65b9\u4ee3\u7801\u5ba1\u8ba1\u5de5\u4f5c\u91cf\uff0c\u5229\u7528Rust\u7c7b\u578b\u7cfb\u7edf\u7279\u6027\u6709\u6548\u8bc6\u522b\u548c\u8ddf\u8e2a\u6f5c\u5728\u5371\u9669\u4ee3\u7801\uff0c\u4e3aRust\u751f\u6001\u7cfb\u7edf\u5b89\u5168\u63d0\u4f9b\u5b9e\u7528\u5de5\u5177"}}
{"id": "2602.06072", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.06072", "abs": "https://arxiv.org/abs/2602.06072", "authors": ["Rui Ning", "Wei Zhang", "Fan Lai"], "title": "PackInfer: Compute- and I/O-Efficient Attention for Batched LLM Inference", "comment": null, "summary": "Attention efficiency is critical to large language model (LLM) inference. While prior advances optimize attention execution for individual requests (e.g., FlashAttention), production LLM serving relies on batching requests with highly heterogeneous sequence lengths for high serving throughput. This mismatch induces severe computation and I/O imbalance, exacerbates stragglers, and underutilizes GPU resources. We present PackInfer, a kernel-level attention framework that enables compute- and I/O-aware execution for heterogeneous batched inference. PackInfer orchestrates batched requests into load-balanced execution groups, effectively saturating GPU utilization by packing multiple requests into unified kernel launches. By constructing attention kernels directly over packed query-key regions, PackInfer eliminates redundant computation and balances thread-block execution. It then incorporates I/O-aware grouping that co-locates shared-prefix requests and reorganizes KV caches into group-contiguous layouts, reducing memory fragmentation and redundant data movement as generation evolves. Evaluations on real-world workloads show that PackInfer reduces inference latency by 13.0-20.1%, and improves throughput by 20% compared to the state-of-the-art FlashAttention.", "AI": {"tldr": "PackInfer\u662f\u4e00\u4e2a\u9488\u5bf9\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u7684\u6ce8\u610f\u529b\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u5f02\u6784\u6279\u5904\u7406\u8bf7\u6c42\u6253\u5305\u6210\u8d1f\u8f7d\u5747\u8861\u7684\u6267\u884c\u7ec4\uff0c\u4f18\u5316\u8ba1\u7b97\u548cI/O\u6548\u7387\uff0c\u663e\u8457\u63d0\u5347\u63a8\u7406\u6027\u80fd\u3002", "motivation": "\u751f\u4ea7\u73af\u5883\u4e2dLLM\u63a8\u7406\u9700\u8981\u6279\u5904\u7406\u4e0d\u540c\u957f\u5ea6\u7684\u5e8f\u5217\u4ee5\u83b7\u5f97\u9ad8\u541e\u5410\u91cf\uff0c\u4f46\u73b0\u6709\u6ce8\u610f\u529b\u4f18\u5316\u6280\u672f\uff08\u5982FlashAttention\uff09\u4e3b\u8981\u9488\u5bf9\u5355\u4e2a\u8bf7\u6c42\uff0c\u5bfc\u81f4\u8ba1\u7b97\u548cI/O\u4e0d\u5e73\u8861\u3001\u62d6\u5c3e\u6548\u5e94\u4e25\u91cd\uff0cGPU\u8d44\u6e90\u5229\u7528\u7387\u4f4e\u3002", "method": "1. \u5c06\u6279\u5904\u7406\u8bf7\u6c42\u7f16\u6392\u6210\u8d1f\u8f7d\u5747\u8861\u7684\u6267\u884c\u7ec4\uff0c\u5c06\u591a\u4e2a\u8bf7\u6c42\u6253\u5305\u5230\u7edf\u4e00\u7684\u5185\u6838\u542f\u52a8\u4e2d\uff1b2. \u76f4\u63a5\u5728\u6253\u5305\u7684\u67e5\u8be2-\u952e\u533a\u57df\u4e0a\u6784\u5efa\u6ce8\u610f\u529b\u5185\u6838\uff0c\u6d88\u9664\u5197\u4f59\u8ba1\u7b97\u5e76\u5e73\u8861\u7ebf\u7a0b\u5757\u6267\u884c\uff1b3. \u91c7\u7528I/O\u611f\u77e5\u5206\u7ec4\uff0c\u5c06\u5171\u4eab\u524d\u7f00\u7684\u8bf7\u6c42\u5171\u7f6e\uff0c\u5e76\u5c06KV\u7f13\u5b58\u91cd\u7ec4\u4e3a\u7ec4\u8fde\u7eed\u5e03\u5c40\uff0c\u51cf\u5c11\u5185\u5b58\u788e\u7247\u548c\u5197\u4f59\u6570\u636e\u79fb\u52a8\u3002", "result": "\u5728\u5b9e\u9645\u5de5\u4f5c\u8d1f\u8f7d\u8bc4\u4f30\u4e2d\uff0cPackInfer\u76f8\u6bd4\u6700\u5148\u8fdb\u7684FlashAttention\uff0c\u63a8\u7406\u5ef6\u8fdf\u964d\u4f4e13.0-20.1%\uff0c\u541e\u5410\u91cf\u63d0\u534720%\u3002", "conclusion": "PackInfer\u901a\u8fc7\u8ba1\u7b97\u548cI/O\u611f\u77e5\u7684\u5f02\u6784\u6279\u5904\u7406\u63a8\u7406\u6267\u884c\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u751f\u4ea7\u73af\u5883\u4e2dLLM\u63a8\u7406\u7684\u6ce8\u610f\u529b\u6548\u7387\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86GPU\u5229\u7528\u7387\u548c\u6574\u4f53\u6027\u80fd\u3002"}}
{"id": "2602.06680", "categories": ["cs.PL", "cs.DC"], "pdf": "https://arxiv.org/pdf/2602.06680", "abs": "https://arxiv.org/abs/2602.06680", "authors": ["Ali Rasim Kocal", "Michael Schwarz", "Simmo Saan", "Helmut Seidl"], "title": "Same Engine, Multiple Gears: Parallelizing Fixpoint Iteration at Different Granularities (Extended Version)", "comment": null, "summary": "Fixpoint iteration constitutes the algorithmic core of static analyzers. Parallelizing the fixpoint engine can significantly reduce analysis times. Previous approaches typically fix the granularity of tasks upfront, e.g., at the level of program threads or procedures - yielding an engine permanently stuck in one gear. Instead, we propose to parallelize a generic fixpoint engine in a way that is parametric in the task granularity - meaning that our engine can be run in different gears. We build on the top-down solver TD, extended with support for mixed-flow sensitivity, and realize two competing philosophies for parallelization, both building on a task pool that schedules tasks to a fixed number of workers. The nature of tasks differs between the philosophies. In the immediate approach, all tasks access a single thread-safe hash table maintaining solver state, while in the independent approach, each task has its own state and exchanges data with other tasks via a publish/subscribe data structure. We have equipped the fixpoint engine of the static analysis framework Goblint with implementations following both philosophies and report on our results for large real-world programs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53c2\u6570\u5316\u4efb\u52a1\u7c92\u5ea6\u7684\u5e76\u884c\u4e0d\u52a8\u70b9\u5f15\u64ce\uff0c\u652f\u6301\u4e24\u79cd\u5e76\u884c\u5316\u54f2\u5b66\uff1a\u5373\u65f6\u65b9\u6cd5\u548c\u72ec\u7acb\u65b9\u6cd5\uff0c\u5e76\u5728Goblint\u9759\u6001\u5206\u6790\u6846\u67b6\u4e2d\u5b9e\u73b0\u3002", "motivation": "\u5e76\u884c\u5316\u4e0d\u52a8\u70b9\u8fed\u4ee3\u53ef\u4ee5\u663e\u8457\u51cf\u5c11\u9759\u6001\u5206\u6790\u65f6\u95f4\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u9884\u5148\u56fa\u5b9a\u4efb\u52a1\u7c92\u5ea6\uff08\u5982\u7a0b\u5e8f\u7ebf\u7a0b\u6216\u8fc7\u7a0b\u7ea7\u522b\uff09\uff0c\u5bfc\u81f4\u5f15\u64ce\u6027\u80fd\u53d7\u9650\u3002\u9700\u8981\u4e00\u79cd\u66f4\u7075\u6d3b\u7684\u53c2\u6570\u5316\u4efb\u52a1\u7c92\u5ea6\u65b9\u6cd5\u3002", "method": "\u57fa\u4e8e\u652f\u6301\u6df7\u5408\u6d41\u654f\u611f\u6027\u7684TD\u6c42\u89e3\u5668\uff0c\u6784\u5efa\u53c2\u6570\u5316\u4efb\u52a1\u7c92\u5ea6\u7684\u5e76\u884c\u4e0d\u52a8\u70b9\u5f15\u64ce\u3002\u5b9e\u73b0\u4e24\u79cd\u5e76\u884c\u5316\u54f2\u5b66\uff1a1\uff09\u5373\u65f6\u65b9\u6cd5\uff1a\u6240\u6709\u4efb\u52a1\u8bbf\u95ee\u5355\u4e2a\u7ebf\u7a0b\u5b89\u5168\u54c8\u5e0c\u8868\uff1b2\uff09\u72ec\u7acb\u65b9\u6cd5\uff1a\u6bcf\u4e2a\u4efb\u52a1\u6709\u81ea\u5df1\u7684\u72b6\u6001\uff0c\u901a\u8fc7\u53d1\u5e03/\u8ba2\u9605\u6570\u636e\u7ed3\u6784\u4ea4\u6362\u6570\u636e\u3002", "result": "\u5728Goblint\u9759\u6001\u5206\u6790\u6846\u67b6\u4e2d\u5b9e\u73b0\u4e86\u4e24\u79cd\u5e76\u884c\u5316\u65b9\u6cd5\uff0c\u5e76\u5728\u5927\u578b\u5b9e\u9645\u7a0b\u5e8f\u4e0a\u8fdb\u884c\u4e86\u6d4b\u8bd5\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u53c2\u6570\u5316\u4efb\u52a1\u7c92\u5ea6\u5e76\u884c\u4e0d\u52a8\u70b9\u5f15\u64ce\u80fd\u591f\u7075\u6d3b\u9002\u5e94\u4e0d\u540c\u5206\u6790\u9700\u6c42\uff0c\u4e24\u79cd\u5e76\u884c\u5316\u54f2\u5b66\u5404\u6709\u4f18\u52bf\uff0c\u4e3a\u9759\u6001\u5206\u6790\u6027\u80fd\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2602.06074", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.06074", "abs": "https://arxiv.org/abs/2602.06074", "authors": ["Mohammad Umar", "Bharat Tripathi"], "title": "Experimental Analysis of Server-Side Caching for Web Performance", "comment": "4 pages, experimental study, server-side caching", "summary": "Performance in web applications is a key aspect of user experience and system scalability. Among the different techniques used to improve web application performance, caching has been widely used. While caching has been widely explored in web performance optimization literature, there is a lack of experimental work that explores the effect of simple inmemory caching in small-scale web applications. This paper fills this research gap by experimentally comparing the performance of two server-side web application configurations: one without caching and another with in-memory caching and a fixed time-tolive. The performance evaluation was conducted using a lightweight web server framework, and response times were measured using repeated HTTP requests under identical environmental conditions. The results show a significant reduction in response time for cached requests, and the findings of this paper provide valuable insights into the effectiveness of simple server-side caching in improving web application performance making it suitable for educational environments and small-scale web applications where simplicity and reproducibility are critical.", "AI": {"tldr": "\u5b9e\u9a8c\u7814\u7a76\u8868\u660e\uff0c\u5728\u5c0f\u578bWeb\u5e94\u7528\u4e2d\u5b9e\u65bd\u7b80\u5355\u7684\u5185\u5b58\u7f13\u5b58\u80fd\u663e\u8457\u964d\u4f4e\u54cd\u5e94\u65f6\u95f4\uff0c\u9002\u5408\u6559\u80b2\u73af\u5883\u548c\u5c0f\u89c4\u6a21\u5e94\u7528", "motivation": "\u5c3d\u7ba1\u7f13\u5b58\u6280\u672f\u5df2\u88ab\u5e7f\u6cdb\u7814\u7a76\uff0c\u4f46\u7f3a\u4e4f\u9488\u5bf9\u5c0f\u578bWeb\u5e94\u7528\u4e2d\u7b80\u5355\u5185\u5b58\u7f13\u5b58\u6548\u679c\u7684\u5b9e\u9a8c\u7814\u7a76\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7814\u7a76\u7a7a\u767d\uff0c\u63a2\u7d22\u7b80\u5355\u670d\u52a1\u5668\u7aef\u7f13\u5b58\u5728\u5c0f\u578b\u5e94\u7528\u4e2d\u7684\u6027\u80fd\u5f71\u54cd", "method": "\u5b9e\u9a8c\u6bd4\u8f83\u4e24\u79cd\u670d\u52a1\u5668\u7aef\u914d\u7f6e\uff1a\u65e0\u7f13\u5b58 vs \u5e26\u5185\u5b58\u7f13\u5b58\u548c\u56fa\u5b9a\u751f\u5b58\u65f6\u95f4\u3002\u4f7f\u7528\u8f7b\u91cf\u7ea7Web\u670d\u52a1\u5668\u6846\u67b6\uff0c\u5728\u76f8\u540c\u73af\u5883\u6761\u4ef6\u4e0b\u901a\u8fc7\u91cd\u590dHTTP\u8bf7\u6c42\u6d4b\u91cf\u54cd\u5e94\u65f6\u95f4", "result": "\u7f13\u5b58\u8bf7\u6c42\u7684\u54cd\u5e94\u65f6\u95f4\u663e\u8457\u964d\u4f4e\uff0c\u8bc1\u660e\u4e86\u7b80\u5355\u670d\u52a1\u5668\u7aef\u7f13\u5b58\u5728\u63d0\u5347Web\u5e94\u7528\u6027\u80fd\u65b9\u9762\u7684\u6709\u6548\u6027", "conclusion": "\u7b80\u5355\u670d\u52a1\u5668\u7aef\u7f13\u5b58\u80fd\u6709\u6548\u6539\u5584Web\u5e94\u7528\u6027\u80fd\uff0c\u7279\u522b\u9002\u5408\u6559\u80b2\u73af\u5883\u548c\u5c0f\u89c4\u6a21\u5e94\u7528\uff0c\u56e0\u4e3a\u5176\u7b80\u5355\u6027\u548c\u53ef\u91cd\u73b0\u6027\u81f3\u5173\u91cd\u8981"}}
{"id": "2602.06715", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2602.06715", "abs": "https://arxiv.org/abs/2602.06715", "authors": ["Toby Ueno", "Ankush Das"], "title": "Practical Refinement Session Type Inference (Extended Version)", "comment": null, "summary": "Session types express and enforce safe communication in concurrent message-passing systems by statically capturing the interaction protocols between processes in the type. Recent works extend session types with arithmetic refinements, which enable additional fine-grained description of communication, but impose additional annotation burden on the programmer. To alleviate this burden, we propose a type inference algorithm for a session type system with arithmetic refinements. We develop a theory of subtyping for session types, including an algorithm which we prove sound with respect to a semantic definition based on type simulation. We also provide a formal inference algorithm that generates type and arithmetic constraints, which are then solved using the Z3 SMT solver. The algorithm has been implemented on top of the Rast language, and includes 3 key optimizations that make inference feasible and practical. We evaluate the efficacy of our inference engine by evaluating it on 6 challenging benchmarks, ranging from unary and binary natural numbers to linear $\u03bb$-calculus. We show the performance benefits provided by our optimizations in coercing Z3 into solving the arithmetic constraints in reasonable time.", "AI": {"tldr": "\u63d0\u51fa\u5e26\u7b97\u672f\u7cbe\u5316\u7684\u4f1a\u8bdd\u7c7b\u578b\u7cfb\u7edf\u7684\u7c7b\u578b\u63a8\u65ad\u7b97\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u7c7b\u578b\u548c\u7b97\u672f\u7ea6\u675f\u5e76\u7528Z3\u6c42\u89e3\uff0c\u51cf\u8f7b\u7a0b\u5e8f\u5458\u6807\u6ce8\u8d1f\u62c5", "motivation": "\u73b0\u6709\u7684\u5e26\u7b97\u672f\u7cbe\u5316\u7684\u4f1a\u8bdd\u7c7b\u578b\u7cfb\u7edf\u867d\u7136\u80fd\u66f4\u7cbe\u7ec6\u5730\u63cf\u8ff0\u901a\u4fe1\u534f\u8bae\uff0c\u4f46\u7ed9\u7a0b\u5e8f\u5458\u589e\u52a0\u4e86\u7e41\u91cd\u7684\u6807\u6ce8\u8d1f\u62c5\uff0c\u9700\u8981\u81ea\u52a8\u63a8\u65ad\u6765\u51cf\u8f7b\u8fd9\u79cd\u8d1f\u62c5", "method": "1. \u5efa\u7acb\u4f1a\u8bdd\u7c7b\u578b\u7684\u5b50\u7c7b\u578b\u7406\u8bba\uff0c\u5305\u62ec\u57fa\u4e8e\u7c7b\u578b\u6a21\u62df\u7684\u8bed\u4e49\u5b9a\u4e49\u548c\u7b97\u6cd5\uff1b2. \u5f00\u53d1\u5f62\u5f0f\u5316\u7684\u63a8\u65ad\u7b97\u6cd5\uff0c\u751f\u6210\u7c7b\u578b\u548c\u7b97\u672f\u7ea6\u675f\uff1b3. \u4f7f\u7528Z3 SMT\u6c42\u89e3\u5668\u89e3\u51b3\u7ea6\u675f\uff1b4. \u5728Rast\u8bed\u8a00\u4e0a\u5b9e\u73b0\uff0c\u5305\u542b3\u4e2a\u5173\u952e\u4f18\u5316", "result": "1. \u5b9e\u73b0\u4e86\u63a8\u65ad\u5f15\u64ce\u5e76\u57286\u4e2a\u6311\u6218\u6027\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8bc4\u4f30\uff1b2. \u5c55\u793a\u4e86\u4ece\u4e00\u5143/\u4e8c\u5143\u81ea\u7136\u6570\u5230\u7ebf\u6027\u03bb\u6f14\u7b97\u7684\u5e94\u7528\uff1b3. \u4f18\u5316\u663e\u8457\u63d0\u9ad8\u4e86Z3\u6c42\u89e3\u7b97\u672f\u7ea6\u675f\u7684\u6548\u7387", "conclusion": "\u63d0\u51fa\u7684\u7c7b\u578b\u63a8\u65ad\u7b97\u6cd5\u80fd\u6709\u6548\u51cf\u8f7b\u5e26\u7b97\u672f\u7cbe\u5316\u7684\u4f1a\u8bdd\u7c7b\u578b\u7cfb\u7edf\u7684\u6807\u6ce8\u8d1f\u62c5\uff0c\u901a\u8fc7\u4f18\u5316\u4f7f\u63a8\u65ad\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u53ef\u884c"}}
{"id": "2602.06075", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.06075", "abs": "https://arxiv.org/abs/2602.06075", "authors": ["Guangyi Liu", "Pengxiang Zhao", "Yaozhen Liang", "Qinyi Luo", "Shunye Tang", "Yuxiang Chai", "Weifeng Lin", "Han Xiao", "WenHao Wang", "Siheng Chen", "Zhengxi Lu", "Gao Wu", "Hao Wang", "Liang Liu", "Yong Liu"], "title": "MemGUI-Bench: Benchmarking Memory of Mobile GUI Agents in Dynamic Environments", "comment": "https://lgy0404.github.io/MemGUI-Bench/", "summary": "Current mobile GUI agent benchmarks systematically fail to assess memory capabilities, with only 5.2-11.8% memory-related tasks and no cross-session learning evaluation. We introduce MemGUI-Bench, a comprehensive memory-centric benchmark with pass@k and staged LLM-as-judge evaluation. Our contributions include: (1) a systematic memory taxonomy analyzing 11 agents across 5 architectures; (2) 128 tasks across 26 applications where 89.8% challenge memory through cross-temporal and cross-spatial retention; (3) MemGUI-Eval, an automated pipeline with Progressive Scrutiny and 7 hierarchical metrics; and (4) RQ-driven assessment of 11 state-of-the-art agents. Our experiments reveal significant memory deficits across all evaluated systems, identify 5 distinct failure modes, and synthesize 5 actionable design implications. All resources including code, benchmark, and evaluation results will be \\textbf{\\textit{fully open-sourced and continuously maintained}} at https://lgy0404.github.io/MemGUI-Bench/.", "AI": {"tldr": "MemGUI-Bench\uff1a\u9996\u4e2a\u4e13\u6ce8\u4e8e\u79fb\u52a8GUI\u4ee3\u7406\u8bb0\u5fc6\u80fd\u529b\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b128\u4e2a\u8de8\u65f6\u7a7a\u8bb0\u5fc6\u4efb\u52a1\uff0c\u63ed\u793a\u4e86\u73b0\u6709\u4ee3\u7406\u5728\u8bb0\u5fc6\u65b9\u9762\u7684\u663e\u8457\u7f3a\u9677", "motivation": "\u73b0\u6709\u79fb\u52a8GUI\u4ee3\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e25\u91cd\u5ffd\u89c6\u8bb0\u5fc6\u80fd\u529b\u8bc4\u4f30\uff0c\u4ec5\u67095.2-11.8%\u7684\u4efb\u52a1\u6d89\u53ca\u8bb0\u5fc6\uff0c\u4e14\u7f3a\u4e4f\u8de8\u4f1a\u8bdd\u5b66\u4e60\u8bc4\u4f30\u3002\u9700\u8981\u5efa\u7acb\u4e13\u95e8\u8bc4\u4f30GUI\u4ee3\u7406\u8bb0\u5fc6\u80fd\u529b\u7684\u57fa\u51c6", "method": "\u63d0\u51faMemGUI-Bench\u57fa\u51c6\uff1a1) \u7cfb\u7edf\u5316\u8bb0\u5fc6\u5206\u7c7b\u6cd5\u5206\u679011\u79cd\u4ee3\u7406\u76845\u79cd\u67b6\u6784\uff1b2) 128\u4e2a\u8de826\u4e2a\u5e94\u7528\u7684\u4efb\u52a1\uff0c89.8%\u6311\u6218\u8de8\u65f6\u7a7a\u8bb0\u5fc6\uff1b3) MemGUI-Eval\u81ea\u52a8\u5316\u8bc4\u4f30\u6d41\u6c34\u7ebf\uff0c\u5305\u542b\u6e10\u8fdb\u5ba1\u67e5\u548c7\u4e2a\u5c42\u6b21\u5316\u6307\u6807\uff1b4) \u57fa\u4e8e\u7814\u7a76\u95ee\u9898\u768411\u79cdSOTA\u4ee3\u7406\u8bc4\u4f30", "result": "\u5b9e\u9a8c\u663e\u793a\u6240\u6709\u8bc4\u4f30\u7cfb\u7edf\u90fd\u5b58\u5728\u663e\u8457\u8bb0\u5fc6\u7f3a\u9677\uff0c\u8bc6\u522b\u51fa5\u79cd\u4e0d\u540c\u7684\u5931\u8d25\u6a21\u5f0f\uff0c\u5e76\u7efc\u5408\u51fa5\u4e2a\u53ef\u64cd\u4f5c\u7684\u8bbe\u8ba1\u542f\u793a", "conclusion": "MemGUI-Bench\u586b\u8865\u4e86GUI\u4ee3\u7406\u8bb0\u5fc6\u8bc4\u4f30\u7684\u7a7a\u767d\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u4ee3\u7406\u5728\u8bb0\u5fc6\u80fd\u529b\u4e0a\u7684\u4e0d\u8db3\uff0c\u4e3a\u672a\u6765GUI\u4ee3\u7406\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u91cd\u8981\u6307\u5bfc\u3002\u6240\u6709\u8d44\u6e90\u5c06\u5b8c\u5168\u5f00\u6e90\u5e76\u6301\u7eed\u7ef4\u62a4"}}
{"id": "2602.06934", "categories": ["cs.PL", "cs.AI", "cs.DC", "cs.LO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.06934", "abs": "https://arxiv.org/abs/2602.06934", "authors": ["Ehud Shapiro"], "title": "Implementing Grassroots Logic Programs with Multiagent Transition Systems and AI", "comment": null, "summary": "Grassroots Logic Programs (GLP) is a concurrent logic programming language with variables partitioned into paired \\emph{readers} and \\emph{writers}, conjuring both linear logic and futures/promises: an assignment is produced at most once via the sole occurrence of a writer (promise) and consumed at most once via the sole occurrence of its paired reader (future), and may contain additional readers and/or writers, enabling the concise expression of rich multidirectional communication modalities.\n  GLP was designed as a language for grassroots platforms -- distributed systems with multiple instances that can operate independently of each other and of any global resource, and can coalesce into ever larger instances -- with its target architecture being smartphones communicating peer-to-peer. The operational semantics of Concurrent (single-agent) GLP and of multiagent GLP (maGLP) were defined via transition systems/multiagent transition systems, respectively.\n  Here, we describe the mathematics developed to facilitate the workstation- and smartphone-based implementations of GLP by AI in Dart. We developed dGLP -- implementation-ready deterministic operational semantics for single-agent GLP -- and proved it correct with respect to the Concurrent GLP operational semantics; dGLP was used by AI as a formal spec, from which it developed a workstation-based implementation of GLP. We developed madGLP -- an implementation-ready multiagent operational semantics for maGLP -- and proved it correct with respect to the maGLP operational semantics; madGLP is deterministic at the agent level (not at the system level due to communication asynchrony), and is being used by AI as a formal spec from which it develops a smartphone-based implementation of maGLP.", "AI": {"tldr": "GLP\u662f\u4e00\u79cd\u5e76\u53d1\u903b\u8f91\u7f16\u7a0b\u8bed\u8a00\uff0c\u4e3a\u5206\u5e03\u5f0f\u8349\u6839\u5e73\u53f0\u8bbe\u8ba1\uff0c\u5177\u6709\u8bfb\u8005-\u5199\u8005\u53d8\u91cf\u5206\u533a\uff0c\u652f\u6301\u4e30\u5bcc\u7684\u591a\u5411\u901a\u4fe1\u6a21\u5f0f\u3002\u672c\u6587\u5f00\u53d1\u4e86dGLP\u548cmadGLP\u4f5c\u4e3a\u5b9e\u73b0\u5c31\u7eea\u7684\u786e\u5b9a\u6027\u64cd\u4f5c\u8bed\u4e49\uff0c\u7528\u4e8e\u5de5\u4f5c\u7ad9\u548c\u667a\u80fd\u624b\u673a\u5b9e\u73b0\u3002", "motivation": "\u4e3a\u8349\u6839\u5e73\u53f0\uff08\u5206\u5e03\u5f0f\u7cfb\u7edf\uff0c\u5404\u5b9e\u4f8b\u53ef\u72ec\u7acb\u8fd0\u884c\u5e76\u53ef\u5408\u5e76\u6210\u66f4\u5927\u5b9e\u4f8b\uff09\u8bbe\u8ba1\u7f16\u7a0b\u8bed\u8a00\uff0c\u7279\u522b\u662f\u9488\u5bf9\u667a\u80fd\u624b\u673a\u70b9\u5bf9\u70b9\u901a\u4fe1\u67b6\u6784\u3002\u9700\u8981\u4e3aAI\u5b9e\u73b0\u63d0\u4f9b\u5f62\u5f0f\u5316\u89c4\u8303\u3002", "method": "\u5f00\u53d1\u4e86dGLP\uff08\u5355\u4ee3\u7406GLP\u7684\u786e\u5b9a\u6027\u64cd\u4f5c\u8bed\u4e49\uff09\u548cmadGLP\uff08\u591a\u4ee3\u7406GLP\u7684\u64cd\u4f5c\u8bed\u4e49\uff09\uff0c\u5e76\u8bc1\u660e\u5b83\u4eec\u76f8\u5bf9\u4e8e\u539f\u59cb\u64cd\u4f5c\u8bed\u4e49\u7684\u6b63\u786e\u6027\u3002\u8fd9\u4e9b\u786e\u5b9a\u6027\u8bed\u4e49\u4f5c\u4e3aAI\u5b9e\u73b0\u7684\u6b63\u5f0f\u89c4\u8303\u3002", "result": "dGLP\u5df2\u6210\u529f\u7528\u4e8eAI\u5f00\u53d1\u57fa\u4e8e\u5de5\u4f5c\u7ad9\u7684GLP\u5b9e\u73b0\uff1bmadGLP\u6b63\u5728\u88abAI\u7528\u4e8e\u5f00\u53d1\u57fa\u4e8e\u667a\u80fd\u624b\u673a\u7684maGLP\u5b9e\u73b0\u3002\u4e24\u79cd\u8bed\u4e49\u90fd\u7ecf\u8fc7\u6b63\u786e\u6027\u8bc1\u660e\u3002", "conclusion": "\u901a\u8fc7\u5f00\u53d1\u5b9e\u73b0\u5c31\u7eea\u7684\u786e\u5b9a\u6027\u64cd\u4f5c\u8bed\u4e49\uff0c\u4e3aGLP\u8bed\u8a00\u5728\u8349\u6839\u5e73\u53f0\u4e0a\u7684\u5b9e\u73b0\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u5f62\u5f0f\u5316\u57fa\u7840\uff0c\u652f\u6301\u4ece\u5de5\u4f5c\u7ad9\u5230\u667a\u80fd\u624b\u673a\u7684\u5206\u5e03\u5f0f\u5b9e\u73b0\u3002"}}
{"id": "2602.06079", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.06079", "abs": "https://arxiv.org/abs/2602.06079", "authors": ["Liangyu Wang", "Siqi Zhang", "Junjie Wang", "Yiming Dong", "Bo Zheng", "Zihan Qiu", "Shengkun Tang", "Di Wang", "Rui Men", "Dayiheng Liu"], "title": "Canzona: A Unified, Asynchronous, and Load-Balanced Framework for Distributed Matrix-based Optimizers", "comment": null, "summary": "The scaling of Large Language Models (LLMs) drives interest in matrix-based optimizers (e.g., Shampoo, Muon, SOAP) for their convergence efficiency; yet their requirement for holistic updates conflicts with the tensor fragmentation in distributed frameworks like Megatron. Existing solutions are suboptimal: synchronous approaches suffer from computational redundancy, while layer-wise partitioning fails to reconcile this conflict without violating the geometric constraints of efficient communication primitives. To bridge this gap, we propose Canzona, a Unified, Asynchronous, and Load-Balanced framework that decouples logical optimizer assignment from physical parameter distribution. For Data Parallelism, we introduce an alpha-Balanced Static Partitioning strategy that respects atomicity while neutralizing the load imbalance. For Tensor Parallelism, we design an Asynchronous Compute pipeline utilizing Micro-Group Scheduling to batch fragmented updates and hide reconstruction overhead. Extensive evaluations on the Qwen3 model family (up to 32B parameters) on 256 GPUs demonstrate that our approach preserves the efficiency of established parallel architectures, achieving a 1.57x speedup in end-to-end iteration time and reducing optimizer step latency by 5.8x compared to the baseline.", "AI": {"tldr": "Canzona\u662f\u4e00\u4e2a\u7edf\u4e00\u3001\u5f02\u6b65\u3001\u8d1f\u8f7d\u5747\u8861\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u5206\u5e03\u5f0f\u8bad\u7ec3\u4e2d\u9ad8\u6548\u5b9e\u73b0\u77e9\u9635\u4f18\u5316\u5668\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u5f20\u91cf\u5e76\u884c\u548c\u6570\u636e\u5e76\u884c\u4e0b\u7684\u51b2\u7a81\u95ee\u9898\u3002", "motivation": "\u77e9\u9635\u4f18\u5316\u5668\uff08\u5982Shampoo\u3001Muon\u3001SOAP\uff09\u5728\u5927\u8bed\u8a00\u6a21\u578b\u6269\u5c55\u4e2d\u5177\u6709\u6536\u655b\u6548\u7387\u4f18\u52bf\uff0c\u4f46\u5176\u6574\u4f53\u66f4\u65b0\u9700\u6c42\u4e0eMegatron\u7b49\u5206\u5e03\u5f0f\u6846\u67b6\u4e2d\u7684\u5f20\u91cf\u788e\u7247\u5316\u5b58\u5728\u51b2\u7a81\u3002\u73b0\u6709\u540c\u6b65\u65b9\u6cd5\u5b58\u5728\u8ba1\u7b97\u5197\u4f59\uff0c\u800c\u5206\u5c42\u5212\u5206\u65b9\u6cd5\u65e0\u6cd5\u5728\u4e0d\u8fdd\u53cd\u9ad8\u6548\u901a\u4fe1\u539f\u8bed\u51e0\u4f55\u7ea6\u675f\u7684\u60c5\u51b5\u4e0b\u89e3\u51b3\u8fd9\u4e00\u51b2\u7a81\u3002", "method": "\u63d0\u51faCanzona\u6846\u67b6\uff0c\u5c06\u903b\u8f91\u4f18\u5316\u5668\u5206\u914d\u4e0e\u7269\u7406\u53c2\u6570\u5206\u5e03\u89e3\u8026\u3002\u9488\u5bf9\u6570\u636e\u5e76\u884c\uff0c\u63d0\u51faalpha\u5e73\u8861\u9759\u6001\u5206\u533a\u7b56\u7565\uff0c\u5728\u4fdd\u6301\u539f\u5b50\u6027\u7684\u540c\u65f6\u5e73\u8861\u8d1f\u8f7d\u3002\u9488\u5bf9\u5f20\u91cf\u5e76\u884c\uff0c\u8bbe\u8ba1\u5f02\u6b65\u8ba1\u7b97\u6d41\u6c34\u7ebf\uff0c\u5229\u7528\u5fae\u7ec4\u8c03\u5ea6\u6279\u91cf\u5904\u7406\u788e\u7247\u5316\u66f4\u65b0\u5e76\u9690\u85cf\u91cd\u6784\u5f00\u9500\u3002", "result": "\u5728256\u4e2aGPU\u4e0a\u5bf9Qwen3\u6a21\u578b\u5bb6\u65cf\uff08\u6700\u9ad8320\u4ebf\u53c2\u6570\uff09\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4fdd\u6301\u4e86\u73b0\u6709\u5e76\u884c\u67b6\u6784\u7684\u6548\u7387\uff0c\u7aef\u5230\u7aef\u8fed\u4ee3\u65f6\u95f4\u52a0\u901f1.57\u500d\uff0c\u4f18\u5316\u5668\u6b65\u9aa4\u5ef6\u8fdf\u964d\u4f4e5.8\u500d\u3002", "conclusion": "Canzona\u6210\u529f\u89e3\u51b3\u4e86\u77e9\u9635\u4f18\u5316\u5668\u5728\u5206\u5e03\u5f0f\u8bad\u7ec3\u4e2d\u7684\u5b9e\u73b0\u51b2\u7a81\uff0c\u901a\u8fc7\u7edf\u4e00\u3001\u5f02\u6b65\u3001\u8d1f\u8f7d\u5747\u8861\u7684\u8bbe\u8ba1\uff0c\u5728\u4fdd\u6301\u5e76\u884c\u67b6\u6784\u6548\u7387\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u8bad\u7ec3\u6027\u80fd\u3002"}}
{"id": "2602.06085", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.06085", "abs": "https://arxiv.org/abs/2602.06085", "authors": ["Maxim Moraru", "Kamalavasan Kamalakkannan", "Jered Dominguez-Trujillo", "Patrick Diehl", "Atanu Barai", "Julien Loiseau", "Zachary Kent Baker", "Howard Pritchard", "Galen M Shipman"], "title": "LAAFD: LLM-based Agents for Accelerated FPGA Design", "comment": null, "summary": "FPGAs offer high performance, low latency, and energy efficiency for accelerated computing, yet adoption in scientific and edge settings is limited by the specialized hardware expertise required. High-level synthesis (HLS) boosts productivity over HDLs, but competitive designs still demand hardware-aware optimizations and careful dataflow design. We introduce LAAFD, an agentic workflow that uses large language models to translate general-purpose C++ into optimized Vitis HLS kernels. LAAFD automates key transfor mations: deep pipelining, vectorization, and dataflow partitioning and closes the loop with HLS co-simulation and synthesis feedback to verify correctness while iteratively improving execution time in cycles. Over a suite of 15 kernels representing common compute patterns in HPC, LAFFD achieves 99.9% geomean performance when compared to the hand tuned baseline for Vitis HLS. For stencil workloads, LAAFD matches the performance of SODA, a state-of-the-art DSL-based HLS code generator for stencil solvers, while yielding more readable kernels. These results suggest LAAFD substantially lowers the expertise barrier to FPGA acceleration without sacrificing efficiency.", "AI": {"tldr": "LAAFD \u662f\u4e00\u4e2a\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u5c06\u901a\u7528 C++ \u4ee3\u7801\u81ea\u52a8\u8f6c\u6362\u4e3a\u4f18\u5316 Vitis HLS \u5185\u6838\u7684\u667a\u80fd\u5de5\u4f5c\u6d41\uff0c\u80fd\u5b9e\u73b0\u6df1\u5ea6\u6d41\u6c34\u7ebf\u3001\u5411\u91cf\u5316\u548c\u6570\u636e\u6d41\u5206\u533a\u7b49\u5173\u952e\u4f18\u5316\uff0c\u5728\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u540c\u65f6\u5927\u5e45\u964d\u4f4e FPGA \u7f16\u7a0b\u95e8\u69db\u3002", "motivation": "FPGA \u5177\u6709\u9ad8\u6027\u80fd\u3001\u4f4e\u5ef6\u8fdf\u548c\u80fd\u6548\u4f18\u52bf\uff0c\u4f46\u5728\u79d1\u5b66\u8ba1\u7b97\u548c\u8fb9\u7f18\u8ba1\u7b97\u4e2d\u7684\u5e94\u7528\u53d7\u5230\u786c\u4ef6\u4e13\u4e1a\u77e5\u8bc6\u95e8\u69db\u7684\u9650\u5236\u3002\u867d\u7136\u9ad8\u7ea7\u7efc\u5408\uff08HLS\uff09\u6bd4\u786c\u4ef6\u63cf\u8ff0\u8bed\u8a00\uff08HDL\uff09\u63d0\u9ad8\u4e86\u751f\u4ea7\u529b\uff0c\u4f46\u8981\u83b7\u5f97\u6709\u7ade\u4e89\u529b\u7684\u8bbe\u8ba1\u4ecd\u9700\u786c\u4ef6\u611f\u77e5\u4f18\u5316\u548c\u7cbe\u7ec6\u7684\u6570\u636e\u6d41\u8bbe\u8ba1\u3002", "method": "LAAFD \u91c7\u7528\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u667a\u80fd\u5de5\u4f5c\u6d41\uff0c\u5c06\u901a\u7528 C++ \u4ee3\u7801\u8f6c\u6362\u4e3a\u4f18\u5316\u7684 Vitis HLS \u5185\u6838\u3002\u8be5\u5de5\u4f5c\u6d41\u81ea\u52a8\u5316\u6267\u884c\u6df1\u5ea6\u6d41\u6c34\u7ebf\u3001\u5411\u91cf\u5316\u548c\u6570\u636e\u6d41\u5206\u533a\u7b49\u5173\u952e\u8f6c\u6362\uff0c\u5e76\u901a\u8fc7 HLS \u534f\u540c\u4eff\u771f\u548c\u7efc\u5408\u53cd\u9988\u5f62\u6210\u95ed\u73af\uff0c\u9a8c\u8bc1\u6b63\u786e\u6027\u5e76\u8fed\u4ee3\u6539\u8fdb\u6267\u884c\u5468\u671f\u3002", "result": "\u5728\u4ee3\u8868 HPC \u5e38\u89c1\u8ba1\u7b97\u6a21\u5f0f\u7684 15 \u4e2a\u5185\u6838\u6d4b\u8bd5\u5957\u4ef6\u4e2d\uff0cLAAFD \u76f8\u6bd4\u624b\u52a8\u8c03\u4f18\u7684 Vitis HLS \u57fa\u51c6\u5b9e\u73b0\u4e86 99.9% \u7684\u51e0\u4f55\u5e73\u5747\u6027\u80fd\u3002\u5bf9\u4e8e\u6a21\u677f\u8ba1\u7b97\u5de5\u4f5c\u8d1f\u8f7d\uff0cLAAFD \u6027\u80fd\u4e0e\u6700\u5148\u8fdb\u7684\u57fa\u4e8e DSL \u7684 HLS \u4ee3\u7801\u751f\u6210\u5668 SODA \u76f8\u5f53\uff0c\u540c\u65f6\u751f\u6210\u66f4\u6613\u8bfb\u7684\u5185\u6838\u3002", "conclusion": "LAAFD \u5728\u4e0d\u727a\u7272\u6548\u7387\u7684\u524d\u63d0\u4e0b\uff0c\u663e\u8457\u964d\u4f4e\u4e86 FPGA \u52a0\u901f\u7684\u4e13\u4e1a\u77e5\u8bc6\u95e8\u69db\uff0c\u4e3a\u66f4\u5e7f\u6cdb\u5730\u91c7\u7528 FPGA \u52a0\u901f\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2602.06498", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.06498", "abs": "https://arxiv.org/abs/2602.06498", "authors": ["Arno Geimer"], "title": "BouquetFL: Emulating diverse participant hardware in Federated Learning", "comment": null, "summary": "In Federated Learning (FL), multiple parties collaboratively train a shared Machine Learning model to encapsulate all private knowledge without exchange of information. While it has seen application in several industrial projects, most FL research considers simulations on a central machine, without considering potential hardware heterogeneity between the involved parties. In this paper, we present BouquetFL, a framework designed to address this methodological gap by simulating heterogeneous client hardware on a single physical machine. By programmatically emulating diverse hardware configurations through resource restriction, BouquetFL enables controlled FL experimentation under realistic hardware diversity. Our tool provides an accessible way to study system heterogeneity in FL without requiring multiple physical devices, thereby bringing experimental practice closer to practical deployment conditions. The target audience are FL researchers studying highly heterogeneous federations. We include a wide range of profiles derived from commonly available consumer and small-lab devices, as well as a custom hardware sampler built on real-world hardware popularity, allowing users to configure the federation according to their preference.", "AI": {"tldr": "BouquetFL\u662f\u4e00\u4e2a\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u5355\u673a\u4e0a\u6a21\u62df\u5f02\u6784\u5ba2\u6237\u7aef\u786c\u4ef6\u6765\u89e3\u51b3\u73b0\u6709\u7814\u7a76\u5ffd\u7565\u786c\u4ef6\u5f02\u8d28\u6027\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u8054\u90a6\u5b66\u4e60\u7814\u7a76\u5927\u591a\u5728\u4e2d\u592e\u673a\u5668\u4e0a\u8fdb\u884c\u6a21\u62df\uff0c\u5ffd\u7565\u4e86\u5b9e\u9645\u90e8\u7f72\u4e2d\u53c2\u4e0e\u65b9\u786c\u4ef6\u5f02\u8d28\u6027\u5bf9\u8bad\u7ec3\u7684\u5f71\u54cd\uff0c\u8fd9\u5bfc\u81f4\u5b9e\u9a8c\u6761\u4ef6\u4e0e\u5b9e\u9645\u60c5\u51b5\u5b58\u5728\u5dee\u8ddd\u3002", "method": "\u901a\u8fc7\u8d44\u6e90\u9650\u5236\u7f16\u7a0b\u6a21\u62df\u4e0d\u540c\u7684\u786c\u4ef6\u914d\u7f6e\uff0c\u5728\u5355\u53f0\u7269\u7406\u673a\u5668\u4e0a\u6a21\u62df\u5f02\u6784\u5ba2\u6237\u7aef\u786c\u4ef6\uff0c\u63d0\u4f9b\u57fa\u4e8e\u771f\u5b9e\u786c\u4ef6\u6d41\u884c\u5ea6\u7684\u81ea\u5b9a\u4e49\u786c\u4ef6\u91c7\u6837\u5668\uff0c\u652f\u6301\u7528\u6237\u6309\u9700\u914d\u7f6e\u8054\u90a6\u73af\u5883\u3002", "result": "\u5f00\u53d1\u4e86BouquetFL\u6846\u67b6\uff0c\u4f7f\u7814\u7a76\u8005\u80fd\u591f\u5728\u4e0d\u9700\u8981\u591a\u53f0\u7269\u7406\u8bbe\u5907\u7684\u60c5\u51b5\u4e0b\uff0c\u5728\u53d7\u63a7\u73af\u5883\u4e2d\u7814\u7a76\u7cfb\u7edf\u5f02\u8d28\u6027\u5bf9\u8054\u90a6\u5b66\u4e60\u7684\u5f71\u54cd\u3002", "conclusion": "BouquetFL\u586b\u8865\u4e86\u8054\u90a6\u5b66\u4e60\u7814\u7a76\u7684\u65b9\u6cd5\u8bba\u7a7a\u767d\uff0c\u4f7f\u5b9e\u9a8c\u6761\u4ef6\u66f4\u63a5\u8fd1\u5b9e\u9645\u90e8\u7f72\u573a\u666f\uff0c\u7279\u522b\u9002\u7528\u4e8e\u7814\u7a76\u9ad8\u5ea6\u5f02\u6784\u8054\u90a6\u7684\u7814\u7a76\u4eba\u5458\u3002"}}
{"id": "2602.06499", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.06499", "abs": "https://arxiv.org/abs/2602.06499", "authors": ["Gyeongseo Park", "Eungyeong Lee", "Song-woo Sok", "Myung-Hoon Cha", "Kwangwon Koh", "Baik-Song An", "Hongyeon Kim", "Ki-Dong Kang"], "title": "FCDP: Fully Cached Data Parallel for Communication-Avoiding Large-Scale Training", "comment": "14 pages,10 figures", "summary": "Training billion-parameter models requires distributing model states across GPUs using fully sharded data parallel (i.e., ZeRO-3). While ZeRO-3 succeeds on clusters with high-bandwidth NVLink and InfiniBand interconnects, researchers with commodity hardware face severe inter-node all-gather bottlenecks. Existing optimizations take two approaches: GPU memory caching (MiCS, ZeRO++) trades memory capacity for reduced communication, triggering out-of-memory failures on large models; host memory offloading (ZeRO-Offload, ZeRO-Infinity) extends capacity but degrades throughput due to PCIe overhead. We observe that on bandwidth-limited clusters, host memory can serve not as an overflow tier but as a fast caching layer that outperforms inter-node communication. Based on this insight, we propose FCDP, which eliminates redundant inter-node communication while preserving ZeRO-3's minimal GPU memory footprint. FCDP caches forward-pass parameters in host memory and reuses them during the backward pass via fast intra-node all-gather, reducing inter-node all-gather by 50%. For parameter-efficient fine-tuning (PEFT), FCDP selectively communicates only trainable parameters to maximize caching, reducing inter-node traffic by over 99%. In our commodity cluster setup, FCDP achieves up to 100x higher throughput than ZeRO-3 and 51x higher than ZeRO++, while maintaining ZeRO-3's maximum batch size.", "AI": {"tldr": "FCDP\u662f\u4e00\u79cd\u65b0\u7684\u5206\u5e03\u5f0f\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u4e3b\u673a\u5185\u5b58\u4e2d\u7f13\u5b58\u524d\u5411\u4f20\u64ad\u53c2\u6570\u5e76\u5728\u53cd\u5411\u4f20\u64ad\u65f6\u91cd\u7528\uff0c\u51cf\u5c1150%\u7684\u8282\u70b9\u95f4\u901a\u4fe1\uff0c\u5728\u5e26\u5bbd\u6709\u9650\u7684\u96c6\u7fa4\u4e0a\u5b9e\u73b0\u6bd4ZeRO-3\u548cZeRO++\u66f4\u9ad8\u7684\u541e\u5410\u91cf\u3002", "motivation": "\u73b0\u6709\u5206\u5e03\u5f0f\u8bad\u7ec3\u65b9\u6cd5\u5728\u5546\u7528\u786c\u4ef6\u4e0a\u9762\u4e34\u74f6\u9888\uff1aZeRO-3\u5728\u5e26\u5bbd\u6709\u9650\u7684\u96c6\u7fa4\u4e0a\u5b58\u5728\u4e25\u91cd\u7684\u8282\u70b9\u95f4\u901a\u4fe1\u74f6\u9888\uff1bZeRO++\u7b49GPU\u5185\u5b58\u7f13\u5b58\u65b9\u6cd5\u4f1a\u89e6\u53d1\u5185\u5b58\u4e0d\u8db3\u95ee\u9898\uff1bZeRO-Offload\u7b49\u4e3b\u673a\u5185\u5b58\u5378\u8f7d\u65b9\u6cd5\u5219\u56e0PCIe\u5f00\u9500\u5bfc\u81f4\u541e\u5410\u91cf\u4e0b\u964d\u3002", "method": "FCDP\u5c06\u4e3b\u673a\u5185\u5b58\u4f5c\u4e3a\u5feb\u901f\u7f13\u5b58\u5c42\u800c\u975e\u6ea2\u51fa\u5c42\uff0c\u5728\u524d\u5411\u4f20\u64ad\u65f6\u5c06\u53c2\u6570\u7f13\u5b58\u5230\u4e3b\u673a\u5185\u5b58\uff0c\u53cd\u5411\u4f20\u64ad\u65f6\u901a\u8fc7\u5feb\u901f\u7684\u8282\u70b9\u5185all-gather\u91cd\u7528\u8fd9\u4e9b\u53c2\u6570\uff0c\u4ece\u800c\u6d88\u9664\u5197\u4f59\u7684\u8282\u70b9\u95f4\u901a\u4fe1\u3002\u5bf9\u4e8e\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff0cFCDP\u9009\u62e9\u6027\u901a\u4fe1\u4ec5\u8bad\u7ec3\u53c2\u6570\u4ee5\u6700\u5927\u5316\u7f13\u5b58\u6548\u679c\u3002", "result": "\u5728\u5546\u7528\u96c6\u7fa4\u8bbe\u7f6e\u4e2d\uff0cFCDP\u76f8\u6bd4ZeRO-3\u5b9e\u73b0\u9ad8\u8fbe100\u500d\u7684\u541e\u5410\u91cf\u63d0\u5347\uff0c\u76f8\u6bd4ZeRO++\u5b9e\u73b051\u500d\u7684\u63d0\u5347\uff0c\u540c\u65f6\u4fdd\u6301ZeRO-3\u7684\u6700\u5927\u6279\u5904\u7406\u5927\u5c0f\u3002\u5bf9\u4e8e\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff0cFCDP\u51cf\u5c11\u8d85\u8fc799%\u7684\u8282\u70b9\u95f4\u901a\u4fe1\u6d41\u91cf\u3002", "conclusion": "FCDP\u901a\u8fc7\u5728\u4e3b\u673a\u5185\u5b58\u4e2d\u667a\u80fd\u7f13\u5b58\u53c2\u6570\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5e26\u5bbd\u6709\u9650\u96c6\u7fa4\u4e0a\u7684\u5206\u5e03\u5f0f\u8bad\u7ec3\u74f6\u9888\uff0c\u5728\u4fdd\u6301\u6700\u5c0fGPU\u5185\u5b58\u5360\u7528\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u8bad\u7ec3\u541e\u5410\u91cf\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u7684\u5927\u89c4\u6a21\u6a21\u578b\u8bad\u7ec3\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.06502", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2602.06502", "abs": "https://arxiv.org/abs/2602.06502", "authors": ["Ying Yuan", "Pengfei Zuo", "Bo Wang", "Zhangyu Chen", "Zhipeng Tan", "Zhou Yu"], "title": "DualMap: Enabling Both Cache Affinity and Load Balancing for Distributed LLM Serving", "comment": "23 pages, 15 figures", "summary": "In LLM serving, reusing the KV cache of prompts across requests is critical for reducing TTFT and serving costs. Cache-affinity scheduling, which co-locates requests with the same prompt prefix to maximize KV cache reuse, often conflicts with load-balancing scheduling that distributes requests evenly across compute instances. Existing schedulers fail to reconcile this trade-off as they operate within a single mapping space, typically applying cache-affinity routing to a subset of requests and load-balanced routing to the rest, without a unified solution to achieve both goals. To address this limitation, we propose DualMap, a dual-mapping scheduling strategy for distributed LLM serving that achieves both cache affinity and load balancing. Its key idea is to map each request to two candidate instances via two independent hash functions based on the request prompt, then intelligently select the better candidate based on current system states. This design increases the likelihood that requests with shared prefixes are co-located, while evenly dispersing distinct prefixes across the cluster via ``the power of two choices''. To make DualMap robust under dynamic and skewed real-world workloads, we incorporate three techniques: 1) SLO-aware request routing, which prioritizes cache affinity but switches to load-aware scheduling when TTFT exceeds the SLO, enhancing load balance without sacrificing cache reuse; 2) hotspot-aware rebalancing, which dynamically migrates requests from overloaded to underloaded instances, mitigating hotspots and rebalancing the system; 3) lightweight dual-hash-ring scaling, which leverages a dual-hash-ring mapping to support fast and low-overhead instance scaling without costly global remapping. Experiments on real-world workloads show that DualMap improves effective request capacity by up to 2.25$\\times$ under the same TTFT SLO constraints compared with SOTA work.", "AI": {"tldr": "DualMap\uff1a\u4e00\u79cd\u7528\u4e8e\u5206\u5e03\u5f0fLLM\u670d\u52a1\u7684\u53cc\u6620\u5c04\u8c03\u5ea6\u7b56\u7565\uff0c\u901a\u8fc7\u53cc\u91cd\u54c8\u5e0c\u6620\u5c04\u548c\u667a\u80fd\u9009\u62e9\u5b9e\u73b0KV\u7f13\u5b58\u4eb2\u548c\u6027\u4e0e\u8d1f\u8f7d\u5747\u8861\u7684\u5e73\u8861", "motivation": "\u5728LLM\u670d\u52a1\u4e2d\uff0c\u8de8\u8bf7\u6c42\u91cd\u7528\u63d0\u793a\u7684KV\u7f13\u5b58\u5bf9\u4e8e\u964d\u4f4eTTFT\u548c\u670d\u52a1\u6210\u672c\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u7684\u8c03\u5ea6\u5668\u65e0\u6cd5\u5728\u7f13\u5b58\u4eb2\u548c\u6027\u8c03\u5ea6\uff08\u5c06\u76f8\u540c\u63d0\u793a\u524d\u7f00\u7684\u8bf7\u6c42\u5171\u7f6e\uff09\u548c\u8d1f\u8f7d\u5747\u8861\u8c03\u5ea6\uff08\u5c06\u8bf7\u6c42\u5747\u5300\u5206\u5e03\u5230\u8ba1\u7b97\u5b9e\u4f8b\uff09\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u56e0\u4e3a\u5b83\u4eec\u901a\u5e38\u5728\u5355\u4e00\u6620\u5c04\u7a7a\u95f4\u4e2d\u64cd\u4f5c\u3002", "method": "\u63d0\u51faDualMap\u53cc\u6620\u5c04\u8c03\u5ea6\u7b56\u7565\uff1a1\uff09\u901a\u8fc7\u4e24\u4e2a\u72ec\u7acb\u7684\u54c8\u5e0c\u51fd\u6570\u5c06\u6bcf\u4e2a\u8bf7\u6c42\u6620\u5c04\u5230\u4e24\u4e2a\u5019\u9009\u5b9e\u4f8b\uff1b2\uff09\u57fa\u4e8e\u5f53\u524d\u7cfb\u7edf\u72b6\u6001\u667a\u80fd\u9009\u62e9\u66f4\u597d\u7684\u5019\u9009\u5b9e\u4f8b\uff1b3\uff09\u5305\u542b\u4e09\u79cd\u6280\u672f\uff1aSLO\u611f\u77e5\u8bf7\u6c42\u8def\u7531\u3001\u70ed\u70b9\u611f\u77e5\u518d\u5e73\u8861\u3001\u8f7b\u91cf\u7ea7\u53cc\u54c8\u5e0c\u73af\u6269\u5c55\u3002", "result": "\u5728\u771f\u5b9e\u5de5\u4f5c\u8d1f\u8f7d\u5b9e\u9a8c\u4e2d\uff0c\u5728\u76f8\u540c\u7684TTFT SLO\u7ea6\u675f\u4e0b\uff0cDualMap\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u5de5\u4f5c\u5c06\u6709\u6548\u8bf7\u6c42\u5bb9\u91cf\u63d0\u9ad8\u4e86\u6700\u591a2.25\u500d\u3002", "conclusion": "DualMap\u901a\u8fc7\u521b\u65b0\u7684\u53cc\u6620\u5c04\u8bbe\u8ba1\uff0c\u6210\u529f\u89e3\u51b3\u4e86LLM\u670d\u52a1\u4e2d\u7f13\u5b58\u4eb2\u548c\u6027\u4e0e\u8d1f\u8f7d\u5747\u8861\u4e4b\u95f4\u7684\u51b2\u7a81\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6027\u80fd\u8868\u73b0\u548c\u7cfb\u7edf\u6548\u7387\u3002"}}
{"id": "2602.06555", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.06555", "abs": "https://arxiv.org/abs/2602.06555", "authors": ["Lanpei Li", "Massimo Coppola", "Malio Li", "Valerio Besozzi", "Jack Bell", "Vincenzo Lomonaco"], "title": "Reinforcement Learning-Based Dynamic Management of Structured Parallel Farm Skeletons on Serverless Platforms", "comment": "Accepted at AHPC3 workshop, PDP 2026", "summary": "We present a framework for dynamic management of structured parallel processing skeletons on serverless platforms. Our goal is to bring HPC-like performance and resilience to serverless and continuum environments while preserving the programmability benefits of skeletons. As a first step, we focus on the well known Farm pattern and its implementation on the open-source OpenFaaS platform, treating autoscaling of the worker pool as a QoS-aware resource management problem. The framework couples a reusable farm template with a Gymnasium-based monitoring and control layer that exposes queue, timing, and QoS metrics to both reactive and learning-based controllers. We investigate the effectiveness of AI-driven dynamic scaling for managing the farm's degree of parallelism via the scalability of serverless functions on OpenFaaS. In particular, we discuss the autoscaling model and its training, and evaluate two reinforcement learning (RL) policies against a baseline of reactive management derived from a simple farm performance model. Our results show that AI-based management can better accommodate platform-specific limitations than purely model-based performance steering, improving QoS while maintaining efficient resource usage and stable scaling behaviour.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u5728\u65e0\u670d\u52a1\u5668\u5e73\u53f0\u4e0a\u52a8\u6001\u7ba1\u7406\u7ed3\u6784\u5316\u5e76\u884c\u5904\u7406\u9aa8\u67b6\u7684\u6846\u67b6\uff0c\u91cd\u70b9\u5173\u6ce8Farm\u6a21\u5f0f\uff0c\u7ed3\u5408AI\u9a71\u52a8\u7684\u52a8\u6001\u6269\u5c55\u6765\u63d0\u5347\u6027\u80fd\u548c\u8d44\u6e90\u6548\u7387\u3002", "motivation": "\u5c06HPC\u7ea7\u522b\u7684\u6027\u80fd\u548c\u5f39\u6027\u5f15\u5165\u65e0\u670d\u52a1\u5668\u548c\u8fde\u7eed\u8ba1\u7b97\u73af\u5883\uff0c\u540c\u65f6\u4fdd\u6301\u9aa8\u67b6\u7f16\u7a0b\u7684\u53ef\u7f16\u7a0b\u6027\u4f18\u52bf\uff0c\u89e3\u51b3\u65e0\u670d\u52a1\u5668\u5e73\u53f0\u4e0a\u5e76\u884c\u5904\u7406\u7684\u6027\u80fd\u7ba1\u7406\u95ee\u9898\u3002", "method": "\u57fa\u4e8eOpenFaaS\u5e73\u53f0\u5b9e\u73b0Farm\u6a21\u5f0f\uff0c\u5c06\u5de5\u4f5c\u6c60\u81ea\u52a8\u6269\u5c55\u4f5c\u4e3aQoS\u611f\u77e5\u7684\u8d44\u6e90\u7ba1\u7406\u95ee\u9898\u3002\u6846\u67b6\u7ed3\u5408\u53ef\u590d\u7528\u7684farm\u6a21\u677f\u548c\u57fa\u4e8eGymnasium\u7684\u76d1\u63a7\u63a7\u5236\u5c42\uff0c\u652f\u6301\u53cd\u5e94\u5f0f\u548c\u57fa\u4e8e\u5b66\u4e60\u7684\u63a7\u5236\u5668\uff0c\u7814\u7a76AI\u9a71\u52a8\u7684\u52a8\u6001\u6269\u5c55\u7b56\u7565\u3002", "result": "AI\u57fa\u7840\u7684\u7ba1\u7406\u76f8\u6bd4\u7eaf\u57fa\u4e8e\u6a21\u578b\u7684\u6027\u80fd\u5f15\u5bfc\u80fd\u66f4\u597d\u5730\u9002\u5e94\u5e73\u53f0\u7279\u5b9a\u9650\u5236\uff0c\u5728\u4fdd\u6301\u9ad8\u6548\u8d44\u6e90\u4f7f\u7528\u548c\u7a33\u5b9a\u6269\u5c55\u884c\u4e3a\u7684\u540c\u65f6\u6539\u5584QoS\u3002", "conclusion": "AI\u9a71\u52a8\u7684\u52a8\u6001\u6269\u5c55\u7b56\u7565\u5728\u65e0\u670d\u52a1\u5668\u5e73\u53f0\u4e0a\u7ba1\u7406\u5e76\u884c\u5904\u7406\u9aa8\u67b6\u662f\u6709\u6548\u7684\uff0c\u80fd\u591f\u5e73\u8861\u6027\u80fd\u3001\u8d44\u6e90\u6548\u7387\u548c\u7a33\u5b9a\u6027\uff0c\u4e3a\u7ed3\u6784\u5316\u5e76\u884c\u8ba1\u7b97\u5728\u65e0\u670d\u52a1\u5668\u73af\u5883\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u5411\u3002"}}
