<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 2]
- [cs.DC](#cs.DC) [Total: 6]
- [cs.AR](#cs.AR) [Total: 3]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Solvable Tuple Patterns and Their Applications to Program Verification](https://arxiv.org/abs/2508.20365)
*Naoki Kobayashi,Ryosuke Sato,Ayumi Shinohara,Ryo Yoshinaka*

Main category: cs.PL

TL;DR: 该论文提出了可解元组模式(STP)的概念，用于表达列表式递归数据结构的程序不变量，无需负样本即可从小样本中高效推断，并集成到CHC求解器中实现自动化程序验证。


<details>
  <summary>Details</summary>
Motivation: 尽管程序验证技术有所进展，但完全自动化验证操作递归数据结构的程序仍然具有挑战性，特别是在处理列表类数据结构的不变量推断方面。

Method: 引入可解元组模式(STP)表达数据结构不变量，开发STP推断算法仅需正样本，利用支持序列理论的SMT求解器验证不变量，并将STP推断集成到支持列表类数据结构的CHC求解器中。

Result: 集成STP推断的CHC求解器在CHC-COMP 2025的ADT-LIN类别中以显著优势获胜，证明了该方法的有效性。

Conclusion: STP提供了一种高效推断递归数据结构不变量的方法，无需负样本且可集成到现有验证框架中，显著提升了自动化程序验证能力。

Abstract: Despite the recent progress of automated program verification techniques,
fully automated verification of programs manipulating recursive data structures
remains a challenge. We introduce the notion of solvable tuple patterns (STPs)
to express invariants between list-like recursive data structures. A
distinguishing feature of STPs is that they can be efficiently inferred from
only a small number of positive samples; no negative samples are required. An
SMT solver that supports the sequence theory can be used to check that an
inferred STP is indeed an inductive invariant. After presenting basic
properties of STPs and an STP inference algorithm, we show how to incorporate
the STP inference into a CHC (Constrained Horn Clauses) solver supporting
list-like data structures, which serves as a uniform backend for automated
program verification tools. A CHC solver incorporating the STP inference has
won the ADT-LIN category of CHC-COMP 2025 by a big margin.

</details>


### [2] [Static Factorisation of Probabilistic Programs With User-Labelled Sample Statements and While Loops](https://arxiv.org/abs/2508.20922)
*Markus Böck,Jürgen Cito*

Main category: cs.PL

TL;DR: 该论文研究了概率程序与贝叶斯网络之间的双向表示关系，提出了支持循环和动态标签的静态分析方法，开发了程序切片技术来优化变分推理、Metropolis Hastings和顺序蒙特卡罗算法。


<details>
  <summary>Details</summary>
Motivation: 解决概率程序（包含用户标记的sample语句和while循环）能否被图形化表示的开源问题，这些特性在Gen、Turing和Pyro等概率编程语言中很常见。

Method: 扩展现有操作语义以支持语言特性，通过将程序转换为控制流图，定义静态分析来近似程序中随机变量的依赖结构，获得静态因子分解，并开发程序切片技术。

Result: 获得了隐式定义程序密度的静态因子分解，对于无循环和常量标签的程序等同于贝叶斯网络因子分解，对于通过循环或动态标签定义无限随机变量的程序提供了新颖的图形表示。

Conclusion: 提出的优化技术被证明是正确的，并在经验上显示能够匹配或超越现有技术，成功降低了变分推理中梯度估计的方差，并加速了单点Metropolis Hastings和顺序蒙特卡罗算法。

Abstract: It is commonly known that any Bayesian network can be implemented as a
probabilistic program, but the reverse direction is not so clear. In this work,
we address the open question to what extent a probabilistic program with
user-labelled sample statements and while loops - features found in languages
like Gen, Turing, and Pyro - can be represented graphically. To this end, we
extend existing operational semantics to support these language features. By
translating a program to its control-flow graph, we define a sound static
analysis that approximates the dependency structure of the random variables in
the program. As a result, we obtain a static factorisation of the implicitly
defined program density, which is equivalent to the known Bayesian network
factorisation for programs without loops and constant labels, but constitutes a
novel graphical representation for programs that define an unbounded number of
random variables via loops or dynamic labels. We further develop a sound
program slicing technique to leverage this structure to statically enable three
well-known optimisations for the considered program class: we reduce the
variance of gradient estimates in variational inference and we speed up both
single-site Metropolis Hastings and sequential Monte Carlo. These optimisations
are proven correct and empirically shown to match or outperform existing
techniques.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [3] [SpeedMalloc: Improving Multi-threaded Applications via a Lightweight Core for Memory Allocation](https://arxiv.org/abs/2508.20253)
*Ruihao Li,Qinzhe Wu,Krishna Kavi,Gayatri Mehta,Jonathan C. Beard,Neeraja J. Yadwadkar,Lizy K. John*

Main category: cs.DC

TL;DR: SpeedMalloc使用轻量级支持核心处理多线程应用的内存分配任务，通过将分配器元数据隔离在专用核心缓存中，减少缓存冲突和跨核同步开销，相比现有软件和硬件分配器实现显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 内存分配虽然只占代码的5%，但对程序性能有蝴蝶效应影响，可达2.7倍性能差异。现有加速器方案对多线程支持有限，跨核同步仍是挑战。

Method: 采用轻量级可编程支持核心处理内存分配任务，该核心具有高效跨核数据同步能力，将所有分配器元数据保存在自身缓存中，避免与用户数据缓存冲突和跨核元数据同步。

Result: 相比Jemalloc、TCMalloc、Mimalloc、Mallacc和Memento五种先进分配器，SpeedMalloc在多线程工作负载上分别实现1.75倍、1.18倍、1.15倍、1.23倍和1.18倍的加速。

Conclusion: SpeedMalloc通过专用支持核心设计有效解决了多线程内存分配的缓存污染和同步开销问题，相比现有方案性能更优，且能适应新的分配器设计。

Abstract: Memory allocation, though constituting only a small portion of the executed
code, can have a "butterfly effect" on overall program performance, leading to
significant and far-reaching impacts. Despite accounting for just approximately
5% of total instructions, memory allocation can result in up to a 2.7x
performance variation depending on the allocator used. This effect arises from
the complexity of memory allocation in modern multi-threaded multi-core
systems, where allocator metadata becomes intertwined with user data, leading
to cache pollution or increased cross-thread synchronization overhead.
Offloading memory allocators to accelerators, e.g., Mallacc and Memento, is a
potential direction to improve the allocator performance and mitigate cache
pollution. However, these accelerators currently have limited support for
multi-threaded applications, and synchronization between cores and accelerators
remains a significant challenge.
  We present SpeedMalloc, using a lightweight support-core to process memory
allocation tasks in multi-threaded applications. The support-core is a
lightweight programmable processor with efficient cross-core data
synchronization and houses all allocator metadata in its own caches. This
design minimizes cache conflicts with user data and eliminates the need for
cross-core metadata synchronization. In addition, using a general-purpose core
instead of domain-specific accelerators makes SpeedMalloc capable of adopting
new allocator designs. We compare SpeedMalloc with state-of-the-art software
and hardware allocators, including Jemalloc, TCMalloc, Mimalloc, Mallacc, and
Memento. SpeedMalloc achieves 1.75x, 1.18x, 1.15x, 1.23x, and 1.18x speedups on
multithreaded workloads over these five allocators, respectively.

</details>


### [4] [SwizzlePerf: Hardware-Aware LLMs for GPU Kernel Performance Optimization](https://arxiv.org/abs/2508.20258)
*Arya Tschand,Muhammad Awad,Ryan Swann,Kesavan Ramakrishnan,Jeffrey Ma,Keith Lowery,Ganesh Dasika,Vijay Janapa Reddi*

Main category: cs.DC

TL;DR: SwizzlePerf是一个利用LLM进行GPU内核性能优化的系统，通过提供硬件感知能力，自动生成空间优化方案，相比传统搜索方法效率大幅提升


<details>
  <summary>Details</summary>
Motivation: 现有基于搜索的LLM性能优化方法缺乏硬件感知能力，而人类性能工程师依赖硬件特性来实现近最优性能。需要让LLM具备硬件意识来进行软件级优化

Method: 通过分析工作负载的内存访问模式、架构规格、过滤的性能日志和历史性能反思，为LLM提供明确的硬件感知能力，自动生成GPU内核的空间优化方案

Result: 对于GEMM内核，SwizzlePerf在5分钟内生成专家需要2周才能找到的最优swizzling模式。在10个多样化内核中，9个实现了最高2.06倍加速和70%的L2命中率提升

Conclusion: 这是首个系统性地创建硬件感知LLM性能工程代理的工作，为未来自动化硬件优化开辟了新方向

Abstract: Large language models (LLMs) have shown progress in GPU kernel performance
engineering using inefficient search-based methods that optimize around
runtime. Any existing approach lacks a key characteristic that human
performance engineers rely on for near-optimal utilization --
hardware-awareness. By leveraging the workload's specific memory access
patterns, architecture specifications, filtered profiling logs, and reflections
on historical performance, we can make software-level optimizations that are
tailored to the underlying hardware. SwizzlePerf automatically generates
spatial optimizations for GPU kernels on disaggregated architectures by giving
LLMs explicit hardware-awareness.
  For a GEMM kernel, SwizzlePerf takes less than 5 minutes to generate the same
hardware-specific optimal swizzling pattern that took expert performance
engineers 2 weeks to find. On a suite of 10 diverse ML and Science kernels,
SwizzlePerf can generate swizzling patterns for 9 of the kernels that achieve
up to a 2.06x speedup and 70% improvement in L2 hit rate. This work is the
first of many steps toward systematically creating hardware-aware LLM
performance engineering agents.

</details>


### [5] [Predictable LLM Serving on GPU Clusters](https://arxiv.org/abs/2508.20274)
*Erfan Darzi,Shreeanant Bharadwaj,Sree Bhargavi Balija*

Main category: cs.DC

TL;DR: 提出了一种基于主机级别的PCIe干扰控制方案，通过动态MIG重配置、PCIe感知放置和轻量级防护机制，有效降低共享A100集群中的尾部延迟和SLO违规率。


<details>
  <summary>Details</summary>
Motivation: 共享A100集群中PCIe结构上的噪声邻居干扰会导致尾部延迟增加和SLO违规，影响延迟敏感型推理任务的性能。

Method: 采用动态多实例GPU(MIG)重配置、PCIe感知的虚拟机放置策略，结合MPS配额和cgroup I/O控制等轻量级防护机制，通过采样租户尾部延迟和系统信号，利用拓扑提示避免PCIe热点。

Result: 在单主机和2节点(16-GPU)集群上，SLO违规率降低约32%，p99延迟改善约15%，吞吐量成本≤5%；LLM服务评估显示TTFT p99改善10-15%，成本≤5%。

Conclusion: 该控制器方案能有效缓解PCIe干扰问题，在保持低吞吐量成本的同时显著改善尾部延迟性能，且无需修改控制器即可适用于LLM服务场景。

Abstract: Latency-sensitive inference on shared A100 clusters often suffers
noisy-neighbor interference on the PCIe fabric, inflating tail latency and SLO
violations. We present a fabric-agnostic, VM-deployable host-level controller
that combines dynamic Multi-Instance GPU (MIG) reconfiguration, PCIe-aware
placement, and lightweight guardrails (MPS quotas, cgroup I/O). It samples
per-tenant tails and system signals, uses topology hints to avoid PCIe hot
spots, and gates actions with dwell/cool-down to avoid thrash. On a single host
and a 2-node (16-GPU) cluster, SLO miss-rate is reduced by \(\approx\)32\%
(\(\approx\)1.5) and p99 latency improves \(\approx\)15\% with \(\leq\)5\%
throughput cost versus static MIG and naive placement; ablations show MIG and
placement contribute comparably. We also evaluate LLM serving with vLLM on OLMo
2 7B Instruct: TTFT p99 improves \(\approx\)10--15\% at \(\leq\)5\% cost
without changing the controller.

</details>


### [6] [CoFormer: Collaborating with Heterogeneous Edge Devices for Scalable Transformer Inference](https://arxiv.org/abs/2508.20375)
*Guanyu Xu,Zhiwei Hao,Li Shen,Yong Luo,Fuhui Sun,Xiaoyan Wang,Han Hu,Yonggang Wen*

Main category: cs.DC

TL;DR: CoFormer是一个协作推理系统，通过将大型transformer模型分解为多个小型模型在边缘设备上分布式推理，解决了计算资源受限环境下的实时推理挑战。


<details>
  <summary>Details</summary>
Motivation: 现有策略要么将transformer计算卸载到其他设备导致通信开销大，要么直接在边缘设备部署压缩模型导致精度和效率权衡不佳。需要一种新方法在资源受限的边缘设备上实现高效推理。

Method: 利用transformer的可分割性和可集成性，将大型transformer分解为多个小型模型进行分布式推理，中间结果聚合生成最终输出。使用DeBo算法解决优化问题，最小化推理延迟和精度损失，并通过渐进校准恢复性能。

Result: 支持多种transformer模型在异构边缘设备上运行，推理速度提升3.1倍，内存需求减少76.3%，能耗降低约40%，同时保持满意的推理性能。

Conclusion: CoFormer为边缘设备上的大型transformer模型推理提供了一种有效的协作解决方案，在性能、效率和资源消耗之间取得了良好平衡。

Abstract: The impressive performance of transformer models has sparked the deployment
of intelligent applications on resource-constrained edge devices. However,
ensuring high-quality service for real-time edge systems is a significant
challenge due to the considerable computational demands and resource
requirements of these models. Existing strategies typically either offload
transformer computations to other devices or directly deploy compressed models
on individual edge devices. These strategies, however, result in either
considerable communication overhead or suboptimal trade-offs between accuracy
and efficiency. To tackle these challenges, we propose a collaborative
inference system for general transformer models, termed CoFormer. The central
idea behind CoFormer is to exploit the divisibility and integrability of
transformer. An off-the-shelf large transformer can be decomposed into multiple
smaller models for distributed inference, and their intermediate results are
aggregated to generate the final output. We formulate an optimization problem
to minimize both inference latency and accuracy degradation under heterogeneous
hardware constraints. DeBo algorithm is proposed to first solve the
optimization problem to derive the decomposition policy, and then progressively
calibrate decomposed models to restore performance. We demonstrate the
capability to support a wide range of transformer models on heterogeneous edge
devices, achieving up to 3.1$\times$ inference speedup with large transformer
models. Notably, CoFormer enables the efficient inference of GPT2-XL with 1.6
billion parameters on edge devices, reducing memory requirements by 76.3\%.
CoFormer can also reduce energy consumption by approximately 40\% while
maintaining satisfactory inference performance.

</details>


### [7] [pdGRASS: A Fast Parallel Density-Aware Algorithm for Graph Spectral Sparsification](https://arxiv.org/abs/2508.20403)
*Tiancheng Zhao,Zekun Yin,Huihai An,Xiaoyu Yang,Zhou Jin,Jiasi Shen,Helen Xu*

Main category: cs.DC

TL;DR: 提出了pdGRASS并行算法，解决了现有feGRASS方法在并行化和偏斜输入上的问题，实现了3.9x-8.8x的加速比和更好的稀疏化质量


<details>
  <summary>Details</summary>
Motivation: 现有feGRASS方法存在两个主要问题：1)恢复步骤难以并行化（严格数据依赖）2)在偏斜输入上性能下降，需要多次遍历

Method: pdGRASS并行算法，将边组织成无数据依赖的独立子任务，支持高效并行化和单次遍历的充分边恢复

Result: 平均加速比3.9x-8.8x，PCG迭代次数改善1.2x-1.8x，最坏情况下实现1000x以上加速

Conclusion: pdGRASS在图谱稀疏化问题上显著提升了可扩展性和性能

Abstract: Graph Spectral Sparsification (GSS) identifies an ultra-sparse subgraph, or
sparsifier, whose Laplacian matrix closely approximates the spectral properties
of the original graph, enabling substantial reductions in computational
complexity for computationally intensive problems in scientific computing. The
state-of-the-art method for efficient GSS is feGRASS, consisting of two steps:
1) spanning tree generation and 2) off-tree edge recovery. However, feGRASS
suffers from two main issues: 1) difficulties in parallelizing the recovery
step for strict data dependencies, and 2) performance degradation on skewed
inputs, often requiring multiple passes to recover sufficient edges. To address
these challenges, we propose parallel density-aware Graph Spectral
Sparsification (pdGRASS), a parallel algorithm that organizes edges into
disjoint subtasks without data dependencies between them, enabling efficient
parallelization and sufficient edge recovery in a single pass. We empirically
evaluate feGRASS and pdGRASS based on 1) off-tree edge-recovery runtime and 2)
sparsifier quality, measured by the iteration count required for convergence in
a preconditioned conjugate gradient (PCG) application. The evaluation
demonstrates that, depending on the number of edges recovered, pdGRASS achieves
average speedups ranging from 3.9x to 8.8x. The resulting sparsifiers also show
between 1.2x higher and 1.8x lower PCG iteration counts, with further
improvements as more edges are recovered. Additionally, pdGRASS mitigates the
worst-case runtimes of feGRASS with over 1000x speedup. These results highlight
pdGRASS's significant improvements in scalability and performance for the graph
spectral sparsification problem.

</details>


### [8] [Collaborative Evolution of Intelligent Agents in Large-Scale Microservice Systems](https://arxiv.org/abs/2508.20508)
*Yilin Li,Song Han,Sibo Wang,Ming Wang,Renzi Meng*

Main category: cs.DC

TL;DR: 基于多代理协同进化机制的微服务智能优化方法，通过图表示学习和游戏驱动策略优化，有效解决大规模微服务系统的管理挑战


<details>
  <summary>Details</summary>
Motivation: 解决大规模微服务架构中复杂的服务依赖关系、动态拓扑结构和波动工作负荷等管理挑战

Method: 将每个服务模型化为代理，使用图表示学习构建服务依赖图，基于Markov决策过程学习策略，采用中央训练分散执行框架，设计游戏驱动的策略优化机制

Result: 在微服务模拟平台上进行实验，在多个关键指标上超过其他先进方法，显著提高了大规模微服务系统的管理效率和运行稳定性

Conclusion: 该方法能够快速响应突发工作负荷、拓扑重配置或资源冲突等场景，实现稳定的策略收敛，具有强烈的实践价值和工程可行性

Abstract: This paper proposes an intelligent service optimization method based on a
multi-agent collaborative evolution mechanism to address governance challenges
in large-scale microservice architectures. These challenges include complex
service dependencies, dynamic topology structures, and fluctuating workloads.
The method models each service as an agent and introduces graph representation
learning to construct a service dependency graph. This enables agents to
perceive and embed structural changes within the system. Each agent learns its
policy based on a Markov Decision Process. A centralized training and
decentralized execution framework is used to integrate local autonomy with
global coordination. To enhance overall system performance and adaptability, a
game-driven policy optimization mechanism is designed. Through a
selection-mutation process, agent strategy distributions are dynamically
adjusted. This supports adaptive collaboration and behavioral evolution among
services. Under this mechanism, the system can quickly respond and achieve
stable policy convergence when facing scenarios such as sudden workload spikes,
topology reconfigurations, or resource conflicts. To evaluate the effectiveness
of the proposed method, experiments are conducted on a representative
microservice simulation platform. Comparative analyses are performed against
several advanced approaches, focusing on coordination efficiency, adaptability,
and policy convergence performance. Experimental results show that the proposed
method outperforms others in several key metrics. It significantly improves
governance efficiency and operational stability in large-scale microservice
systems. The method demonstrates strong practical value and engineering
feasibility.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [9] [Testing and Fault Tolerance Techniques for Carbon Nanotube-Based FPGAs](https://arxiv.org/abs/2508.20304)
*Siyuan Lu,Kangwei Xu,Peng Xie,Rui Wang,Yuanqing Cheng*

Main category: cs.AR

TL;DR: 本文提出了一系列测试技术和备份架构，用于检测修复基于碳纳米管的FPGA中的延迟故障和金属性碳纳米管故障，显著提高了测试效率和产品良率。


<details>
  <summary>Details</summary>
Motivation: 随着半导体制造进入纳米级，传统CMOS FPGA在性能和功耗方面面临挑战。多壁碳纳米管(MWCNT)和碳纳米管场效应线性管(CNFET)作为有前景的替代者，但因制造过程不成熟导致了延迟故障和金属性碳纳米管(m-CNTs)故障。

Method: 提出基于环形振荡器(RO)的测试技术检测MWCNT互连线延迟故障；为CLB中的运行链提出有效测试技术；采用改进的查找表(LUT)电路设计加快测试速度；提出检测m-CNTs的测试算法；设计了冗余备份行共享架构提高产品良率。

Result: 实验结果显示，6输入LUT的测试时间比传统测试减少35.49%，提出的算法能够以少量开销实现高测试覆盖率，冗余架构能够高效地修复故障段。

Conclusion: 该研究为碳纳米管基FPGA提供了一套完整的测试和修复方案，有效解决了因制造过程不成熟导致的故障问题，显著提高了测试效率和产品良率，促进了碳纳米管技术在FPGA领域的应用。

Abstract: As the semiconductor manufacturing process technology node shrinks into the
nanometer-scale, the CMOS-based Field Programmable Gate Arrays (FPGAs) face big
challenges in scalability of performance and power consumption. Multi-walled
Carbon Nanotube (MWCNT) serves as a promising candidate for Cu interconnects
thanks to the superior conductivity. Moreover, Carbon Nanotube Field Transistor
(CNFET) also emerges as a prospective alternative to the conventional CMOS
device because of high power efficiency and large noise margin. The combination
of MWCNT and CNFET enables the promising CNT-based FPGAs. However, the MWCNT
interconnects exhibit significant process variations due to immature
fabrication process, leading to delay faults. Also, the non-ideal CNFET
fabrication process may generate a few metallic CNTs (m-CNTs), rendering
correlated faulty blocks. In this article, we propose a ring oscillator (RO)
based testing technique to detect delay faults due to the process variation of
MWCNT interconnects. Furthermore, we propose an effective testing technique for
the carry chains in CLBs, and an improved circuit design based on the lookup
table (LUT) is applied to speed up the fault testing of CNT-based FPGAs. In
addition, we propose a testing algorithm to detect m-CNTs in CLBs. Finally, we
propose a redundant spare row sharing architecture to improve the yield of
CNT-based FPGA further. Experimental results show that the test time for a
6-input LUT can be reduced by 35.49% compared with conventional testing, and
the proposed algorithm can achieve a high test coverage with little overhead.
The proposed redundant architecture can repair the faulty segment effectively
and efficiently.

</details>


### [10] [The Future of Memory: Limits and Opportunities](https://arxiv.org/abs/2508.20425)
*Shuhan Liu,Samuel Dayo,Peijing Li,Philip Levis,Subhasish Mitra,Thierry Tambe,David Tennenhouse,H. -S. Philip Wong*

Main category: cs.AR

TL;DR: 这篇论文提出了一种新的计算-内存节点架构，通过将内存分割成更小的切片并与计算元素紧密耦合，来解决大规模共享内存系统的扩展性和信令挑战。


<details>
  <summary>Details</summary>
Motivation: 内存延迟、带宽、容量和能耗问题不断到制系统性能，需要重新考虑系统架构来解决这些挑战。

Method: 利用2.5D/3D集成技术，构建计算-内存节点，提供私有本地内存和包内共享内存元素，让软件明确管理数据位置和移动。

Result: 这种方法能够实现微米级距离的节点独占数据访问，显著降低访问成本，并提供比DRAM更好的带宽和能量效率。

Conclusion: 通过将内存分割成小片与计算紧密耦合，让软件明确管理内存层级，可以更有效地解决内存性能和能耗问题。

Abstract: Memory latency, bandwidth, capacity, and energy increasingly limit
performance. In this paper, we reconsider proposed system architectures that
consist of huge (many-terabyte to petabyte scale) memories shared among large
numbers of CPUs. We argue two practical engineering challenges, scaling and
signaling, limit such designs. We propose the opposite approach. Rather than
create large, shared, homogenous memories, systems explicitly break memory up
into smaller slices more tightly coupled with compute elements. Leveraging
advances in 2.5D/3D integration, this compute-memory node provisions private
local memory, enabling accesses of node-exclusive data through micrometer-scale
distances, and dramatically reduced access cost. In-package memory elements
support shared state within a processor, providing far better bandwidth and
energy-efficiency than DRAM, which is used as main memory for large working
sets and cold data. Hardware making memory capacities and distances explicit
allows software to efficiently compose this hierarchy, managing data placement
and movement.

</details>


### [11] [Microarchitecture Design and Benchmarking of Custom SHA-3 Instruction for RISC-V](https://arxiv.org/abs/2508.20653)
*Alperen Bolat,Sakir Sezer,Kieran McLaughlin,Henry Hui*

Main category: cs.AR

TL;DR: 该研究探讨了在RISC-V CPU架构中集成SHA-3加密加速指令的微架构挑战，通过定制指令实现显著性能提升。


<details>
  <summary>Details</summary>
Motivation: SHA-3加密由于其独特的基于置换的结构和内存访问模式，现有解决方案主要依赖独立协处理器或软件优化，缺乏直接的微架构集成研究。

Method: 在RISC-V CPU架构中设计和原型化SHA-3定制指令，使用GEM5周期精确模拟和FPGA原型验证，重点关注流水线并行执行、存储利用和硬件成本。

Result: 性能提升达8.02倍（RISC-V优化软件）和46.31倍（Keccak特定软件），寄存器使用仅增加15.09%，LUT利用率增加11.51%。

Conclusion: 研究证明了在微架构级别实现SHA-3加速的可行性，为未来密码学指令集扩展提供了关键设计见解。

Abstract: Integrating cryptographic accelerators into modern CPU architectures presents
unique microarchitectural challenges, particularly when extending instruction
sets with complex and multistage operations. Hardware-assisted cryptographic
instructions, such as Intel's AES-NI and ARM's custom instructions for
encryption workloads, have demonstrated substantial performance improvements.
However, efficient SHA-3 acceleration remains an open problem due to its
distinct permutation-based structure and memory access patterns. Existing
solutions primarily rely on standalone coprocessors or software optimizations,
often avoiding the complexities of direct microarchitectural integration. This
study investigates the architectural challenges of embedding a SHA-3
permutation operation as a custom instruction within a general-purpose
processor, focusing on pipelined simultaneous execution, storage utilization,
and hardware cost. In this paper, we investigated and prototyped a SHA-3 custom
instruction for the RISC-V CPU architecture. Using cycle-accurate GEM5
simulations and FPGA prototyping, our results demonstrate performance
improvements of up to 8.02x for RISC-V optimized SHA-3 software workloads and
up to 46.31x for Keccak-specific software workloads, with only a 15.09%
increase in registers and a 11.51% increase in LUT utilization. These findings
provide critical insights into the feasibility and impact of SHA-3 acceleration
at the microarchitectural level, highlighting practical design considerations
for future cryptographic instruction set extensions.

</details>
