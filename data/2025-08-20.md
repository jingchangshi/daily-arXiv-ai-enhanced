<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 2]
- [cs.DC](#cs.DC) [Total: 10]
- [cs.AR](#cs.AR) [Total: 13]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Reactive Semantics for User Interface Description Languages](https://arxiv.org/abs/2508.13610)
*Basile Pesin,Celia Picard,Cyril Allignol*

Main category: cs.PL

TL;DR: 本文提出了一种用于核心反应式UIDL语言Smalite的辛拉语义模型，以支持对用户界面描述语言的正式化验证。


<details>
  <summary>Details</summary>
Motivation: 虽然用户界面描述语言(UIDL)已广泛用于安全关键GUI开发，但对其正式化和验证的研究还很缺乏。

Method: 设计了一种辛拉语义模型，用于核心反应式UIDL语言Smalite，该语言足够表达更复杂语言的构造。

Result: 提供了一个正式的语义基础，为后续开发经过正式验证的UIDL编译器奠定基础。

Conclusion: 这项预研究工作为实现高可靠性的用户界面开发工具提供了重要的理论支撑。

Abstract: User Interface Description Languages (UIDLs) are high-level languages that
facilitate the development of Human-Machine Interfaces, such as Graphical User
Interface (GUI) applications. They usually provide first-class primitives to
specify how the program reacts to an external event (user input, network
message), and how data flows through the program. Although these
domain-specific languages are now widely used to implement safety-critical
GUIs, little work has been invested in their formalization and verification.
  In this paper, we propose a denotational semantic model for a core reactive
UIDL, Smalite, which we argue is expressive enough to encode constructs from
more realistic languages. This preliminary work may be used as a stepping stone
to produce a formally verified compiler for UIDLs.

</details>


### [2] [Bisimilarity and Simulatability of Processes Parameterized by Join Interactions](https://arxiv.org/abs/2508.13611)
*Clemens Grabmayer,Maurizio Murgia*

Main category: cs.PL

TL;DR: 本文探讨了参数化二似性的自然弱化版本：无限制结合交互二似性，并证明它在确定性环境下与原概念一致，但在一般情况下是更粗糕的等价关系


<details>
  <summary>Details</summary>
Motivation: 从Larsen的参数化二似性概念出发，探索其自然弱化版本，即无限制结合交互二似性，以更好地理解进程在环境中的行为等价性

Method: 通过定义结合交互进程(p & e和q & e)的二似性来形成新的等价关系，并与原始参数化二似性进行对比分析，同时扩展到模拟关系和模态逻辑特征化

Result: 结合交互参数化二似性在确定性环境下与参数化二似性一致，但在一般情况下是更粗糕的等价关系；在模拟关系上两者完全一致；获得了与Larsen相同的环境区分预序结果

Conclusion: 结合交互参数化二似性是一种有效的弱化概念，通过确定化交互可以恢复到原始概念，并在模拟关系上保持了完全一致性，为进一步研究提供了基础

Abstract: Departing from Larsen's concept of parameterized bisimilarity of processes
with respect to interaction with environments, we start an exploration of its
natural weakening: bisimilarity of unrestricted join interactions with
environments. Parameterized bisimilarity relates processes p and q with respect
to an environment e if p and q behave bi-similarly while joining --
respectively the same -- transitions from e. The weakened variant relates
processes p and q with respect to environment e if the join-interaction
processes p & e and q & e of p and q with e are bisimilar. (Hereby join
interactions r & f facilitate a step with label a to r' & f' if and only if r
and f permit a-steps to r' and f' , respectively.) Join-interaction
parameterized (ji-parameterized) bisimilarity coincides with parameterized
bisimilarity for deterministic environments, but that it is a coarser
equivalence in general. We explain how Larsen's concept can be recovered from
ji-parameterized bisimilarity by 'determinizing' interactions. We show that by
adaptation to simulatability (simulation preorder) the same concept arises:
parameterized simulatability coincides with ji-parameterized simulatability.
For the discrimination preorder of (ji-)parameterized simulatability on
environments we obtain the same result as Larsen did for parameterized
bisimilarity. Also, we give a modal-logic characterization of
(ji-)parameterized simulatability. Finally we gather open problems, and provide
an outlook on our current related work.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [3] [Harnessing the Full Potential of RRAMs through Scalable and Distributed In-Memory Computing with Integrated Error Correction](https://arxiv.org/abs/2508.13298)
*Huynh Q. N. Vo,Md Tawsif Rahman Chowdhury,Paritosh Ramanan,Murat Yildirim,Gozde Tutuncuoglu*

Main category: cs.DC

TL;DR: MELISO+是一个全栈分布式内存计算框架，通过两层纠错机制和分布式RRAM架构，解决了阻变存储器非理想性问题，实现大规模矩阵计算，显著提升能效和降低延迟。


<details>
  <summary>Details</summary>
Motivation: 传统架构因数据移动能耗高而无法满足指数增长的计算需求，RRAM内存计算面临设备非理想性和大规模计算可扩展性差的挑战。

Method: 提出MELISO+框架，包含新颖的两层纠错机制来缓解设备非理想性，开发分布式RRAM计算框架支持超过65,000×65,000维度的矩阵计算。

Result: 设备非理想性导致的一阶和二阶算术误差减少90%以上，能效提升3-5个数量级，延迟降低100倍，低精度RRAM设备在精度、能耗和延迟方面超越高精度替代方案。

Conclusion: MELISO+通过算法-硬件协同设计和可扩展架构，显著推进了适用于大语言模型和生成式AI的可持续高维计算。

Abstract: Exponential growth in global computing demand is exacerbated due to the
higher-energy requirements of conventional architectures, primarily due to
energy-intensive data movement. In-memory computing with Resistive Random
Access Memory (RRAM) addresses this by co-integrating memory and processing,
but faces significant hurdles related to device-level non-idealities and poor
scalability for large computing tasks. Here, we introduce \textbf{MELISO+}
(In-\textbf{Me}mory \textbf{Li}near \textbf{So}lver), a full-stack, distributed
framework for energy-efficient in-memory computing. MELISO+ proposes a novel
two-tier error correction mechanism to mitigate device non-idealities and
develops a distributed RRAM computing framework to enable matrix computations
exceeding dimensions of $65,000 \times 65,000$. This approach reduces first-
and second-order arithmetic errors due to device non-idealities by over 90\%,
enhances energy efficiency by three to five orders of magnitude, and decreases
latency 100-fold. Hence, MELISO+ allows lower-precision RRAM devices to
outperform high-precision device alternatives in accuracy, energy and latency
metrics. By unifying algorithm-hardware co-design with scalable architecture,
MELISO+ significantly advances sustainable, high-dimensional computing suitable
for applications like large language models and generative AI.

</details>


### [4] [Persistent and Partitioned MPI for Stencil Communication](https://arxiv.org/abs/2508.13370)
*Gerald Collom,Jason Burmark,Olga Pearce,Amanda Bienz*

Main category: cs.DC

TL;DR: 论文分析了迭代模板操作的MPI通信优化性能，比较了非阻塞、持久性和分区通信方法，在多种规模下测试了进程数、线程数和消息大小的影响，发现持久通信提速37%，分区通信提速68%。


<details>
  <summary>Details</summary>
Motivation: 大规模并行应用中迭代模板操作的性能主要受通信开销影响，需要研究MPI优化技术（如持久通信和分区通信）来降低开销和提高通信效率。

Method: 使用Comb基准测试套件评估非阻塞、持久性和分区通信的性能，分析不同规模下的优化效果，并研究进程数、线程数和消息大小对分区通信的影响。

Result: 测量结果显示，持久MPI通信相比基线MPI通信可提速37%，分区MPI通信可提速68%。

Conclusion: 持久通信和分区通信是有效的MPI优化技术，能显著提升大规模迭代模板应用的通信性能，分区通信的优化效果尤为突出。

Abstract: Many parallel applications rely on iterative stencil operations, whose
performance are dominated by communication costs at large scales. Several MPI
optimizations, such as persistent and partitioned communication, reduce
overheads and improve communication efficiency through amortized setup costs
and reduced synchronization of threaded sends. This paper presents the
performance of stencil communication in the Comb benchmarking suite when using
non blocking, persistent, and partitioned communication routines. The impact of
each optimization is analyzed at various scales. Further, the paper presents an
analysis of the impact of process count, thread count, and message size on
partitioned communication routines. Measured timings show that persistent MPI
communication can provide a speedup of up to 37% over the baseline MPI
communication, and partitioned MPI communication can provide a speedup of up to
68%.

</details>


### [5] [OrbitChain: Orchestrating In-orbit Real-time Analytics of Earth Observation Data](https://arxiv.org/abs/2508.13374)
*Zhouyu Li,Zhijing Yang,Huayue Gu,Xiaojian Wang,Yuchen Liu,Ruozhou Yu*

Main category: cs.DC

TL;DR: OrbitChain是一个跨卫星协作分析框架，通过将分析任务分解为微服务并优化通信路由，实现地球观测数据的实时分析，比现有系统提升60%分析工作量并减少72%通信开销。


<details>
  <summary>Details</summary>
Motivation: 现有地球观测卫星系统由于带宽限制和连接时间有限，数据下载和分析需要数小时甚至数天，无法满足灾害响应等实时应用需求。

Method: 开发OrbitChain框架，将分析应用分解为微服务，在多卫星间分配计算资源，设计流量路由算法最小化星间通信开销，采用流水线工作流实现实时任务完成。

Result: 实验显示系统能比现有地球观测分析框架完成多达60%的分析工作量，同时减少高达72%的通信开销。

Conclusion: OrbitChain通过跨卫星协作和优化通信，成功实现了地球观测数据的实时分析，为时间敏感应用和星座间协作提供了有效解决方案。

Abstract: Earth observation analytics have the potential to serve many time-sensitive
applications. However, due to limited bandwidth and duration of
ground-satellite connections, it takes hours or even days to download and
analyze data from existing Earth observation satellites, making real-time
demands like timely disaster response impossible. Toward real-time analytics,
we introduce OrbitChain, a collaborative analytics framework that orchestrates
computational resources across multiple satellites in an Earth observation
constellation. OrbitChain decomposes analytics applications into microservices
and allocates computational resources for time-constrained analysis. A traffic
routing algorithm is devised to minimize the inter-satellite communication
overhead. OrbitChain adopts a pipeline workflow that completes Earth
observation tasks in real-time, facilitates time-sensitive applications and
inter-constellation collaborations such as tip-and-cue. To evaluate OrbitChain,
we implement a hardware-in-the-loop orbital computing testbed. Experiments show
that our system can complete up to 60% analytics workload than existing Earth
observation analytics framework while reducing the communication overhead by up
to 72%.

</details>


### [6] [Optimizing Allreduce Operations for Heterogeneous Architectures with Multiple Processes per GPU](https://arxiv.org/abs/2508.13397)
*Michael Adams,Amanda Bienz*

Main category: cs.DC

TL;DR: 通过利用多个CPU核心加速GPU集群通信操作，在大规模all-reduce操作中获得显著性能提升


<details>
  <summary>Details</summary>
Motivation: 现代异构计算节点中GPU通信成为瓶颈，而大量CPU核心空闲未被利用

Method: 扩展车道意识减少技术到GPU，并使用多个CPU核心为每个GPU加速通信操作

Result: 在Delta超算机上获得2.45倍速度提升，在NVIDIA和AMD集群通信库中分别获得1.77倍和1.71倍速度提升

Conclusion: 利用异构计算节点中空闲的CPU资源可以显著提升GPU集群通信性能

Abstract: Large inter-GPU all-reduce operations, prevalent throughout deep learning,
are bottlenecked by communication costs. Emerging heterogeneous architectures
are comprised of complex nodes, often containing $4$ GPUs and dozens to
hundreds of CPU cores per node. Parallel applications are typically accelerated
on the available GPUs, using only a single CPU core per GPU while the remaining
cores sit idle. This paper presents novel optimizations to large GPU-aware
all-reduce operations, extending lane-aware reductions to the GPUs, and notably
using multiple CPU cores per GPU to accelerate these operations. These
multi-CPU-accelerated GPU-aware lane all-reduces yield speedup of up to $2.45$x
for large MPI all-reduces across the NVIDIA A100 GPUs of NCSA's Delta
supercomputer. Finally, the approach is extended to NVIDIA's and AMD's
collective communication libraries, achieving speedup of up to $1.77$x and
$1.71$x, respectively, across $2$ state-of-the-art supercomputers.

</details>


### [7] [DDoS Attacks in Cloud Computing: Detection and Prevention](https://arxiv.org/abs/2508.13522)
*Zain Ahmad,Musab Ahmad,Bilal Ahmad*

Main category: cs.DC

TL;DR: 该研究给出了DDoS攻击的全面概述，包括不同类型攻击的特征、检测技术和防范方法，为组织提供了安全防护指南


<details>
  <summary>Details</summary>
Motivation: DDoS攻击作为常见且危害极大的网络安全威胁，其复杂性和频率在近年显著增加，需要有效的检测和减少方案

Method: 分析了不同类型的DDoS攻击（浓度型、协议层、应用层），评估现有检测技术（包过滤、入侵检测系统、机器学习方法）和防范技术（防火墙、速率限制、CPP、ELD机制）

Result: 研究评估了各种方法的效果性和适用性，为不同类型攻击和环境提供了适合的解决方案

Conclusion: 该研究提供了对DDoS攻击及其防御技术的全面理解，为组织和个人提升网络安全防护能力提供了有价值的指南和见解

Abstract: DDoS attacks are one of the most prevalent and harmful cybersecurity threats
faced by organizations and individuals today. In recent years, the complexity
and frequency of DDoS attacks have increased significantly, making it
challenging to detect and mitigate them effectively. The study analyzes various
types of DDoS attacks, including volumetric, protocol, and application layer
attacks, and discusses the characteristics, impact, and potential targets of
each type. It also examines the existing techniques used for DDoS attack
detection, such as packet filtering, intrusion detection systems, and machine
learning-based approaches, and their strengths and limitations. Moreover, the
study explores the prevention techniques employed to mitigate DDoS attacks,
such as firewalls, rate limiting , CPP and ELD mechanism. It evaluates the
effectiveness of each approach and its suitability for different types of
attacks and environments. In conclusion, this study provides a comprehensive
overview of the different types of DDoS attacks, their detection, and
prevention techniques. It aims to provide insights and guidelines for
organizations and individuals to enhance their cybersecurity posture and
protect against DDoS attacks.

</details>


### [8] [LAMMPS-KOKKOS: Performance Portable Molecular Dynamics Across Exascale Architectures](https://arxiv.org/abs/2508.13523)
*Anders Johansson,Evan Weinberg,Christian R. Trott,Megan J. McCarthy,Stan G. Moore*

Main category: cs.DC

TL;DR: LAMMPS分子动力学软件通过集成Kokkos性能可移植库，在现代异构计算环境中实现了跨GPU平台的高性能计算，并在美国三大百亿亿次超级计算机上展示了强大的扩展性能。


<details>
  <summary>Details</summary>
Motivation: 随着LAMMPS发展成为世界级的分子动力学代码，需要适应现代异构计算环境，在不同GPU硬件平台上实现性能可移植性。

Method: 将Kokkos性能可移植库集成到现有的C++代码中，研究简单对势、多体反应势和机器学习力场三种原子间势的性能可移植性。

Result: 在不同厂商和世代的GPU上获得了良好的性能表现，分析了浮点运算吞吐量、内存带宽、缓存能力和线程原子操作性能，并在美国三大百亿亿次超级计算机上展示了强大的扩展性能。

Conclusion: 通过Kokkos库的集成，LAMMPS成功实现了在现代异构计算平台上的高性能可移植计算，为大规模分子动力学模拟提供了有效的解决方案。

Abstract: Since its inception in 1995, LAMMPS has grown to be a world-class molecular
dynamics code, with thousands of users, over one million lines of code, and
multi-scale simulation capabilities. We discuss how LAMMPS has adapted to the
modern heterogeneous computing landscape by integrating the Kokkos performance
portability library into the existing C++ code. We investigate performance
portability of simple pairwise, many-body reactive, and machine-learned
force-field interatomic potentials. We present results on GPUs across different
vendors and generations, and analyze performance trends, probing FLOPS
throughput, memory bandwidths, cache capabilities, and thread-atomic operation
performance. Finally, we demonstrate strong scaling on all current US exascale
machines -- OLCF Frontier, and ALCF Aurora, and NNSA El Capitan -- for the
three potentials.

</details>


### [9] [LUNDIsim: model meshes for flow simulation and scientific data compression benchmarks](https://arxiv.org/abs/2508.13636)
*Laurent Duval,Frédéric Payan,Christophe Preux,Lauriane Bouard*

Main category: cs.DC

TL;DR: 这篇论文提出了LUNDIsim数据集，为地质学和油气工程领域的数据压缩算法提供了标准化的性能评估平台


<details>
  <summary>Details</summary>
Motivation: 科学数据量快速增长导致计算性、解释性和可持续性挑战，特别是在地球科学和气候研究中。需要标准化的数据集来评估数据压缩算法的性能

Method: 发布LUNDIsim数据集，包含四种不同的地下环境模型，具有多分辨率的六面体缩放表示、孔隙度/渗透率数据集，以及两相流水动力学模拟的完整设置

Result: 提供了一个遵循FAIR原则的开放数据集，包含最小可复现示例(MRE)附属数据，适用于数据缩放、网格压缩算法评估、可视化和机器学习等多种地质学工作流

Conclusion: LUNDIsim数据集为地质学和油气工程领域的数据压缩算法提供了一个标准化的性能评估平台，有助于解决大规模数据处理的效率、净度、多样性、可解释性和可用性挑战

Abstract: The volume of scientific data produced for and by numerical simulation
workflows is increasing at an incredible rate. This raises concerns either in
computability, interpretability, and sustainability. This is especially
noticeable in earth science (geology, meteorology, oceanography, and
astronomy), notably with climate studies.
  We highlight five main evaluation issues: efficiency, discrepancy, diversity,
interpretability, availability.
  Among remedies, lossless and lossy compression techniques are becoming
popular to better manage dataset volumes. Performance assessment -- with
comparative benchmarks -- require open datasets shared under FAIR principles
(Findable, Accessible, Interoperable, Reusable), with MRE (Minimal Reproducible
Example) ancillary data for reuse. We share LUNDIsim, an exemplary faulted
geological mesh. It is inspired by SPE10 comparative Challenge. Enhanced by
porosity/permeability datasets, this dataset proposes four distinct subsurface
environments. They were primarily designed for flow simulation in porous media.
Several consistent resolutions (with HexaShrink multiscale representations) are
proposed for each model. We also provide a set of reservoir features for
reproducing typical two-phase flow simulations on all LUNDIsim models in a
reservoir engineering context. This dataset is chiefly meant for benchmarking
and evaluating data size reduction (upscaling) or genuine composite mesh
compression algorithms. It is also suitable for other advanced mesh processing
workflows in geology and reservoir engineering, from visualization to machine
learning.
  LUNDIsim meshes are available at https://doi.org/10.5281/zenodo.14641958

</details>


### [10] [Estimating CO$_2$ emissions of distributed applications and platforms with SimGrid/Batsim](https://arxiv.org/abs/2508.13693)
*Gabriella Saraiva,Miguel Vasconcelos,Sarita Mazzini Bruschi,Danilo Carastan-Santos,Daniel Cordeiro*

Main category: cs.DC

TL;DR: 开发了一个用于Batsim模拟器的碳足迹插件，通过计算CO2排放来评估数据中心任务和资源管理策略的环境影响。


<details>
  <summary>Details</summary>
Motivation: 全面评估数据中心任务和资源管理策略相关的环境影响，特别是碳足迹方面。

Method: 在SimGrid框架内开发插件，基于模拟平台的能耗和模拟机器的碳强度因子计算碳排放，并与Batsim集成保持兼容性。

Result: 成功实现了碳足迹计算功能，使研究人员能够评估调度策略的碳效率。

Conclusion: 该插件扩展了Batsim模拟器的功能，为数据中心环境影响的量化评估提供了有效工具。

Abstract: This work presents a carbon footprint plugin designed to extend the
capabilities of the Batsim simulator by allowing the calculation of CO$_2$
emissions during simulation runs. The goal is to comprehensively assess the
environmental impact associated with task and resource management strategies in
data centers. The plugin is developed within SimGrid -- the underlying
simulation framework of Batsim -- and computes carbon emissions based on the
simulated platform's energy consumption and carbon intensity factor of the
simulated machines. Once implemented, it is integrated into Batsim, ensuring
compatibility with existing simulation workflows and enabling researchers to
assess the carbon efficiency of their scheduling strategies.

</details>


### [11] [CaPGNN: Optimizing Parallel Graph Neural Network Training with Joint Caching and Resource-Aware Graph Partitioning](https://arxiv.org/abs/2508.13716)
*Xianfeng Song,Yi Zou,Zheng Shi*

Main category: cs.DC

TL;DR: CaPGNN是一个用于多GPU单服务器环境下高效并行全批次GNN训练的新框架，通过自适应缓存和资源感知图分区算法，显著减少通信开销并平衡计算负载


<details>
  <summary>Details</summary>
Motivation: 传统全批次GNN训练在分布式环境中面临高通信开销和负载不均衡的问题，限制了其可扩展性

Method: 提出联合自适应缓存算法利用CPU和GPU内存减少顶点特征的重复传输，以及资源感知图分区算法根据GPU异构能力动态调整子图大小

Result: 在大规模基准数据集上，CaPGNN将通信成本降低高达96%，训练速度提升高达12.7倍

Conclusion: 自适应缓存和资源感知分区技术能够促进全批次GNN训练在分布式计算环境中的可扩展、高效和实际部署

Abstract: Graph Neural Networks (GNNs) have shown remarkable capabilities in processing
graph-structured data prevalent in various real-world applications. However,
the scalability of full-batch GNN training becomes severely limited by high
communication overhead and load imbalance in distributed environments. In this
paper, we present CaPGNN, a novel framework for efficient parallel full-batch
GNN training on single-server with multi-GPU, designed specifically to reduce
redundant inter-GPU communication and balance computational workloads. We
propose a joint adaptive caching algorithm that leverages both CPU and GPU
memory to significantly reduce the repetitive transmission of vertex features
across partitions. Additionally, we introduce a resource-aware graph
partitioning algorithm that adjusts subgraph sizes dynamically according to the
heterogeneous computational and communication capacities of GPUs. Extensive
experiments on large-scale benchmark datasets demonstrate that CaPGNN
effectively reduces communication costs by up to 96% and accelerates GNN
training by up to 12.7 times compared to state-of-the-art approaches. Our
results highlight the potential of adaptive caching and resource-aware
partitioning to facilitate scalable, efficient, and practical deployment of
full-batch GNN training in distributed computing environments.

</details>


### [12] [Is RISC-V ready for High Performance Computing? An evaluation of the Sophon SG2044](https://arxiv.org/abs/2508.13840)
*Nick Brown*

Main category: cs.DC

TL;DR: SG2044是SOPHGO新一代64核高性能RISC-V处理器，相比前代SG2042在HPC性能上有显著提升，特别是在高核心数场景下性能提升达4.91倍，主要得益于RVV v1.0支持和内存子系统改进。


<details>
  <summary>Details</summary>
Motivation: RISC-V在嵌入式领域取得成功，但在高性能计算(HPC)领域尚未普及，需要评估新一代RISC-V处理器SG2044在HPC工作负载下的性能表现。

Method: 对SG2044进行首个HPC性能研究，与SG2042和其他架构进行对比测试，重点关注高核心数场景下的性能表现。

Result: SG2044在高核心数(64核)场景下性能比SG2042提升4.91倍，显著缩小了与其他架构的性能差距，特别是在计算密集型工作负载方面。

Conclusion: SG2044通过RVV v1.0支持和内存子系统升级，显著提升了HPC性能，使RISC-V在高性能计算领域更具竞争力。

Abstract: The pace of RISC-V adoption continues to grow rapidly, yet for the successes
enjoyed in areas such as embedded computing, RISC-V is yet to gain ubiquity in
High Performance Computing (HPC). The Sophon SG2044 is SOPHGO's next generation
64-core high performance CPU that has been designed for workstation and server
grade workloads. Building upon the SG2042, subsystems that were a bottleneck in
the previous generation have been upgraded.
  In this paper we undertake the first performance study of the SG2044 for HPC.
Comparing against the SG2042 and other architectures, we find that the SG2044
is most advantageous when running at higher core counts, delivering up to 4.91
greater performance than the SG2042 over 64-cores. Two of the most important
upgrades in the SG2044 are support for RVV v1.0 and an enhanced memory
subsystem. This results in the SG2044 significantly closing the performance gap
with other architectures, especially for compute-bound workloads.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [13] [EvoVerilog: Large Langugage Model Assisted Evolution of Verilog Code](https://arxiv.org/abs/2508.13156)
*Ping Guo,Yiting Wang,Wanghao Ye,Yexiao He,Ziyao Wang,Xiaopeng Dai,Ang Li,Qingfu Zhang*

Main category: cs.AR

TL;DR: EvoVerilog是一个结合大语言模型和进化算法的新框架，用于自动生成和优化Verilog代码，在硬件设计自动化方面取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM方法主要依赖人工干预和精选数据集进行微调，限制了在自动化设计工作流程中的可扩展性。虽然最近出现了迭代搜索技术，但它们往往无法探索多样化的设计解决方案，可能不如简单的重复提示方法。

Method: EvoVerilog采用多目标、基于种群的搜索策略，结合LLM的推理能力和进化算法，自动生成和精炼Verilog代码，无需人工干预即可探索广泛的设计可能性。

Result: 在VerilogEval-Machine和VerilogEval-Human基准测试中，EvoVerilog分别达到了89.1和80.2的pass@10分数，表现出最先进的性能。该框架还展示了同时生成多种功能Verilog代码并优化资源利用的能力。

Conclusion: EvoVerilog通过结合LLM和进化算法，有效解决了现有方法在自动硬件设计中的局限性，实现了高性能和多样化的设计探索，为自动化硬件设计工作流程提供了有前景的解决方案。

Abstract: Large Language Models (LLMs) have demonstrated great potential in automating
the generation of Verilog hardware description language code for hardware
design. This automation is critical to reducing human effort in the complex and
error-prone process of hardware design.
  However, existing approaches predominantly rely on human intervention and
fine-tuning using curated datasets, limiting their scalability in automated
design workflows.
  Although recent iterative search techniques have emerged, they often fail to
explore diverse design solutions and may underperform simpler approaches such
as repeated prompting.
  To address these limitations, we introduce EvoVerilog, a novel framework that
combines the reasoning capabilities of LLMs with evolutionary algorithms to
automatically generate and refine Verilog code.
  EvoVerilog utilizes a multiobjective, population-based search strategy to
explore a wide range of design possibilities without requiring human
intervention.
  Extensive experiments demonstrate that EvoVerilog achieves state-of-the-art
performance, with pass@10 scores of 89.1 and 80.2 on the VerilogEval-Machine
and VerilogEval-Human benchmarks, respectively. Furthermore, the framework
showcases its ability to explore diverse designs by simultaneously generating a
variety of functional Verilog code while optimizing resource utilization.

</details>


### [14] [Image2Net: Datasets, Benchmark and Hybrid Framework to Convert Analog Circuit Diagrams into Netlists](https://arxiv.org/abs/2508.13157)
*Haohang Xu,Chengjie Liu,Qihang Wang,Wenhao Huang,Yongjian Xu,Weiyu Chen,Anlan Peng,Zhijun Li,Bo Li,Lei Qi,Jun Yang,Yuan Du,Li Du*

Main category: cs.AR

TL;DR: 该论文提出了Image2Net混合框架，用于将图像电路图转换为文本网表，以支持LLM在模拟集成电路设计中的应用，并构建了包含丰富样式的新数据集。


<details>
  <summary>Details</summary>
Motivation: 现有模拟IC大多以图像电路图形式存在，而非文本网表，这限制了LLM在模拟IC设计中的进一步发展。需要将电路图转换为网表来丰富LLM的知识。

Method: 构建了包含丰富样式电路图的新数据集，提出了Image2Net混合框架进行电路图到网表的转换，并引入了网表编辑距离(NED)来精确评估转换结果。

Result: Image2Net实现了80.77%的成功率，比之前工作提高了34.62%-45.19%；平均NED为0.116，比现有最佳方法降低了62.1%-69.6%。

Conclusion: Image2Net框架在电路图到网表转换任务上表现出色，为LLM在模拟IC设计中的应用提供了有效支持，显著优于现有方法。

Abstract: Large Language Model (LLM) exhibits great potential in designing of analog
integrated circuits (IC) because of its excellence in abstraction and
generalization for knowledge. However, further development of LLM-based analog
ICs heavily relies on textual description of analog ICs, while existing analog
ICs are mostly illustrated in image-based circuit diagrams rather than
text-based netlists. Converting circuit diagrams to netlists help LLMs to
enrich the knowledge of analog IC. Nevertheless, previously proposed conversion
frameworks face challenges in further application because of limited support of
image styles and circuit elements. Up to now, it still remains a challenging
task to effectively convert complex circuit diagrams into netlists. To this
end, this paper constructs and opensources a new dataset with rich styles of
circuit diagrams as well as balanced distribution of simple and complex analog
ICs. And a hybrid framework, named Image2Net, is proposed for practical
conversion from circuit diagrams to netlists. The netlist edit distance (NED)
is also introduced to precisely assess the difference between the converted
netlists and ground truth. Based on our benchmark, Image2Net achieves 80.77\%
successful rate, which is 34.62\%-45.19\% higher than previous works.
Specifically, the proposed work shows 0.116 averaged NED, which is
62.1\%-69.6\% lower than state-of-the-arts.

</details>


### [15] [Fine Grain 3D Integration for Microarchitecture Design Through Cube Packing Exploration](https://arxiv.org/abs/2508.13158)
*Yongxiang Liu,Yuchun Ma,Eren Kurshan,Glenn Reinman,Jason Cong*

Main category: cs.AR

TL;DR: 这篇论文提出了一种新的3D IC设计方法，允许逻辑块跨越多个硅层，通过立方包装式引擎优化性能、面积和温度，实现了36%性能提升和30%功耗降低。


<details>
  <summary>Details</summary>
Motivation: 传统3D IC研究仅关注堆叠2D硅层，连接减少限于块间延迟。细粒度3D集成虽可提升性能和功耗，但缺少对应的建模和工具支持。

Method: 开发了一个立方包装式引擎，能够同时优化物理和架构设计，采用热感知布局和热通孔插入技术控制温度。

Result: 实验结果显示相比2D设计性能提升36%（BIPS），相比单层块3D设计提升14%，多层块能够降低30%的功耗散失，并通过热感知布局控制最高温度。

Conclusion: 该方法通过允许逻辑块跨越多个硅层，实现了更高效的3D集成，在性能、面积和热管理方面都取得了显著改善。

Abstract: Most previous 3D IC research focused on stacking traditional 2D silicon
layers, so the interconnect reduction is limited to inter-block delays. In this
paper, we propose techniques that enable efficient exploration of the 3D design
space where each logical block can span more than one silicon layers. Although
further power and performance improvement is achievable through fine grain 3D
integration, the necessary modeling and tool infrastructure has been mostly
missing. We develop a cube packing engine which can simultaneously optimize
physical and architectural design for effective utilization of 3D in terms of
performance, area and temperature. Our experimental results using a design
driver show 36% performance improvement (in BIPS) over 2D and 14% over 3D with
single layer blocks. Additionally multi-layer blocks can provide up to 30%
reduction in power dissipation compared to the single-layer alternatives. Peak
temperature of the design is kept within limits as a result of thermal-aware
floorplanning and thermal via insertion techniques.

</details>


### [16] [Accelerating Transistor-Level Simulation of Integrated Circuits via Equivalence of RC Long-Chain Structures](https://arxiv.org/abs/2508.13159)
*Ruibai Tang,Wenlai Zhao*

Main category: cs.AR

TL;DR: 这篇论文提出了三种新的约简方法，专门用于处理RC长链结构的传续器级模拟优化，在仅享受0.7%相对误差的情况下实现了平均8.8%的性能提升。


<details>
  <summary>Details</summary>
Motivation: RC长链结构在集成电路中占平坈6.34%（最高达12%）的节点数量，而传续器级模拟计算成本高很昂贵，需要有效的约简方法来提高模拟效率。

Method: 提出了三种新题的约简方法，特别为不同时间常数规模的RC长链结构进行优化设计。

Result: 在包含ALU、加法器、乘法器、SEC/DED检查器和中断控制器等多种功能模块的基准电路上，实验结果显示平坈8.8%（最高达22%）的性能提升，误差仅为0.7%。

Conclusion: 该方法能够在保持高模拟精度的前提下，显著提高RC长链结构的传续器级模拟效率，对于大规模集成电路的物理验证具有重要价值。

Abstract: Transistor-level simulation plays a vital role in validating the physical
correctness of integrated circuits. However, such simulations are
computationally expensive. This paper proposes three novel reduction methods
specifically tailored to RC long-chain structures with different scales of time
constant. Such structures account for an average of 6.34\% (up to 12\%) of the
total nodes in the benchmark circuits. Experimental results demonstrate that
our methods yields an average performance improvement of 8.8\% (up to 22\%) on
simulating benchmark circuits which include a variety of functional modules
such as ALUs, adders, multipliers, SEC/DED checkers, and interrupt controllers,
with only 0.7\% relative error.

</details>


### [17] [Through Silicon Via Aware Design Planning for Thermally Efficient 3-D Integrated Circuits](https://arxiv.org/abs/2508.13160)
*Yibo Chen,Eren Kurshan,Dave Motschman,Charles Johnson,Yuan Xie*

Main category: cs.AR

TL;DR: 通过热敏感的TSV农场位置优化技术，减少3D集成电路中密集信号总线TSV结构造成的横向热阻塞效应


<details>
  <summary>Details</summary>
Motivation: 随着TSV尺寸缩小到微米级别，密集的TSV农场会在薄化硅基版中造成明显的横向热阻塞，加剧局部热点问题

Method: 提出了一种热敏感的通孔农场位置技术，专门用于最小化由密集信号总线TSV结构引起的横向热阻塞效应

Result: 该技术能够有效减少3D集成电路中的热阻塞问题，改善热管理性能

Conclusion: 热敏感的TSV农场位置是解决3D IC热管理挑战的关键技术，对于高密度集成的热效果优化至关重要

Abstract: 3-D integrated circuits (3-D ICs) offer performance advantages due to their
increased bandwidth and reduced wire-length enabled by through-silicon-via
structures (TSVs). Traditionally TSVs have been considered to improve the
thermal conductivity in the vertical direction. However, the lateral thermal
blockage effect becomes increasingly important for TSV via farms (a cluster of
TSV vias used for signal bus connections between layers) because the TSV size
and pitch continue to scale in {\mu}m range and the metal to insulator ratio
becomes smaller. Consequently, dense TSV farms can create lateral thermal
blockages in thinned silicon substrate and exacerbate the local hotspots. In
this paper, we propose a thermal-aware via farm placement technique for 3-D ICs
to minimize lateral heat blockages caused by dense signal bus TSV structures.

</details>


### [18] [Piano: A Multi-Constraint Pin Assignment-Aware Floorplanner](https://arxiv.org/abs/2508.13161)
*Zhexuan Xu,Kexin Zhou,Jie Wang,Zijie Geng,Siyuan Xu,Shixiong Kai,Mingxuan Yuan,Feng Wu*

Main category: cs.AR

TL;DR: Piano是一个同时优化模块布局和引脚分配的VLSI布图规划框架，通过图基方法在多种约束下显著改善布线性能


<details>
  <summary>Details</summary>
Motivation: 传统布图规划器在现代约束条件下往往忽略引脚分配，这影响了后续详细布局和布线的性能

Method: 构建基于模块几何关系和网表连接的图，迭代搜索最短路径确定引脚分配，采用空白空间移除策略和三个局部优化器

Result: 平均减少6.81% HPWL、13.39%馈通线长、16.36%馈通模块数量和21.21%未放置引脚，同时保持零空白空间

Conclusion: Piano框架有效解决了现代VLSI布图规划中的多约束问题，显著提升了布局质量

Abstract: Floorplanning is a critical step in VLSI physical design, increasingly
complicated by modern constraints such as fixed-outline requirements,
whitespace removal, and the presence of pre-placed modules. In addition, the
assignment of pins on module boundaries significantly impacts the performance
of subsequent stages, including detailed placement and routing. However,
traditional floorplanners often overlook pin assignment with modern constraints
during the floorplanning stage. In this work, we introduce Piano, a
floorplanning framework that simultaneously optimizes module placement and pin
assignment under multiple constraints. Specifically, we construct a graph based
on the geometric relationships among modules and their netlist connections,
then iteratively search for shortest paths to determine pin assignments. This
graph-based method also enables accurate evaluation of feedthrough and unplaced
pins, thereby guiding overall layout quality. To further improve the design, we
adopt a whitespace removal strategy and employ three local optimizers to
enhance layout metrics under multi-constraint scenarios. Experimental results
on widely used benchmark circuits demonstrate that Piano achieves an average
6.81% reduction in HPWL, a 13.39% decrease in feedthrough wirelength, a 16.36%
reduction in the number of feedthrough modules, and a 21.21% drop in unplaced
pins, while maintaining zero whitespace.

</details>


### [19] [FedChip: Federated LLM for Artificial Intelligence Accelerator Chip Design](https://arxiv.org/abs/2508.13162)
*Mahmoud Nazzal,Khoa Nguyen,Deepak Vungarala,Ramtin Zand,Shaahin Angizi,Hai Phan,Abdallah Khreishah*

Main category: cs.AR

TL;DR: FedChip是一个联邦学习框架，允许多个芯片设计方在保护专有数据隐私的前提下，协作微调共享LLM用于自动化硬件设计生成，设计质量提升77%以上


<details>
  <summary>Details</summary>
Motivation: 解决AI硬件设计中LLM应用面临的数据隐私问题和领域专业知识缺乏的挑战，实现多方协作的芯片设计自动化

Method: 提出FedChip联邦微调方法，创建APTPU-Gen数据集（3万设计变体），引入Chip@k评估指标来平衡多个质量指标

Result: 实验结果显示FedChip相比高端LLM设计质量提升超过77%，同时保持数据隐私

Conclusion: FedChip为芯片设计自动化提供了有效的隐私保护协作学习方案，显著提升设计质量

Abstract: AI hardware design is advancing rapidly, driven by the promise of design
automation to make chip development faster, more efficient, and more accessible
to a wide range of users. Amongst automation tools, Large Language Models
(LLMs) offer a promising solution by automating and streamlining parts of the
design process. However, their potential is hindered by data privacy concerns
and the lack of domain-specific training. To address this, we introduce
FedChip, a Federated fine-tuning approach that enables multiple Chip design
parties to collaboratively enhance a shared LLM dedicated for automated
hardware design generation while protecting proprietary data. FedChip enables
parties to train the model on proprietary local data and improve the shared
LLM's performance. To exemplify FedChip's deployment, we create and release
APTPU-Gen, a dataset of 30k design variations spanning various performance
metric values such as power, performance, and area (PPA). To encourage the LLM
to generate designs that achieve a balance across multiple quality metrics, we
propose a new design evaluation metric, Chip@k, which statistically evaluates
the quality of generated designs against predefined acceptance criteria.
Experimental results show that FedChip improves design quality by more than 77%
over high-end LLMs while maintaining data privacy

</details>


### [20] [Sustainable AI Training via Hardware-Software Co-Design on NVIDIA, AMD, and Emerging GPU Architectures](https://arxiv.org/abs/2508.13163)
*Yashasvi Makin,Rahul Maliakkal*

Main category: cs.AR

TL;DR: 这篇论文探讨了基于GPU架构的硬件-软件协同设计方法，通过内存优化、核心运算改进等技术来提升AI模型训练的能源效率，降低环境影响。


<details>
  <summary>Details</summary>
Motivation: 大规模深度学习和AI模型训练消耗巨大计算资源和能源，模型复杂度快速增长导致能源消耗指数增长，必须寻找提升计算效率和降低环境影响的技术方案。

Method: 采用硬件-软件协同设计方法，涉及专用张量/矩阵核心评估、高级内存优化技术、创新集成方案，以及混合精度算术、能源感知调度算法、编译器驱动的核心改进等软件层优化。

Result: 通过硬件-软件协同设计实现了显著的能源效率提升，在保持性能的同时降低了AI训练的环境影响，并通过实际案例验证了这些可持续AI训练方法的实际效果。

Conclusion: 硬件-软件协同设计能够在不抑制性能的前提下实现AI训练效率的大幅提升，为构建真正可持续的人工智能系统提供了重要技术路径，同时指出了未来的研究方向。

Abstract: In particular, large-scale deep learning and artificial intelligence model
training uses a lot of computational power and energy, so it poses serious
sustainability issues. The fast rise in model complexity has resulted in
exponential increases in energy consumption, increasing the demand for
techniques maximizing computational efficiency and lowering environmental
impact. This work explores environmentally driven performance optimization
methods especially intended for advanced GPU architectures from NVIDIA, AMD,
and other emerging GPU architectures. Our main focus is on investigating
hardware-software co-design techniques meant to significantly increase
memory-level and kernel-level operations, so improving performance-per-watt
measures. Our thorough research encompasses evaluations of specialized tensor
and matrix cores, advanced memory optimization methods, and creative
integration approaches that taken together result in notable energy efficiency
increases. We also discuss important software-level optimizations that augment
hardware capability including mixed-precision arithmetic, advanced energy-aware
scheduling algorithms, and compiler-driven kernel enhancements. Moreover, we
methodically point out important research gaps and suggest future directions
necessary to create really sustainable artificial intelligence systems. This
paper emphasizes how major increases in training efficiency can be obtained by
co-design of hardware and software, so lowering the environmental impact of
artificial intelligence without compromising performance. To back up our
analysis, we use real-world case studies from top companies like Meta, Google,
Amazon, and others that show how these sustainable AI training methods are used
in the real world.

</details>


### [21] [White-Box Reasoning: Synergizing LLM Strategy and gm/Id Data for Automated Analog Circuit Design](https://arxiv.org/abs/2508.13172)
*Jianqiu Chen,Siqi Li,Xu He*

Main category: cs.AR

TL;DR: 基于gm/Id方法和LLM协同的模拟电路设计框架，实现了比传统方法效率10倍提升的专业级模拟电路设计


<details>
  <summary>Details</summary>
Motivation: 传统模拟电路设计依赖经验且模拟效率低，而直接使用大语言模型只能做出猜测而缺乏工程原理支撑

Method: 采用"协同推理"框架，将LLM的战略思维与gm/Id方法的物理精度相结合，通过gm/Id查找表使LLM成为数据驱动的设计合作伙伴

Result: 在两级运放大器设计中，框架让Gemini模型在5次迭代内满足所有TT角点规格，并扩展到所有PVT角点优化，效率比高级工程师提升10倍

Conclusion: 这项工作验证了通过结合LLM推理和科学电路设计方法来实现真正模拟电路设计自动化的可行路径

Abstract: Analog IC design is a bottleneck due to its reliance on experience and
inefficient simulations, as traditional formulas fail in advanced nodes.
Applying Large Language Models (LLMs) directly to this problem risks mere
"guessing" without engineering principles. We present a "synergistic reasoning"
framework that integrates an LLM's strategic reasoning with the physical
precision of the gm/Id methodology. By empowering the LLM with gm/Id lookup
tables, it becomes a quantitative, data-driven design partner.
  We validated this on a two-stage op-amp, where our framework enabled the
Gemini model to meet all TT corner specs in 5 iterations and extended
optimization to all PVT corners. A crucial ablation study proved gm/Id data is
key for this efficiency and precision; without it, the LLM is slower and
deviates. Compared to a senior engineer's design, our framework achieves
quasi-expert quality with an order-of-magnitude improvement in efficiency. This
work validates a path for true analog design automation by combining LLM
reasoning with scientific circuit design methodologies.

</details>


### [22] [Low-power, Energy-efficient, Cardiologist-level Atrial Fibrillation Detection for Wearable Devices](https://arxiv.org/abs/2508.13181)
*Dominik Loroch,Johannes Feldmann,Vladimir Rybalkin,Norbert Wehn*

Main category: cs.AR

TL;DR: 新型可穿戴心电图监测设备，采用FPGA和深度学习算法，以极低功耗（3.8mW）实现连续3周心戱抑勘检测，准确率达95%，超越心血管专家水平。


<details>
  <summary>Details</summary>
Motivation: 心戱抑勘是常见心律失常，但现有检测技术在大规模普及方面面临挑战，需要更能源效的可穿戴解决方案。

Method: 采用硬件-软件协同设计，基于FPGA的贴片式可穿戴设备，嵌入深度学习AF检测算法，通过硬件感知神经网络架构搜索优化功耗管理。

Result: 设备功耗仅3.8mW，比当前最佳技术低1-3个数量级，可连续工作3周以上，AF检测准确率达95%，超过心血管专家表现。

Conclusion: 该技术为可扩展、可靠和可持续的心戱抑勘监测提供了重要进展，有力推动AF检测技术的普及应用。

Abstract: Atrial fibrillation (AF) is a common arrhythmia and major risk factor for
cardiovascular complications. While commercially available devices and
supporting Artificial Intelligence (AI) algorithms exist for reliable detection
of AF, the scaling of this technology to the amount of people who need this
diagnosis is still a major challenge. This paper presents a novel wearable
device, designed specifically for the early and reliable detection of AF. We
present an FPGA-based patch-style wearable monitor with embedded deep
learning-based AF detection. Operating with 3.8mW system power, which is 1-3
orders of magnitude lower than the state-of-the-art, the device enables
continuous AF detection for over three weeks while achieving 95% accuracy,
surpassing cardiologist-level performance. A key innovation is the combination
of energy-efficient hardware-software co-design and optimized power management
through the application of hardware-aware neural architecture search. This
advancement represents a significant step toward scalable, reliable, and
sustainable AF monitoring.

</details>


### [23] [Accelerating LLM Inference via Dynamic KV Cache Placement in Heterogeneous Memory System](https://arxiv.org/abs/2508.13231)
*Yunhua Fang,Rui Xie,Asad Ul Haq,Linsen Ma,Kaoutar El Maghraoui,Naigang Wang,Meng Wang,Liu Liu,Tong Zhang*

Main category: cs.AR

TL;DR: 这篇论文研究了在异构内存系统中动态调度LLM推理的KV缓存放置问题，通过数学形式化推导理论上限，揭示了运行时优化的巨大潜力空间。


<details>
  <summary>Details</summary>
Motivation: 解决LLM推理中因额外访问KV缓存导致的内存带宽压力，利用现代AI硬件的异构内存系统来提升带宽利用率。

Method: 将KV缓存放置问题数学形式化，推导出理论上限来评估最优调度策略的潜力性能。

Result: 发现现有调度方案距离理论上限还有很大的优化空间，证明了动态KV缓存调度的重要性。

Conclusion: 这是首次对异构内存系统中LLM推理KV缓存动态调度的正式研究，为未来运行时优化策略的设计奠定了理论基础。

Abstract: Large Language Model (LLM) inference is increasingly constrained by memory
bandwidth, with frequent access to the key-value (KV) cache dominating data
movement. While attention sparsity reduces some memory traffic, the relevance
of past tokens varies over time, requiring the full KV cache to remain
accessible and sustaining pressure on both bandwidth and capacity. With
advances in interconnects such as NVLink and LPDDR5X, modern AI hardware now
integrates high-bandwidth memory (HBM) with high-speed off-package DRAM, making
heterogeneous memory systems a practical solution. This work investigates
dynamic KV cache placement across such systems to maximize aggregated bandwidth
utilization under capacity constraints. Rather than proposing a specific
scheduling policy, we formulate the placement problem mathematically and derive
a theoretical upper bound, revealing substantial headroom for runtime
optimization. To our knowledge, this is the first formal treatment of dynamic
KV cache scheduling in heterogeneous memory systems for LLM inference.

</details>


### [24] [Sub-Millisecond Event-Based Eye Tracking on a Resource-Constrained Microcontroller](https://arxiv.org/abs/2508.13244)
*Marco Giordano,Pietro Bonazzi,Luca Benini,Michele Magno*

Main category: cs.AR

TL;DR: 基于微控制器的事件驱动眼动跟踪系统，具备实时、低延迟、低功耗特性，通过优化CNN模型在边缘设备上实现高效眼动跟踪。


<details>
  <summary>Details</summary>
Motivation: 解决嵌入式系统中实时眼动跟踪的低延迟、低功耗挑战，适用于智能眼镜等可穿戴设备。

Method: 采用DVXplorer Micro动态视觉传感器，统于STM32N6微控制器，设计了专门优化的卷积神经网络模型处理事件数据，利用AI硬件加速器实现实时推理。

Result: 在Ini-30数据集上实现平均学生预测误差5.99像素，中位数误差5.73像素，端到端推理延迟仅385微秒，每周期执行52次MAC操作，消耗能量仅155微焦耳。

Conclusion: 该系统成功实现了高效能嵌入式眼动跟踪方案，为可穿戴设备提供了技术支撑，具有应用前景。

Abstract: This paper presents a novel event-based eye-tracking system deployed on a
resource-constrained microcontroller, addressing the challenges of real-time,
low-latency, and low-power performance in embedded systems. The system
leverages a Dynamic Vision Sensor (DVS), specifically the DVXplorer Micro, with
an average temporal resolution of 200 {\mu}s, to capture rapid eye movements
with extremely low latency. The system is implemented on a novel low-power and
high-performance microcontroller from STMicroelectronics, the STM32N6. The
microcontroller features an 800 MHz Arm Cortex-M55 core and AI hardware
accelerator, the Neural-ART Accelerator, enabling real-time inference with
milliwatt power consumption. The paper propose a hardware-aware and
sensor-aware compact Convolutional Neuron Network (CNN) optimized for
event-based data, deployed at the edge, achieving a mean pupil prediction error
of 5.99 pixels and a median error of 5.73 pixels on the Ini-30 dataset. The
system achieves an end-to-end inference latency of just 385 {\mu}s and a neural
network throughput of 52 Multiply and Accumulate (MAC) operations per cycle
while consuming just 155 {\mu}J of energy. This approach allows for the
development of a fully embedded, energy-efficient eye-tracking solution
suitable for applications such as smart glasses and wearable devices.

</details>


### [25] [ViTAD: Timing Violation-Aware Debugging of RTL Code using Large Language Models](https://arxiv.org/abs/2508.13257)
*Wenhao Lv,Yingjie Xia,Xiyuan Chen,Li Kuang*

Main category: cs.AR

TL;DR: ViTAD是一种自动化时序违规修复方法，通过构建信号时序依赖图和使用LLM分析违规根因，结合领域知识库生成定制修复方案，在真实数据集上达到73.68%的成功率。


<details>
  <summary>Details</summary>
Motivation: 现代VLSI电路设计对时序要求极高，传统时序优化依赖人工经验，需要自动化方法来提高效率和准确性。

Method: 解析Verilog代码和时序报告构建STDG图，使用LLM分析违规路径和根因，从领域知识库检索相关知识生成定制修复策略。

Result: 在包含54个违规案例的数据集上，ViTAD达到73.68%的修复成功率，比仅使用LLM的基线方法提高19.30%。

Conclusion: ViTAD方法通过结合图分析和LLM技术，有效实现了时序违规的自动化分析和修复，显著提升了VLSI设计流程的效率。

Abstract: In modern Very Large Scale Integrated (VLSI) circuit design flow, the
Register-Transfer Level (RTL) stage presents a critical opportunity for timing
optimization. Addressing timing violations at this early stage is essential, as
modern systems demand higher speeds, where even minor timing violations can
lead to functional failures or system crashes. However, traditional timing
optimization heavily relies on manual expertise, requiring engineers to
iteratively analyze timing reports and debug. To automate this process, this
paper proposes ViTAD, a method that efficiently analyzes the root causes of
timing violations and dynamically generates targeted repair strategies.
Specifically, we first parse Verilog code and timing reports to construct a
Signal Timing Dependency Graph (STDG). Based on the STDG, we perform violation
path analysis and use large language models (LLMs) to infer the root causes of
violations. Finally, by analyzing the causes of violations, we selectively
retrieve relevant debugging knowledge from a domain-specific knowledge base to
generate customized repair solutions. To evaluate the effectiveness of our
method, we construct a timing violation dataset based on real-world open-source
projects. This dataset contains 54 cases of violations. Experimental results
show that our method achieves a 73.68% success rate in repairing timing
violations, while the baseline using only LLM is 54.38%. Our method improves
the success rate by 19.30%.

</details>
