{"id": "2507.16109", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2507.16109", "abs": "https://arxiv.org/abs/2507.16109", "authors": ["Zihao Chen", "Mohammad Goudarzi", "Adel Nadjaran Toosi"], "title": "Resilience Evaluation of Kubernetes in Cloud-Edge Environments via Failure Injection", "comment": null, "summary": "Kubernetes has emerged as an essential platform for deploying containerised\napplications across cloud and edge infrastructures. As Kubernetes gains\nincreasing adoption for mission-critical microservices, evaluating system\nresilience under realistic fault conditions becomes crucial. However,\nsystematic resilience assessments of Kubernetes in hybrid cloud-edge\nenvironments are currently limited in research. To address this gap, a novel\nresilience evaluation framework integrates mainstream fault injection tools\nwith automated workload generation for comprehensive cloud-edge Kubernetes\ntesting. Multiple fault injection platforms, including Chaos Mesh, Gremlin, and\nChaosBlade are combined with realistic traffic simulation tools, enabling\nautomated orchestration of complex failure scenarios. Through this framework,\ncomprehensive experiments are conducted that systematically target node-level,\npod-level, and network failures across cloud and cloud-edge environments. The\nfirst comprehensive resilience dataset for hybrid cloud-edge Kubernetes\ndeployments is created, comprising over 30 GB of performance data from 11,965\nfault injection scenarios including response times, failure rates, and error\npatterns. Analysis reveals that cloud-edge deployments demonstrate 80% superior\nresponse stability under network delay and partition conditions, while cloud\ndeployments exhibit 47% better resilience under bandwidth limitations,\nproviding quantitative guidance for architectural decision-making in cloud-edge\ndeployments."}
{"id": "2507.16165", "categories": ["cs.DC", "cs.GR", "gr-qc"], "pdf": "https://arxiv.org/pdf/2507.16165", "abs": "https://arxiv.org/abs/2507.16165", "authors": ["Liam Naddell", "Marcelo Ponce"], "title": "Parallel Ray Tracing of Black Hole Images Using the Schwarzschild Metric", "comment": "Published and presented at PEARC '25: Practice and Experience in\n  Advanced Research Computing 2025: \"The Power of Collaboration\"", "summary": "Rendering images of black holes by utilizing ray tracing techniques is a\ncommon methodology employed in many aspects of scientific and astrophysical\nvisualizations. Similarly, general ray tracing techniques are widely used in\nareas related to computer graphics. In this work we describe the implementation\nof a parallel open-source program that can ray trace images in the presence of\na black hole geometry. We do this by combining a couple of different techniques\nusually present in parallel scientific computing, such as, mathematical\napproximations, utilization of scientific libraries, shared-memory and\ndistributed-memory parallelism."}
{"id": "2507.16350", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2507.16350", "abs": "https://arxiv.org/abs/2507.16350", "authors": ["Serdar Metin"], "title": "Autonomous Dominant Resource Fairness for Blockchain Ecosystems", "comment": "10 pages, 3 figures", "summary": "Blockchain systems have been a part of mainstream academic research, and a\nhot topic at that. It has spread to almost every subfield in the computer\nscience literature, as well as economics and finance. Especially in a world\nwhere digital trust is much sought for, blockchains offer a rich variety of\ndesired properties, such as immutability, public auditing, decentralised record\nkeeping, among others. Not only has it been a research topic of its own, the\nintegration of blockchains into other systems has been proposed as solutions in\nmany areas, ranging from grid computing, cloud and fog computing, to internet\nof things, self driving vehicles , and smart cities. In many cases the primary\nfunction attributed to blockchains in these contexts is resource management.\nAlthough much attention is paid to this topic, the focus is on single resource\nallocation scenarios. Even the cases where multiple resource types are to be\nallocated, are treated as single resource type scenarios, and problems are\nformulated as allocating standardised bundles consisting of a fixed amount of\neach of them, such as virtual machines. The present study addresses the problem\nof allocating multiple resource types among tasks with heterogeneous resource\ndemands with a smart contract adaptation of Precomputed Dominant Resource\nFairness; an algorithm that approximates Dominant Resource Fairness, without\nloop iterations, which makes it preferable in the blockchain context because of\nthe block gas limit. We present the resulting algorithm, Autonomous Dominant\nResource Fairness, along with the empirical data collected from the tests run\non the algorithm. The results show that Autonomous Dominant Resource Fairness\nis a gas-cost efficient algorithm, which can be used to manage hundreds of\nresource types for unlimited number of users."}
{"id": "2507.16668", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2507.16668", "abs": "https://arxiv.org/abs/2507.16668", "authors": ["Somayeh Sobati-M"], "title": "FOGNITE: Federated Learning-Enhanced Fog-Cloud Architecture", "comment": null, "summary": "Modern smart grids demand fast, intelligent, and energy-aware computing at\nthe edge to manage real time fluctuations and ensure reliable operation. This\npaper introduces FOGNITE Fog-based Grid In intelligence with Neural Integration\nand Twin based Execution a next-generation fog cloud framework designed to\nenhance autonomy, resilience, and efficiency in distributed energy systems.\nFOGNITE combines three core components: federated learning, reinforcement\nlearning, and digital twin validation. Each fog node trains a local CNN LSTM\nmodel on private energy consumption data, enabling predictive intelligence\nwhile preserving data privacy through federated aggregation. A reinforcement\nlearning agent dynamically schedules tasks based on current system load and\nenergy conditions, optimizing for performance under uncertainty.\n  To prevent unsafe or inefficient decisions, a hierarchical digital twin layer\nsimulates potential actions before deployment, significantly reducing execution\nerrors and energy waste. We evaluate FOGNITE on a real world testbed of\nRaspberry Pi devices, showing up to a 93.7% improvement in load balancing\naccuracy and a 63.2% reduction in energy waste compared to conventional\narchitectures. By shifting smart grid control from reactive correction to\nproactive optimization, FOGNITE represents a step toward more intelligent,\nadaptive, and sustainable energy infrastructures"}
{"id": "2507.16051", "categories": ["cs.PL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2507.16051", "abs": "https://arxiv.org/abs/2507.16051", "authors": ["Juan Altmayer Pizzorno", "Emery D. Berger"], "title": "RightTyper: Effective and Efficient Type Annotation for Python", "comment": null, "summary": "Python type annotations bring the benefits of static type checking to the\nlanguage. However, manually writing annotations can be time-consuming and\ntedious. The result is that most real-world Python code remains largely\nuntyped. Past approaches to annotating types in Python code fall short in a\nnumber of ways. Static approaches struggle with dynamic features and infer\noverly broad types. AI-based methods are inherently unsound and can miss rare\nor user-defined types. Dynamic methods can impose extreme runtime overheads,\ndegrading performance by up to 270x, abort execution as they exhaust resources,\nand even infer incorrect types that lead to runtime errors. Crucially, all\nprior work assumes implicitly that the code to be annotated is already correct.\nThis assumption is generally unwarranted, especially for large codebases that\nhave been untyped.\n  This paper presents RightTyper, a novel approach for Python that overcomes\nthese disadvantages. RightTyper not only generates precise type annotations\nbased on actual program behavior, improving recall in type checking relative to\nprior approaches. It also turns type checking into anomaly detection, allowing\nthe type checker to identify corner cases that the programmer can audit for\nunintended behavior. RightTyper is also fast and space-efficient, imposing just\n30% performance overhead on average. RightTyper achieves these characteristics\nby a principled yet pervasive use of sampling--guided by self-profiling--along\nwith statistical filtering and careful resolution and aggregation of type\ninformation."}
{"id": "2507.16177", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2507.16177", "abs": "https://arxiv.org/abs/2507.16177", "authors": ["Yifan Zhang", "Xiaoyu Niu", "Hongzheng Tian", "Yanjun Zhang", "Bo Yu", "Shaoshan Liu", "Sitao Huang"], "title": "A Sparsity-Aware Autonomous Path Planning Accelerator with HW/SW Co-Design and Multi-Level Dataflow Optimization", "comment": "Accepted by ACM Transactions on Architecture and Code Optimization\n  (ACM TACO)", "summary": "Path planning is critical for autonomous driving, generating smooth,\ncollision-free, feasible paths based on perception and localization inputs.\nHowever, its computationally intensive nature poses significant challenges for\nresource-constrained autonomous driving hardware. This paper presents an\nend-to-end FPGA-based acceleration framework targeting the quadratic\nprogramming (QP), core of optimization-based path planning. We employ a\nhardware-friendly alternating direction method of multipliers (ADMM) for QP\nsolving and a parallelizable preconditioned conjugate gradient (PCG) method for\nlinear systems. By analyzing sparse matrix patterns, we propose customized\nstorage schemes and efficient sparse matrix multiplication units, significantly\nreducing resource usage and accelerating matrix operations. Our multi-level\ndataflow optimization strategy incorporates intra-operator parallelization and\npipelining, inter-operator fine-grained pipelining, and CPU-FPGA system-level\ntask mapping. Implemented on the AMD ZCU102 platform, our framework achieves\nstate-of-the-art latency and energy efficiency, including 1.48x faster\nperformance than the best FPGA-based design, 2.89x over an Intel i7-11800H CPU,\n5.62x over an ARM Cortex-A57 embedded CPU, and 1.56x over a state-of-the-art\nGPU solution, along with a 2.05x throughput improvement over existing\nFPGA-based designs."}
{"id": "2507.16710", "categories": ["cs.DC", "cs.PF"], "pdf": "https://arxiv.org/pdf/2507.16710", "abs": "https://arxiv.org/abs/2507.16710", "authors": ["Andrei-Leonard Nicusan", "Dominik Werner", "Simon Branford", "Simon Hartley", "Andrew J. Morris", "Kit Windows-Yule"], "title": "AcceleratedKernels.jl: Cross-Architecture Parallel Algorithms from a Unified, Transpiled Codebase", "comment": null, "summary": "AcceleratedKernels.jl is introduced as a backend-agnostic library for\nparallel computing in Julia, natively targeting NVIDIA, AMD, Intel, and Apple\naccelerators via a unique transpilation architecture. Written in a unified,\ncompact codebase, it enables productive parallel programming with minimised\nimplementation and usage complexities. Benchmarks of arithmetic-heavy kernels\nshow performance on par with C and OpenMP-multithreaded CPU implementations,\nwith Julia sometimes offering more consistent and predictable numerical\nperformance than conventional C compilers. Exceptional composability is\nhighlighted as simultaneous CPU-GPU co-processing is achievable - such as\nCPU-GPU co-sorting - with transparent use of hardware-specialised MPI\nimplementations. Tests on the Baskerville Tier 2 UK HPC cluster achieved\nworld-class sorting throughputs of 538-855 GB/s using 200 NVIDIA A100 GPUs,\ncomparable to the highest literature-reported figure of 900 GB/s achieved on\n262,144 CPU cores. The use of direct NVLink GPU-to-GPU interconnects resulted\nin a 4.93x speedup on average; normalised by a combined capital, running and\nenvironmental cost, communication-heavy HPC tasks only become economically\nviable on GPUs if GPUDirect interconnects are employed."}
{"id": "2507.16086", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2507.16086", "abs": "https://arxiv.org/abs/2507.16086", "authors": ["Andrew Marmaduke", "Apoorv Ingle", "J. Garrett Morris"], "title": "Understanding Haskell-style Overloading via Open Data and Open Functions", "comment": null, "summary": "We present a new, uniform semantics for Haskell-style overloading. We realize\nour approach in a new core language, System F$_\\mathrm{D}$, whose metatheory we\nmechanize in the Lean4 interactive theorem prover. System F$_\\mathrm{D}$ is\ndistinguished by its open data types and open functions, each given by a\ncollection of instances rather than by a single definition. We show that System\nF$_\\mathrm{D}$ can encode advanced features of Haskell's of type class systems,\nmore expressively than current semantics of these features, and without\nassuming additional type equality axioms."}
{"id": "2507.16326", "categories": ["cs.AR", "B.5.0"], "pdf": "https://arxiv.org/pdf/2507.16326", "abs": "https://arxiv.org/abs/2507.16326", "authors": ["Daniel Bascones", "Borja Morcillo"], "title": "Hourglass Sorting: A novel parallel sorting algorithm and its implementation", "comment": "6 pages, 5 figures", "summary": "Sorting is one of the fundamental problems in computer science. Playing a\nrole in many processes, it has a lower complexity bound imposed by\n$\\mathcal{O}(n\\log{n})$ when executing on a sequential machine. This limit can\nbe brought down to sub-linear times thanks to parallelization techniques that\nincrease the number of comparisons done in parallel. This, however, increases\nthe cost of implementation, which limits the application of such techniques.\nMoreover, as the size of the arrays increases, a bottleneck arises in moving\nthe vast quantities of data required at the input, and generated at the output\nof such sorter. This might impose time requirements much stricter than those of\nthe sorting itself. In this paper, a novel parallel sorter is proposed for the\nspecific case where the input is parallel, but the output is serial. The design\nis then implemented and verified on an FPGA within the context of a quantum\nLDPC decoder. A latency of $\\log{n}$ is achieved for the output of the first\nelement, after which the rest stream out for a total sorting time of\n$n+\\log{n}$. Contrary to other parallel sorting methods, clock speed does not\ndegrade with $n$, and resources scale linearly with input size."}
{"id": "2507.16731", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2507.16731", "abs": "https://arxiv.org/abs/2507.16731", "authors": ["Senyao Li", "Haozhao Wang", "Wenchao Xu", "Rui Zhang", "Song Guo", "Jingling Yuan", "Xian Zhong", "Tianwei Zhang", "Ruixuan Li"], "title": "Collaborative Inference and Learning between Edge SLMs and Cloud LLMs: A Survey of Algorithms, Execution, and Open Challenges", "comment": "35 pages, 9 figures", "summary": "As large language models (LLMs) evolve, deploying them solely in the cloud or\ncompressing them for edge devices has become inadequate due to concerns about\nlatency, privacy, cost, and personalization. This survey explores a\ncollaborative paradigm in which cloud-based LLMs and edge-deployed small\nlanguage models (SLMs) cooperate across both inference and training. We present\na unified taxonomy of edge-cloud collaboration strategies. For inference, we\ncategorize approaches into task assignment, task division, and mixture-based\ncollaboration at both task and token granularity, encompassing adaptive\nscheduling, resource-aware offloading, speculative decoding, and modular\nrouting. For training, we review distributed adaptation techniques, including\nparameter alignment, pruning, bidirectional distillation, and\nsmall-model-guided optimization. We further summarize datasets, benchmarks, and\ndeployment cases, and highlight privacy-preserving methods and vertical\napplications. This survey provides the first systematic foundation for LLM-SLM\ncollaboration, bridging system and algorithm co-design to enable efficient,\nscalable, and trustworthy edge-cloud intelligence."}
{"id": "2507.16089", "categories": ["cs.PL", "cs.DB"], "pdf": "https://arxiv.org/pdf/2507.16089", "abs": "https://arxiv.org/abs/2507.16089", "authors": ["Michael J. Sullivan", "Zhibo Chen", "Elvis Pranskevichus", "Robert J. Simmons", "Victor Petrovykh", "Aljaž Mur Eržen", "Yury Selivanov"], "title": "Querying Graph-Relational Data", "comment": null, "summary": "For applications that store structured data in relational databases, there is\nan impedance mismatch between the flat representations encouraged by relational\ndata models and the deeply nested information that applications expect to\nreceive. In this work, we present the graph-relational database model, which\nprovides a flexible, compositional, and strongly-typed solution to this\n\"object-relational mismatch.\" We formally define the graph-relational database\nmodel and present a static and dynamic semantics for queries. In addition, we\ndiscuss the realization of the graph-relational database model in EdgeQL, a\ngeneral-purpose SQL-style query language, and the Gel system, which compiles\nEdgeQL schemas and queries into PostgreSQL queries. Gel facilitates the kind of\nobject-shaped data manipulation that is frequently provided inefficiently by\nobject-relational mapping (ORM) technologies, while achieving most of the\nefficiency that comes from require writing complex PostgreSQL queries directly."}
{"id": "2507.16379", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2507.16379", "abs": "https://arxiv.org/abs/2507.16379", "authors": ["Ondrej Vlcek", "Vojtech Mrazek"], "title": "ApproxGNN: A Pretrained GNN for Parameter Prediction in Design Space Exploration for Approximate Computing", "comment": "To appear at ICCAD 2025", "summary": "Approximate computing offers promising energy efficiency benefits for\nerror-tolerant applications, but discovering optimal approximations requires\nextensive design space exploration (DSE). Predicting the accuracy of circuits\ncomposed of approximate components without performing complete synthesis\nremains a challenging problem. Current machine learning approaches used to\nautomate this task require retraining for each new circuit configuration,\nmaking them computationally expensive and time-consuming. This paper presents\nApproxGNN, a construction methodology for a pre-trained graph neural network\nmodel predicting QoR and HW cost of approximate accelerators employing\napproximate adders from a library. This approach is applicable in DSE for\nassignment of approximate components to operations in accelerator. Our approach\nintroduces novel component feature extraction based on learned embeddings\nrather than traditional error metrics, enabling improved transferability to\nunseen circuits. ApproxGNN models can be trained with a small number of\napproximate components, supports transfer to multiple prediction tasks,\nutilizes precomputed embeddings for efficiency, and significantly improves\naccuracy of the prediction of approximation error. On a set of image\nconvolutional filters, our experimental results demonstrate that the proposed\nembeddings improve prediction accuracy (mean square error) by 50% compared to\nconventional methods. Furthermore, the overall prediction accuracy is 30%\nbetter than statistical machine learning approaches without fine-tuning and 54%\nbetter with fast finetuning."}
{"id": "2507.16781", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2507.16781", "abs": "https://arxiv.org/abs/2507.16781", "authors": ["Imran Latif", "Muhammad Ali Shafique", "Hayat Ullah", "Alex C. Newkirk", "Xi Yu", "Arslan Munir"], "title": "Cooling Matters: Benchmarking Large Language Models and Vision-Language Models on Liquid-Cooled Versus Air-Cooled H100 GPU Systems", "comment": "11 pages", "summary": "The unprecedented growth in artificial intelligence (AI) workloads, recently\ndominated by large language models (LLMs) and vision-language models (VLMs),\nhas intensified power and cooling demands in data centers. This study\nbenchmarks LLMs and VLMs on two HGX nodes, each with 8x NVIDIA H100 graphics\nprocessing units (GPUs), using liquid and air cooling. Leveraging GPU Burn,\nWeights and Biases, and IPMItool, we collect detailed thermal, power, and\ncomputation data. Results show that the liquid-cooled systems maintain GPU\ntemperatures between 41-50 degrees Celsius, while the air-cooled counterparts\nfluctuate between 54-72 degrees Celsius under load. This thermal stability of\nliquid-cooled systems yields 17 percent higher performance (54 TFLOPs per GPU\nvs. 46 TFLOPs per GPU), improved performance per watt, reduced energy overhead,\nand greater system efficiency than the air-cooled counterparts. These findings\nunderscore the energy and sustainability benefits of liquid cooling, offering a\ncompelling path forward for hyperscale data centers s"}
{"id": "2507.16660", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2507.16660", "abs": "https://arxiv.org/abs/2507.16660", "authors": ["Xuran Cai"], "title": "Enhancing Compiler Optimization Efficiency through Grammatical Decompositions of Control-Flow Graphs", "comment": null, "summary": "This thesis addresses the complexities of compiler optimizations, such as\nregister allocation and Lifetime-optimal Speculative Partial Redundancy\nElimination (LOSPRE), which are often handled using tree decomposition\nalgorithms. However, these methods frequently overlook important sparsity\naspects of Control Flow Graphs (CFGs) and result in high computational costs.\nWe introduce the SPL (Series-Parallel-Loop) decomposition, a novel framework\nthat offers optimal solutions to these challenges. A key contribution is the\nformulation of a general solution for Partial Constraint Satisfaction Problems\n(PCSPs) within graph structures, applied to three optimization problems. First,\nSPL decomposition enhances register allocation by accurately modeling variable\ninterference graphs, leading to efficient register assignments and improved\nperformance across benchmarks. Second, it optimizes LOSPRE by effectively\nidentifying and eliminating redundancies in program execution. Finally, the\nthesis focuses on optimizing the placement of bank selection instructions to\nenhance data retrieval efficiency and reduce latency. Extensive experimentation\ndemonstrates significant performance improvements over existing methods,\nestablishing SPL decomposition as a powerful tool for complex compiler\noptimizations, including register allocation, LOSPRE, and bank selection."}
{"id": "2507.16391", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2507.16391", "abs": "https://arxiv.org/abs/2507.16391", "authors": ["Chenqi Lin", "Kang Yang", "Tianshi Xu", "Ling Liang", "Yufei Wang", "Zhaohui Chen", "Runsheng Wang", "Mingyu Gao", "Meng Li"], "title": "Ironman: Accelerating Oblivious Transfer Extension for Privacy-Preserving AI with Near-Memory Processing", "comment": null, "summary": "With the wide application of machine learning (ML), privacy concerns arise\nwith user data as they may contain sensitive information. Privacy-preserving ML\n(PPML) based on cryptographic primitives has emerged as a promising solution in\nwhich an ML model is directly computed on the encrypted data to provide a\nformal privacy guarantee. However, PPML frameworks heavily rely on the\noblivious transfer (OT) primitive to compute nonlinear functions. OT mainly\ninvolves the computation of single-point correlated OT (SPCOT) and learning\nparity with noise (LPN) operations. As OT is still computed extensively on\ngeneral-purpose CPUs, it becomes the latency bottleneck of modern PPML\nframeworks.\n  In this paper, we propose a novel OT accelerator, dubbed Ironman, to\nsignificantly increase the efficiency of OT and the overall PPML framework. We\nobserve that SPCOT is computation-bounded, and thus propose a hardware-friendly\nSPCOT algorithm with a customized accelerator to improve SPCOT computation\nthroughput. In contrast, LPN is memory-bandwidth-bounded due to irregular\nmemory access patterns. Hence, we further leverage the near-memory processing\n(NMP) architecture equipped with memory-side cache and index sorting to improve\neffective memory bandwidth. With extensive experiments, we demonstrate Ironman\nachieves a 39.2-237.4 times improvement in OT throughput across different NMP\nconfigurations compared to the full-thread CPU implementation. For different\nPPML frameworks, Ironman demonstrates a 2.1-3.4 times reduction in end-to-end\nlatency for both CNN and Transformer models."}
{"id": "2507.16628", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2507.16628", "abs": "https://arxiv.org/abs/2507.16628", "authors": ["Rajpreet Singh", "Vidhi Kothari"], "title": "Augmenting Von Neumann's Architecture for an Intelligent Future", "comment": "6 pages, 2 figures", "summary": "This work presents a novel computer architecture that extends the Von Neumann\nmodel with a dedicated Reasoning Unit (RU) to enable native artificial general\nintelligence capabilities. The RU functions as a specialized co-processor that\nexecutes symbolic inference, multi-agent coordination, and hybrid\nsymbolic-neural computation as fundamental architectural primitives. This\nhardware-embedded approach allows autonomous agents to perform goal-directed\nplanning, dynamic knowledge manipulation, and introspective reasoning directly\nwithin the computational substrate at system scale. The architecture\nincorporates a reasoning-specific instruction set architecture, parallel\nsymbolic processing pipelines, agent-aware kernel abstractions, and a unified\nmemory hierarchy that seamlessly integrates cognitive and numerical workloads.\nThrough systematic co-design across hardware, operating system, and agent\nruntime layers, this architecture establishes a computational foundation where\nreasoning, learning, and adaptation emerge as intrinsic execution properties\nrather than software abstractions, potentially enabling the development of\ngeneral-purpose intelligent machines."}
{"id": "2507.16793", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2507.16793", "abs": "https://arxiv.org/abs/2507.16793", "authors": ["Jianqiao Mo", "Alhad Daftardar", "Joey Ah-kiow", "Kaiyue Guo", "Benedikt Bünz", "Siddharth Garg", "Brandon Reagen"], "title": "MTU: The Multifunction Tree Unit in zkSpeed for Accelerating HyperPlonk", "comment": null, "summary": "Zero-Knowledge Proofs (ZKPs) are critical for privacy preservation and\nverifiable computation. Many ZKPs rely on kernels such as the SumCheck protocol\nand Merkle Tree commitments, which enable their security properties. These\nkernels exhibit balanced binary tree computational patterns, which enable\nefficient hardware acceleration. Prior work has investigated accelerating these\nkernels as part of an overarching ZKP protocol; however, a focused study of how\nto best exploit the underlying tree pattern for hardware efficiency remains\nlimited. We conduct a systematic evaluation of these tree-based workloads under\ndifferent traversal strategies, analyzing performance on multi-threaded CPUs\nand a hardware accelerator, the Multifunction Tree Unit (MTU). We introduce a\nhardware-friendly Hybrid Traversal for binary tree that improves parallelism\nand scalability while significantly reducing memory traffic on hardware. Our\nresults show that MTU achieves up to 1478$\\times$ speedup over CPU at DDR-level\nbandwidth and that our hybrid traversal outperforms as standalone approach by\nup to 3$\\times$. These findings offer practical guidance for designing\nefficient hardware accelerators for ZKP workloads with binary tree structures."}
