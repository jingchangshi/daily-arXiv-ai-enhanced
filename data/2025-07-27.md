<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 1]
- [cs.DC](#cs.DC) [Total: 15]
- [cs.AR](#cs.AR) [Total: 3]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Higher-Order Behavioural Conformances via Fibrations](https://arxiv.org/abs/2507.18509)
*Henning Urbat*

Main category: cs.PL

TL;DR: 本文提出了一种统一的范畴化方法（Howe's method），用于证明高阶语言中行为一致性（如关系或度量）的程序同余性。


<details>
  <summary>Details</summary>
Motivation: 随着具有定量特征（如概率性）语言的兴起，需要扩展共归纳方法以支持更精细的行为一致性，并确保其作为程序同余性的正确性。

Method: 通过抽象高阶规范（AHOS）建模语言，并使用纤维化建模行为一致性，提出一种通用的范畴化方法。

Result: 在自然条件下，AHOS建模的语言的最大行为（双）一致性形成同余性。

Conclusion: 该方法适用于概率高阶语言，成功证明了双相似性和行为伪度量的同余性。

Abstract: Coinduction is a widely used technique for establishing behavioural
equivalence of programs in higher-order languages. In recent years, the rise of
languages with quantitative (e.g.~probabilistic) features has led to extensions
of coinductive methods to more refined types of behavioural conformances, most
notably notions of behavioural distance. To guarantee soundness of coinductive
reasoning, one needs to show that the behavioural conformance at hand forms a
program congruence, i.e. it is suitably compatible with the operations of the
language. This is usually achieved by a complex proof technique known as
\emph{Howe's method}, which needs to be carefully adapted to both the specific
language and the targeted notion of behavioural conformance. We develop a
uniform categorical approach to Howe's method that features two orthogonal
dimensions of abstraction: (1) the underlying higher-order language is modelled
by an \emph{abstract higher-order specification} (AHOS), a novel and very
general categorical account of operational semantics, and (2) notions of
behavioural conformance (such as relations or metrics) are modelled via
fibrations over the base category of an AHOS. Our main result is a fundamental
congruence theorem at this level of generality: Under natural conditions on the
categorical ingredients and the operational rules of a language modelled by an
AHOS, the greatest behavioural (bi)conformance on its operational model forms a
congruence. We illustrate our theory by deriving congruence of bisimilarity and
behavioural pseudometrics for probabilistic higher-order languages.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [2] [Incentivised Orchestrated Training Architecture (IOTA): A Technical Primer for Release](https://arxiv.org/abs/2507.17766)
*Felix Quinque,Alan Aboudib,Szymon Fonau,Rodrigo Lopez Portillo Alcocer,Brian McCrindle,Steffen Cruz*

Main category: cs.DC

TL;DR: Bittensor的SN9展示了分布式网络预训练大型语言模型的可行性，但存在模型本地化和奖励分配问题。IOTA架构通过协作训练、公平奖励和优化技术解决了这些问题。


<details>
  <summary>Details</summary>
Motivation: 解决SN9中模型本地化和奖励分配的问题，实现更高效、公平的分布式预训练。

Method: 采用数据并行和流水线并行的SWARM架构，结合激活压缩、Butterfly All-Reduce和CLASP公平奖励机制。

Result: 实现了模型规模的动态扩展、通信带宽优化、线性可扩展性及公平贡献评估。

Conclusion: IOTA通过协作和优化技术显著提升了分布式预训练的效率和公平性。

Abstract: In August 2024, Bittensor's Subnet 9 (SN9) demonstrated that a distributed
network of incentivized, permissionless actors could each pretrain large
language models (LLMs) ranging from 700 million to 14 billion parameters, while
surpassing established baselines. While that work validated blockchain-based
decentralized pretraining as viable, it contained core issues: (i) every miner
had to fit an entire model locally, and (ii) "winner-takes-all" rewards
encouraged model hoarding.
  Here we introduce IOTA (Incentivized Orchestrated Training Architecture), an
architecture that addresses these limitations by transforming SN9's previously
isolated competitors into a single cooperating unit that can scale arbitrarily
while still rewarding each contributor fairly.
  Key preliminary results: (1) Data- and Pipeline-parallel SWARM architecture -
An orchestrator distributes model layers across heterogeneous miners and
streams activations between them, enabling model sizes to scale with the number
of participants rather than being constrained by the VRAM of a single machine;
(2) Granular, continuous incentives - Validators measure each miner's
contribution and allocate token emissions proportionally; (3) Activation
compression - We used model-bottlenecks to cut communication bandwidths of
activations by up to 128x, vastly improving training speed; (4) Butterfly
All-Reduce - Miners average disjoint parameter slices in O(1) bandwidth,
offering linear scalability, redundancy and built-in collusion detection; (5)
CLASP (Contribution Loss Assessment via Sampling of Pathways) - A fair
attribution scheme assigns credit to miners proportional to their marginal
utility and detects exploits, even when contributions are interdependent across
the pipeline.

</details>


### [3] [PolyServe: Efficient Multi-SLO Serving at Scale](https://arxiv.org/abs/2507.17769)
*Kan Zhu,Haiyang Shi,Le Xu,Jiaxin Shan,Arvind Krishnamurthy,Baris Kasikci,Liguang Xie*

Main category: cs.DC

TL;DR: PolyServe是一种新型的多SLO调度策略，旨在高效处理具有不同延迟需求的LLM请求，通过分组、负载均衡和资源共享优化吞吐量和SLO达成率。


<details>
  <summary>Details</summary>
Motivation: 现有系统将工作负载简单分为延迟敏感（LS）和尽力而为（BE），忽略了延迟敏感类别内的多样性，导致用户体验和调度机会不佳。

Method: PolyServe将请求按每令牌延迟需求分组，调度到服务器子集，并通过负载梯度、资源共享和动态填充预测优化调度。

Result: PolyServe相比现有策略实现了1.23倍的吞吐量增益，达到最优吞吐量的92.5%。

Conclusion: PolyServe通过精细化的多SLO调度策略，显著提升了LLM请求处理的效率和用户满意度。

Abstract: Advances in Large Language Models (LLMs) have led to a surge of LLM-powered
applications. These applications have diverse token-generation latency
requirements. As a result, simply classifying workloads as latency-sensitive
(LS) or best-effort (BE) overlooks the nuances within the latency-sensitive
category and results in suboptimal user experiences and scheduling
opportunities. However, efficiently serving requests with multiple SLO
requirements poses significant challenges. First, all requests within a batch
generate new tokens simultaneously, which can misalign them with their distinct
SLO requirements. Moreover, while existing systems focus on auto-scaling for
handling various overall request rates, the diversity of SLOs necessitates
fine-grained auto-scaling among these SLO tiers. Finally, unlike LS/BE
scenarios, where BE requests can be aborted at any time to ensure the SLO
attainment of LS requests, those with different latency-sensitive SLOs cannot
tolerate prolonged delays, and tail latency must be controlled.
  To tackle these challenges, we propose PolyServe, a novel multi-SLO
scheduling policy at scale that maintains high SLO attainment while maximizing
throughput. PolyServe first groups requests into multiple bins based on their
per-token latency requirement, then schedules each bin to a subset of the
server fleet. PolyServe routes requests to the highest-load but still
SLO-attainable server to create a load gradient that facilitates auto-scaling.
To increase utilization, PolyServe permits looser-SLO requests to share
tighter-SLO instances when their own servers are saturated. PolyServe uses
profiling data to guide scheduling decisions and manage tail latency through
request-wait-time-aware scheduling, dynamic chunking, and continuous chunked
prefill prediction. PolyServe achieves 1.23x goodput gain compared to existing
policies, achieving up to 92.5% of optimal goodput.

</details>


### [4] [Comparative Evaluation of PyTorch, JAX, SciPy, and Neal for Solving QUBO Problems at Scale](https://arxiv.org/abs/2507.17770)
*Pei-Kun Yang*

Main category: cs.DC

TL;DR: 该研究比较了五种基于软件的QUBO求解器在随机生成的QUBO矩阵上的性能，发现Neal在解质量上最优但可扩展性差，PyTorch在可扩展性和运行时间上表现最佳。


<details>
  <summary>Details</summary>
Motivation: 评估不同软件求解器在解决大规模QUBO问题时的性能差异，为实际应用提供选择依据。

Method: 生成随机QUBO矩阵（1000x1000到45000x45000），在六种收敛阈值下测试五种求解器的解质量和计算时间。

Result: Neal解质量最优但仅支持6000变量；PyTorch解质量稍逊但可扩展至45000变量且运行时间短；JAX介于两者之间；SciPy表现最差。

Conclusion: PyTorch在大规模QUBO问题中表现最均衡，适合资源充足时使用。

Abstract: Quadratic Unconstrained Binary Optimization (QUBO) is a versatile framework
for modeling combinatorial optimization problems. This study benchmarks five
software-based QUBO solvers: Neal, PyTorch (CPU), PyTorch (GPU), JAX, and
SciPy, on randomly generated QUBO matrices ranging from 1000x1000 to
45000x45000, under six convergence thresholds from 10^-1 to 10^-6. We evaluate
their performance in terms of solution quality (energy) and computational time.
Among the solvers tested, Neal achieved the lowest energy values but was
limited to problems with up to 6000 variables due to high memory consumption.
PyTorch produced slightly higher energy results than Neal but demonstrated
superior scalability, solving instances with up to 45000 variables. Its support
for GPU acceleration and CPU multi-threading also resulted in significantly
shorter runtimes. JAX yielded energy values slightly above those of PyTorch and
was limited to 25000 variables, with runtimes comparable to PyTorch on GPU.
SciPy was the most constrained solver, handling only up to 6000 variables and
consistently producing the highest energy values with the longest computation
times. These findings highlight trade-offs between solution quality,
scalability, and runtime efficiency, and suggest that PyTorch is the most
balanced choice for large-scale QUBO problems when computational resources
permit.

</details>


### [5] [Flexible Vector Integration in Embedded RISC-V SoCs for End to End CNN Inference Acceleration](https://arxiv.org/abs/2507.17771)
*Dmitri Lyalikov*

Main category: cs.DC

TL;DR: 论文探讨了异构和领域特定架构在深度学习推理中的潜力，重点解决了资源受限嵌入式平台上CNN部署的系统集成和编译/执行模型问题，展示了RISC-V Vector 1.0扩展在减少预处理瓶颈和CPU回退过程中的优势。


<details>
  <summary>Details</summary>
Motivation: 随着深度学习加速器和神经处理单元的多样化，传统硅缩放接近极限，嵌入式SoC需要解决性能/功耗权衡问题，但缺乏高效的系统集成和编译/执行模型。

Method: 利用RISC-V Vector 1.0扩展，设计灵活的编程模型和缓存层次结构，优化预处理和CPU回退过程。

Result: 实验显示图像预处理速度提升9倍，YOLOv3回退层执行速度提升3倍，同时功耗低于传统并行执行平台。

Conclusion: RISC-V Vector 1.0扩展为加速器丰富的嵌入式SoC提供了平衡计算和内存占用的灵活编程模型，显著提升了深度学习数据流的效率。

Abstract: The emergence of heterogeneity and domain-specific architectures targeting
deep learning inference show great potential for enabling the deployment of
modern CNNs on resource-constrained embedded platforms. A significant
development is the diversification of custom hardware solely targeting the most
expensive parts of CNNs. DLAs (deep learning accelerators) and NPUs (neural
processing units), among others, can overcome the approaching limits of
traditional silicon scaling and provide a solution to the power/performance
tradeoff within embedded SoCs. Efficient DSA utilization requires proper system
integration and a compilation/execution model for balanced execution in these
heterogeneous architectures. There is a critical need for proper system
integration and an efficient compilation/execution model for balanced execution
in these heterogeneous architectures. This work highlights the hardware
integration challenges for efficiently placing these units within the memory
hierarchy and correct proximity to other execution blocks. We experimentally
verify performance bottlenecks in CNN execution and pre/post-processing at
runtime, where previous attention has generally been given to accelerator
speedup alone. This work takes advantage of the ratification of the RISC-V
Vector 1.0 extension and demonstrates its potential as a flexible target within
a well-suited cache hierarchy scheme to reduce pre-processing bottlenecks and
CPU fallback processes. Our results show up to a 9x speedup of image
pre-processing and YOLOv3 fallback layer execution by up to 3x compared to CPU.
We demonstrate RVV-1.0 in exposing a flexible programming model that can enable
a balanced computation and memory footprint on accelerator-rich embedded SoCs
supporting modern deep-learning dataflows while consuming less power than
traditional parallel execution platforms.

</details>


### [6] [Caching Techniques for Reducing the Communication Cost of Federated Learning in IoT Environments](https://arxiv.org/abs/2507.17772)
*Ahmad Alhonainy,Praveen Rao*

Main category: cs.DC

TL;DR: 论文提出了一种基于缓存的联邦学习方法（FL），通过FIFO、LRU和优先级策略减少不必要的模型更新传输，降低通信成本，同时保持模型精度。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在分布式设备上训练共享模型时，通信成本是主要瓶颈，尤其在资源受限环境中。

Method: 引入FIFO、LRU和基于优先级的缓存策略，选择性转发重要模型更新。

Result: 在CIFAR-10和医疗数据集上的实验表明，通信量减少且精度损失极小。

Conclusion: 智能缓存提升了可扩展性和内存效率，支持边缘物联网中的可靠联邦学习，适用于智慧城市和医疗等延迟敏感场景。

Abstract: Federated Learning (FL) allows multiple distributed devices to jointly train
a shared model without centralizing data, but communication cost remains a
major bottleneck, especially in resource-constrained environments. This paper
introduces caching strategies - FIFO, LRU, and Priority-Based - to reduce
unnecessary model update transmissions. By selectively forwarding significant
updates, our approach lowers bandwidth usage while maintaining model accuracy.
Experiments on CIFAR-10 and medical datasets show reduced communication with
minimal accuracy loss. Results confirm that intelligent caching improves
scalability, memory efficiency, and supports reliable FL in edge IoT networks,
making it practical for deployment in smart cities, healthcare, and other
latency-sensitive applications.

</details>


### [7] [MultiKernelBench: A Multi-Platform Benchmark for Kernel Generation](https://arxiv.org/abs/2507.17773)
*Zhongzhen Wen,Yinghui Zhang,Zhong Li,Zhongxin Liu,Linna Xie,Tian Zhang*

Main category: cs.DC

TL;DR: MultiKernelBench是一个全面的多平台基准测试，用于评估基于LLM的深度学习内核生成，解决了现有基准测试的局限性。


<details>
  <summary>Details</summary>
Motivation: 减少手动编写高性能深度学习内核的工作量，并解决现有基准测试在硬件支持、任务覆盖和分类上的不足。

Method: 设计了MultiKernelBench，支持285个任务和14个内核类别，覆盖三种硬件平台，并引入了模块化后端抽象层和类别感知的一样本提示方法。

Result: 评估了七种先进LLM，揭示了任务难度差异、对新硬件平台的泛化能力不足，以及针对性提示策略的有效性。

Conclusion: MultiKernelBench为LLM在深度学习内核生成领域的评估提供了全面且可扩展的解决方案。

Abstract: The automatic generation of deep learning (DL) kernels using large language
models (LLMs) has emerged as a promising approach to reduce the manual effort
and hardware-specific expertise required for writing high-performance operator
implementations. However, existing benchmarks for evaluating LLMs in this
domain suffer from limited hardware support, coarse-grained kernel
categorization, and imbalanced task coverage. To address these limitations, we
introduce MultiKernelBench, the first comprehensive, multi-platform benchmark
for LLM-based DL kernel generation. MultiKernelBench spans 285 tasks across 14
well-defined kernel categories and supports three major hardware platforms:
Nvidia GPUs, Huawei NPUs, and Google TPUs. To enable future extensibility, we
design a modular backend abstraction layer that decouples platform-specific
logic from the core benchmarking infrastructure, allowing easy integration of
new hardware platforms. We further propose a simple yet effective
category-aware one-shot prompting method that improves generation quality by
providing in-category exemplars. Through systematic evaluations of seven
state-of-the-art LLMs, we reveal significant variation in task difficulty, poor
generalization to platforms with less training exposure, and the effectiveness
of targeted prompting strategies. MultiKernelBench is publicly available at
https://github.com/wzzll123/MultiKernelBench.

</details>


### [8] [CHAMP: A Configurable, Hot-Swappable Edge Architecture for Adaptive Biometric Tasks](https://arxiv.org/abs/2507.17793)
*Joel Brogan,Matthew Yohe,David Cornett*

Main category: cs.DC

TL;DR: CHAMP是一个模块化边缘计算平台，支持动态更换AI功能模块，适用于灵活的高性能边缘AI系统。


<details>
  <summary>Details</summary>
Motivation: 为现场操作员提供灵活、高性能的边缘AI系统，支持即时适应不同任务需求。

Method: 采用基于FPGA的低功耗加速器和高吞吐总线，结合定制操作系统VDiSK，实现即插即用的AI流水线和加密生物识别数据存储。

Result: 实验显示，1至5个神经计算加速器的吞吐量接近线性扩展，同时揭示了USB3总线的性能增益和饱和限制。

Conclusion: CHAMP在生物识别、监控和灾难响应中有广泛应用前景，未来可改进总线协议、模块功能和系统软件。

Abstract: What if you could piece together your own custom biometrics and AI analysis
system, a bit like LEGO blocks? We aim to bring that technology to field
operators in the field who require flexible, high-performance edge AI system
that can be adapted on a moment's notice. This paper introduces CHAMP
(Configurable Hot-swappable Architecture for Machine Perception), a modular
edge computing platform that allows operators to dynamically swap in
specialized AI "capability cartridges" for tasks like face recognition, object
tracking, and document analysis. CHAMP leverages low-power FPGA-based
accelerators on a high-throughput bus, orchestrated by a custom operating
system (VDiSK) to enable plug-and-play AI pipelines and cryptographically
secured biometric datasets. In this paper we describe the CHAMP design,
including its modular scaling with multiple accelerators and the VDiSK
operating system for runtime reconfiguration, along with its cryptographic
capabilities to keep data stored on modules safe and private. Experiments
demonstrate near-linear throughput scaling from 1 to 5 neural compute
accelerators, highlighting both the performance gains and saturation limits of
the USB3-based bus. Finally, we discuss applications of CHAMP in field
biometrics, surveillance, and disaster response, and outline future
improvements in bus protocols, cartridge capabilities, and system software.

</details>


### [9] [Optimizing Edge Gaming Slices through an Enhanced User Plane Function and Analytics in Beyond-5G Networks](https://arxiv.org/abs/2507.17843)
*Bruno Marques da Silva,Larissa Ferreira Rodrigues Moreira,Flávio de Oliveira Silva,Rodrigo Moreira*

Main category: cs.DC

TL;DR: 论文提出了一种闭环架构，整合NWDAF和UPF以估计用户延迟并增强5G控制平面，通过AI模型实现游戏分类。


<details>
  <summary>Details</summary>
Motivation: 解决移动用户在服务管理和SLA合规性方面的挑战，特别是边缘游戏中的延迟问题。

Method: 提出闭环架构，结合NWDAF和UPF，嵌入AI模型进行游戏分类和延迟估计。

Result: 结果表明，AI模型能有效分类游戏，为移动边缘游戏研究开辟新途径。

Conclusion: 该架构成功增强了5G控制平面的延迟感知能力，为未来研究提供了方向。

Abstract: The latest generation of games and pervasive communication technologies poses
challenges in service management and Service-Level Agreement compliance for
mobile users. State-of-the-art edge-gaming techniques enhance throughput,
reduce latency, and leverage cloud computing. However, further development of
core functions such as the User Plane Function (UPF) is needed for
non-intrusive user latency measurement. This paper proposes a closed-loop
architecture integrating the Network Data Analytics Function (NWDAF) and UPF to
estimate user latency and enhance the 5G control plane by making it
latency-aware. The results show that embedding an artificial intelligence model
within NWDAF enables game classification and opens new avenues for mobile edge
gaming research.

</details>


### [10] [PowerTrip: Exploiting Federated Heterogeneous Datacenter Power for Distributed ML Training](https://arxiv.org/abs/2507.17904)
*Talha Mehboob,Luanzheng Guo,Nathan Tallent,Michael Zink,David Irwin*

Main category: cs.DC

TL;DR: PowerTrip系统动态选择地理分布式站点以优化大模型训练中的电力-通信权衡，提升训练效率。


<details>
  <summary>Details</summary>
Motivation: 大规模AI模型的电力需求可能超过单个数据中心的容量，而地理分布式训练虽能解决电力限制，却带来通信开销的挑战。现有方法忽略电力供应的异构性。

Method: PowerTrip基于电力-成本启发式动态选择站点，优先高电力可用性和低网络延迟，采用动态贪婪方法优化训练效率。

Result: 使用真实Google电力数据评估，PowerTrip比基线策略减少50%的时间达到目标精度。

Conclusion: PowerTrip有效解决了电力受限的分布式训练中的电力-通信权衡问题，显著提升训练效率。

Abstract: The exponential growth of large-scale AI models has led to computational and
power demands that can exceed the capacity of a single data center. This is due
to the limited power supplied by regional grids that leads to limited regional
computational power. Consequently, distributing training workloads across
geographically distributed sites has become essential. However, this approach
introduces a significant challenge in the form of communication overhead,
creating a fundamental trade-off between the performance gains from accessing
greater aggregate power and the performance losses from increased network
latency. Although prior work has focused on reducing communication volume or
using heuristics for distribution, these methods assume constant homogeneous
power supplies and ignore the challenge of heterogeneous power availability
between sites.
  To address the challenge of training large models in power-constrained,
geo-distributed environments, we introduce PowerTrip, a system that dynamically
selects a subset of sites during runtime to optimize the power-communication
trade-off. Specifically, PowerTrip selects sites based on a power-to-cost
heuristic, prioritizing those with high power availability and low network
latency. PowerTrip employs a dynamic greedy approach and uses the marginal gain
in training efficiency, i.e., accuracy improvement per unit of time, to
optimize for the number of sites where the performance penalty from network
overhead negates the benefit of adding more computational power. Our
evaluation, which uses real-world Google power traces to model realistic power
capacity constraints, demonstrates that PowerTrip can reduce time-to-accuracy
by up to 50% compared to existing baseline policies.

</details>


### [11] [C-Koordinator: Interference-aware Management for Large-scale and Co-located Microservice Clusters](https://arxiv.org/abs/2507.18005)
*Shengye Song,Minxian Xu,Zuowei Zhang,Chengxi Gao,Fansong Zeng,Yu Ding,Kejiang Ye,Chengzhong Xu*

Main category: cs.DC

TL;DR: 论文提出了一种基于CPI（每指令周期）的干扰预测方法，并设计了C-Koordinator平台，用于优化大规模共置微服务集群的资源利用和干扰缓解。


<details>
  <summary>Details</summary>
Motivation: 共置微服务集群虽然提高了资源利用率，但也带来了资源竞争和干扰问题，尤其是在大规模、多样化和异构环境下。

Method: 通过分析大规模共置微服务集群的特性，采用CPI作为干扰度量指标，并利用多维指标实现CPI的准确预测。基于此，设计了C-Koordinator平台。

Result: 干扰预测模型准确率超过90.3%，显著降低了应用延迟，响应时间（P50、P90、P99）在不同系统负载下提升了16.7%至36.1%。

Conclusion: C-Koordinator平台能有效缓解共置环境中的干扰，提升应用性能。

Abstract: Microservices transform traditional monolithic applications into lightweight,
loosely coupled application components and have been widely adopted in many
enterprises. Cloud platform infrastructure providers enhance the resource
utilization efficiency of microservices systems by co-locating different
microservices. However, this approach also introduces resource competition and
interference among microservices. Designing interference-aware strategies for
large-scale, co-located microservice clusters is crucial for enhancing resource
utilization and mitigating competition-induced interference. These challenges
are further exacerbated by unreliable metrics, application diversity, and node
heterogeneity.
  In this paper, we first analyze the characteristics of large-scale and
co-located microservices clusters at Alibaba and further discuss why cycle per
instruction (CPI) is adopted as a metric for interference measurement in
large-scale production clusters, as well as how to achieve accurate prediction
of CPI through multi-dimensional metrics. Based on CPI interference prediction
and analysis, we also present the design of the C-Koordinator platform, an
open-source solution utilized in Alibaba cluster, which incorporates
co-location and interference mitigation strategies. The interference prediction
models consistently achieve over 90.3% accuracy, enabling precise prediction
and rapid mitigation of interference in operational environments. As a result,
application latency is reduced and stabilized across all percentiles (P50, P90,
P99) response time (RT), achieving improvements ranging from 16.7% to 36.1%
under various system loads compared with state-of-the-art system. These results
demonstrate the system's ability to maintain smooth application performance in
co-located environments.

</details>


### [12] [Unlock the Potential of Fine-grained LLM Serving via Dynamic Module Scaling](https://arxiv.org/abs/2507.18006)
*Jingfeng Wu,Yiyuan He,Minxian Xu,Xitong Gao,Kejiang Ye,Chengzhong Xu*

Main category: cs.DC

TL;DR: CoCoServe是一个弹性系统，通过模块级操作实现动态和细粒度的扩展，优化大型语言模型（LLM）的资源管理和性能。


<details>
  <summary>Details</summary>
Motivation: 当前LLM服务系统在资源管理和动态负载适应方面存在不足，导致资源利用率和性能下降。

Method: 提出模块级复制和迁移操作，开发自动扩展机制，动态调节资源分配和性能优化。

Result: CoCoServe可降低成本46%，延迟减少14%-75%，吞吐量提升1.16x-4x。

Conclusion: CoCoServe显著提升了LLM服务的效率和成本效益，优于现有系统。

Abstract: The rise of large language models (LLMs) has created new opportunities across
various fields but has also introduced significant challenges in resource
management. Current LLM serving systems face a fundamental tension: balancing
serving demands with limited resources while adapting to unpredictable traffic
patterns. Static deployments lead to suboptimal resource utilization and
performance degradation under dynamic workloads. Furthermore, the high cost of
adjusting instances hinders dynamic scaling, limiting the true potential of
efficient LLM serving.
  To address this, we propose CoCoServe, an elastic system that facilitates
dynamic and fine-grained scaling. Its key innovation lies in the module-level
operations for the replication and migration of LLM modules, such as decoder
layers and projections. Through a comprehensive analysis of the trade-offs
associated with these operations, we develop an auto-scaling mechanism that
dynamically regulates module-level resource allocation and performance
optimization, enabling a more cost-effective deployment of LLMs. Our evaluation
demonstrates that the scaling operations employed by CoCoServe exhibit
excellent scalability and can reduce costs by 46% while maintaining
availability. Compared to state-of-the-art LLM serving systems (e.g., Hugging
Face Transformers and vLLM), our approach reduces latency by 14%-75% and
achieves 1.16x-4x throughput on average across different model sizes and
workloads.

</details>


### [13] [Cloud Native System for LLM Inference Serving](https://arxiv.org/abs/2507.18007)
*Minxian Xu,Junhan Liao,Jingfeng Wu,Yiyuan He,Kejiang Ye,Chengzhong Xu*

Main category: cs.DC

TL;DR: 本文探讨了如何利用云原生技术（如容器化、微服务和动态调度）优化大语言模型（LLM）的推理服务，解决传统方法中的资源效率低下问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLM）的高计算需求在云环境中部署时面临资源效率低、成本高和延迟问题，亟需更高效的解决方案。

Method: 采用云原生技术（如Kubernetes自动扩展）动态调整资源分配，优化LLM推理服务的性能和可扩展性。

Result: 实验证明，云原生架构能有效减少延迟、提高吞吐量，并动态适应工作负载变化。

Conclusion: 云原生框架为LLM推理服务的未来提供了可扩展的解决方案，对云计算和AI领域的研究者和从业者具有重要参考价值。

Abstract: Large Language Models (LLMs) are revolutionizing numerous industries, but
their substantial computational demands create challenges for efficient
deployment, particularly in cloud environments. Traditional approaches to
inference serving often struggle with resource inefficiencies, leading to high
operational costs, latency issues, and limited scalability. This article
explores how Cloud Native technologies, such as containerization,
microservices, and dynamic scheduling, can fundamentally improve LLM inference
serving. By leveraging these technologies, we demonstrate how a Cloud Native
system enables more efficient resource allocation, reduces latency, and
enhances throughput in high-demand scenarios. Through real-world evaluations
using Kubernetes-based autoscaling, we show that Cloud Native architectures can
dynamically adapt to workload fluctuations, mitigating performance bottlenecks
while optimizing LLM inference serving performance. This discussion provides a
broader perspective on how Cloud Native frameworks could reshape the future of
scalable LLM inference serving, offering key insights for researchers,
practitioners, and industry leaders in cloud computing and artificial
intelligence.

</details>


### [14] [FCPO: Federated Continual Policy Optimization for Real-Time High-Throughput Edge Video Analytics](https://arxiv.org/abs/2507.18047)
*Lucas Liebe,Thanh-Tung Nguyen,Dongman Lee*

Main category: cs.DC

TL;DR: FCPO结合持续强化学习（CRL）和联邦强化学习（FRL），优化边缘视频分析（EVA）的实时推理服务，显著提升吞吐量、降低延迟并减少内存消耗。


<details>
  <summary>Details</summary>
Motivation: 边缘视频分析（EVA）的复杂性增加，现有调度系统在动态环境中表现不佳，局部强化学习（RL）存在扩展性和适应性不足的问题。

Method: FCPO通过CRL和FRL动态调整推理批次大小、输入分辨率和多线程处理，结合特定代理聚合方案和多样性感知经验缓冲。

Result: 实验显示，FCPO在吞吐量、延迟和收敛速度上显著优于现有RL方法，内存消耗减少10倍。

Conclusion: FCPO为动态边缘环境提供了一种高效、可扩展的实时推理解决方案。

Abstract: The growing complexity of Edge Video Analytics (EVA) facilitates new kind of
intelligent applications, but creates challenges in real-time inference serving
systems. State-of-the-art (SOTA) scheduling systems optimize global workload
distributions for heterogeneous devices but often suffer from extended
scheduling cycles, leading to sub-optimal processing in rapidly changing Edge
environments. Local Reinforcement Learning (RL) enables quick adjustments
between cycles but faces scalability, knowledge integration, and adaptability
issues. Thus, we propose FCPO, which combines Continual RL (CRL) with Federated
RL (FRL) to address these challenges. This integration dynamically adjusts
inference batch sizes, input resolutions, and multi-threading during pre- and
post-processing. CRL allows agents to learn from changing Markov Decision
Processes, capturing dynamic environmental variations, while FRL improves
generalization and convergence speed by integrating experiences across
inference models. FCPO combines these via an agent-specific aggregation scheme
and a diversity-aware experience buffer. Experiments on a real-world EVA
testbed showed over 5 times improvement in effective throughput, 60% reduced
latency, and 20% faster convergence with up to 10 times less memory consumption
compared to SOTA RL-based approaches.

</details>


### [15] [A large-scale distributed parallel discrete event simulation engines based on Warped2 for Wargaming simulation](https://arxiv.org/abs/2507.18050)
*Xiaoning Jia,Ruilin Kong,Guangya Si,Bilong Shen,Zhe Ji*

Main category: cs.DC

TL;DR: 论文提出了一种优化的并行离散事件仿真框架，解决了传统PDES引擎在资源分配和复杂实体交互中的局限性，通过四项改进显著提升了性能和效率。


<details>
  <summary>Details</summary>
Motivation: 传统并行离散事件仿真（PDES）引擎在资源分配和复杂实体交互中存在效率问题，限制了大规模仿真的可扩展性。

Method: 提出了四项改进：异步监听线程、METIS负载均衡策略、实体交互求解器和空间哈希算法。

Result: 实验验证显示，框架在性能上比基线实现快16倍，同步开销减少58.18%，负载均衡贡献了57%的改进。

Conclusion: 优化框架为大规模仿真提供了高效的PDES解决方案。

Abstract: Rising demand for complex simulations highlights conventional
engines'scalability limits, spurring Parallel Discrete Event Simulation (PDES)
adoption.Warped2, a PDES engine leveraging Time Warp synchronization with
Pending Event Set optimization, delivers strong performance, it struggles with
inherent wargaming limitations: inefficient LP resource allocation during
synchronization and unaddressed complex entity interaction patterns. To address
these challenges, we present an optimized framework featuring four synergistic
improvements: (1) Asynchronous listener threads are introduced to address event
monitoring latency in large-scale scenarios, instead of synchronous polling
mechanisms, (2) METIS-based load rebalancing strategy is incorporated to
address the issue of dynamic event allocation during real-world simulation, (3)
Entity interaction solver with constraint satisfaction mechanisms is designed
to mitigate state conflicts, and (4) Spatial hashing algorithm to overcome
O(n^2) complexity bottlenecks in large-scale nearest-neighbor searches.
Experimental validation through a GridWorld demo demonstrates significant
enhancements in temporal fidelity and computational efficiency. Benchmark
results show our framework achieves 16x acceleration over baseline
implementations and maintains 8x speedup over 1-thread configuration across MPI
and Pthreads implementations.The combined load balancing and LP migration
strategy reduces synchronization overhead by 58.18%, with load balancing
accounting for 57% of the total improvement as the dominant optimization
factor. These improvements provide an enhanced solution for PDES implementation
in large-scale simulation scenarios.

</details>


### [16] [Towards Designing an Energy Aware Data Replication Strategy for Cloud Systems Using Reinforcement Learning](https://arxiv.org/abs/2507.18459)
*Amir Najjar,Riad Mokadem,Jean-Marc Pierson*

Main category: cs.DC

TL;DR: 提出了一种基于强化学习的数据复制策略，用于云系统动态适应工作负载变化，优化服务质量与资源利用。


<details>
  <summary>Details</summary>
Motivation: 全球数据量快速增长，传统阈值机制依赖人工调整，难以适应动态变化。

Method: 采用强化学习模型，定义状态、动作和奖励，自动学习系统特性并适应工作负载。

Result: 策略在服务质量与资源利用间取得平衡，兼顾提供商利润与环境影响。

Conclusion: 强化学习为数据复制提供了动态自适应解决方案，优于传统阈值机制。

Abstract: The rapid growth of global data volumes has created a demand for scalable
distributed systems that can maintain a high quality of service. Data
replication is a widely used technique that provides fault tolerance, improved
performance and higher availability. Traditional implementations often rely on
threshold-based activation mechanisms, which can vary depending on workload
changes and system architecture. System administrators typically bear the
responsibility of adjusting these thresholds. To address this challenge,
reinforcement learning can be used to dynamically adapt to workload changes and
different architectures. In this paper, we propose a novel data replication
strategy for cloud systems that employs reinforcement learning to automatically
learn system characteristics and adapt to workload changes. The strategy's aim
is to provide satisfactory Quality of Service while optimizing a trade-off
between provider profit and environmental impact. We present the architecture
behind our solution and describe the reinforcement learning model by defining
the states, actions and rewards.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [17] [Designing High-Performance and Thermally Feasible Multi-Chiplet Architectures enabled by Non-bendable Glass Interposer](https://arxiv.org/abs/2507.18040)
*Harsh Sharma,Janardhan Rao Doppa,Umit Y. Ogras,Partha Pratim Pande*

Main category: cs.AR

TL;DR: 玻璃中介层多芯片架构在电气性能和能耗方面优于硅中介层系统，但随着系统尺寸增大，封装变形成为关键挑战。本文提出了一种热、变形和性能感知的设计框架，通过架构与封装协同优化，实现了性能提升和功耗降低。


<details>
  <summary>Details</summary>
Motivation: 解决玻璃中介层多芯片系统因尺寸增大导致的封装变形问题，同时优化性能和功耗。

Method: 提出了一种热、变形和性能感知的设计框架，通过架构与封装协同优化，平衡性能、功耗和结构可靠性。

Result: 实验表明，优化后的多芯片架构在深度神经网络任务中性能提升64.7%，功耗降低40%，且制造成本更低。

Conclusion: 该设计框架为玻璃中介层多芯片系统提供了有效的解决方案，实现了性能、功耗和可靠性的优化。

Abstract: Multi-chiplet architectures enabled by glass interposer offer superior
electrical performance, enable higher bus widths due to reduced crosstalk, and
have lower capacitance in the redistribution layer than current silicon
interposer-based systems. These advantages result in lower energy per bit,
higher communication frequencies, and extended interconnect range. However,
deformation of the package (warpage) in glass interposer-based systems becomes
a critical challenge as system size increases, leading to severe mechanical
stress and reliability concerns. Beyond a certain size, conventional packaging
techniques fail to manage warpage effectively, necessitating new approaches to
mitigate warpage induced bending with scalable performance for glass interposer
based multi-chiplet systems. To address these inter-twined challenges, we
propose a thermal-, warpage-, and performance-aware design framework that
employs architecture and packaging co-optimization. The proposed framework
disintegrates the surface and embedded chiplets to balance conflicting design
objectives, ensuring optimal trade-offs between performance, power, and
structural reliability. Our experiments demonstrate that optimized
multi-chiplet architectures from our design framework achieve up to 64.7%
performance improvement and 40% power reduction compared to traditional 2.5D
systems to execute deep neural network workloads with lower fabrication costs.

</details>


### [18] [Sandwich: Separating Prefill-Decode Compilation for Efficient CPU LLM Serving](https://arxiv.org/abs/2507.18454)
*Juntao Zhao,Jiuru Li,Chuan Wu*

Main category: cs.AR

TL;DR: Sandwich是一种基于CPU的LLM服务引擎，针对预填充和解码阶段采用不同的执行计划，显著提升了吞吐量和延迟性能。


<details>
  <summary>Details</summary>
Motivation: 现有CPU解决方案忽略了LLM推理中预填充和解码阶段的工作负载差异，导致性能不佳。

Method: 提出Sandwich引擎，分别优化预填充和解码阶段的执行计划，并在多种CPU平台上进行评估。

Result: Sandwich在吞吐量、延迟性能和内核效率方面显著优于现有解决方案。

Conclusion: Sandwich通过硬件优化和动态执行计划，为CPU上的LLM服务提供了高效解决方案。

Abstract: Utilizing CPUs to serve large language models (LLMs) is a resource-friendly
alternative to GPU serving. Existing CPU-based solutions ignore workload
differences between the prefill and the decode phases of LLM inference,
applying a static per-NUMA (Non-Uniform Memory Access) node model partition and
utilizing vendor libraries for operator-level execution, which is suboptimal.
We propose Sandwich, a hardware-centric CPU-based LLM serving engine that uses
different execution plans for the prefill and decode phases and optimizes them
separately.
  We evaluate Sandwich across diverse baselines and datasets on five CPU
platforms, including x86 with AVX-2 and AVX-512, as well as ARM with NEON.
Sandwich achieves an average 2.01x throughput improvement and 90% satisfactory
time-to-first-token (TTFT) and time-per-output-token (TPOT) latencies with up
to 3.40x lower requirements in single sequence serving, and significant
improvement in Goodput in continuous-batching serving. The GEMM kernels
generated by Sandwich outperform representative vendor kernels and other
dynamic shape solutions, achieving performance comparable to static compilers
with three orders of magnitude less kernel tuning costs.

</details>


### [19] [PRACtical: Subarray-Level Counter Update and Bank-Level Recovery Isolation for Efficient PRAC Rowhammer Mitigation](https://arxiv.org/abs/2507.18581)
*Ravan Nazaraliyev,Saber Ganjisaffar,Nurlan Nazaraliyev,Nael Abu-Ghazaleh*

Main category: cs.AR

TL;DR: PRACtical优化了DDR5中的PRAC+ABO机制，通过减少计数器更新延迟和实现银行级粒度缓解，提升了性能并降低了能耗。


<details>
  <summary>Details</summary>
Motivation: 随着DRAM密度增加，Rowhammer问题加剧，现有DDR5标准中的PRAC机制虽能缓解但带来性能开销。

Method: 提出PRACtical方法：1）引入集中式增量电路减少计数器更新延迟；2）通过DRAM寄存器实现银行级粒度缓解。

Result: 性能平均提升8%（最高20%），能耗降低19%，攻击下的性能下降限制在6%以内。

Conclusion: PRACtical在保持Rowhammer防护的同时，显著优化了性能和能耗。

Abstract: As DRAM density increases, Rowhammer becomes more severe due to heightened
charge leakage, reducing the number of activations needed to induce bit flips.
The DDR5 standard addresses this threat with in-DRAM per-row activation
counters (PRAC) and the Alert Back-Off (ABO) signal to trigger mitigation.
However, PRAC adds performance overhead by incrementing counters during the
precharge phase, and recovery refreshes stalls the entire memory channel, even
if only one bank is under attack.
  We propose PRACtical, a performance-optimized approach to PRAC+ABO that
maintains the same security guarantees. First, we reduce counter update latency
by introducing a centralized increment circuit, enabling overlap between
counter updates and subsequent row activations in other subarrays. Second, we
enhance the $RFM_{ab}$ mitigation by enabling bank-level granularity: instead
of stalling the entire channel, only affected banks are paused. This is
achieved through a DRAM-resident register that identifies attacked banks.
  PRACtical improves performance by 8% on average (up to 20%) over the
state-of-the-art, reduces energy by 19%, and limits performance degradation
from aggressive performance attacks to less than 6%, all while preserving
Rowhammer protection.

</details>
