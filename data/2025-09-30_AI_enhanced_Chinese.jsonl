{"id": "2509.22679", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.22679", "abs": "https://arxiv.org/abs/2509.22679", "authors": ["Abdessalam Benhari", "Yves Denneulin", "Fr\u00e9d\u00e9ric Desprez", "Fanny Dufoss\u00e9", "Denis Trystram"], "title": "Analysis of the carbon footprint of HPC", "comment": null, "summary": "The demand in computing power has never stopped growing over the years.\nToday, the performance of the most powerful systems exceeds the exascale.\nUnfortunately, this growth also comes with ever-increasing energy costs,\nleading to a high carbon footprint. This paper investigates the evolution of\nhigh performance systems in terms of carbon emissions. A lot of studies focus\non Top500 (and Green500) as the tip of an iceberg to identify trends in the\ndomain in terms of computing performance. We propose here to go further in\nconsidering the whole span life of several large scale systems and to link the\nevolution with trajectory toward 2030. More precisely, we introduce the energy\nmix in the analysis of Top500 systems and we derive a predictive model for\nestimating the weight of HPC for the next 5 years.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u9ad8\u6027\u80fd\u8ba1\u7b97\u7cfb\u7edf\u7684\u78b3\u6392\u653e\u6f14\u53d8\uff0c\u901a\u8fc7\u5206\u6790Top500\u7cfb\u7edf\u7684\u80fd\u6e90\u7ed3\u6784\uff0c\u5efa\u7acb\u4e86\u9884\u6d4b\u672a\u67655\u5e74HPC\u78b3\u6392\u653e\u7684\u6a21\u578b\u3002", "motivation": "\u9ad8\u6027\u80fd\u8ba1\u7b97\u7cfb\u7edf\u7684\u8ba1\u7b97\u80fd\u529b\u6301\u7eed\u589e\u957f\uff0c\u4f46\u80fd\u6e90\u6210\u672c\u548c\u78b3\u8db3\u8ff9\u4e5f\u968f\u4e4b\u589e\u52a0\uff0c\u9700\u8981\u7814\u7a76\u5176\u78b3\u6392\u653e\u6f14\u53d8\u8d8b\u52bf\u3002", "method": "\u8003\u8651\u591a\u4e2a\u5927\u89c4\u6a21\u7cfb\u7edf\u7684\u5b8c\u6574\u751f\u547d\u5468\u671f\uff0c\u7ed3\u5408Top500\u7cfb\u7edf\u7684\u80fd\u6e90\u7ed3\u6784\u5206\u6790\uff0c\u5efa\u7acb\u9884\u6d4b\u6a21\u578b\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u9884\u6d4b\u672a\u67655\u5e74HPC\u78b3\u6392\u653e\u7684\u6a21\u578b\uff0c\u5e76\u5206\u6790\u4e86\u54112030\u5e74\u53d1\u5c55\u7684\u8f68\u8ff9\u3002", "conclusion": "\u9ad8\u6027\u80fd\u8ba1\u7b97\u7cfb\u7edf\u7684\u78b3\u6392\u653e\u95ee\u9898\u65e5\u76ca\u91cd\u8981\uff0c\u9700\u8981\u7efc\u5408\u8003\u8651\u80fd\u6e90\u7ed3\u6784\u548c\u751f\u547d\u5468\u671f\u6765\u8bc4\u4f30\u5176\u73af\u5883\u5f71\u54cd\u3002"}}
{"id": "2509.22681", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.22681", "abs": "https://arxiv.org/abs/2509.22681", "authors": ["Xianwen Guo", "Bin Huang", "Xiaomeng Wu", "Guanlin Wu", "Fangjian Li", "Shijia Wang", "Qiang Xiao", "Chuanjiang Luo", "Yong Li"], "title": "FLAME: A Serving System Optimized for Large-Scale Generative Recommendation with Efficiency", "comment": null, "summary": "Generative recommendation (GR) models possess greater scaling power compared\nto traditional deep learning recommendation models (DLRMs), yet they also\nimpose a tremendous increase in computational burden. Measured in FLOPs, a\ntypical GR model's workload sits in $10^9 \\sim 10^{11}$ range, roughly four\norders of magnitude higher than traditional DLRMs. Delivering accurate results\nin a few tens of milliseconds while processing billions of such requests per\nday puts extreme demands on the performance of the online serving system.\nTherefore, for industry practitioners, the alluring gains of GR models are\ntempered by the formidable challenge of online deployment at scale in\nproduction services. In this work, we introduce a comprehensive solution of\nonline serving system tailored For Large-scale GenerAtive RecoMmendation with\nEfficiency (FLAME). Specifically, we leveraging CPU-GPU heterogeneous hardware\nto decouple feature pre-processing and model computation. We encapsulated\nseveral memory optimization features as the Proximal Data Accelerator (PDA)\nmodule to make full use of limited bandwidth and storage resources, which\nachieves a 1.9x throughput gain and a 1.7x latency reduction. We implement the\nFused Kernel Engine (FKE) module based on the functionality and interface of\nNVIDIA TensorRT to boost model computation, delivering a speedup ratio of\n4.6x-6.1x, throughput gain ratio of 4.7x-6.3x one step further. In addition, we\ndesign the Dynamic Stream Orchestrator (DSO) module to coordinate concurrent\nrequests, enhancing the system throughput performance with 1.3x improvement in\nthroughput and 2.3x speed-up under non-uniform distribution of upstream\ncandidates. Comprehensive evaluations demonstrate that our FLAME effectively\nsupports large-scale online deployment of GR models and achieves remarkable\nimprovements in system performance.", "AI": {"tldr": "FLAME\u662f\u4e00\u4e2a\u9488\u5bf9\u5927\u89c4\u6a21\u751f\u6210\u5f0f\u63a8\u8350\u6a21\u578b\u7684\u9ad8\u6548\u5728\u7ebf\u670d\u52a1\u7cfb\u7edf\uff0c\u901a\u8fc7CPU-GPU\u5f02\u6784\u786c\u4ef6\u3001\u5185\u5b58\u4f18\u5316\u548c\u52a8\u6001\u6d41\u534f\u8c03\u7b49\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u541e\u5410\u91cf\u548c\u5ef6\u8fdf\u6027\u80fd\u3002", "motivation": "\u751f\u6210\u5f0f\u63a8\u8350\u6a21\u578b\u76f8\u6bd4\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u63a8\u8350\u6a21\u578b\u5177\u6709\u66f4\u5f3a\u7684\u6269\u5c55\u80fd\u529b\uff0c\u4f46\u8ba1\u7b97\u8d1f\u62c5\u589e\u52a0\u4e864\u4e2a\u6570\u91cf\u7ea7\uff0c\u5728\u4fdd\u6301\u6beb\u79d2\u7ea7\u54cd\u5e94\u65f6\u95f4\u7684\u540c\u65f6\u5904\u7406\u6570\u5341\u4ebf\u8bf7\u6c42\u5bf9\u5728\u7ebf\u670d\u52a1\u7cfb\u7edf\u63d0\u51fa\u4e86\u6781\u9ad8\u8981\u6c42\u3002", "method": "\u91c7\u7528CPU-GPU\u5f02\u6784\u786c\u4ef6\u89e3\u8026\u7279\u5f81\u9884\u5904\u7406\u548c\u6a21\u578b\u8ba1\u7b97\uff1b\u4f7f\u7528Proximal Data Accelerator\u6a21\u5757\u8fdb\u884c\u5185\u5b58\u4f18\u5316\uff1b\u57fa\u4e8eNVIDIA TensorRT\u5b9e\u73b0Fused Kernel Engine\u6a21\u5757\u52a0\u901f\u6a21\u578b\u8ba1\u7b97\uff1b\u8bbe\u8ba1Dynamic Stream Orchestrator\u6a21\u5757\u534f\u8c03\u5e76\u53d1\u8bf7\u6c42\u3002", "result": "PDA\u6a21\u5757\u5b9e\u73b01.9\u500d\u541e\u5410\u91cf\u589e\u76ca\u548c1.7\u500d\u5ef6\u8fdf\u964d\u4f4e\uff1bFKE\u6a21\u5757\u63d0\u4f9b4.6x-6.1x\u52a0\u901f\u6bd4\u548c4.7x-6.3x\u541e\u5410\u91cf\u589e\u76ca\uff1bDSO\u6a21\u5757\u5728\u975e\u5747\u5300\u5206\u5e03\u573a\u666f\u4e0b\u5b9e\u73b01.3\u500d\u541e\u5410\u91cf\u63d0\u5347\u548c2.3\u500d\u52a0\u901f\u3002", "conclusion": "FLAME\u7cfb\u7edf\u6709\u6548\u652f\u6301\u5927\u89c4\u6a21\u751f\u6210\u5f0f\u63a8\u8350\u6a21\u578b\u7684\u5728\u7ebf\u90e8\u7f72\uff0c\u5728\u7cfb\u7edf\u6027\u80fd\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\u3002"}}
{"id": "2509.22684", "categories": ["cs.DC", "cs.AR", "cs.CR", "cs.PF"], "pdf": "https://arxiv.org/pdf/2509.22684", "abs": "https://arxiv.org/abs/2509.22684", "authors": ["Tarunesh Verma", "Yichao Yuan", "Nishil Talati", "Todd Austin"], "title": "ZKProphet: Understanding Performance of Zero-Knowledge Proofs on GPUs", "comment": "To appear at 2025 IEEE International Symposium on Workload\n  Characterization", "summary": "Zero-Knowledge Proofs (ZKP) are protocols which construct cryptographic\nproofs to demonstrate knowledge of a secret input in a computation without\nrevealing any information about the secret. ZKPs enable novel applications in\nprivate and verifiable computing such as anonymized cryptocurrencies and\nblockchain scaling and have seen adoption in several real-world systems. Prior\nwork has accelerated ZKPs on GPUs by leveraging the inherent parallelism in\ncore computation kernels like Multi-Scalar Multiplication (MSM). However, we\nfind that a systematic characterization of execution bottlenecks in ZKPs, as\nwell as their scalability on modern GPU architectures, is missing in the\nliterature. This paper presents ZKProphet, a comprehensive performance study of\nZero-Knowledge Proofs on GPUs. Following massive speedups of MSM, we find that\nZKPs are bottlenecked by kernels like Number-Theoretic Transform (NTT), as they\naccount for up to 90% of the proof generation latency on GPUs when paired with\noptimized MSM implementations. Available NTT implementations under-utilize GPU\ncompute resources and often do not employ architectural features like\nasynchronous compute and memory operations. We observe that the arithmetic\noperations underlying ZKPs execute exclusively on the GPU's 32-bit integer\npipeline and exhibit limited instruction-level parallelism due to data\ndependencies. Their performance is thus limited by the available integer\ncompute units. While one way to scale the performance of ZKPs is adding more\ncompute units, we discuss how runtime parameter tuning for optimizations like\nprecomputed inputs and alternative data representations can extract additional\nspeedup. With this work, we provide the ZKP community a roadmap to scale\nperformance on GPUs and construct definitive GPU-accelerated ZKPs for their\napplication requirements and available hardware resources.", "AI": {"tldr": "ZKProphet\uff1a\u5bf9GPU\u4e0a\u96f6\u77e5\u8bc6\u8bc1\u660e\u6027\u80fd\u7684\u5168\u9762\u7814\u7a76\uff0c\u53d1\u73b0NTT\u5185\u6838\u6210\u4e3a\u4e3b\u8981\u74f6\u9888\uff0c\u5e76\u63d0\u51fa\u901a\u8fc7\u53c2\u6570\u8c03\u4f18\u548c\u67b6\u6784\u4f18\u5316\u6765\u63d0\u5347\u6027\u80fd\u7684\u8def\u7ebf\u56fe", "motivation": "\u73b0\u6709\u6587\u732e\u7f3a\u4e4f\u5bf9\u96f6\u77e5\u8bc6\u8bc1\u660e\u5728GPU\u67b6\u6784\u4e0a\u6267\u884c\u74f6\u9888\u548c\u53ef\u6269\u5c55\u6027\u7684\u7cfb\u7edf\u8868\u5f81\uff0c\u7279\u522b\u662f\u5728MSM\u5927\u5e45\u52a0\u901f\u540e\uff0cNTT\u7b49\u5185\u6838\u6210\u4e3a\u65b0\u7684\u6027\u80fd\u74f6\u9888", "method": "\u901a\u8fc7\u7cfb\u7edf\u6027\u80fd\u5206\u6790\uff0c\u8bc6\u522bZKPs\u5728GPU\u4e0a\u7684\u6267\u884c\u74f6\u9888\uff0c\u5206\u6790NTT\u7b49\u5185\u6838\u7684\u8d44\u6e90\u5229\u7528\u4e0d\u8db3\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u8fd0\u884c\u65f6\u53c2\u6570\u8c03\u4f18\u3001\u9884\u8ba1\u7b97\u8f93\u5165\u548c\u66ff\u4ee3\u6570\u636e\u8868\u793a\u7b49\u4f18\u5316\u65b9\u6cd5", "result": "\u53d1\u73b0NTT\u5185\u6838\u5360\u7528GPU\u8bc1\u660e\u751f\u6210\u5ef6\u8fdf\u768490%\uff0c\u73b0\u6709\u5b9e\u73b0\u672a\u5145\u5206\u5229\u7528GPU\u8ba1\u7b97\u8d44\u6e90\uff0c\u7b97\u672f\u64cd\u4f5c\u4ec5\u9650\u4e8e32\u4f4d\u6574\u6570\u6d41\u6c34\u7ebf\u4e14\u6307\u4ee4\u7ea7\u5e76\u884c\u6027\u6709\u9650", "conclusion": "\u4e3aZKP\u793e\u533a\u63d0\u4f9b\u4e86\u5728GPU\u4e0a\u6269\u5c55\u6027\u80fd\u7684\u8def\u7ebf\u56fe\uff0c\u901a\u8fc7\u53c2\u6570\u8c03\u4f18\u548c\u67b6\u6784\u4f18\u5316\u53ef\u4ee5\u6784\u5efa\u9488\u5bf9\u7279\u5b9a\u5e94\u7528\u9700\u6c42\u548c\u786c\u4ef6\u8d44\u6e90\u7684GPU\u52a0\u901fZKP"}}
{"id": "2509.22701", "categories": ["cs.DC", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.22701", "abs": "https://arxiv.org/abs/2509.22701", "authors": ["Leszek Sliwko", "Jolanta Mizera-Pietraszko"], "title": "Enhancing Cluster Scheduling in HPC: A Continuous Transfer Learning for Real-Time Optimization", "comment": "This is the accepted version of the paper published in 2025 IEEE\n  International Parallel and Distributed Processing Symposium Workshops\n  (IPDPSW). The final version is available at:\n  https://doi.org/10.1109/IPDPSW66978.2025.00056", "summary": "This study presents a machine learning-assisted approach to optimize task\nscheduling in cluster systems, focusing on node-affinity constraints.\nTraditional schedulers like Kubernetes struggle with real-time adaptability,\nwhereas the proposed continuous transfer learning model evolves dynamically\nduring operations, minimizing retraining needs. Evaluated on Google Cluster\nData, the model achieves over 99% accuracy, reducing computational overhead and\nimproving scheduling latency for constrained tasks. This scalable solution\nenables real-time optimization, advancing machine learning integration in\ncluster management and paving the way for future adaptive scheduling\nstrategies.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u96c6\u7fa4\u4efb\u52a1\u8c03\u5ea6\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u6301\u7eed\u8fc1\u79fb\u5b66\u4e60\u52a8\u6001\u9002\u5e94\u8282\u70b9\u4eb2\u548c\u6027\u7ea6\u675f\uff0c\u5728Google\u96c6\u7fa4\u6570\u636e\u4e0a\u5b9e\u73b099%\u4ee5\u4e0a\u51c6\u786e\u7387\uff0c\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\u5e76\u6539\u5584\u8c03\u5ea6\u5ef6\u8fdf\u3002", "motivation": "\u4f20\u7edf\u8c03\u5ea6\u5668\uff08\u5982Kubernetes\uff09\u5728\u5b9e\u65f6\u9002\u5e94\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u65e0\u6cd5\u6709\u6548\u5904\u7406\u8282\u70b9\u4eb2\u548c\u6027\u7ea6\u675f\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u52a8\u6001\u6f14\u5316\u7684\u8c03\u5ea6\u4f18\u5316\u65b9\u6848\u3002", "method": "\u91c7\u7528\u6301\u7eed\u8fc1\u79fb\u5b66\u4e60\u6a21\u578b\uff0c\u5728\u64cd\u4f5c\u8fc7\u7a0b\u4e2d\u52a8\u6001\u6f14\u5316\uff0c\u6700\u5c0f\u5316\u91cd\u65b0\u8bad\u7ec3\u9700\u6c42\uff0c\u4e13\u6ce8\u4e8e\u8282\u70b9\u4eb2\u548c\u6027\u7ea6\u675f\u7684\u4f18\u5316\u3002", "result": "\u5728Google\u96c6\u7fa4\u6570\u636e\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u6a21\u578b\u8fbe\u5230\u8d85\u8fc799%\u7684\u51c6\u786e\u7387\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u5f00\u9500\uff0c\u6539\u5584\u4e86\u53d7\u9650\u4efb\u52a1\u7684\u8c03\u5ea6\u5ef6\u8fdf\u3002", "conclusion": "\u8be5\u53ef\u6269\u5c55\u89e3\u51b3\u65b9\u6848\u5b9e\u73b0\u4e86\u5b9e\u65f6\u4f18\u5316\uff0c\u63a8\u52a8\u4e86\u673a\u5668\u5b66\u4e60\u5728\u96c6\u7fa4\u7ba1\u7406\u4e2d\u7684\u96c6\u6210\uff0c\u4e3a\u672a\u6765\u81ea\u9002\u5e94\u8c03\u5ea6\u7b56\u7565\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2509.22980", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2509.22980", "abs": "https://arxiv.org/abs/2509.22980", "authors": ["Jingyao Zhang", "Elaheh Sadredini"], "title": "\\textit{No One-Size-Fits-All}: A Workload-Driven Characterization of Bit-Parallel vs. Bit-Serial Data Layouts for Processing-using-Memory", "comment": null, "summary": "Processing-in-Memory (PIM) is a promising approach to overcoming the\nmemory-wall bottleneck. However, the PIM community has largely treated its two\nfundamental data layouts, Bit-Parallel (BP) and Bit-Serial (BS), as if they\nwere interchangeable. This implicit \"one-layout-fits-all\" assumption, often\nhard-coded into existing evaluation frameworks, creates a critical gap:\narchitects lack systematic, workload-driven guidelines for choosing the optimal\ndata layout for their target applications.\n  To address this gap, this paper presents the first systematic,\nworkload-driven characterization of BP and BS PIM architectures. We develop\niso-area, cycle-accurate BP and BS PIM architectural models and conduct a\ncomprehensive evaluation using a diverse set of benchmarks. Our suite includes\nboth fine-grained microworkloads from MIMDRAM to isolate specific operational\ncharacteristics, and large-scale applications from the PIMBench suite, such as\nthe VGG network, to represent realistic end-to-end workloads.\n  Our results quantitatively demonstrate that no single layout is universally\nsuperior; the optimal choice is strongly dependent on workload characteristics.\nBP excels on control-flow-intensive tasks with irregular memory access\npatterns, whereas BS shows substantial advantages in massively parallel,\nlow-precision (e.g., INT4/INT8) computations common in AI. Based on this\ncharacterization, we distill a set of actionable design guidelines for\narchitects. This work challenges the prevailing one-size-fits-all view on PIM\ndata layouts and provides a principled foundation for designing\nnext-generation, workload-aware, and potentially hybrid PIM systems.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u7cfb\u7edf\u6027\u5730\u5206\u6790\u4e86PIM\u67b6\u6784\u4e2d\u7684\u4e24\u79cd\u57fa\u672c\u6570\u636e\u5e03\u5c40\uff08\u4f4d\u5e76\u884cBP\u548c\u4f4d\u4e32\u884cBS\uff09\uff0c\u53d1\u73b0\u6ca1\u6709\u5355\u4e00\u5e03\u5c40\u9002\u7528\u4e8e\u6240\u6709\u573a\u666f\uff0c\u6700\u4f18\u9009\u62e9\u53d6\u51b3\u4e8e\u5de5\u4f5c\u8d1f\u8f7d\u7279\u6027\u3002", "motivation": "PIM\u793e\u533a\u957f\u671f\u4ee5\u6765\u5c06BP\u548cBS\u5e03\u5c40\u89c6\u4e3a\u53ef\u4e92\u6362\u7684\uff0c\u8fd9\u79cd\"\u4e00\u79cd\u5e03\u5c40\u9002\u7528\u6240\u6709\"\u7684\u5047\u8bbe\u7f3a\u4e4f\u7cfb\u7edf\u6027\u7684\u5de5\u4f5c\u8d1f\u8f7d\u9a71\u52a8\u6307\u5bfc\uff0c\u5bfc\u81f4\u67b6\u6784\u5e08\u65e0\u6cd5\u4e3a\u7279\u5b9a\u5e94\u7528\u9009\u62e9\u6700\u4f18\u6570\u636e\u5e03\u5c40\u3002", "method": "\u5f00\u53d1\u4e86\u7b49\u9762\u79ef\u3001\u5468\u671f\u7cbe\u786e\u7684BP\u548cBS PIM\u67b6\u6784\u6a21\u578b\uff0c\u4f7f\u7528\u591a\u6837\u5316\u57fa\u51c6\u6d4b\u8bd5\u5957\u4ef6\u8fdb\u884c\u7efc\u5408\u8bc4\u4f30\uff0c\u5305\u62ecMIMDRAM\u7684\u7ec6\u7c92\u5ea6\u5fae\u5de5\u4f5c\u8d1f\u8f7d\u548cPIMBench\u5957\u4ef6\u7684\u5927\u89c4\u6a21\u5e94\u7528\uff08\u5982VGG\u7f51\u7edc\uff09\u3002", "result": "BP\u5728\u63a7\u5236\u6d41\u5bc6\u96c6\u578b\u4efb\u52a1\u548c\u4e0d\u89c4\u5219\u5185\u5b58\u8bbf\u95ee\u6a21\u5f0f\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u800cBS\u5728AI\u5e38\u89c1\u7684\u5927\u89c4\u6a21\u5e76\u884c\u3001\u4f4e\u7cbe\u5ea6\u8ba1\u7b97\uff08\u5982INT4/INT8\uff09\u4e2d\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002", "conclusion": "\u6311\u6218\u4e86PIM\u6570\u636e\u5e03\u5c40\u7684\u901a\u7528\u9002\u7528\u6027\u89c2\u70b9\uff0c\u4e3a\u8bbe\u8ba1\u4e0b\u4e00\u4ee3\u5de5\u4f5c\u8d1f\u8f7d\u611f\u77e5\u548c\u6f5c\u5728\u6df7\u5408PIM\u7cfb\u7edf\u63d0\u4f9b\u4e86\u539f\u5219\u6027\u57fa\u7840\uff0c\u5e76\u63d0\u70bc\u51fa\u4e00\u5957\u53ef\u64cd\u4f5c\u7684\u8bbe\u8ba1\u6307\u5357\u3002"}}
{"id": "2509.22982", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2509.22982", "abs": "https://arxiv.org/abs/2509.22982", "authors": ["David M Kahn", "Jan Hoffmann", "Thomas Reps", "Jessie Grosen"], "title": "Efficient Cost Bounds with Linear Maps", "comment": null, "summary": "The Automatic Amortized Resource Analysis (AARA) derives program-execution\ncost bounds using types. To do so, AARA often makes use of cost-free types,\nwhich are critical for the composition of types and cost bounds. However,\ninferring cost-free types using the current state-of-the-art algorithm is\nexpensive due to recursive dependence on additional cost-free types.\nFurthermore, that algorithm uses a heuristic only applicable to polynomial cost\nbounds, and not, e.g., exponential bounds. This paper presents a new approach\nto these problems by representing the cost-free types of a function in a new\nway: with a linear map, which can stand for infinitely many cost-free types.\nSuch maps enable an algebraic flavor of reasoning about cost bounds (including\nnon-polynomial bounds) via matrix inequalities. These inequalities can be\nsolved with off-the-shelf linear-programming tools for many programs, so that\ntypes can always be efficiently checked and often be efficiently inferred. An\nexperimental evaluation with a prototype implementation shows that-when it is\napplicable-the inference of linear maps is exponentially more efficient than\nthe state-of-the-art algorithm.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7ebf\u6027\u6620\u5c04\u65b9\u6cd5\u6765\u8868\u793a\u51fd\u6570\u7684\u65e0\u6210\u672c\u7c7b\u578b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709AARA\u65b9\u6cd5\u4e2d\u65e0\u6210\u672c\u7c7b\u578b\u63a8\u65ad\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\uff0c\u652f\u6301\u591a\u9879\u5f0f\u548c\u975e\u591a\u9879\u5f0f\u6210\u672c\u8fb9\u754c\u5206\u6790\u3002", "motivation": "\u73b0\u6709\u7684\u81ea\u52a8\u644a\u9500\u8d44\u6e90\u5206\u6790(AARA)\u65b9\u6cd5\u5728\u63a8\u65ad\u65e0\u6210\u672c\u7c7b\u578b\u65f6\u6548\u7387\u4f4e\u4e0b\uff0c\u5b58\u5728\u9012\u5f52\u4f9d\u8d56\u95ee\u9898\uff0c\u4e14\u73b0\u6709\u542f\u53d1\u5f0f\u65b9\u6cd5\u4ec5\u9002\u7528\u4e8e\u591a\u9879\u5f0f\u6210\u672c\u8fb9\u754c\uff0c\u65e0\u6cd5\u5904\u7406\u6307\u6570\u7ea7\u7b49\u975e\u591a\u9879\u5f0f\u8fb9\u754c\u3002", "method": "\u4f7f\u7528\u7ebf\u6027\u6620\u5c04\u6765\u8868\u793a\u51fd\u6570\u7684\u65e0\u6210\u672c\u7c7b\u578b\uff0c\u8fd9\u79cd\u6620\u5c04\u53ef\u4ee5\u4ee3\u8868\u65e0\u9650\u591a\u4e2a\u65e0\u6210\u672c\u7c7b\u578b\u3002\u901a\u8fc7\u77e9\u9635\u4e0d\u7b49\u5f0f\u8fdb\u884c\u4ee3\u6570\u63a8\u7406\uff0c\u5229\u7528\u73b0\u6210\u7684\u7ebf\u6027\u89c4\u5212\u5de5\u5177\u6c42\u89e3\u6210\u672c\u8fb9\u754c\u95ee\u9898\u3002", "result": "\u539f\u578b\u5b9e\u73b0\u5b9e\u9a8c\u8868\u660e\uff0c\u5f53\u9002\u7528\u65f6\uff0c\u7ebf\u6027\u6620\u5c04\u63a8\u65ad\u6bd4\u73b0\u6709\u6700\u5148\u8fdb\u7b97\u6cd5\u6548\u7387\u5448\u6307\u6570\u7ea7\u63d0\u5347\uff0c\u80fd\u591f\u9ad8\u6548\u68c0\u67e5\u548c\u63a8\u65ad\u7c7b\u578b\u3002", "conclusion": "\u7ebf\u6027\u6620\u5c04\u65b9\u6cd5\u4e3aAARA\u63d0\u4f9b\u4e86\u66f4\u9ad8\u6548\u7684\u65e0\u6210\u672c\u7c7b\u578b\u8868\u793a\u548c\u63a8\u65ad\u673a\u5236\uff0c\u652f\u6301\u66f4\u5e7f\u6cdb\u7684\u6210\u672c\u8fb9\u754c\u5206\u6790\uff0c\u5305\u62ec\u975e\u591a\u9879\u5f0f\u8fb9\u754c\u3002"}}
{"id": "2509.22704", "categories": ["cs.DC", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.22704", "abs": "https://arxiv.org/abs/2509.22704", "authors": ["Leszek Sliwko"], "title": "Intelligent Load Balancing in Cloud Computer Systems", "comment": "A thesis submitted in partial fulfilment of the requirements of the\n  University of Westminster for the degree of Doctor of Philosophy", "summary": "Cloud computing is an established technology allowing users to share\nresources on a large scale, never before seen in IT history. A cloud system\nconnects multiple individual servers in order to process related tasks in\nseveral environments at the same time. Clouds are typically more cost-effective\nthan single computers of comparable computing performance. The sheer physical\nsize of the system itself means that thousands of machines may be involved. The\nfocus of this research was to design a strategy to dynamically allocate tasks\nwithout overloading Cloud nodes which would result in system stability being\nmaintained at minimum cost. This research has added the following new\ncontributions to the state of knowledge: (i) a novel taxonomy and\ncategorisation of three classes of schedulers, namely OS-level, Cluster and Big\nData, which highlight their unique evolution and underline their different\nobjectives; (ii) an abstract model of cloud resources utilisation is specified,\nincluding multiple types of resources and consideration of task migration\ncosts; (iii) a virtual machine live migration was experimented with in order to\ncreate a formula which estimates the network traffic generated by this process;\n(iv) a high-fidelity Cloud workload simulator, based on a month-long workload\ntraces from Google's computing cells, was created; (v) two possible approaches\nto resource management were proposed and examined in the practical part of the\nmanuscript: the centralised metaheuristic load balancer and the decentralised\nagent-based system. The project involved extensive experiments run on the\nUniversity of Westminster HPC cluster, and the promising results are presented\ntogether with detailed discussions and a conclusion.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u4efb\u52a1\u5206\u914d\u7b56\u7565\uff0c\u4ee5\u9632\u6b62\u4e91\u8282\u70b9\u8fc7\u8f7d\u5e76\u7ef4\u6301\u7cfb\u7edf\u7a33\u5b9a\u6027\uff0c\u540c\u65f6\u6700\u5c0f\u5316\u6210\u672c\u3002\u7814\u7a76\u5305\u62ec\u8c03\u5ea6\u5668\u5206\u7c7b\u3001\u4e91\u8d44\u6e90\u5229\u7528\u6a21\u578b\u3001\u865a\u62df\u673a\u8fc1\u79fb\u7f51\u7edc\u6d41\u91cf\u4f30\u7b97\u3001\u9ad8\u4fdd\u771f\u4e91\u5de5\u4f5c\u8d1f\u8f7d\u6a21\u62df\u5668\u4ee5\u53ca\u4e24\u79cd\u8d44\u6e90\u7ba1\u7406\u65b9\u6cd5\u3002", "motivation": "\u4e91\u8ba1\u7b97\u7cfb\u7edf\u8fde\u63a5\u5927\u91cf\u670d\u52a1\u5668\u5904\u7406\u76f8\u5173\u4efb\u52a1\uff0c\u867d\u7136\u6bd4\u5355\u673a\u66f4\u5177\u6210\u672c\u6548\u76ca\uff0c\u4f46\u7cfb\u7edf\u89c4\u6a21\u5e9e\u5927\u53ef\u80fd\u5bfc\u81f4\u8282\u70b9\u8fc7\u8f7d\u3002\u7814\u7a76\u65e8\u5728\u8bbe\u8ba1\u52a8\u6001\u4efb\u52a1\u5206\u914d\u7b56\u7565\uff0c\u5728\u7ef4\u6301\u7cfb\u7edf\u7a33\u5b9a\u6027\u7684\u540c\u65f6\u6700\u5c0f\u5316\u6210\u672c\u3002", "method": "1) \u63d0\u51fa\u4e09\u7c7b\u8c03\u5ea6\u5668\uff08OS\u7ea7\u3001\u96c6\u7fa4\u548c\u5927\u6570\u636e\uff09\u7684\u5206\u7c7b\u6cd5\uff1b2) \u5efa\u7acb\u5305\u542b\u591a\u79cd\u8d44\u6e90\u548c\u4efb\u52a1\u8fc1\u79fb\u6210\u672c\u7684\u4e91\u8d44\u6e90\u5229\u7528\u62bd\u8c61\u6a21\u578b\uff1b3) \u901a\u8fc7\u865a\u62df\u673a\u5b9e\u65f6\u8fc1\u79fb\u5b9e\u9a8c\u4f30\u7b97\u7f51\u7edc\u6d41\u91cf\uff1b4) \u57fa\u4e8eGoogle\u8ba1\u7b97\u5355\u5143\u4e00\u4e2a\u6708\u5de5\u4f5c\u8d1f\u8f7d\u75d5\u8ff9\u521b\u5efa\u9ad8\u4fdd\u771f\u4e91\u5de5\u4f5c\u8d1f\u8f7d\u6a21\u62df\u5668\uff1b5) \u63d0\u51fa\u5e76\u6d4b\u8bd5\u96c6\u4e2d\u5f0f\u5143\u542f\u53d1\u5f0f\u8d1f\u8f7d\u5747\u8861\u5668\u548c\u5206\u6563\u5f0f\u57fa\u4e8e\u4ee3\u7406\u7cfb\u7edf\u4e24\u79cd\u8d44\u6e90\u7ba1\u7406\u65b9\u6cd5\u3002", "result": "\u5728\u5a01\u65af\u654f\u65af\u7279\u5927\u5b66HPC\u96c6\u7fa4\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u83b7\u5f97\u4e86\u6709\u524d\u666f\u7684\u7ed3\u679c\uff0c\u8868\u660e\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u7ba1\u7406\u4e91\u8d44\u6e90\u5e76\u7ef4\u6301\u7cfb\u7edf\u7a33\u5b9a\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u6210\u529f\u5f00\u53d1\u4e86\u52a8\u6001\u4efb\u52a1\u5206\u914d\u7b56\u7565\uff0c\u901a\u8fc7\u5206\u7c7b\u8c03\u5ea6\u5668\u3001\u5efa\u7acb\u8d44\u6e90\u6a21\u578b\u3001\u4f30\u7b97\u8fc1\u79fb\u6210\u672c\u3001\u521b\u5efa\u6a21\u62df\u5668\u4ee5\u53ca\u6d4b\u8bd5\u4e24\u79cd\u7ba1\u7406\u65b9\u6cd5\uff0c\u4e3a\u4e91\u8ba1\u7b97\u8d44\u6e90\u7ba1\u7406\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u5728\u7ef4\u6301\u7cfb\u7edf\u7a33\u5b9a\u6027\u7684\u540c\u65f6\u6700\u5c0f\u5316\u6210\u672c\u3002"}}
{"id": "2509.22999", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2509.22999", "abs": "https://arxiv.org/abs/2509.22999", "authors": ["Sachin Sachdeva", "Jincong Lu", "Wantong Li", "Sheldon X. -D. Tan"], "title": "Enhanced Hybrid Temporal Computing Using Deterministic Summations for Ultra-Low-Power Accelerators", "comment": "8 pages", "summary": "This paper presents an accuracy-enhanced Hybrid Temporal Computing (E-HTC)\nframework for ultra-low-power hardware accelerators with deterministic\nadditions. Inspired by the recently proposed HTC architecture, which leverages\npulse-rate and temporal data encoding to reduce switching activity and energy\nconsumption but loses accuracy due to its multiplexer (MUX)-based scaled\naddition, we propose two bitstream addition schemes: (1) an Exact\nMultiple-input Binary Accumulator (EMBA), which performs precise binary\naccumulation, and (2) a Deterministic Threshold-based Scaled Adder (DTSA),\nwhich employs threshold logic for scaled addition. These adders are integrated\ninto a multiplier accumulator (MAC) unit supporting both unipolar and bipolar\nencodings. To validate the framework, we implement two accelerators: a Finite\nImpulse Response (FIR) filter and an 8-point Discrete Cosine Transform\n(DCT)/iDCT engine. Results on a 4x4 MAC show that, in unipolar mode, E-HTC\nmatches the RMSE of state-of-the-art Counter-Based Stochastic Computing (CBSC)\nMAC, improves accuracy by 94% over MUX-based HTC, and reduces power and area by\n23% and 7% compared to MUX-based HTC and 64% and 74% compared to CBSC. In\nbipolar mode, E-HTC MAC achieves 2.09% RMSE -- an 83% improvement over\nMUX-based HTC -- and approaches CBSC's 1.40% RMSE with area and power savings\nof 28% and 43% vs. MUX-based HTC and about 76% vs. CBSC. In FIR experiments,\nboth E-HTC variants yield PSNR gains of 3--5 dB (30--45% RMSE reduction) while\nsaving 13% power and 3% area. For DCT/iDCT, E-HTC boosts PSNR by 10--13 dB\n(70--75% RMSE reduction) while saving area and power over both MUX- and\nCBSC-based designs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u7cbe\u5ea6\u589e\u5f3a\u7684\u6df7\u5408\u65f6\u5e8f\u8ba1\u7b97\u6846\u67b6E-HTC\uff0c\u901a\u8fc7\u4e24\u79cd\u65b0\u578b\u52a0\u6cd5\u5668\u65b9\u6848\uff08EMBA\u548cDTSA\uff09\u6539\u8fdb\u4e86HTC\u67b6\u6784\u7684\u7cbe\u5ea6\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u4f4e\u529f\u8017\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u8ba1\u7b97\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709HTC\u67b6\u6784\u5229\u7528\u8109\u51b2\u7387\u548c\u65f6\u5e8f\u6570\u636e\u7f16\u7801\u964d\u4f4e\u5f00\u5173\u6d3b\u52a8\u548c\u80fd\u8017\uff0c\u4f46\u56e0\u5176\u57fa\u4e8eMUX\u7684\u7f29\u653e\u52a0\u6cd5\u5bfc\u81f4\u7cbe\u5ea6\u635f\u5931\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u7cbe\u786e\u7684\u52a0\u6cd5\u65b9\u6848\u3002", "method": "\u63d0\u51faEMBA\u7cbe\u786e\u591a\u8f93\u5165\u4e8c\u8fdb\u5236\u7d2f\u52a0\u5668\u548cDTSA\u786e\u5b9a\u6027\u9608\u503c\u7f29\u653e\u52a0\u6cd5\u5668\uff0c\u96c6\u6210\u5230\u652f\u6301\u5355\u6781\u548c\u53cc\u6781\u7f16\u7801\u7684MAC\u5355\u5143\u4e2d\uff0c\u5e76\u5728FIR\u6ee4\u6ce2\u5668\u548cDCT/iDCT\u5f15\u64ce\u4e0a\u9a8c\u8bc1\u3002", "result": "\u57284x4 MAC\u4e2d\uff0c\u5355\u6781\u6a21\u5f0f\u4e0bE-HTC\u4e0eCBSC\u7684RMSE\u76f8\u5f53\uff0c\u6bd4MUX-HTC\u7cbe\u5ea6\u63d0\u534794%\uff0c\u529f\u8017\u548c\u9762\u79ef\u5206\u522b\u51cf\u5c1123%\u548c7%\uff1b\u53cc\u6781\u6a21\u5f0f\u4e0bRMSE\u4e3a2.09%\uff0c\u6bd4MUX-HTC\u63d0\u534783%\u3002\u5728FIR\u548cDCT/iDCT\u5b9e\u9a8c\u4e2d\u5747\u83b7\u5f97\u663e\u8457PSNR\u63d0\u5347\u548c\u529f\u8017\u9762\u79ef\u8282\u7701\u3002", "conclusion": "E-HTC\u6846\u67b6\u6210\u529f\u89e3\u51b3\u4e86HTC\u67b6\u6784\u7684\u7cbe\u5ea6\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u8d85\u4f4e\u529f\u8017\u4f18\u52bf\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u63a5\u8fd1CBSC\u7684\u7cbe\u5ea6\u6c34\u5e73\uff0c\u4e3a\u8d85\u4f4e\u529f\u8017\u786c\u4ef6\u52a0\u901f\u5668\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.23061", "categories": ["cs.PL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23061", "abs": "https://arxiv.org/abs/2509.23061", "authors": ["Xu Xu", "Xin Li", "Xingwei Qu", "Jie Fu", "Binhang Yuan"], "title": "Local Success Does Not Compose: Benchmarking Large Language Models for Compositional Formal Verification", "comment": null, "summary": "We introduce DafnyCOMP, a benchmark for evaluating large language models\n(LLMs) on compositional specification generation in Dafny. Unlike prior\nbenchmarks that focus on single-function tasks, DafnyCOMP targets programs\ncomposed of multiple interacting functions with data dependencies, requiring\nreasoning across component boundaries. The benchmark consists of 300\nautomatically synthesized multi-function programs. We evaluate several\nstate-of-the-art LLM families and find that, while they perform well on\nsingle-function verification, their performance drops sharply on compositional\ntasks. Analysis reveals systematic failures in cross-functional reasoning,\nincluding fragile specifications, misalignment between implementations and\nproofs, and unstable reasoning. DafnyCOMP thus provides a diagnostic tool for\nmeasuring progress toward reliable, verifiable, and compositional code\ngeneration with LLMs.", "AI": {"tldr": "DafnyCOMP\u662f\u4e00\u4e2a\u8bc4\u4f30LLM\u5728Dafny\u7ec4\u5408\u5f0f\u89c4\u8303\u751f\u6210\u80fd\u529b\u7684\u57fa\u51c6\uff0c\u5305\u542b300\u4e2a\u81ea\u52a8\u5408\u6210\u7684\u591a\u51fd\u6570\u7a0b\u5e8f\uff0c\u53d1\u73b0LLM\u5728\u7ec4\u5408\u4efb\u52a1\u4e0a\u8868\u73b0\u663e\u8457\u4e0b\u964d\u3002", "motivation": "\u73b0\u6709\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u5355\u51fd\u6570\u4efb\u52a1\uff0c\u7f3a\u4e4f\u8bc4\u4f30LLM\u5728\u5177\u6709\u6570\u636e\u4f9d\u8d56\u7684\u591a\u51fd\u6570\u4ea4\u4e92\u7a0b\u5e8f\u4e0a\u7684\u7ec4\u5408\u63a8\u7406\u80fd\u529b\u3002", "method": "\u6784\u5efa\u5305\u542b300\u4e2a\u81ea\u52a8\u5408\u6210\u7684\u591a\u51fd\u6570Dafny\u7a0b\u5e8f\u7684\u57fa\u51c6\uff0c\u8bc4\u4f30\u591a\u4e2a\u5148\u8fdbLLM\u5728\u7ec4\u5408\u89c4\u8303\u751f\u6210\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002", "result": "LLM\u5728\u5355\u51fd\u6570\u9a8c\u8bc1\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5728\u7ec4\u5408\u4efb\u52a1\u4e0a\u6027\u80fd\u6025\u5267\u4e0b\u964d\uff0c\u5b58\u5728\u8de8\u51fd\u6570\u63a8\u7406\u7684\u7cfb\u7edf\u6027\u5931\u8d25\u3002", "conclusion": "DafnyCOMP\u4e3a\u8861\u91cfLLM\u5728\u53ef\u9760\u3001\u53ef\u9a8c\u8bc1\u548c\u7ec4\u5408\u4ee3\u7801\u751f\u6210\u65b9\u9762\u7684\u8fdb\u5c55\u63d0\u4f9b\u4e86\u8bca\u65ad\u5de5\u5177\u3002"}}
{"id": "2509.22707", "categories": ["cs.DC", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2509.22707", "abs": "https://arxiv.org/abs/2509.22707", "authors": ["Jinqi Yan", "Fang He", "Qianlong Sang", "Bifeng Tong", "Peng Sun", "Yili Gong", "Chuang Hu", "Dazhao Cheng"], "title": "Metadata-Guided Adaptable Frequency Scaling across Heterogeneous Applications and Devices", "comment": null, "summary": "Dynamic Voltage and Frequency Scaling is essential for enhancing energy\nefficiency in mobile platforms. However, traditional heuristic-based governors\nare increasingly inadequate for managing the complexity of heterogeneous\nSystem-on-Chip designs and diverse application workloads. Although\nreinforcement learning approaches offer improved performance, their poor\ngeneralization capability and reliance on extensive retraining for each\nhardware and application combination leads to significant deployment costs. In\nthis work, we observe that device and application metadata inherently\nencapsulate valuable knowledge for DVFS, presenting an opportunity to overcome\nthese limitations. We formulate DVFS for heterogeneous devices and applications\nas a multi-task reinforcement learning problem. We introduce MetaDVFS, which is\na metadata-guided framework that systematically leverages metadata to discover\nand transfer shared knowledge across DVFS tasks. MetaDVFS can output a set of\nDVFS models with significant generalization capability for various applications\nof heterogeneous devices. Evaluations on five Google Pixel devices running six\napplications show that MetaDVFS achieves up to 17% improvement in\nPerformance-Power Ratio and up to 26% improvement in Quality of Experience.\nCompared to state-of-the-art methods, MetaDVFS delivers 70.8% faster adaptation\nand 5.8-27.6% higher performance over standalone device-application specific\ntraining, while avoiding negative transfer effects. These results establish\nMetaDVFS as an effective and scalable solution for DVFS deployment in\nheterogeneous mobile environments.", "AI": {"tldr": "MetaDVFS\u662f\u4e00\u4e2a\u57fa\u4e8e\u5143\u6570\u636e\u7684\u52a8\u6001\u7535\u538b\u9891\u7387\u7f29\u653e\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u4efb\u52a1\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u89e3\u51b3\u5f02\u6784\u8bbe\u5907\u548c\u5e94\u7528\u7684DVFS\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u529f\u8017\u6bd4\u548c\u7528\u6237\u4f53\u9a8c\u8d28\u91cf\u3002", "motivation": "\u4f20\u7edf\u542f\u53d1\u5f0fDVFS\u8c03\u63a7\u5668\u96be\u4ee5\u5e94\u5bf9\u5f02\u6784SoC\u8bbe\u8ba1\u548c\u591a\u6837\u5316\u5e94\u7528\u8d1f\u8f7d\u7684\u590d\u6742\u6027\uff0c\u800c\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u6cdb\u5316\u80fd\u529b\u5dee\u4e14\u90e8\u7f72\u6210\u672c\u9ad8\u3002", "method": "\u5c06DVFS\u5efa\u6a21\u4e3a\u591a\u4efb\u52a1\u5f3a\u5316\u5b66\u4e60\u95ee\u9898\uff0c\u5229\u7528\u8bbe\u5907\u548c\u5e94\u7528\u5143\u6570\u636e\u6765\u53d1\u73b0\u548c\u8fc1\u79fb\u5171\u4eab\u77e5\u8bc6\uff0c\u8f93\u51fa\u5177\u6709\u5f3a\u6cdb\u5316\u80fd\u529b\u7684DVFS\u6a21\u578b\u96c6\u3002", "result": "\u57285\u6b3eGoogle Pixel\u8bbe\u5907\u548c6\u4e2a\u5e94\u7528\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cMetaDVFS\u5728\u6027\u80fd\u529f\u8017\u6bd4\u4e0a\u63d0\u5347\u8fbe17%\uff0c\u7528\u6237\u4f53\u9a8c\u8d28\u91cf\u63d0\u5347\u8fbe26%\uff0c\u9002\u914d\u901f\u5ea6\u6bd4\u73b0\u6709\u65b9\u6cd5\u5feb70.8%\u3002", "conclusion": "MetaDVFS\u4e3a\u5f02\u6784\u79fb\u52a8\u73af\u5883\u4e2d\u7684DVFS\u90e8\u7f72\u63d0\u4f9b\u4e86\u6709\u6548\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u907f\u514d\u4e86\u8d1f\u8fc1\u79fb\u6548\u5e94\u3002"}}
{"id": "2509.23179", "categories": ["cs.AR", "cs.CR"], "pdf": "https://arxiv.org/pdf/2509.23179", "abs": "https://arxiv.org/abs/2509.23179", "authors": ["Jingyao Zhang", "Elaheh Sadredini"], "title": "A Near-Cache Architectural Framework for Cryptographic Computing", "comment": null, "summary": "Recent advancements in post-quantum cryptographic algorithms have led to\ntheir standardization by the National Institute of Standards and Technology\n(NIST) to safeguard information security in the post-quantum era. These\nalgorithms, however, employ public keys and signatures that are 3 to 9$\\times$\nlonger than those used in pre-quantum cryptography, resulting in significant\nperformance and energy efficiency overheads. A critical bottleneck identified\nin our analysis is the cache bandwidth. This limitation motivates the adoption\nof on-chip in-/near-cache computing, a computing paradigm that offers\nhigh-performance, exceptional energy efficiency, and flexibility to accelerate\npost-quantum cryptographic algorithms. Our analysis of existing works reveals\nchallenges in integrating in-/near-cache computing into modern computer systems\nand performance limitations due to external bandwidth limitation, highlighting\nthe need for innovative solutions that can seamlessly integrate into existing\nsystems without performance and energy efficiency issues. In this paper, we\nintroduce a near-cache-slice computing paradigm with support of customization\nand virtual address, named Crypto-Near-Cache (CNC), designed to accelerate\npost-quantum cryptographic algorithms and other applications. By placing SRAM\narrays with bitline computing capability near cache slices, high internal\nbandwidth and short data movement are achieved with native support of virtual\naddressing. An ISA extension to facilitate CNC is also proposed, with detailed\ndiscussion on the implementation aspects of the core/cache datapath.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCrypto-Near-Cache (CNC)\u7684\u8fd1\u7f13\u5b58\u5207\u7247\u8ba1\u7b97\u8303\u5f0f\uff0c\u901a\u8fc7\u5c06\u5177\u6709\u4f4d\u7ebf\u8ba1\u7b97\u80fd\u529b\u7684SRAM\u9635\u5217\u653e\u7f6e\u5728\u7f13\u5b58\u5207\u7247\u9644\u8fd1\uff0c\u6765\u52a0\u901f\u540e\u91cf\u5b50\u5bc6\u7801\u7b97\u6cd5\uff0c\u89e3\u51b3\u7f13\u5b58\u5e26\u5bbd\u74f6\u9888\u95ee\u9898\u3002", "motivation": "\u540e\u91cf\u5b50\u5bc6\u7801\u7b97\u6cd5\u7684\u516c\u94a5\u548c\u7b7e\u540d\u957f\u5ea6\u6bd4\u524d\u91cf\u5b50\u5bc6\u7801\u957f3-9\u500d\uff0c\u5bfc\u81f4\u663e\u8457\u7684\u6027\u80fd\u548c\u80fd\u6548\u5f00\u9500\u3002\u5206\u6790\u53d1\u73b0\u7f13\u5b58\u5e26\u5bbd\u662f\u5173\u952e\u74f6\u9888\uff0c\u8fd9\u4fc3\u4f7f\u91c7\u7528\u7247\u4e0a\u8fd1\u7f13\u5b58\u8ba1\u7b97\u8303\u5f0f\u6765\u52a0\u901f\u540e\u91cf\u5b50\u5bc6\u7801\u7b97\u6cd5\u3002", "method": "\u8bbe\u8ba1CNC\u8fd1\u7f13\u5b58\u5207\u7247\u8ba1\u7b97\u8303\u5f0f\uff0c\u5728\u7f13\u5b58\u5207\u7247\u9644\u8fd1\u653e\u7f6e\u5177\u6709\u4f4d\u7ebf\u8ba1\u7b97\u80fd\u529b\u7684SRAM\u9635\u5217\uff0c\u5b9e\u73b0\u9ad8\u5185\u90e8\u5e26\u5bbd\u548c\u77ed\u6570\u636e\u79fb\u52a8\uff0c\u652f\u6301\u865a\u62df\u5730\u5740\uff0c\u5e76\u63d0\u51fa\u4e86\u76f8\u5e94\u7684ISA\u6269\u5c55\u3002", "result": "\u901a\u8fc7CNC\u67b6\u6784\u5b9e\u73b0\u4e86\u9ad8\u5185\u90e8\u5e26\u5bbd\u548c\u77ed\u6570\u636e\u79fb\u52a8\uff0c\u80fd\u591f\u6709\u6548\u52a0\u901f\u540e\u91cf\u5b50\u5bc6\u7801\u7b97\u6cd5\u548c\u5176\u4ed6\u5e94\u7528\uff0c\u89e3\u51b3\u4e86\u5916\u90e8\u5e26\u5bbd\u9650\u5236\u5e26\u6765\u7684\u6027\u80fd\u95ee\u9898\u3002", "conclusion": "CNC\u63d0\u4f9b\u4e86\u4e00\u79cd\u521b\u65b0\u7684\u8fd1\u7f13\u5b58\u8ba1\u7b97\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u65e0\u7f1d\u96c6\u6210\u5230\u73b0\u6709\u7cfb\u7edf\u4e2d\uff0c\u89e3\u51b3\u540e\u91cf\u5b50\u5bc6\u7801\u7b97\u6cd5\u7684\u6027\u80fd\u548c\u80fd\u6548\u95ee\u9898\uff0c\u540c\u65f6\u652f\u6301\u865a\u62df\u5730\u5740\u548c\u5b9a\u5236\u5316\u9700\u6c42\u3002"}}
{"id": "2509.23229", "categories": ["cs.PL", "D.2.4; F.3.1"], "pdf": "https://arxiv.org/pdf/2509.23229", "abs": "https://arxiv.org/abs/2509.23229", "authors": ["Yawen Guan", "Cl\u00e9ment Pit-Claudel"], "title": "Fine-Grained Reasoning About Container-Internal Pointers with Logical Pinning", "comment": null, "summary": "Most separation logics hide container-internal pointers for modularity. This\nmakes it difficult to specify container APIs that temporarily expose those\npointers to the outside, and to verify programs that use these APIs. We present\nlogical pinning, a lightweight borrowing model for sequential programs that\nallows users to selectively track container-internal pointers at the logical\nlevel. Our model generalizes the magic-wand operator, making it easy to write\nand prove precise specifications, including pointer-stability properties.\nBecause it only changes how representation predicates and specifications are\nwritten, our approach is compatible with most separation logic variants. We\ndemonstrate the practicality of logical pinning by verifying small but\nrepresentative pointer-manipulating programs, and deriving more precise\nversions of common container specifications. In doing so, we show that our\napproach subsumes some well-known proof patterns, simplifies some complex\nproofs, and enables reasoning about program patterns not supported by\ntraditional specifications. All of our results are mechanized in the Rocq proof\nassistant, using the CFML library.", "AI": {"tldr": "\u63d0\u51fa\u903b\u8f91\u56fa\u5b9a(logical pinning)\u65b9\u6cd5\uff0c\u5141\u8bb8\u5728\u5206\u79bb\u903b\u8f91\u4e2d\u6709\u9009\u62e9\u5730\u8ddf\u8e2a\u5bb9\u5668\u5185\u90e8\u6307\u9488\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u5206\u79bb\u903b\u8f91\u9690\u85cf\u5185\u90e8\u6307\u9488\u5bfc\u81f4\u7684API\u89c4\u8303\u548c\u9a8c\u8bc1\u56f0\u96be\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u5206\u79bb\u903b\u8f91\u4e3a\u4e86\u6a21\u5757\u6027\u9690\u85cf\u5bb9\u5668\u5185\u90e8\u6307\u9488\uff0c\u8fd9\u4f7f\u5f97\u96be\u4ee5\u89c4\u8303\u90a3\u4e9b\u4e34\u65f6\u66b4\u9732\u5185\u90e8\u6307\u9488\u7684\u5bb9\u5668API\uff0c\u4e5f\u96be\u4ee5\u9a8c\u8bc1\u4f7f\u7528\u8fd9\u4e9bAPI\u7684\u7a0b\u5e8f\u3002", "method": "\u903b\u8f91\u56fa\u5b9a\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u501f\u7528\u6a21\u578b\uff0c\u6269\u5c55\u4e86\u9b54\u6cd5\u68d2\u64cd\u4f5c\u7b26\uff0c\u5141\u8bb8\u5728\u903b\u8f91\u5c42\u9762\u6709\u9009\u62e9\u5730\u8ddf\u8e2a\u5bb9\u5668\u5185\u90e8\u6307\u9488\uff0c\u53ea\u9700\u6539\u53d8\u8868\u793a\u8c13\u8bcd\u548c\u89c4\u8303\u5199\u6cd5\u3002", "result": "\u9a8c\u8bc1\u4e86\u5c0f\u578b\u4f46\u5177\u6709\u4ee3\u8868\u6027\u7684\u6307\u9488\u64cd\u4f5c\u7a0b\u5e8f\uff0c\u63a8\u5bfc\u51fa\u66f4\u7cbe\u786e\u7684\u5e38\u89c1\u5bb9\u5668\u89c4\u8303\uff0c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5305\u542b\u4e86\u4e00\u4e9b\u77e5\u540d\u8bc1\u660e\u6a21\u5f0f\uff0c\u7b80\u5316\u4e86\u590d\u6742\u8bc1\u660e\uff0c\u652f\u6301\u4f20\u7edf\u89c4\u8303\u65e0\u6cd5\u5904\u7406\u7684\u7a0b\u5e8f\u6a21\u5f0f\u63a8\u7406\u3002", "conclusion": "\u903b\u8f91\u56fa\u5b9a\u65b9\u6cd5\u5b9e\u7528\u4e14\u517c\u5bb9\u5927\u591a\u6570\u5206\u79bb\u903b\u8f91\u53d8\u4f53\uff0c\u5728Rocq\u8bc1\u660e\u52a9\u624b\u4e2d\u4f7f\u7528CFML\u5e93\u5b9e\u73b0\u4e86\u6240\u6709\u7ed3\u679c\u3002"}}
{"id": "2509.22832", "categories": ["cs.DC", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.22832", "abs": "https://arxiv.org/abs/2509.22832", "authors": ["Biyao Zhang", "Mingkai Zheng", "Debargha Ganguly", "Xuecen Zhang", "Vikash Singh", "Vipin Chaudhary", "Zhao Zhang"], "title": "Efficient Fine-Grained GPU Performance Modeling for Distributed Deep Learning of LLM", "comment": null, "summary": "Training Large Language Models(LLMs) is one of the most compute-intensive\ntasks in high-performance computing. Predicting end-to-end training time for\nmulti-billion parameter models distributed across hundreds of GPUs remains\nchallenging due to complex interactions between transformer components,\nparallelism strategies(data, model, pipeline, tensor), and multi-tier\ncommunication. Learned models require costly sampling, while analytical models\noften struggle with real-world network and hardware complexities. We address\nthis by decomposing LLMs into core computational primitives and modeling them\nwith: (1) operator-level decomposition for fine-grained analysis; (2)\nlightweight sampling based hardware-aware prediction models for key operations;\n(3) an end-to-end prediction system integrating these components across complex\nparallelization strategies. Crucially, our methodology has been validated on\ntwo large-scale HPC systems. Our framework achieves low average prediction\nerrors-4.98\\% on Perlmutter(A100) and 9.38\\% on Vista(GH200)-for models up to\n20B parameters across 128 GPUs. Importantly, it runs entirely on CPUs, enabling\nrapid iteration over hardware configurations and training strategies without\ncostly on-cluster experimentation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9884\u6d4b\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u65f6\u95f4\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u89e3LLM\u4e3a\u8ba1\u7b97\u539f\u8bed\uff0c\u7ed3\u5408\u8f7b\u91cf\u7ea7\u91c7\u6837\u548c\u786c\u4ef6\u611f\u77e5\u9884\u6d4b\uff0c\u5728CPU\u4e0a\u5b9e\u73b0\u51c6\u786e\u9884\u6d4b\uff0c\u907f\u514d\u6602\u8d35\u7684\u96c6\u7fa4\u5b9e\u9a8c\u3002", "motivation": "\u9884\u6d4b\u591aGPU\u5206\u5e03\u5f0f\u8bad\u7ec3\u6570\u5341\u4ebf\u53c2\u6570LLM\u7684\u7aef\u5230\u7aef\u8bad\u7ec3\u65f6\u95f4\u5177\u6709\u6311\u6218\u6027\uff0c\u73b0\u6709\u5b66\u4e60\u65b9\u6cd5\u6210\u672c\u9ad8\uff0c\u5206\u6790\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u73b0\u5b9e\u7f51\u7edc\u548c\u786c\u4ef6\u590d\u6742\u6027\u3002", "method": "\u5c06LLM\u5206\u89e3\u4e3a\u6838\u5fc3\u8ba1\u7b97\u539f\u8bed\uff0c\u91c7\u7528\u7b97\u5b50\u7ea7\u5206\u89e3\u8fdb\u884c\u7ec6\u7c92\u5ea6\u5206\u6790\uff0c\u57fa\u4e8e\u8f7b\u91cf\u7ea7\u91c7\u6837\u7684\u786c\u4ef6\u611f\u77e5\u9884\u6d4b\u6a21\u578b\uff0c\u96c6\u6210\u7aef\u5230\u7aef\u9884\u6d4b\u7cfb\u7edf\u652f\u6301\u590d\u6742\u5e76\u884c\u7b56\u7565\u3002", "result": "\u5728\u4e24\u4e2a\u5927\u89c4\u6a21HPC\u7cfb\u7edf\u4e0a\u9a8c\u8bc1\uff0cPerlmutter(A100)\u5e73\u5747\u9884\u6d4b\u8bef\u5dee4.98%\uff0cVista(GH200)\u5e73\u5747\u9884\u6d4b\u8bef\u5dee9.38%\uff0c\u652f\u630120B\u53c2\u6570\u6a21\u578b\u5728128 GPU\u4e0a\u7684\u9884\u6d4b\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728CPU\u4e0a\u8fd0\u884c\uff0c\u80fd\u591f\u5feb\u901f\u8fed\u4ee3\u786c\u4ef6\u914d\u7f6e\u548c\u8bad\u7ec3\u7b56\u7565\uff0c\u65e0\u9700\u6602\u8d35\u7684\u96c6\u7fa4\u5b9e\u9a8c\uff0c\u4e3aLLM\u8bad\u7ec3\u65f6\u95f4\u9884\u6d4b\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.23674", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2509.23674", "abs": "https://arxiv.org/abs/2509.23674", "authors": ["Hongqin Lyu", "Yonghao Wang", "Yunlin Du", "Mingyu Shi", "Zhiteng Chao", "Wenxing Li", "Tiancheng Wang", "Huawei Li"], "title": "AssertGen: Enhancement of LLM-aided Assertion Generation through Cross-Layer Signal Bridging", "comment": "6 pages, 7 figures", "summary": "Assertion-based verification (ABV) serves as a crucial technique for ensuring\nthat register-transfer level (RTL) designs adhere to their specifications.\nWhile Large Language Model (LLM) aided assertion generation approaches have\nrecently achieved remarkable progress, existing methods are still unable to\neffectively identify the relationship between design specifications and RTL\ndesigns, which leads to the insufficiency of the generated assertions. To\naddress this issue, we propose AssertGen, an assertion generation framework\nthat automatically generates SystemVerilog assertions (SVA). AssertGen first\nextracts verification objectives from specifications using a chain-of-thought\n(CoT) reasoning strategy, then bridges corresponding signals between these\nobjectives and the RTL code to construct a cross-layer signal chain, and\nfinally generates SVAs based on the LLM. Experimental results demonstrate that\nAssertGen outperforms the existing state-of-the-art methods across several key\nmetrics, such as pass rate of formal property verification (FPV), cone of\ninfluence (COI), proof core and mutation testing coverage.", "AI": {"tldr": "AssertGen\u662f\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u65ad\u8a00\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u63d0\u53d6\u9a8c\u8bc1\u76ee\u6807\u3001\u6784\u5efa\u8de8\u5c42\u4fe1\u53f7\u94fe\u6765\u751f\u6210SystemVerilog\u65ad\u8a00\uff0c\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709LLM\u8f85\u52a9\u7684\u65ad\u8a00\u751f\u6210\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u8bc6\u522b\u8bbe\u8ba1\u89c4\u8303\u4e0eRTL\u8bbe\u8ba1\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u5bfc\u81f4\u751f\u6210\u7684\u65ad\u8a00\u4e0d\u5145\u5206\u3002", "method": "\u9996\u5148\u4f7f\u7528\u601d\u7ef4\u94fe\u63a8\u7406\u4ece\u89c4\u8303\u4e2d\u63d0\u53d6\u9a8c\u8bc1\u76ee\u6807\uff0c\u7136\u540e\u5728\u76ee\u6807\u4e0eRTL\u4ee3\u7801\u4e4b\u95f4\u6865\u63a5\u4fe1\u53f7\u6784\u5efa\u8de8\u5c42\u4fe1\u53f7\u94fe\uff0c\u6700\u540e\u57fa\u4e8eLLM\u751f\u6210SVA\u65ad\u8a00\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eAssertGen\u5728\u5f62\u5f0f\u5c5e\u6027\u9a8c\u8bc1\u901a\u8fc7\u7387\u3001\u5f71\u54cd\u9525\u3001\u8bc1\u660e\u6838\u5fc3\u548c\u53d8\u5f02\u6d4b\u8bd5\u8986\u76d6\u7387\u7b49\u5173\u952e\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "AssertGen\u6846\u67b6\u901a\u8fc7\u6709\u6548\u8fde\u63a5\u89c4\u8303\u4e0eRTL\u8bbe\u8ba1\uff0c\u663e\u8457\u63d0\u5347\u4e86\u65ad\u8a00\u751f\u6210\u7684\u8d28\u91cf\u548c\u6548\u679c\u3002"}}
{"id": "2509.25114", "categories": ["cs.PL", "cs.SC", "math.AG"], "pdf": "https://arxiv.org/pdf/2509.25114", "abs": "https://arxiv.org/abs/2509.25114", "authors": ["Erdenebayar Bayarmagnai", "Fatemeh Mohammadi", "R\u00e9mi Pr\u00e9bet"], "title": "From Affine to Polynomial: Synthesizing Loops with Branches via Algebraic Geometry", "comment": null, "summary": "Ensuring software correctness remains a fundamental challenge in formal\nprogram verification. One promising approach relies on finding polynomial\ninvariants for loops. Polynomial invariants are properties of a program loop\nthat hold before and after each iteration. Generating such invariants is a\ncrucial task in loop analysis, but it is undecidable in the general case.\nRecently, an alternative approach to this problem has emerged, focusing on\nsynthesizing loops from invariants. However, existing methods only synthesize\naffine loops without guard conditions from polynomial invariants. In this\npaper, we address a more general problem, allowing loops to have polynomial\nupdate maps with a given structure, inequations in the guard condition, and\npolynomial invariants of arbitrary form.\n  We use algebraic geometry tools to design and implement an algorithm that\ncomputes a finite set of polynomial equations whose solutions correspond to all\nnondeterministic branching loops satisfying the given invariants. Furthermore,\nwe introduce a new class of invariants for which we present a significantly\nmore efficient algorithm. In other words, we reduce the problem of synthesizing\nloops to find solutions of multivariate polynomial systems with rational\nentries. This final step is handled in our software using an SMT solver.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u591a\u9879\u5f0f\u4e0d\u53d8\u91cf\u5408\u6210\u5faa\u73af\u7684\u65b0\u65b9\u6cd5\uff0c\u5141\u8bb8\u5faa\u73af\u5177\u6709\u591a\u9879\u5f0f\u66f4\u65b0\u6620\u5c04\u3001\u4e0d\u7b49\u5f0f\u4fdd\u62a4\u6761\u4ef6\u548c\u4efb\u610f\u5f62\u5f0f\u7684\u591a\u9879\u5f0f\u4e0d\u53d8\u91cf\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u53ea\u80fd\u4ece\u591a\u9879\u5f0f\u4e0d\u53d8\u91cf\u5408\u6210\u65e0\u4fdd\u62a4\u6761\u4ef6\u7684\u4eff\u5c04\u5faa\u73af\uff0c\u9700\u8981\u89e3\u51b3\u66f4\u4e00\u822c\u7684\u5faa\u73af\u5408\u6210\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u4ee3\u6570\u51e0\u4f55\u5de5\u5177\u8bbe\u8ba1\u7b97\u6cd5\uff0c\u5c06\u5faa\u73af\u5408\u6210\u95ee\u9898\u8f6c\u5316\u4e3a\u6c42\u89e3\u6709\u7406\u6570\u591a\u5143\u591a\u9879\u5f0f\u7cfb\u7edf\uff0c\u5e76\u7528SMT\u6c42\u89e3\u5668\u5904\u7406\u3002", "result": "\u5f00\u53d1\u4e86\u80fd\u8ba1\u7b97\u6ee1\u8db3\u7ed9\u5b9a\u4e0d\u53d8\u91cf\u7684\u6240\u6709\u975e\u786e\u5b9a\u6027\u5206\u652f\u5faa\u73af\u7684\u7b97\u6cd5\uff0c\u5e76\u5bf9\u7279\u5b9a\u4e0d\u53d8\u91cf\u7c7b\u63d0\u51fa\u4e86\u66f4\u9ad8\u6548\u7684\u7b97\u6cd5\u3002", "conclusion": "\u6210\u529f\u5c06\u5faa\u73af\u5408\u6210\u95ee\u9898\u7b80\u5316\u4e3a\u591a\u9879\u5f0f\u7cfb\u7edf\u6c42\u89e3\uff0c\u4e3a\u8f6f\u4ef6\u6b63\u786e\u6027\u9a8c\u8bc1\u63d0\u4f9b\u4e86\u66f4\u5f3a\u5927\u7684\u5de5\u5177\u3002"}}
{"id": "2509.22922", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.22922", "abs": "https://arxiv.org/abs/2509.22922", "authors": ["Pranjal Naman", "Yogesh Simmhan"], "title": "OptimES: Optimizing Federated Learning Using Remote Embeddings for Graph Neural Networks", "comment": "Extended full-length version of paper that appeared at Euro-Par 2024:\n  \"Optimizing Federated Learning Using Remote Embeddings for Graph Neural\n  Networks\", Pranjal Naman and Yogesh Simmhan, in International European\n  Conference on Parallel and Distributed Computing (Euro-Par), 2024. DOI:\n  https://doi.org/10.1007/978-3-031-69766-1_32", "summary": "Graph Neural Networks (GNNs) have experienced rapid advancements in recent\nyears due to their ability to learn meaningful representations from graph data\nstructures. However, in most real-world settings, such as financial transaction\nnetworks and healthcare networks, this data is localized to different data\nowners and cannot be aggregated due to privacy concerns. Federated Learning\n(FL) has emerged as a viable machine learning approach for training a shared\nmodel that iteratively aggregates local models trained on decentralized data.\nThis addresses privacy concerns while leveraging parallelism. State-of-the-art\nmethods enhance the privacy-respecting convergence accuracy of federated GNN\ntraining by sharing remote embeddings of boundary vertices through a server\n(EmbC). However, they are limited by diminished performance due to large\ncommunication costs. In this article, we propose OptimES, an optimized\nfederated GNN training framework that employs remote neighbourhood pruning,\noverlapping the push of embeddings to the server with local training, and\ndynamic pulling of embeddings to reduce network costs and training time. We\nperform a rigorous evaluation of these strategies for four common graph\ndatasets with up to $111M$ vertices and $1.8B$ edges. We see that a modest drop\nin per-round accuracy due to the preemptive push of embeddings is out-stripped\nby the reduction in per-round training time for large and dense graphs like\nReddit and Products, converging up to $\\approx 3.5\\times$ faster than EmbC and\ngiving up to $\\approx16\\%$ better accuracy than the default federated GNN\nlearning. While accuracy improvements over default federated GNNs are modest\nfor sparser graphs like Arxiv and Papers, they achieve the target accuracy\nabout $\\approx11\\times$ faster than EmbC.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4f18\u5316\u7684\u8054\u90a6\u56fe\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u6846\u67b6OptimES\uff0c\u901a\u8fc7\u8fdc\u7a0b\u90bb\u57df\u526a\u679d\u3001\u5d4c\u5165\u63a8\u9001\u4e0e\u672c\u5730\u8bad\u7ec3\u91cd\u53e0\u3001\u52a8\u6001\u5d4c\u5165\u62c9\u53d6\u7b49\u7b56\u7565\uff0c\u5728\u4fdd\u6301\u9690\u79c1\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u901a\u4fe1\u6210\u672c\u548c\u8bad\u7ec3\u65f6\u95f4\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u56fe\u6570\u636e\u901a\u5e38\u5206\u6563\u5728\u4e0d\u540c\u6570\u636e\u6240\u6709\u8005\u624b\u4e2d\uff0c\u7531\u4e8e\u9690\u79c1\u95ee\u9898\u65e0\u6cd5\u96c6\u4e2d\u3002\u73b0\u6709\u8054\u90a6GNN\u65b9\u6cd5\u867d\u7136\u4fdd\u62a4\u9690\u79c1\uff0c\u4f46\u901a\u4fe1\u6210\u672c\u9ad8\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u91c7\u7528\u8fdc\u7a0b\u90bb\u57df\u526a\u679d\u3001\u5c06\u5d4c\u5165\u63a8\u9001\u5230\u670d\u52a1\u5668\u4e0e\u672c\u5730\u8bad\u7ec3\u91cd\u53e0\u3001\u52a8\u6001\u62c9\u53d6\u5d4c\u5165\u7b49\u4f18\u5316\u7b56\u7565\u6765\u51cf\u5c11\u7f51\u7edc\u6210\u672c\u548c\u8bad\u7ec3\u65f6\u95f4\u3002", "result": "\u5728\u56db\u4e2a\u56fe\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u5bf9\u4e8e\u5927\u578b\u5bc6\u96c6\u56fe\uff08\u5982Reddit\u548cProducts\uff09\uff0c\u8bad\u7ec3\u901f\u5ea6\u6bd4EmbC\u5feb\u7ea63.5\u500d\uff0c\u51c6\u786e\u7387\u6bd4\u9ed8\u8ba4\u8054\u90a6GNN\u63d0\u9ad8\u7ea616%\uff1b\u5bf9\u4e8e\u7a00\u758f\u56fe\uff08\u5982Arxiv\u548cPapers\uff09\uff0c\u8fbe\u5230\u76ee\u6807\u51c6\u786e\u7387\u7684\u901f\u5ea6\u6bd4EmbC\u5feb\u7ea611\u500d\u3002", "conclusion": "OptimES\u6846\u67b6\u901a\u8fc7\u4f18\u5316\u7b56\u7565\u6709\u6548\u89e3\u51b3\u4e86\u8054\u90a6GNN\u8bad\u7ec3\u4e2d\u7684\u901a\u4fe1\u74f6\u9888\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u9690\u79c1\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u8bad\u7ec3\u6548\u7387\u548c\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2509.23693", "categories": ["cs.AR", "cs.OS"], "pdf": "https://arxiv.org/pdf/2509.23693", "abs": "https://arxiv.org/abs/2509.23693", "authors": ["Tao Lu", "Jiapin Wang", "Yelin Shan", "Xiangping Zhang", "Xiang Chen"], "title": "ASIC-based Compression Accelerators for Storage Systems: Design, Placement, and Profiling Insights", "comment": "16 pages", "summary": "Lossless compression imposes significant computational over head on\ndatacenters when performed on CPUs. Hardware compression and decompression\nprocessing units (CDPUs) can alleviate this overhead, but optimal algorithm\nselection, microarchitectural design, and system-level placement of CDPUs are\nstill not well understood. We present the design of an ASIC-based in-storage\nCDPU and provide a comprehensive end-to-end evaluation against two leading ASIC\naccelerators, Intel QAT 8970 and QAT 4xxx. The evaluation spans three dominant\nCDPU placement regimes: peripheral, on-chip, and in-storage. Our results\nreveal: (i) acute sensitivity of throughput and latency to CDPU placement and\ninterconnection, (ii) strong correlation between compression efficiency and\ndata patterns/layouts, (iii) placement-driven divergences between\nmicrobenchmark gains and real-application speedups, (iv) discrepancies between\nmodule and system-level power efficiency, and (v) scalability and multi-tenant\ninterference is sues of various CDPUs. These findings motivate a\nplacement-aware, cross-layer rethinking of hardware (de)compression for\nhyperscale storage infrastructures.", "AI": {"tldr": "\u672c\u6587\u8bc4\u4f30\u4e86\u786c\u4ef6\u538b\u7f29\u89e3\u538b\u5904\u7406\u5355\u5143(CDPU)\u7684\u4e0d\u540c\u90e8\u7f72\u4f4d\u7f6e(\u5916\u8bbe\u3001\u7247\u4e0a\u3001\u5b58\u50a8\u5185)\u5bf9\u6027\u80fd\u548c\u6548\u7387\u7684\u5f71\u54cd\uff0c\u63ed\u793a\u4e86\u90e8\u7f72\u4f4d\u7f6e\u5bf9\u541e\u5410\u91cf\u3001\u5ef6\u8fdf\u3001\u529f\u8017\u6548\u7387\u7684\u91cd\u8981\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u4e86\u9488\u5bf9\u8d85\u5927\u89c4\u6a21\u5b58\u50a8\u57fa\u7840\u8bbe\u65bd\u7684\u8de8\u5c42\u91cd\u65b0\u601d\u8003\u3002", "motivation": "\u65e0\u635f\u538b\u7f29\u5728CPU\u4e0a\u6267\u884c\u4f1a\u7ed9\u6570\u636e\u4e2d\u5fc3\u5e26\u6765\u663e\u8457\u8ba1\u7b97\u5f00\u9500\uff0c\u786c\u4ef6CDPU\u53ef\u4ee5\u7f13\u89e3\u8fd9\u4e00\u5f00\u9500\uff0c\u4f46\u6700\u4f18\u7b97\u6cd5\u9009\u62e9\u3001\u5fae\u67b6\u6784\u8bbe\u8ba1\u548c\u7cfb\u7edf\u7ea7\u90e8\u7f72\u4f4d\u7f6e\u4ecd\u4e0d\u660e\u786e\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u57fa\u4e8eASIC\u7684\u5b58\u50a8\u5185CDPU\uff0c\u5e76\u4e0eIntel QAT 8970\u548cQAT 4xxx\u4e24\u4e2a\u9886\u5148\u7684ASIC\u52a0\u901f\u5668\u8fdb\u884c\u5168\u9762\u7684\u7aef\u5230\u7aef\u8bc4\u4f30\uff0c\u6db5\u76d6\u4e09\u79cd\u4e3b\u8981CDPU\u90e8\u7f72\u673a\u5236\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff1a(i)\u541e\u5410\u91cf\u548c\u5ef6\u8fdf\u5bf9CDPU\u90e8\u7f72\u4f4d\u7f6e\u548c\u4e92\u8fde\u6781\u5ea6\u654f\u611f\uff1b(ii)\u538b\u7f29\u6548\u7387\u4e0e\u6570\u636e\u6a21\u5f0f/\u5e03\u5c40\u5f3a\u76f8\u5173\uff1b(iii)\u5fae\u57fa\u51c6\u6d4b\u8bd5\u589e\u76ca\u4e0e\u5b9e\u9645\u5e94\u7528\u52a0\u901f\u5b58\u5728\u90e8\u7f72\u9a71\u52a8\u7684\u5dee\u5f02\uff1b(iv)\u6a21\u5757\u7ea7\u548c\u7cfb\u7edf\u7ea7\u529f\u8017\u6548\u7387\u5b58\u5728\u5dee\u5f02\uff1b(v)\u5404\u79cdCDPU\u7684\u53ef\u6269\u5c55\u6027\u548c\u591a\u79df\u6237\u5e72\u6270\u95ee\u9898\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u4fc3\u4f7f\u5bf9\u8d85\u5927\u89c4\u6a21\u5b58\u50a8\u57fa\u7840\u8bbe\u65bd\u7684\u786c\u4ef6\u538b\u7f29\u89e3\u538b\u8fdb\u884c\u90e8\u7f72\u611f\u77e5\u7684\u8de8\u5c42\u91cd\u65b0\u601d\u8003\u3002"}}
{"id": "2509.23013", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.23013", "abs": "https://arxiv.org/abs/2509.23013", "authors": ["Varad Kulkarni", "Nikhil Reddy", "Tuhin Khare", "Abhinandan S. Prasad", "Chitra Babu", "Yogesh Simmhan"], "title": "Characterizing FaaS Workflows on Public Clouds: The Good, the Bad and the Ugly", "comment": null, "summary": "Function-as-a-service (FaaS) is a popular serverless computing paradigm for\ndeveloping event-driven functions that elastically scale on public clouds. FaaS\nworkflows, such as AWS Step Functions and Azure Durable Functions, are composed\nfrom FaaS functions, like AWS Lambda and Azure Functions, to build practical\napplications. But, the complex interactions between functions in the workflow\nand the limited visibility into the internals of proprietary FaaS platforms are\nmajor impediments to gaining a deeper understanding of FaaS workflow platforms.\nWhile several works characterize FaaS platforms to derive such insights, there\nis a lack of a principled and rigorous study for FaaS workflow platforms, which\nhave unique scaling, performance and costing behavior influenced by the\nplatform design, dataflow and workloads. In this article, we perform extensive\nevaluations of three popular FaaS workflow platforms from AWS and Azure,\nrunning 25 micro-benchmark and application workflows over 132k invocations. Our\ndetailed analysis confirms some conventional wisdom but also uncovers unique\ninsights on the function execution, workflow orchestration, inter-function\ninteractions, cold-start scaling and monetary costs. Our observations help\ndevelopers better configure and program these platforms, set performance and\nscalability expectations, and identify research gaps on enhancing the\nplatforms.", "AI": {"tldr": "\u5bf9AWS\u548cAzure\u7684\u4e09\u79cd\u6d41\u884cFaaS\u5de5\u4f5c\u6d41\u5e73\u53f0\u8fdb\u884c\u5927\u89c4\u6a21\u8bc4\u4f30\uff0c\u901a\u8fc713.2\u4e07\u6b21\u8c03\u7528\u8fd0\u884c25\u4e2a\u5fae\u57fa\u51c6\u548c\u5e94\u7528\u5de5\u4f5c\u6d41\uff0c\u63ed\u793a\u4e86\u51fd\u6570\u6267\u884c\u3001\u5de5\u4f5c\u6d41\u7f16\u6392\u3001\u51b7\u542f\u52a8\u6269\u5c55\u548c\u6210\u672c\u65b9\u9762\u7684\u72ec\u7279\u89c1\u89e3\u3002", "motivation": "FaaS\u5de5\u4f5c\u6d41\u5e73\u53f0\uff08\u5982AWS Step Functions\u548cAzure Durable Functions\uff09\u7684\u590d\u6742\u51fd\u6570\u4ea4\u4e92\u548c\u4e13\u6709\u5e73\u53f0\u5185\u90e8\u53ef\u89c1\u6027\u6709\u9650\uff0c\u963b\u788d\u4e86\u5bf9\u8fd9\u4e9b\u5e73\u53f0\u7684\u6df1\u5165\u7406\u89e3\u3002\u73b0\u6709\u7814\u7a76\u7f3a\u4e4f\u5bf9FaaS\u5de5\u4f5c\u6d41\u5e73\u53f0\u7684\u7cfb\u7edf\u6027\u548c\u4e25\u8c28\u7814\u7a76\u3002", "method": "\u5bf9AWS\u548cAzure\u7684\u4e09\u79cd\u6d41\u884cFaaS\u5de5\u4f5c\u6d41\u5e73\u53f0\u8fdb\u884c\u5e7f\u6cdb\u8bc4\u4f30\uff0c\u8fd0\u884c25\u4e2a\u5fae\u57fa\u51c6\u548c\u5e94\u7528\u5de5\u4f5c\u6d41\uff0c\u5171\u8fdb\u884c132,000\u6b21\u8c03\u7528\uff0c\u8be6\u7ec6\u5206\u6790\u51fd\u6570\u6267\u884c\u3001\u5de5\u4f5c\u6d41\u7f16\u6392\u7b49\u5404\u4e2a\u65b9\u9762\u3002", "result": "\u7814\u7a76\u65e2\u8bc1\u5b9e\u4e86\u4e00\u4e9b\u4f20\u7edf\u89c2\u70b9\uff0c\u4e5f\u53d1\u73b0\u4e86\u5173\u4e8e\u51fd\u6570\u6267\u884c\u3001\u5de5\u4f5c\u6d41\u7f16\u6392\u3001\u51fd\u6570\u95f4\u4ea4\u4e92\u3001\u51b7\u542f\u52a8\u6269\u5c55\u548c\u8d27\u5e01\u6210\u672c\u7684\u72ec\u7279\u89c1\u89e3\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5e2e\u52a9\u5f00\u53d1\u8005\u66f4\u597d\u5730\u914d\u7f6e\u548c\u7f16\u7a0b\u8fd9\u4e9b\u5e73\u53f0\uff0c\u8bbe\u5b9a\u6027\u80fd\u548c\u53ef\u6269\u5c55\u6027\u9884\u671f\uff0c\u5e76\u8bc6\u522b\u589e\u5f3a\u5e73\u53f0\u7684\u7814\u7a76\u7a7a\u767d\u3002"}}
{"id": "2509.23972", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2509.23972", "abs": "https://arxiv.org/abs/2509.23972", "authors": ["Hongqin Lyu", "Yunlin Du", "Yonghao Wang", "Zhiteng Chao", "Tiancheng Wang", "Huawei Li"], "title": "AssertFix: Empowering Automated Assertion Fix via Large Language Models", "comment": "6 pages, 6 figures", "summary": "Assertion-based verification (ABV) is critical in ensuring that\nregister-transfer level (RTL) designs conform to their functional\nspecifications. SystemVerilog Assertions (SVA) effectively specify design\nproperties, but writing and maintaining them manually is challenging and\nerror-prone. Although recent progress of assertion generation methods\nleveraging large language models (LLMs) have shown great potential in improving\nassertion quality, they typically treat assertion generation as a final step,\nleaving the burden of fixing of the incorrect assertions to human effects,\nwhich may significantly limits the application of these methods. To address the\nabove limitation, we propose an automatic assertion fix framework based on\nLLMs, named AssertFix. AsserFix accurately locates the RTL code related to the\nincorrect assertion, systematically identifies the root causes of the assertion\nerrors, classifies the error type and finally applies dedicated fix strategies\nto automatically correct these errors, improving the overall quality of the\ngenerated assertions. Experimental results show that AssertFix achieves\nnoticeable improvements in both fix rate and verification coverage across the\nOpencore benchmarks.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eLLM\u7684\u81ea\u52a8\u65ad\u8a00\u4fee\u590d\u6846\u67b6AssertFix\uff0c\u901a\u8fc7\u5b9a\u4f4dRTL\u4ee3\u7801\u3001\u8bc6\u522b\u9519\u8bef\u6839\u6e90\u3001\u5206\u7c7b\u9519\u8bef\u7c7b\u578b\u5e76\u5e94\u7528\u4e13\u7528\u4fee\u590d\u7b56\u7565\uff0c\u81ea\u52a8\u4fee\u6b63\u9519\u8bef\u65ad\u8a00\uff0c\u63d0\u9ad8\u65ad\u8a00\u751f\u6210\u8d28\u91cf\u3002", "motivation": "\u4f20\u7edf\u65ad\u8a00\u751f\u6210\u65b9\u6cd5\u5c06\u65ad\u8a00\u751f\u6210\u4f5c\u4e3a\u6700\u7ec8\u6b65\u9aa4\uff0c\u9700\u8981\u4eba\u5de5\u4fee\u590d\u9519\u8bef\u65ad\u8a00\uff0c\u9650\u5236\u4e86\u8fd9\u4e9b\u65b9\u6cd5\u7684\u5e94\u7528\u3002", "method": "AssertFix\u6846\u67b6\uff1a1) \u51c6\u786e\u5b9a\u4f4d\u4e0e\u9519\u8bef\u65ad\u8a00\u76f8\u5173\u7684RTL\u4ee3\u7801\uff1b2) \u7cfb\u7edf\u8bc6\u522b\u65ad\u8a00\u9519\u8bef\u7684\u6839\u672c\u539f\u56e0\uff1b3) \u5206\u7c7b\u9519\u8bef\u7c7b\u578b\uff1b4) \u5e94\u7528\u4e13\u7528\u4fee\u590d\u7b56\u7565\u81ea\u52a8\u4fee\u6b63\u9519\u8bef\u3002", "result": "\u5728Opencore\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cAssertFix\u5728\u4fee\u590d\u7387\u548c\u9a8c\u8bc1\u8986\u76d6\u7387\u65b9\u9762\u90fd\u53d6\u5f97\u4e86\u663e\u8457\u63d0\u5347\u3002", "conclusion": "AssertFix\u80fd\u591f\u6709\u6548\u81ea\u52a8\u4fee\u590d\u9519\u8bef\u65ad\u8a00\uff0c\u63d0\u9ad8\u65ad\u8a00\u751f\u6210\u7684\u6574\u4f53\u8d28\u91cf\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u9700\u8981\u4eba\u5de5\u5e72\u9884\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2509.23241", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.23241", "abs": "https://arxiv.org/abs/2509.23241", "authors": ["Ankita Dutta", "Nabendu Chaki", "Rajat K. De"], "title": "Memory Efficient and Staleness Free Pipeline Parallel DNN Training Framework with Improved Convergence Speed", "comment": null, "summary": "High resource requirement for Deep Neural Network (DNN) training across\nmultiple GPUs necessitates development of various parallelism techniques. In\nthis paper, we introduce two interconnected DNN training frameworks, namely,\nV-TiMePReSt and I-TiMePReSt, based on pipeline parallelism, a variant of model\nparallelism. V-TiMePReSt is a completely staleness-free system which enables\nthe DNNs to be trained on the latest updated weights in each stage of all\nforward and backward passes. Developing staleness-aware systems at the expense\nof weight stashing reduces GPU-memory consumption, however, increases the\nnumber of epochs to converge. Thus, we introduce I-TiMePReSt, which is also a\nstaleness-aware system, but not at the expense of weight stashing. It does not\nrely solely on the stale weights or the latest updated weights. I-TiMePReSt\ncomputes an intermediate weight towards the latter and performs backward pass\non it. Additionally, we formulate the significance of the stale weights\nmathematically depending on the degree of staleness. In contrast to\nV-TiMePReSt, I-TiMePReSt works based on the assumption that stale weights have\na significant contribution in training, which can be quantified mathematically\nbased on the degree of staleness, although there are other contributory factors\nwhich should not be ignored. Experimental results show that V-TiMePReSt is\nadvantageous over existing models in terms of $1)$ the extent of staleness of\nthe weight parameter values and $2)$ GPU memory efficiency, while I-TiMePReSt\nis superior in terms of $1)$ removing staleness of the weight parameters\nwithout removing weight stashing and $2)$ maintaining the trade-off between GPU\nmemory consumption and convergence speed (number of epochs).", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e24\u79cd\u57fa\u4e8e\u6d41\u6c34\u7ebf\u5e76\u884c\u7684DNN\u8bad\u7ec3\u6846\u67b6\uff1aV-TiMePReSt\uff08\u5b8c\u5168\u65e0\u9648\u65e7\u7cfb\u7edf\uff09\u548cI-TiMePReSt\uff08\u9648\u65e7\u611f\u77e5\u7cfb\u7edf\uff09\uff0c\u65e8\u5728\u89e3\u51b3\u591aGPU\u8bad\u7ec3\u4e2d\u7684\u8d44\u6e90\u9700\u6c42\u548c\u6536\u655b\u6548\u7387\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728\u591aGPU\u8bad\u7ec3\u4e2d\u7684\u9ad8\u8d44\u6e90\u9700\u6c42\uff0c\u5f00\u53d1\u66f4\u9ad8\u6548\u7684\u5e76\u884c\u8bad\u7ec3\u6280\u672f\uff0c\u5e73\u8861GPU\u5185\u5b58\u6d88\u8017\u4e0e\u8bad\u7ec3\u6536\u655b\u901f\u5ea6\u3002", "method": "V-TiMePReSt\u91c7\u7528\u5b8c\u5168\u65e0\u9648\u65e7\u8bbe\u8ba1\uff0c\u5728\u6240\u6709\u524d\u5411\u548c\u540e\u5411\u4f20\u64ad\u9636\u6bb5\u4f7f\u7528\u6700\u65b0\u6743\u91cd\uff1bI-TiMePReSt\u662f\u9648\u65e7\u611f\u77e5\u7cfb\u7edf\uff0c\u57fa\u4e8e\u9648\u65e7\u7a0b\u5ea6\u6570\u5b66\u91cf\u5316\u9648\u65e7\u6743\u91cd\u7684\u91cd\u8981\u6027\uff0c\u8ba1\u7b97\u4e2d\u95f4\u6743\u91cd\u8fdb\u884c\u540e\u5411\u4f20\u64ad\u3002", "result": "\u5b9e\u9a8c\u8868\u660eV-TiMePReSt\u5728\u6743\u91cd\u9648\u65e7\u7a0b\u5ea6\u548cGPU\u5185\u5b58\u6548\u7387\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff1bI-TiMePReSt\u5728\u6d88\u9664\u6743\u91cd\u9648\u65e7\u6027\u540c\u65f6\u4fdd\u6301GPU\u5185\u5b58\u6d88\u8017\u4e0e\u6536\u655b\u901f\u5ea6\u4e4b\u95f4\u7684\u5e73\u8861\u65b9\u9762\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u4e24\u79cd\u6846\u67b6\u5404\u6709\u4f18\u52bf\uff1aV-TiMePReSt\u9002\u5408\u8ffd\u6c42\u6700\u65b0\u6743\u91cd\u548c\u5185\u5b58\u6548\u7387\u7684\u573a\u666f\uff0cI-TiMePReSt\u5728\u5e73\u8861\u5185\u5b58\u6d88\u8017\u548c\u6536\u655b\u901f\u5ea6\u65b9\u9762\u8868\u73b0\u66f4\u597d\uff0c\u4e3aDNN\u5e76\u884c\u8bad\u7ec3\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2509.24929", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2509.24929", "abs": "https://arxiv.org/abs/2509.24929", "authors": ["Hongwei Zhao", "Vianney Lapotre", "Guy Gogniat"], "title": "Fault Injection in On-Chip Interconnects: A Comparative Study of Wishbone, AXI-Lite, and AXI", "comment": "12 pages, 7 tables", "summary": "Fault injection attacks exploit physical disturbances to compromise the\nfunctionality and security of integrated circuits. As System on Chip (SoC)\narchitectures grow in complexity, the vulnerability of on chip communication\nfabrics has become increasingly prominent. Buses, serving as interconnects\namong various IP cores, represent potential vectors for fault-based\nexploitation. In this study, we perform simulation-driven fault injection\nacross three mainstream bus protocols Wishbone, AXI Lite, and AXI. We\nsystematically examine fault success rates, spatial vulnerability\ndistributions, and timing dependencies to characterize how faults interact with\nbus-level transactions. The results uncover consistent behavioral patterns\nacross protocols, offering practical insights for both attack modeling and the\ndevelopment of resilient SoC designs.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u4eff\u771f\u9a71\u52a8\u7684\u6545\u969c\u6ce8\u5165\u5206\u6790\u4e86\u4e09\u79cd\u4e3b\u6d41\u603b\u7ebf\u534f\u8bae(Wishbone\u3001AXI Lite\u3001AXI)\u7684\u6f0f\u6d1e\uff0c\u63ed\u793a\u4e86\u603b\u7ebf\u534f\u8bae\u5728\u6545\u969c\u653b\u51fb\u4e0b\u7684\u884c\u4e3a\u6a21\u5f0f\u548c\u65f6\u7a7a\u4f9d\u8d56\u6027\u3002", "motivation": "\u968f\u7740SoC\u67b6\u6784\u590d\u6742\u5ea6\u589e\u52a0\uff0c\u7247\u4e0a\u901a\u4fe1\u603b\u7ebf\u4f5c\u4e3aIP\u6838\u95f4\u4e92\u8fde\u7684\u6f5c\u5728\u6545\u969c\u653b\u51fb\u5411\u91cf\uff0c\u5176\u8106\u5f31\u6027\u65e5\u76ca\u7a81\u51fa\uff0c\u9700\u8981\u7cfb\u7edf\u6027\u5730\u8bc4\u4f30\u603b\u7ebf\u534f\u8bae\u7684\u5b89\u5168\u6f0f\u6d1e\u3002", "method": "\u91c7\u7528\u4eff\u771f\u9a71\u52a8\u7684\u6545\u969c\u6ce8\u5165\u65b9\u6cd5\uff0c\u5bf9\u4e09\u79cd\u4e3b\u6d41\u603b\u7ebf\u534f\u8bae(Wishbone\u3001AXI Lite\u3001AXI)\u8fdb\u884c\u7cfb\u7edf\u6027\u7684\u6545\u969c\u6ce8\u5165\u5b9e\u9a8c\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u4e86\u8de8\u534f\u8bae\u7684\u4e00\u81f4\u884c\u4e3a\u6a21\u5f0f\uff0c\u5305\u62ec\u6545\u969c\u6210\u529f\u7387\u3001\u7a7a\u95f4\u8106\u5f31\u6027\u5206\u5e03\u548c\u65f6\u5e8f\u4f9d\u8d56\u6027\uff0c\u63ed\u793a\u4e86\u6545\u969c\u4e0e\u603b\u7ebf\u7ea7\u4e8b\u52a1\u4ea4\u4e92\u7684\u7279\u5f81\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u653b\u51fb\u5efa\u6a21\u548c\u5f39\u6027SoC\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u5b9e\u7528\u89c1\u89e3\uff0c\u5f3a\u8c03\u4e86\u603b\u7ebf\u534f\u8bae\u5728\u6545\u969c\u6ce8\u5165\u653b\u51fb\u4e0b\u7684\u7cfb\u7edf\u6027\u6f0f\u6d1e\u7279\u5f81\u3002"}}
{"id": "2509.23324", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23324", "abs": "https://arxiv.org/abs/2509.23324", "authors": ["Zixu Hao", "Jianyu Wei", "Tuowei Wang", "Minxing Huang", "Huiqiang Jiang", "Shiqi Jiang", "Ting Cao", "Ju Ren"], "title": "Scaling LLM Test-Time Compute with Mobile NPU on Smartphones", "comment": null, "summary": "Deploying Large Language Models (LLMs) on mobile devices faces the challenge\nof insufficient performance in smaller models and excessive resource\nconsumption in larger ones. This paper highlights that mobile Neural Processing\nUnits (NPUs) have underutilized computational resources, particularly their\nmatrix multiplication units, during typical LLM inference. To leverage this\nwasted compute capacity, we propose applying parallel test-time scaling\ntechniques on mobile NPUs to enhance the performance of smaller LLMs. However,\nthis approach confronts inherent NPU challenges, including inadequate hardware\nsupport for fine-grained quantization and low efficiency in general-purpose\ncomputations. To overcome these, we introduce two key techniques: a\nhardware-aware tile quantization scheme that aligns group quantization with NPU\nmemory access patterns, and efficient LUT-based replacements for complex\noperations such as Softmax and dequantization. We design and implement an\nend-to-end inference system that leverages the NPU's compute capability to\nsupport test-time scaling on Qualcomm Snapdragon platforms. Experiments show\nour approach brings significant speedups: up to 19.0 for mixed-precision GEMM\nand 2.2 for Softmax. More importantly, we demonstrate that smaller models using\ntest-time scaling can match or exceed the accuracy of larger models, achieving\na new performance-cost Pareto frontier.", "AI": {"tldr": "\u5728\u79fb\u52a8\u8bbe\u5907\u4e0a\u90e8\u7f72\u5927\u578b\u8bed\u8a00\u6a21\u578b\u65f6\uff0c\u901a\u8fc7\u5229\u7528NPU\u672a\u5145\u5206\u5229\u7528\u7684\u8ba1\u7b97\u8d44\u6e90\uff0c\u5e94\u7528\u5e76\u884c\u6d4b\u8bd5\u65f6\u7f29\u653e\u6280\u672f\u63d0\u5347\u5c0f\u6a21\u578b\u6027\u80fd\uff0c\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u52a0\u901f\u6548\u679c\u3002", "motivation": "\u79fb\u52a8\u8bbe\u5907\u90e8\u7f72LLM\u9762\u4e34\u5c0f\u6a21\u578b\u6027\u80fd\u4e0d\u8db3\u548c\u5927\u6a21\u578b\u8d44\u6e90\u6d88\u8017\u8fc7\u5927\u7684\u6311\u6218\uff0c\u540c\u65f6\u53d1\u73b0\u79fb\u52a8NPU\u5728LLM\u63a8\u7406\u4e2d\u5b58\u5728\u8ba1\u7b97\u8d44\u6e90\u672a\u5145\u5206\u5229\u7528\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u786c\u4ef6\u611f\u77e5\u7684\u74e6\u7247\u91cf\u5316\u65b9\u6848\uff0c\u5c06\u7ec4\u91cf\u5316\u4e0eNPU\u5185\u5b58\u8bbf\u95ee\u6a21\u5f0f\u5bf9\u9f50\uff1b\u4f7f\u7528\u57fa\u4e8e\u67e5\u627e\u8868\u7684\u9ad8\u6548\u66ff\u4ee3\u65b9\u6848\u5904\u7406\u590d\u6742\u64cd\u4f5c\uff1b\u8bbe\u8ba1\u7aef\u5230\u7aef\u63a8\u7406\u7cfb\u7edf\u5728\u9a81\u9f99\u5e73\u53f0\u4e0a\u652f\u6301\u6d4b\u8bd5\u65f6\u7f29\u653e\u3002", "result": "\u5b9e\u73b0\u4e86\u663e\u8457\u52a0\u901f\uff1a\u6df7\u5408\u7cbe\u5ea6GEMM\u6700\u9ad8\u52a0\u901f19.0\u500d\uff0cSoftmax\u52a0\u901f2.2\u500d\uff1b\u5c0f\u6a21\u578b\u4f7f\u7528\u6d4b\u8bd5\u65f6\u7f29\u653e\u540e\u80fd\u591f\u8fbe\u5230\u6216\u8d85\u8fc7\u5927\u6a21\u578b\u7684\u51c6\u786e\u5ea6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u79fb\u52a8NPU\u4e0a\u6210\u529f\u5e94\u7528\u6d4b\u8bd5\u65f6\u7f29\u653e\u6280\u672f\uff0c\u4e3a\u5c0f\u6a21\u578b\u6027\u80fd\u63d0\u5347\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\uff0c\u5efa\u7acb\u4e86\u65b0\u7684\u6027\u80fd-\u6210\u672c\u5e15\u7d2f\u6258\u524d\u6cbf\u3002"}}
{"id": "2509.23384", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.23384", "abs": "https://arxiv.org/abs/2509.23384", "authors": ["Yue Zhang", "Yuansheng Chen", "Xuan Mo", "Alex Xi", "Jialun Li", "WeiGang Wu"], "title": "A Predictive and Synergistic Two-Layer Scheduling Framework for LLM Serving", "comment": null, "summary": "LLM inference serving typically scales out with a two-tier architecture: a\ncluster router distributes requests to multiple inference engines, each of\nwhich then in turn performs its own internal scheduling. However, this commonly\nused paradigm suffers from critical, systemic inefficiency caused by the\ninformation gaps across two layers. At the cluster-layer, the router mainly\nrelies on lagging, coarse-grained metrics, such as average latency and queue\nlength to make decisions, resulting in \"decision lag\" that leads to suboptimal\nrequest routing. At the engine-layer, static heuristic scheduling policies\ncannot effectively handle the dynamic workloads, leading a poor balance between\nlatency and throughput. Besides, these gaps may cause SLO violations and\nresource waste, especially in heterogeneous cloud environments.\n  To bridge such gaps, we propose SynergySched, a cross-layer framework that\nshifts LLM serving system from reactive load balancing to predictive\norchestration. The core of SynergySched lies in a structurally-informed online\nperformance model that provides accurate, forward-looking per-step latency and\ncapacity estimations. This model empowers two key components. At the\nengine-layer, LENS performs SLO-aware, adaptive scheduling, dynamically\noptimizing batching to meet SLOs under real-time loads. At the cluster-layer,\nPRISM uses predictive signals to perform state-driven routing, maximizing\ncluster-wide performance and SLO attainment. Performance evaluations show that\nSynergySched improves SLO attainment by 43% on average and achieves up to 3x\nthroughput speedup in long-context and heterogeneous scenarios. Besides, we\nalso deploy SynergySched on FlowGPT's clusters to demonstrate its advantages in\nproduction environment.", "AI": {"tldr": "SynergySched\u662f\u4e00\u4e2a\u8de8\u5c42\u8c03\u5ea6\u6846\u67b6\uff0c\u901a\u8fc7\u9884\u6d4b\u6027\u7f16\u6392\u89e3\u51b3LLM\u63a8\u7406\u670d\u52a1\u4e2d\u7684\u4e24\u5c42\u67b6\u6784\u6548\u7387\u95ee\u9898\uff0c\u63d0\u9ad8SLO\u8fbe\u6210\u7387\u548c\u541e\u5410\u91cf\u3002", "motivation": "\u4f20\u7edfLLM\u63a8\u7406\u670d\u52a1\u7684\u4e24\u5c42\u67b6\u6784\u5b58\u5728\u4fe1\u606f\u9e3f\u6c9f\uff1a\u96c6\u7fa4\u5c42\u8def\u7531\u5668\u4f9d\u8d56\u6ede\u540e\u6307\u6807\u5bfc\u81f4\u51b3\u7b56\u5ef6\u8fdf\uff0c\u5f15\u64ce\u5c42\u9759\u6001\u8c03\u5ea6\u7b56\u7565\u65e0\u6cd5\u6709\u6548\u5904\u7406\u52a8\u6001\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u9020\u6210SLO\u8fdd\u89c4\u548c\u8d44\u6e90\u6d6a\u8d39\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u7ed3\u6784\u611f\u77e5\u7684\u5728\u7ebf\u6027\u80fd\u6a21\u578b\uff0c\u63d0\u4f9b\u51c6\u786e\u7684\u9010\u6b65\u5ef6\u8fdf\u548c\u5bb9\u91cf\u9884\u6d4b\u3002\u5f15\u64ce\u5c42LENS\u7ec4\u4ef6\u5b9e\u73b0SLO\u611f\u77e5\u7684\u81ea\u9002\u5e94\u8c03\u5ea6\uff0c\u96c6\u7fa4\u5c42PRISM\u7ec4\u4ef6\u4f7f\u7528\u9884\u6d4b\u4fe1\u53f7\u8fdb\u884c\u72b6\u6001\u9a71\u52a8\u8def\u7531\u3002", "result": "\u5728\u957f\u4e0a\u4e0b\u6587\u548c\u5f02\u6784\u573a\u666f\u4e2d\uff0cSLO\u8fbe\u6210\u7387\u5e73\u5747\u63d0\u9ad843%\uff0c\u541e\u5410\u91cf\u6700\u9ad8\u63d0\u53473\u500d\u3002\u5728FlowGPT\u751f\u4ea7\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86\u5176\u4f18\u52bf\u3002", "conclusion": "SynergySched\u901a\u8fc7\u8de8\u5c42\u9884\u6d4b\u6027\u7f16\u6392\uff0c\u6709\u6548\u89e3\u51b3\u4e86LLM\u63a8\u7406\u670d\u52a1\u4e2d\u7684\u7cfb\u7edf\u6548\u7387\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u548c\u8d44\u6e90\u5229\u7528\u7387\u3002"}}
{"id": "2509.23419", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23419", "abs": "https://arxiv.org/abs/2509.23419", "authors": ["Asadullah Tariq", "Tariq Qayyum", "Mohamed Adel Serhani", "Farag Sallabi", "Ikbal Taleb", "Ezedin S. Barka"], "title": "Enhancing Communication Efficiency in FL with Adaptive Gradient Quantization and Communication Frequency Optimization", "comment": null, "summary": "Federated Learning (FL) enables participant devices to collaboratively train\ndeep learning models without sharing their data with the server or other\ndevices, effectively addressing data privacy and computational concerns.\nHowever, FL faces a major bottleneck due to high communication overhead from\nfrequent model updates between devices and the server, limiting deployment in\nresource-constrained wireless networks. In this paper, we propose a three-fold\nstrategy. Firstly, an Adaptive Feature-Elimination Strategy to drop less\nimportant features while retaining high-value ones; secondly, Adaptive Gradient\nInnovation and Error Sensitivity-Based Quantization, which dynamically adjusts\nthe quantization level for innovative gradient compression; and thirdly,\nCommunication Frequency Optimization to enhance communication efficiency. We\nevaluated our proposed model's performance through extensive experiments,\nassessing accuracy, loss, and convergence compared to baseline techniques. The\nresults show that our model achieves high communication efficiency in the\nframework while maintaining accuracy.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4e09\u9636\u6bb5\u7b56\u7565\u6765\u4f18\u5316\u8054\u90a6\u5b66\u4e60\u7684\u901a\u4fe1\u6548\u7387\uff1a\u81ea\u9002\u5e94\u7279\u5f81\u6d88\u9664\u3001\u81ea\u9002\u5e94\u68af\u5ea6\u91cf\u5316\u548c\u901a\u4fe1\u9891\u7387\u4f18\u5316\uff0c\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u901a\u4fe1\u5f00\u9500\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u9762\u4e34\u7684\u4e3b\u8981\u74f6\u9888\u662f\u8bbe\u5907\u4e0e\u670d\u52a1\u5668\u4e4b\u95f4\u9891\u7e41\u6a21\u578b\u66f4\u65b0\u5e26\u6765\u7684\u9ad8\u901a\u4fe1\u5f00\u9500\uff0c\u8fd9\u5728\u8d44\u6e90\u53d7\u9650\u7684\u65e0\u7ebf\u7f51\u7edc\u4e2d\u9650\u5236\u4e86\u90e8\u7f72\u3002", "method": "1. \u81ea\u9002\u5e94\u7279\u5f81\u6d88\u9664\u7b56\u7565\uff1a\u4e22\u5f03\u4e0d\u91cd\u8981\u7279\u5f81\uff0c\u4fdd\u7559\u9ad8\u4ef7\u503c\u7279\u5f81\uff1b2. \u81ea\u9002\u5e94\u68af\u5ea6\u521b\u65b0\u548c\u8bef\u5dee\u654f\u611f\u91cf\u5316\uff1a\u52a8\u6001\u8c03\u6574\u91cf\u5316\u7ea7\u522b\u8fdb\u884c\u68af\u5ea6\u538b\u7f29\uff1b3. \u901a\u4fe1\u9891\u7387\u4f18\u5316\uff1a\u63d0\u9ad8\u901a\u4fe1\u6548\u7387\u3002", "result": "\u901a\u8fc7\u5e7f\u6cdb\u5b9e\u9a8c\u8bc4\u4f30\uff0c\u4e0e\u57fa\u7ebf\u6280\u672f\u76f8\u6bd4\uff0c\u8be5\u6a21\u578b\u5728\u4fdd\u6301\u51c6\u786e\u6027\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u9ad8\u901a\u4fe1\u6548\u7387\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u4e09\u9636\u6bb5\u7b56\u7565\u6709\u6548\u89e3\u51b3\u4e86\u8054\u90a6\u5b66\u4e60\u7684\u901a\u4fe1\u74f6\u9888\u95ee\u9898\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u901a\u4fe1\u6548\u7387\u3002"}}
{"id": "2509.23448", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.23448", "abs": "https://arxiv.org/abs/2509.23448", "authors": ["Hao Hao", "Dahlia Malkhi", "Maofan Yin", "Lizan Zhou"], "title": "Lyte Quorum: Off-Chain Ready Smart Contract Hosted with Choice", "comment": null, "summary": "This paper introduces Lyquor, a decentralized platform that reimagines\nblockchain infrastructure through a service-centric model where nodes\nselectively host smart contracts (called Lyquids) while preserving global\ncomposability. We present three key innovations: (1) Fate-Constrained Ordering\n(FCO), which decouples consensus from execution to enable selective hosting\nwithout sacrificing Layer-1 grade composability; (2) Direct Memory Architecture\n(DMA), which eliminates state access bottlenecks by providing each contract\nwith persistent, byte-addressable virtual memory; and (3) Universal Procedure\nCall (UPC), which enables fault-tolerant, programmable coordination across\ndistributed off-chain computation. Together, these components are powered by a\nRust-macroed unified programming model where on-chain and off-chain logic\ncoexist seamlessly, supporting both traditional smart contract patterns and\nnovel distributed applications. Lyquor addresses critical limitations in\nexisting systems while maintaining compatibility with Ethereum APIs, offering a\npath toward truly scalable decentralized computation.", "AI": {"tldr": "Lyquor\u662f\u4e00\u4e2a\u53bb\u4e2d\u5fc3\u5316\u5e73\u53f0\uff0c\u901a\u8fc7\u670d\u52a1\u4e2d\u5fc3\u6a21\u578b\u91cd\u65b0\u6784\u60f3\u533a\u5757\u94fe\u57fa\u7840\u8bbe\u65bd\uff0c\u8282\u70b9\u9009\u62e9\u6027\u6258\u7ba1\u667a\u80fd\u5408\u7ea6\uff08\u79f0\u4e3aLyquids\uff09\u540c\u65f6\u4fdd\u6301\u5168\u5c40\u53ef\u7ec4\u5408\u6027\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u533a\u5757\u94fe\u7cfb\u7edf\u7684\u5173\u952e\u9650\u5236\uff0c\u7279\u522b\u662f\u72b6\u6001\u8bbf\u95ee\u74f6\u9888\u548c\u53ef\u6269\u5c55\u6027\u95ee\u9898\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u4ee5\u592a\u574aAPI\u7684\u517c\u5bb9\u6027\u3002", "method": "\u63d0\u51fa\u4e09\u4e2a\u5173\u952e\u521b\u65b0\uff1aFate-Constrained Ordering\uff08FCO\uff09\u5c06\u5171\u8bc6\u4e0e\u6267\u884c\u89e3\u8026\uff1bDirect Memory Architecture\uff08DMA\uff09\u6d88\u9664\u72b6\u6001\u8bbf\u95ee\u74f6\u9888\uff1bUniversal Procedure Call\uff08UPC\uff09\u5b9e\u73b0\u8de8\u5206\u5e03\u5f0f\u94fe\u4e0b\u8ba1\u7b97\u7684\u5bb9\u9519\u53ef\u7f16\u7a0b\u534f\u8c03\u3002", "result": "\u6784\u5efa\u4e86\u4e00\u4e2a\u652f\u6301\u4f20\u7edf\u667a\u80fd\u5408\u7ea6\u6a21\u5f0f\u548c\u65b0\u578b\u5206\u5e03\u5f0f\u5e94\u7528\u7684\u7edf\u4e00\u7f16\u7a0b\u6a21\u578b\uff0c\u5176\u4e2d\u94fe\u4e0a\u548c\u94fe\u4e0b\u903b\u8f91\u65e0\u7f1d\u5171\u5b58\u3002", "conclusion": "Lyquor\u4e3a\u771f\u6b63\u53ef\u6269\u5c55\u7684\u53bb\u4e2d\u5fc3\u5316\u8ba1\u7b97\u63d0\u4f9b\u4e86\u4e00\u6761\u8def\u5f84\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u7cfb\u7edf\u7684\u5173\u952e\u9650\u5236\uff0c\u540c\u65f6\u4fdd\u6301\u517c\u5bb9\u6027\u3002"}}
{"id": "2509.23706", "categories": ["cs.DC", "cs.DS"], "pdf": "https://arxiv.org/pdf/2509.23706", "abs": "https://arxiv.org/abs/2509.23706", "authors": ["Bogdan-Ioan Popa", "Adrian-Marius Dumitran", "Livia Magureanu"], "title": "Parallel Algorithms for the One Sided Crossing Minimization Problem", "comment": null, "summary": "The One Sided Crossing Minimization (OSCM) problem is an optimization problem\nin graph drawing that aims to minimize the number of edge crossings in\nbipartite graph layouts. It has practical applications in areas such as network\nvisualization and VLSI (Very Large Scale Integration) design, where reducing\nedge crossings improves the arrangement of circuit components and their\ninterconnections. Despite the rise of multi-core systems, the parallelization\nof exact and fixed-parameter tractable (FPT) algorithms for OSCM remains\nlargely unexplored. Parallel variants offer significant potential for scaling\nto larger graphs but require careful handling of synchronization and memory\nmanagement. In this paper, we explore various previously studied exact and FPT\nalgorithms for OSCM, implementing and analyzing them in both sequential and\nparallel forms. Our main contribution lies in empirically proving that these\nalgorithms can achieve close to linear speedup under parallelization. In\nparticular, our best result achieves a speedup of nearly 19 on a 16-core,\n32-thread machine. We further investigate and discuss the reasons why linear\nspeedup is not always attained.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5355\u8fb9\u4ea4\u53c9\u6700\u5c0f\u5316\u95ee\u9898\u7684\u5e76\u884c\u5316\uff0c\u5b9e\u73b0\u4e86\u591a\u79cd\u7cbe\u786e\u548c\u56fa\u5b9a\u53c2\u6570\u53ef\u5904\u7406\u7b97\u6cd5\uff0c\u572816\u683832\u7ebf\u7a0b\u673a\u5668\u4e0a\u5b9e\u73b0\u4e86\u63a5\u8fd119\u500d\u7684\u52a0\u901f\uff0c\u8bc1\u660e\u4e86\u5e76\u884c\u5316\u7684\u6709\u6548\u6027\u3002", "motivation": "\u5c3d\u7ba1\u591a\u6838\u7cfb\u7edf\u5174\u8d77\uff0c\u4f46OSCM\u95ee\u9898\u7684\u7cbe\u786e\u548cFPT\u7b97\u6cd5\u7684\u5e76\u884c\u5316\u7814\u7a76\u4ecd\u5f88\u7f3a\u4e4f\u3002\u5e76\u884c\u53d8\u4f53\u5728\u5904\u7406\u66f4\u5927\u89c4\u6a21\u56fe\u65f6\u5177\u6709\u663e\u8457\u6f5c\u529b\uff0c\u4f46\u9700\u8981\u4ed4\u7ec6\u5904\u7406\u540c\u6b65\u548c\u5185\u5b58\u7ba1\u7406\u95ee\u9898\u3002", "method": "\u63a2\u7d22\u4e86\u591a\u79cd\u5148\u524d\u7814\u7a76\u7684OSCM\u7cbe\u786e\u548cFPT\u7b97\u6cd5\uff0c\u5b9e\u73b0\u4e86\u5b83\u4eec\u7684\u987a\u5e8f\u548c\u5e76\u884c\u7248\u672c\uff0c\u5e76\u8fdb\u884c\u5bf9\u6bd4\u5206\u6790\u3002", "result": "\u7ecf\u9a8c\u8bc1\u660e\u8fd9\u4e9b\u7b97\u6cd5\u5728\u5e76\u884c\u5316\u4e0b\u53ef\u4ee5\u5b9e\u73b0\u63a5\u8fd1\u7ebf\u6027\u7684\u52a0\u901f\uff0c\u6700\u4f73\u7ed3\u679c\u572816\u683832\u7ebf\u7a0b\u673a\u5668\u4e0a\u5b9e\u73b0\u4e86\u8fd119\u500d\u7684\u52a0\u901f\u3002", "conclusion": "\u5e76\u884c\u5316OSCM\u7b97\u6cd5\u662f\u6709\u6548\u7684\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u6027\u80fd\uff0c\u4f46\u7ebf\u6027\u52a0\u901f\u5e76\u4e0d\u603b\u80fd\u5b9e\u73b0\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7814\u7a76\u5176\u539f\u56e0\u3002"}}
{"id": "2509.23722", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2509.23722", "abs": "https://arxiv.org/abs/2509.23722", "authors": ["Jihu Guo", "Tenghui Ma", "Wei Gao", "Peng Sun", "Jiaxing Li", "Xun Chen", "Yuyang Jin", "Dahua Lin"], "title": "AdaPtis: Reducing Pipeline Bubbles with Adaptive Pipeline Parallelism on Heterogeneous Models", "comment": "13 pages, 15 Figures; Under Review;", "summary": "Pipeline parallelism is widely used to train large language models (LLMs).\nHowever, increasing heterogeneity in model architectures exacerbates pipeline\nbubbles, thereby reducing training efficiency. Existing approaches overlook the\nco-optimization of model partition, model placement, and workload scheduling,\nresulting in limited efficiency improvement or even performance degradation. To\nrespond, we propose AdaPtis, an LLM training system that supports adaptive\npipeline parallelism. First, we develop a pipeline performance model to\naccurately estimate training throughput. Second, AdaPtis jointly optimizes\nmodel partition, model placement, and workload scheduling policies guided by\nthis performance model. Third, we design a unified pipeline executor that\nefficiently supports the execution of diverse pipeline strategies. Extensive\nexperiments show that AdaPtis achieves an average speedup of 1.42x (up to\n2.14x) over Megatron-LM I-1F1B across various LLM architectures and scales.", "AI": {"tldr": "AdaPtis\u662f\u4e00\u4e2a\u81ea\u9002\u5e94\u6d41\u6c34\u7ebf\u5e76\u884c\u8bad\u7ec3\u7cfb\u7edf\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u6a21\u578b\u5212\u5206\u3001\u6a21\u578b\u653e\u7f6e\u548c\u5de5\u4f5c\u8d1f\u8f7d\u8c03\u5ea6\uff0c\u663e\u8457\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u6d41\u6c34\u7ebf\u5e76\u884c\u65b9\u6cd5\u5ffd\u89c6\u4e86\u6a21\u578b\u5212\u5206\u3001\u6a21\u578b\u653e\u7f6e\u548c\u5de5\u4f5c\u8d1f\u8f7d\u8c03\u5ea6\u7684\u534f\u540c\u4f18\u5316\uff0c\u5bfc\u81f4\u6548\u7387\u63d0\u5347\u6709\u9650\u751a\u81f3\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u5f00\u53d1\u4e86\u6d41\u6c34\u7ebf\u6027\u80fd\u6a21\u578b\u6765\u51c6\u786e\u4f30\u8ba1\u8bad\u7ec3\u541e\u5410\u91cf\uff0c\u8054\u5408\u4f18\u5316\u6a21\u578b\u5212\u5206\u3001\u6a21\u578b\u653e\u7f6e\u548c\u5de5\u4f5c\u8d1f\u8f7d\u8c03\u5ea6\u7b56\u7565\uff0c\u5e76\u8bbe\u8ba1\u4e86\u7edf\u4e00\u7684\u6d41\u6c34\u7ebf\u6267\u884c\u5668\u3002", "result": "\u5728\u5404\u79cdLLM\u67b6\u6784\u548c\u89c4\u6a21\u4e0b\uff0cAdaPtis\u76f8\u6bd4Megatron-LM I-1F1B\u5b9e\u73b0\u4e86\u5e73\u57471.42\u500d\uff08\u6700\u9ad82.14\u500d\uff09\u7684\u52a0\u901f\u3002", "conclusion": "AdaPtis\u901a\u8fc7\u81ea\u9002\u5e94\u6d41\u6c34\u7ebf\u5e76\u884c\u6709\u6548\u89e3\u51b3\u4e86\u6a21\u578b\u67b6\u6784\u5f02\u8d28\u6027\u5e26\u6765\u7684\u6d41\u6c34\u7ebf\u6c14\u6ce1\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8bad\u7ec3\u6548\u7387\u3002"}}
{"id": "2509.24030", "categories": ["cs.DC", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2509.24030", "abs": "https://arxiv.org/abs/2509.24030", "authors": ["Anjus George", "Michael Brim", "Christopher Zimmer", "David Rogers", "Sarp Oral", "Zach Mayes"], "title": "From Edge to HPC: Investigating Cross-Facility Data Streaming Architectures", "comment": null, "summary": "In this paper, we investigate three cross-facility data streaming\narchitectures, Direct Streaming (DTS), Proxied Streaming (PRS), and Managed\nService Streaming (MSS). We examine their architectural variations in data flow\npaths and deployment feasibility, and detail their implementation using the\nData Streaming to HPC (DS2HPC) architectural framework and the SciStream\nmemory-to-memory streaming toolkit on the production-grade Advanced Computing\nEcosystem (ACE) infrastructure at Oak Ridge Leadership Computing Facility\n(OLCF). We present a workflow-specific evaluation of these architectures using\nthree synthetic workloads derived from the streaming characteristics of\nscientific workflows. Through simulated experiments, we measure streaming\nthroughput, round-trip time, and overhead under work sharing, work sharing with\nfeedback, and broadcast and gather messaging patterns commonly found in AI-HPC\ncommunication motifs. Our study shows that DTS offers a minimal-hop path,\nresulting in higher throughput and lower latency, whereas MSS provides greater\ndeployment feasibility and scalability across multiple users but incurs\nsignificant overhead. PRS lies in between, offering a scalable architecture\nwhose performance matches DTS in most cases.", "AI": {"tldr": "\u672c\u6587\u6bd4\u8f83\u4e86\u4e09\u79cd\u8de8\u8bbe\u65bd\u6570\u636e\u6d41\u67b6\u6784\uff08DTS\u3001PRS\u3001MSS\uff09\uff0c\u901a\u8fc7\u6a21\u62df\u5b9e\u9a8c\u8bc4\u4f30\u5b83\u4eec\u5728AI-HPC\u901a\u4fe1\u6a21\u5f0f\u4e0b\u7684\u6027\u80fd\u8868\u73b0\u3002", "motivation": "\u7814\u7a76\u8de8\u8bbe\u65bd\u6570\u636e\u6d41\u67b6\u6784\u7684\u6027\u80fd\u5dee\u5f02\uff0c\u4e3a\u79d1\u5b66\u5de5\u4f5c\u6d41\u63d0\u4f9b\u6700\u4f73\u7684\u6570\u636e\u6d41\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4f7f\u7528DS2HPC\u67b6\u6784\u6846\u67b6\u548cSciStream\u5de5\u5177\u5305\uff0c\u5728OLCF\u7684ACE\u57fa\u7840\u8bbe\u65bd\u4e0a\u5b9e\u73b0\u4e09\u79cd\u67b6\u6784\uff0c\u5e76\u901a\u8fc7\u4e09\u79cd\u5408\u6210\u5de5\u4f5c\u8d1f\u8f7d\u8fdb\u884c\u6a21\u62df\u5b9e\u9a8c\u3002", "result": "DTS\u63d0\u4f9b\u6700\u77ed\u8def\u5f84\uff0c\u5177\u6709\u66f4\u9ad8\u541e\u5410\u91cf\u548c\u66f4\u4f4e\u5ef6\u8fdf\uff1bMSS\u90e8\u7f72\u53ef\u884c\u6027\u66f4\u597d\u4f46\u5f00\u9500\u663e\u8457\uff1bPRS\u6027\u80fd\u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b\u4e0eDTS\u76f8\u5f53\u3002", "conclusion": "\u4e0d\u540c\u67b6\u6784\u5404\u6709\u4f18\u52a3\uff0cDTS\u9002\u5408\u6027\u80fd\u654f\u611f\u573a\u666f\uff0cMSS\u9002\u5408\u591a\u7528\u6237\u53ef\u6269\u5c55\u573a\u666f\uff0cPRS\u5728\u6027\u80fd\u548c\u53ef\u6269\u5c55\u6027\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002"}}
{"id": "2509.24063", "categories": ["cs.DC", "cs.CE", "cs.MA", "cs.PF", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2509.24063", "abs": "https://arxiv.org/abs/2509.24063", "authors": ["Lukas Breitwieser", "Ahmad Hesam", "Abdullah Giray Ya\u011fl\u0131k\u00e7\u0131", "Mohammad Sadrosadati", "Fons Rademakers", "Onur Mutlu"], "title": "TeraAgent: A Distributed Agent-Based Simulation Engine for Simulating Half a Trillion Agents", "comment": null, "summary": "Agent-based simulation is an indispensable paradigm for studying complex\nsystems. These systems can comprise billions of agents, requiring the computing\nresources of multiple servers to simulate. Unfortunately, the state-of-the-art\nplatform, BioDynaMo, does not scale out across servers due to its\nshared-memory-based implementation.\n  To overcome this key limitation, we introduce TeraAgent, a distributed\nagent-based simulation engine. A critical challenge in distributed execution is\nthe exchange of agent information across servers, which we identify as a major\nperformance bottleneck. We propose two solutions: 1) a tailored serialization\nmechanism that allows agents to be accessed and mutated directly from the\nreceive buffer, and 2) leveraging the iterative nature of agent-based\nsimulations to reduce data transfer with delta encoding.\n  Built on our solutions, TeraAgent enables extreme-scale simulations with half\na trillion agents (an 84x improvement), reduces time-to-result with additional\ncompute nodes, improves interoperability with third-party tools, and provides\nusers with more hardware flexibility.", "AI": {"tldr": "TeraAgent\u662f\u4e00\u4e2a\u5206\u5e03\u5f0f\u4ee3\u7406\u4eff\u771f\u5f15\u64ce\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u5e73\u53f0BioDynaMo\u65e0\u6cd5\u8de8\u670d\u52a1\u5668\u6269\u5c55\u7684\u95ee\u9898\uff0c\u652f\u6301\u4e07\u4ebf\u7ea7\u4ee3\u7406\u4eff\u771f\u3002", "motivation": "\u73b0\u6709\u4ee3\u7406\u4eff\u771f\u5e73\u53f0BioDynaMo\u7531\u4e8e\u57fa\u4e8e\u5171\u4eab\u5185\u5b58\u5b9e\u73b0\uff0c\u65e0\u6cd5\u5728\u591a\u4e2a\u670d\u52a1\u5668\u95f4\u6269\u5c55\uff0c\u9650\u5236\u4e86\u5927\u89c4\u6a21\u590d\u6742\u7cfb\u7edf\u7684\u4eff\u771f\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u89e3\u51b3\u65b9\u6848\uff1a1\uff09\u5b9a\u5236\u5316\u5e8f\u5217\u5316\u673a\u5236\uff0c\u5141\u8bb8\u4ece\u63a5\u6536\u7f13\u51b2\u533a\u76f4\u63a5\u8bbf\u95ee\u548c\u4fee\u6539\u4ee3\u7406\uff1b2\uff09\u5229\u7528\u4ee3\u7406\u4eff\u771f\u7684\u8fed\u4ee3\u7279\u6027\uff0c\u901a\u8fc7\u589e\u91cf\u7f16\u7801\u51cf\u5c11\u6570\u636e\u4f20\u8f93\u3002", "result": "TeraAgent\u5b9e\u73b0\u4e86\u4e07\u4ebf\u7ea7\u4ee3\u7406\u4eff\u771f\uff08\u63d0\u534784\u500d\uff09\uff0c\u901a\u8fc7\u589e\u52a0\u8ba1\u7b97\u8282\u70b9\u51cf\u5c11\u7ed3\u679c\u65f6\u95f4\uff0c\u63d0\u9ad8\u4e86\u4e0e\u7b2c\u4e09\u65b9\u5de5\u5177\u7684\u4e92\u64cd\u4f5c\u6027\uff0c\u5e76\u4e3a\u7528\u6237\u63d0\u4f9b\u66f4\u591a\u786c\u4ef6\u7075\u6d3b\u6027\u3002", "conclusion": "TeraAgent\u6210\u529f\u89e3\u51b3\u4e86\u5206\u5e03\u5f0f\u4ee3\u7406\u4eff\u771f\u7684\u5173\u952e\u6027\u80fd\u74f6\u9888\uff0c\u5b9e\u73b0\u4e86\u524d\u6240\u672a\u6709\u7684\u4eff\u771f\u89c4\u6a21\uff0c\u4e3a\u590d\u6742\u7cfb\u7edf\u7814\u7a76\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u8ba1\u7b97\u5e73\u53f0\u3002"}}
{"id": "2509.24381", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.24381", "abs": "https://arxiv.org/abs/2509.24381", "authors": ["Tianyu Guo", "Tianming Xu", "Xianjie Chen", "Junru Chen", "Nong Xiao", "Xianwei Zhang"], "title": "RServe: Overlapping Encoding and Prefill for Efficient LMM Inference", "comment": null, "summary": "Large multimodal models (LMMs) typically employ an encoding module to\ntransform multimodal data inputs into embeddings, which are then fed to\nlanguage models for further processing. However, efficiently serving LMMs\nremains highly challenging due to the inherent complexity of their inference\npipelines. Traditional serving engines co-locate the encoding module and the\nlanguage model, leading to significant resource interference and tight data\ndependency. Recent studies have alleviated this issue by disaggregating the\nencoding module from the model, following a design style of prefill-decode\ndisaggregation. Nevertheless, these approaches fail to fully exploit\nparallelism both within individual requests (intra-request) and across multiple\nrequests (inter-request).\n  To overcome the limitation, we propose REDServe, an LMM inference system that\nefficiently orchestrates intra- and inter-request pipelines. REDServe is\ndesigned to reduce low latency and maximize parallelism at both intra- and\ninter-request granularities. Built on the disaggregated architecture of the\nencoding module and language model, REDServe adopts a fine-grained scheduling\nmethod that overlaps multimodal encoding with the forward computation of the\nlanguage model within a single request. For inter-request pipeline, REDServe\nleverages schedulable tokens and token budgets to balance computational loads\nacross micro-batches. Combined with chunked prefill, this enables a novel\nscheduling strategy that coordinates the execution of intra- and inter-request\npipelines. Experimental evaluations on representative LMMs show that REDServe\nachieves substantial latency reduction of up to 66% while improving throughput\nby up to 109%, significantly outperforming existing serving approaches.", "AI": {"tldr": "REDServe\u662f\u4e00\u4e2a\u9ad8\u6548\u7684\u591a\u6a21\u6001\u6a21\u578b\u63a8\u7406\u7cfb\u7edf\uff0c\u901a\u8fc7\u7ec6\u7c92\u5ea6\u8c03\u5ea6\u65b9\u6cd5\u5728\u7f16\u7801\u6a21\u5757\u548c\u8bed\u8a00\u6a21\u578b\u89e3\u8026\u67b6\u6784\u4e0a\u5b9e\u73b0\u8bf7\u6c42\u5185\u548c\u8bf7\u6c42\u95f4\u7684\u5e76\u884c\u4f18\u5316\uff0c\u663e\u8457\u964d\u4f4e\u5ef6\u8fdf\u5e76\u63d0\u9ad8\u541e\u5410\u91cf\u3002", "motivation": "\u4f20\u7edf\u591a\u6a21\u6001\u6a21\u578b\u670d\u52a1\u5f15\u64ce\u5b58\u5728\u7f16\u7801\u6a21\u5757\u4e0e\u8bed\u8a00\u6a21\u578b\u7d27\u5bc6\u8026\u5408\u5bfc\u81f4\u7684\u8d44\u6e90\u5e72\u6270\u548c\u6570\u636e\u4f9d\u8d56\u95ee\u9898\uff0c\u73b0\u6709\u89e3\u8026\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u5229\u7528\u8bf7\u6c42\u5185\u548c\u8bf7\u6c42\u95f4\u7684\u5e76\u884c\u6027\u3002", "method": "\u91c7\u7528\u7f16\u7801\u6a21\u5757\u4e0e\u8bed\u8a00\u6a21\u578b\u89e3\u8026\u67b6\u6784\uff0c\u901a\u8fc7\u7ec6\u7c92\u5ea6\u8c03\u5ea6\u65b9\u6cd5\u91cd\u53e0\u591a\u6a21\u6001\u7f16\u7801\u4e0e\u8bed\u8a00\u6a21\u578b\u524d\u5411\u8ba1\u7b97\uff0c\u5229\u7528\u53ef\u8c03\u5ea6\u4ee4\u724c\u548c\u4ee4\u724c\u9884\u7b97\u5e73\u8861\u5fae\u6279\u6b21\u95f4\u7684\u8ba1\u7b97\u8d1f\u8f7d\uff0c\u7ed3\u5408\u5206\u5757\u9884\u586b\u5145\u5b9e\u73b0\u8bf7\u6c42\u5185\u5916\u7ba1\u9053\u7684\u534f\u8c03\u6267\u884c\u3002", "result": "\u5728\u4ee3\u8868\u6027\u591a\u6a21\u6001\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8bc4\u4f30\u663e\u793a\uff0cREDServe\u5b9e\u73b0\u9ad8\u8fbe66%\u7684\u5ef6\u8fdf\u964d\u4f4e\u548c109%\u7684\u541e\u5410\u91cf\u63d0\u5347\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u670d\u52a1\u65b9\u6cd5\u3002", "conclusion": "REDServe\u901a\u8fc7\u9ad8\u6548\u7684\u8bf7\u6c42\u5185\u5916\u7ba1\u9053\u7f16\u6392\uff0c\u5728\u591a\u6a21\u6001\u6a21\u578b\u63a8\u7406\u670d\u52a1\u4e2d\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u6539\u8fdb\uff0c\u8bc1\u660e\u4e86\u7ec6\u7c92\u5ea6\u8c03\u5ea6\u7b56\u7565\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2509.24626", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.24626", "abs": "https://arxiv.org/abs/2509.24626", "authors": ["Qihui Zhou", "Peiqi Yin", "Pengfei Zuo", "James Cheng"], "title": "SparseServe: Unlocking Parallelism for Dynamic Sparse Attention in Long-Context LLM Serving", "comment": "14 pages, 16 figures", "summary": "Serving long-context LLMs is costly because attention computation grows\nlinearly with context length. Dynamic sparse attention algorithms (DSAs)\nmitigate this by attending only to the key-value (KV) cache of critical tokens.\nHowever, with DSAs, the main performance bottleneck shifts from HBM bandwidth\nto HBM capacity: KV caches for unselected tokens must remain in HBM for\nlow-latency decoding, constraining parallel batch size and stalling further\nthroughput gains. Offloading these underutilized KV caches to DRAM could free\nHBM capacity, allowing larger parallel batch sizes. Yet, achieving such\nhierarchical HBM-DRAM storage raises new challenges, including fragmented KV\ncache access, HBM cache contention, and high HBM demands of hybrid batching,\nthat remain unresolved in prior work.\n  This paper proposes SparseServe, an LLM serving system that unlocks the\nparallel potential of DSAs through efficient hierarchical HBM-DRAM management.\nSparseServe introduces three key innovations to address the challenges\nmentioned above: (1) fragmentation-aware KV cache transfer, which accelerates\nHBM-DRAM data movement through GPU-direct loading (FlashH2D) and CPU-assisted\nsaving (FlashD2H); (2) working-set-aware batch size control that adjusts batch\nsizes based on real-time working set estimation to minimize HBM cache\nthrashing; (3) layer-segmented prefill that bounds HBM use during prefill to a\nsingle layer, enabling efficient execution even for long prompts. Extensive\nexperimental results demonstrate that SparseServe achieves up to 9.26x lower\nmean time-to-first-token (TTFT) latency and up to 3.14x higher token generation\nthroughput compared to state-of-the-art LLM serving systems.", "AI": {"tldr": "SparseServe\u901a\u8fc7\u9ad8\u6548\u7684HBM-DRAM\u5206\u5c42\u7ba1\u7406\u89e3\u51b3\u4e86\u52a8\u6001\u7a00\u758f\u6ce8\u610f\u529b\u7b97\u6cd5\u4e2d\u7684KV\u7f13\u5b58\u5bb9\u91cf\u74f6\u9888\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u957f\u4e0a\u4e0b\u6587LLM\u7684\u670d\u52a1\u6027\u80fd\u3002", "motivation": "\u52a8\u6001\u7a00\u758f\u6ce8\u610f\u529b\u7b97\u6cd5\u867d\u7136\u51cf\u5c11\u4e86\u8ba1\u7b97\u5f00\u9500\uff0c\u4f46\u5c06\u6027\u80fd\u74f6\u9888\u4eceHBM\u5e26\u5bbd\u8f6c\u79fb\u5230\u4e86HBM\u5bb9\u91cf\uff0c\u672a\u9009\u4e2d\u7684KV\u7f13\u5b58\u5360\u7528\u5927\u91cfHBM\u7a7a\u95f4\uff0c\u9650\u5236\u4e86\u5e76\u884c\u6279\u5904\u7406\u5927\u5c0f\u3002", "method": "\u63d0\u51fa\u4e09\u4e2a\u5173\u952e\u6280\u672f\uff1a\u788e\u7247\u611f\u77e5KV\u7f13\u5b58\u4f20\u8f93\uff08FlashH2D\u548cFlashD2H\uff09\u3001\u5de5\u4f5c\u96c6\u611f\u77e5\u6279\u5904\u7406\u5927\u5c0f\u63a7\u5236\u3001\u5c42\u5206\u6bb5\u9884\u586b\u5145\u3002", "result": "\u76f8\u6bd4\u6700\u5148\u8fdb\u7684LLM\u670d\u52a1\u7cfb\u7edf\uff0cSparseServe\u5b9e\u73b0\u4e86\u9ad8\u8fbe9.26\u500d\u7684\u9996\u4ee4\u724c\u5ef6\u8fdf\u964d\u4f4e\u548c3.14\u500d\u7684\u4ee4\u724c\u751f\u6210\u541e\u5410\u91cf\u63d0\u5347\u3002", "conclusion": "SparseServe\u901a\u8fc7\u9ad8\u6548\u7684HBM-DRAM\u5206\u5c42\u5b58\u50a8\u7ba1\u7406\uff0c\u6210\u529f\u91ca\u653e\u4e86\u52a8\u6001\u7a00\u758f\u6ce8\u610f\u529b\u7b97\u6cd5\u7684\u5e76\u884c\u6f5c\u529b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u957f\u4e0a\u4e0b\u6587LLM\u7684\u670d\u52a1\u6548\u7387\u3002"}}
{"id": "2509.24859", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.24859", "abs": "https://arxiv.org/abs/2509.24859", "authors": ["Antian Liang", "Zhigang Zhao", "Kai Zhang", "Xuri Shi", "Chuantao Li", "Chunxiao Wang", "Zhenying He", "Yinan Jing", "X. Sean Wang"], "title": "HAPT: Heterogeneity-Aware Automated Parallel Training on Heterogeneous Clusters", "comment": null, "summary": "With the rapid evolution of GPU architectures, the heterogeneity of model\ntraining infrastructures is steadily increasing. In such environments,\neffectively utilizing all available heterogeneous accelerators becomes critical\nfor distributed model training. However, existing frameworks, which are\nprimarily designed for homogeneous clusters, often exhibit significant resource\nunderutilization when deployed on heterogeneous accelerators and networks. In\nthis paper, we present Hapt, an automated parallel training framework designed\nspecifically for heterogeneous clusters. Hapt introduces a fine-grained planner\nthat efficiently searches a wide space for the inter-operator parallel\nstrategy, enabling Hapt to alleviate communication overheads while maintaining\nbalanced loads across heterogeneous accelerators. In addition, Hapt implements\na heterogeneity-aware 1F1B scheduler that adaptively adjusts the execution\ntiming and ordering of microbatches based on network characteristics,\nmaximizing computation-communication overlap under cross-cluster interconnects\nwhile incurring only minimal memory overhead. Our evaluation results show that\nHapt can deliver 1.3x-1.6x higher performance on heterogeneous clusters than\nstate-of-the-art training frameworks.", "AI": {"tldr": "Hapt\u662f\u4e00\u4e2a\u4e13\u4e3a\u5f02\u6784\u96c6\u7fa4\u8bbe\u8ba1\u7684\u81ea\u52a8\u5e76\u884c\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u7ec6\u7c92\u5ea6\u89c4\u5212\u5668\u548c\u5f02\u6784\u611f\u77e5\u8c03\u5ea6\u5668\uff0c\u5728\u5f02\u6784\u52a0\u901f\u5668\u548c\u7f51\u7edc\u73af\u5883\u4e0b\u5b9e\u73b0\u9ad8\u6027\u80fd\u8bad\u7ec3\u3002", "motivation": "\u968f\u7740GPU\u67b6\u6784\u7684\u5feb\u901f\u6f14\u8fdb\uff0c\u8bad\u7ec3\u57fa\u7840\u8bbe\u65bd\u7684\u5f02\u6784\u6027\u4e0d\u65ad\u589e\u52a0\u3002\u73b0\u6709\u4e3b\u8981\u9488\u5bf9\u540c\u6784\u96c6\u7fa4\u8bbe\u8ba1\u7684\u6846\u67b6\u5728\u5f02\u6784\u52a0\u901f\u5668\u548c\u7f51\u7edc\u4e0a\u90e8\u7f72\u65f6\u5b58\u5728\u663e\u8457\u7684\u8d44\u6e90\u5229\u7528\u4e0d\u8db3\u95ee\u9898\u3002", "method": "Hapt\u5f15\u5165\u7ec6\u7c92\u5ea6\u89c4\u5212\u5668\u641c\u7d22\u5e7f\u6cdb\u7684\u7b97\u5b50\u95f4\u5e76\u884c\u7b56\u7565\uff0c\u540c\u65f6\u91c7\u7528\u5f02\u6784\u611f\u77e5\u76841F1B\u8c03\u5ea6\u5668\uff0c\u6839\u636e\u7f51\u7edc\u7279\u6027\u81ea\u9002\u5e94\u8c03\u6574\u5fae\u6279\u6b21\u7684\u6267\u884c\u65f6\u95f4\u548c\u987a\u5e8f\u3002", "result": "\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\uff0cHapt\u5728\u5f02\u6784\u96c6\u7fa4\u4e0a\u7684\u6027\u80fd\u6bd4\u6700\u5148\u8fdb\u7684\u8bad\u7ec3\u6846\u67b6\u9ad8\u51fa1.3\u500d\u52301.6\u500d\u3002", "conclusion": "Hapt\u6846\u67b6\u80fd\u591f\u6709\u6548\u5229\u7528\u5f02\u6784\u96c6\u7fa4\u8d44\u6e90\uff0c\u5728\u4fdd\u6301\u8d1f\u8f7d\u5e73\u8861\u7684\u540c\u65f6\u6700\u5927\u5316\u8ba1\u7b97-\u901a\u4fe1\u91cd\u53e0\uff0c\u663e\u8457\u63d0\u5347\u5206\u5e03\u5f0f\u6a21\u578b\u8bad\u7ec3\u6027\u80fd\u3002"}}
{"id": "2509.24932", "categories": ["cs.DC", "cs.LG", "cs.NI"], "pdf": "https://arxiv.org/pdf/2509.24932", "abs": "https://arxiv.org/abs/2509.24932", "authors": ["Fardis Nadimi", "Payam Abdisarabshali", "Jacob Chakareski", "Nicholas Mastronarde", "Seyyedali Hosseinalipour"], "title": "Graph Theory Meets Federated Learning over Satellite Constellations: Spanning Aggregations, Network Formation, and Performance Optimization", "comment": "8 Figures, 6 Appendix", "summary": "We introduce Fed-Span, a novel federated/distributed learning framework\ndesigned for low Earth orbit satellite constellations. By leveraging\ngraph-theoretic principles, Fed-Span addresses critical challenges inherent to\ndistributed learning in dynamic satellite networks, including intermittent\nsatellite connectivity, heterogeneous computational capabilities of satellites,\nand time-varying satellites' datasets. At its core, Fed-Span builds upon\nminimum spanning tree (MST) and minimum spanning forest (MSF) topologies,\nenabling spanning model aggregation and dispatching processes for distributed\nlearning. To formalize Fed-Span, we offer a fresh perspective on MST/MSF\ntopologies by formulating them through a set of continuous constraint\nrepresentations (CCRs), thereby devising graph-theoretical abstractions into an\noptimizable framework for satellite networks. Using these CCRs, we obtain the\nenergy consumption and latency of operations in Fed-Span. Moreover, we derive\nnovel convergence bounds for non-convex machine learning loss functions,\naccommodating the key system characteristics and degrees of freedom of\nFed-Span. Finally, we propose a comprehensive optimization problem that jointly\nminimizes model prediction loss, energy consumption, and latency of Fed-Span.\nWe unveil that this problem is NP-hard and develop a systematic approach to\ntransform it into a geometric programming formulation, solved via successive\nconvex optimization with performance guarantees. Through evaluations on\nreal-world datasets, we demonstrate that Fed-Span outperforms existing methods,\nwith faster model convergence, greater energy efficiency, and reduced latency.\nThese results highlight Fed-Span as a novel solution for efficient distributed\nlearning in satellite networks.", "AI": {"tldr": "Fed-Span\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u8054\u90a6/\u5206\u5e03\u5f0f\u5b66\u4e60\u6846\u67b6\uff0c\u4e13\u4e3a\u4f4e\u5730\u7403\u8f68\u9053\u536b\u661f\u661f\u5ea7\u8bbe\u8ba1\u3002\u5b83\u5229\u7528\u56fe\u8bba\u539f\u7406\u89e3\u51b3\u536b\u661f\u7f51\u7edc\u4e2d\u7684\u8fde\u63a5\u95f4\u6b47\u6027\u3001\u8ba1\u7b97\u80fd\u529b\u5f02\u6784\u6027\u548c\u6570\u636e\u96c6\u65f6\u53d8\u6027\u7b49\u6311\u6218\uff0c\u901a\u8fc7\u6700\u5c0f\u751f\u6210\u6811/\u68ee\u6797\u62d3\u6251\u5b9e\u73b0\u6a21\u578b\u805a\u5408\u548c\u5206\u53d1\uff0c\u5e76\u4f18\u5316\u80fd\u8017\u3001\u5ef6\u8fdf\u548c\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u4f4e\u5730\u7403\u8f68\u9053\u536b\u661f\u7f51\u7edc\u4e2d\u5206\u5e03\u5f0f\u5b66\u4e60\u9762\u4e34\u7684\u72ec\u7279\u6311\u6218\uff1a\u536b\u661f\u8fde\u63a5\u95f4\u6b47\u6027\u3001\u536b\u661f\u8ba1\u7b97\u80fd\u529b\u5f02\u6784\u6027\u3001\u536b\u661f\u6570\u636e\u96c6\u65f6\u53d8\u6027\uff0c\u8fd9\u4e9b\u56e0\u7d20\u4f7f\u5f97\u4f20\u7edf\u5206\u5e03\u5f0f\u5b66\u4e60\u65b9\u6cd5\u5728\u536b\u661f\u7f51\u7edc\u4e2d\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u57fa\u4e8e\u6700\u5c0f\u751f\u6210\u6811(MST)\u548c\u6700\u5c0f\u751f\u6210\u68ee\u6797(MSF)\u62d3\u6251\u6784\u5efa\u5206\u5e03\u5f0f\u5b66\u4e60\u6846\u67b6\uff1b\u901a\u8fc7\u8fde\u7eed\u7ea6\u675f\u8868\u793a(CCRs)\u5f62\u5f0f\u5316\u56fe\u8bba\u62bd\u8c61\uff1b\u63a8\u5bfc\u975e\u51f8\u635f\u5931\u51fd\u6570\u7684\u6536\u655b\u8fb9\u754c\uff1b\u5c06\u8054\u5408\u4f18\u5316\u95ee\u9898\u8f6c\u5316\u4e3a\u51e0\u4f55\u89c4\u5212\u5f62\u5f0f\uff0c\u4f7f\u7528\u9010\u6b21\u51f8\u4f18\u5316\u6c42\u89e3\u3002", "result": "\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cFed-Span\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5177\u6709\u66f4\u5feb\u7684\u6a21\u578b\u6536\u655b\u901f\u5ea6\u3001\u66f4\u9ad8\u7684\u80fd\u6548\u548c\u66f4\u4f4e\u7684\u5ef6\u8fdf\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u536b\u661f\u7f51\u7edc\u4e2d\u5206\u5e03\u5f0f\u5b66\u4e60\u7684\u6709\u6548\u6027\u3002", "conclusion": "Fed-Span\u4e3a\u536b\u661f\u7f51\u7edc\u4e2d\u7684\u9ad8\u6548\u5206\u5e03\u5f0f\u5b66\u4e60\u63d0\u4f9b\u4e86\u4e00\u4e2a\u65b0\u9896\u89e3\u51b3\u65b9\u6848\uff0c\u901a\u8fc7\u56fe\u8bba\u62bd\u8c61\u548c\u7cfb\u7edf\u4f18\u5316\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u52a8\u6001\u536b\u661f\u7f51\u7edc\u4e2d\u7684\u5173\u952e\u6311\u6218\u3002"}}
{"id": "2509.25041", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.25041", "abs": "https://arxiv.org/abs/2509.25041", "authors": ["Yu Han", "Lehan Pan", "Jie Peng", "Ziyang Tao", "Wuyang Zhang", "Yanyong Zhang"], "title": "GRACE-MoE: Grouping and Replication with Locality-Aware Routing for Efficient Distributed MoE Inference", "comment": null, "summary": "Sparse Mixture of Experts (SMoE) performs conditional computation by\nselectively activating a subset of experts, thereby enabling scalable parameter\ngrowth in large language models (LLMs). However, the expanded parameter scale\nexceeds the memory capacity of a single device, necessitating distributed\ndeployment for inference. This setup introduces two critical challenges: (1)\nCommunication Issue: Transferring features to devices with activated experts\nleads to significant communication overhead. (2) Computational Load Issue:\nSkewed expert activation overloads certain GPUs, resulting in load imbalance\nacross devices. Among these, communication overhead is identified as the main\nbottleneck in SMoE inference. Nevertheless, reducing communication between\ndevices may exacerbate computational load imbalance, leading to device idleness\nand resource waste. Therefore, we present GRACE-MoE, short for Grouping and\nReplication with Locality-Aware Routing for SMoE inference. GRACE-MoE is a\nco-optimization framework that jointly reduces communication overhead and\nalleviates computational load imbalance. Specifically, the framework comprises\ntwo key phases: (1) Grouping & Replication: This phase groups experts based on\ntheir affinity to reduce cross-device communication. Additionally, dynamic\nreplication is applied to address load skew, improving computational load\nbalance across GPUs. (2) Routing: This phase employs a locality-aware routing\nstrategy with load prediction. It prioritizes local replicas to minimize\ncommunication overhead and balances requests across remote replicas when\nnecessary. Experiments on diverse models and multi-node, multi-GPU environments\ndemonstrate that GRACE-MoE efficiently reduces end-to-end inference latency,\nachieving up to 3.79x speedup over state-of-the-art systems. Code for GRACE-MoE\nwill be released upon acceptance.", "AI": {"tldr": "GRACE-MoE\u662f\u4e00\u4e2a\u9488\u5bf9\u7a00\u758f\u4e13\u5bb6\u6df7\u5408\u6a21\u578b\u63a8\u7406\u7684\u534f\u540c\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u7ec4\u590d\u5236\u548c\u5c40\u90e8\u611f\u77e5\u8def\u7531\u6765\u51cf\u5c11\u901a\u4fe1\u5f00\u9500\u5e76\u7f13\u89e3\u8ba1\u7b97\u8d1f\u8f7d\u4e0d\u5e73\u8861\uff0c\u5b9e\u73b0\u6700\u9ad83.79\u500d\u7684\u52a0\u901f\u3002", "motivation": "\u7a00\u758f\u4e13\u5bb6\u6df7\u5408\u6a21\u578b\u5728\u5206\u5e03\u5f0f\u90e8\u7f72\u65f6\u9762\u4e34\u4e24\u4e2a\u5173\u952e\u6311\u6218\uff1a\u901a\u4fe1\u5f00\u9500\u5927\u548c\u8ba1\u7b97\u8d1f\u8f7d\u4e0d\u5747\u8861\uff0c\u5176\u4e2d\u901a\u4fe1\u5f00\u9500\u662f\u4e3b\u8981\u74f6\u9888\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a1) \u57fa\u4e8e\u4e13\u5bb6\u4eb2\u548c\u6027\u7684\u5206\u7ec4\u548c\u52a8\u6001\u590d\u5236\u6765\u51cf\u5c11\u8de8\u8bbe\u5907\u901a\u4fe1\uff1b2) \u5e26\u8d1f\u8f7d\u9884\u6d4b\u7684\u5c40\u90e8\u611f\u77e5\u8def\u7531\u7b56\u7565\uff0c\u4f18\u5148\u4f7f\u7528\u672c\u5730\u526f\u672c\u3002", "result": "\u5728\u591a\u8282\u70b9\u591aGPU\u73af\u5883\u4e0b\u7684\u5b9e\u9a8c\u8868\u660e\uff0cGRACE-MoE\u80fd\u6709\u6548\u51cf\u5c11\u7aef\u5230\u7aef\u63a8\u7406\u5ef6\u8fdf\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u7cfb\u7edf\u5b9e\u73b0\u6700\u9ad83.79\u500d\u52a0\u901f\u3002", "conclusion": "GRACE-MoE\u901a\u8fc7\u534f\u540c\u4f18\u5316\u901a\u4fe1\u548c\u8ba1\u7b97\u8d1f\u8f7d\uff0c\u6210\u529f\u89e3\u51b3\u4e86SMoE\u63a8\u7406\u4e2d\u7684\u5173\u952e\u74f6\u9888\u95ee\u9898\u3002"}}
{"id": "2509.25121", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2509.25121", "abs": "https://arxiv.org/abs/2509.25121", "authors": ["Anvitha Ramachandran", "Dhruv Parikh", "Viktor Prasanna"], "title": "Accelerating Dynamic Image Graph Construction on FPGA for Vision GNNs", "comment": "IEEE HPEC 2025", "summary": "Vision Graph Neural Networks (Vision GNNs, or ViGs) represent images as\nunstructured graphs, achieving state of the art performance in computer vision\ntasks such as image classification, object detection, and instance\nsegmentation. Dynamic Image Graph Construction (DIGC) builds image graphs by\nconnecting patches (nodes) based on feature similarity, and is dynamically\nrepeated in each ViG layer following GNN based patch (node) feature updates.\nHowever, DIGC constitutes over 50% of end to end ViG inference latency, rising\nto 95% at high image resolutions, making it the dominant computational\nbottleneck. While hardware acceleration holds promise, prior works primarily\noptimize graph construction algorithmically, often compromising DIGC\nflexibility, accuracy, or generality. To address these limitations, we propose\na streaming, deeply pipelined FPGA accelerator for DIGC, featuring on chip\nbuffers that process input features in small, uniform blocks. Our design\nminimizes external memory traffic via localized computation and performs\nefficient parallel sorting with local merge sort and global k way merging\ndirectly on streaming input blocks via heap insertion. This modular\narchitecture scales seamlessly across image resolutions, ViG layer types, and\nmodel sizes and variants, and supports DIGC across diverse ViG based vision\nbackbones. The design achieves high clock frequencies post place and route due\nto the statically configured parallelism minimizing critical path delay and\ndelivers up to 16.6x and 6.8x speedups over optimized CPU and GPU DIGC\nbaselines.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u52a8\u6001\u56fe\u50cf\u56fe\u6784\u5efa\uff08DIGC\uff09\u7684FPGA\u52a0\u901f\u5668\uff0c\u89e3\u51b3\u4e86DIGC\u5728Vision GNNs\u4e2d\u5360\u4e3b\u5bfc\u5730\u4f4d\u7684\u8ba1\u7b97\u74f6\u9888\u95ee\u9898\uff0c\u76f8\u6bd4CPU\u548cGPU\u5b9e\u73b0\u4e86\u663e\u8457\u52a0\u901f\u3002", "motivation": "DIGC\u5728Vision GNNs\u4e2d\u5360\u636e\u4e86\u8d85\u8fc750%\u7684\u7aef\u5230\u7aef\u63a8\u7406\u5ef6\u8fdf\uff0c\u5728\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u4e0b\u751a\u81f3\u8fbe\u523095%\uff0c\u6210\u4e3a\u4e3b\u8981\u8ba1\u7b97\u74f6\u9888\u3002\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u901a\u8fc7\u7b97\u6cd5\u4f18\u5316\uff0c\u4f46\u5f80\u5f80\u727a\u7272\u4e86DIGC\u7684\u7075\u6d3b\u6027\u3001\u51c6\u786e\u6027\u6216\u901a\u7528\u6027\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u6d41\u5f0f\u3001\u6df1\u5ea6\u6d41\u6c34\u7ebf\u7684FPGA\u52a0\u901f\u5668\uff0c\u91c7\u7528\u7247\u4e0a\u7f13\u51b2\u5668\u4ee5\u5c0f\u5757\u5747\u5300\u5757\u5904\u7406\u8f93\u5165\u7279\u5f81\u3002\u901a\u8fc7\u5c40\u90e8\u5316\u8ba1\u7b97\u6700\u5c0f\u5316\u5916\u90e8\u5185\u5b58\u6d41\u91cf\uff0c\u4f7f\u7528\u5c40\u90e8\u5f52\u5e76\u6392\u5e8f\u548c\u5168\u5c40k\u8def\u5408\u5e76\u8fdb\u884c\u9ad8\u6548\u5e76\u884c\u6392\u5e8f\u3002", "result": "\u8be5\u8bbe\u8ba1\u5728\u5e03\u5c40\u5e03\u7ebf\u540e\u5b9e\u73b0\u4e86\u9ad8\u65f6\u949f\u9891\u7387\uff0c\u9759\u6001\u914d\u7f6e\u7684\u5e76\u884c\u6027\u6700\u5c0f\u5316\u4e86\u5173\u952e\u8def\u5f84\u5ef6\u8fdf\uff0c\u76f8\u6bd4\u4f18\u5316\u7684CPU\u548cGPU DIGC\u57fa\u7ebf\u5b9e\u73b0\u4e86\u6700\u9ad816.6\u500d\u548c6.8\u500d\u7684\u52a0\u901f\u6bd4\u3002", "conclusion": "\u63d0\u51fa\u7684\u6a21\u5757\u5316\u67b6\u6784\u80fd\u591f\u65e0\u7f1d\u6269\u5c55\u5230\u4e0d\u540c\u56fe\u50cf\u5206\u8fa8\u7387\u3001ViG\u5c42\u7c7b\u578b\u548c\u6a21\u578b\u5927\u5c0f\uff0c\u652f\u6301\u591a\u79cd\u57fa\u4e8eViG\u7684\u89c6\u89c9\u9aa8\u5e72\u7f51\u7edc\u4e2d\u7684DIGC\u64cd\u4f5c\u3002"}}
{"id": "2509.25155", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2509.25155", "abs": "https://arxiv.org/abs/2509.25155", "authors": ["Neelesh Gupta", "Rakshith Jayanth", "Dhruv Parikh", "Viktor Prasanna"], "title": "Context-Driven Performance Modeling for Causal Inference Operators on Neural Processing Units", "comment": "IEEE HiPC 2025", "summary": "The proliferation of large language models (LLMs) has driven demand for long\ncontext inference on resource constrained edge devices. However, deploying\nthese models on Neural Processing Units (NPUs) presents significant challenges\ndue to the architectural mismatch: quadratic complexity of standard attention\nmechanisms conflicts with memory and compute patterns of edge accelerators.\nThis paper presents a comprehensive performance analysis of various causal\ninference operators on a modern NPU. We benchmark standard quadratic attention\nagainst several sub-quadratic alternatives, including structured state-space\nand linear attention models. Our analysis reveals that while sub-quadratic\nmethods offer superior scalability, they introduce distinct computational\nbottlenecks on the NPU's specialized execution units. We identify that\nquadratic attention becomes severely memory-bound, suffering from cache\ninefficiency and pipeline stalls exceeding 95% at long contexts. In contrast,\nsub-quadratic models can become compute-bound on programmable vector cores.\nThese findings provide critical insights for the co-design of hardware-aware\nmodels and optimization strategies to enable on-device AI inference with\nlong-contexts.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u5728NPU\u4e0a\u90e8\u7f72\u957f\u4e0a\u4e0b\u6587LLM\u7684\u6027\u80fd\u95ee\u9898\uff0c\u6bd4\u8f83\u4e86\u6807\u51c6\u4e8c\u6b21\u6ce8\u610f\u529b\u4e0e\u591a\u79cd\u6b21\u4e8c\u6b21\u66ff\u4ee3\u65b9\u6cd5\u7684\u6027\u80fd\u8868\u73b0\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u7684\u666e\u53ca\uff0c\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\u8fdb\u884c\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u9700\u6c42\u589e\u52a0\uff0c\u4f46\u6807\u51c6\u6ce8\u610f\u529b\u673a\u5236\u7684\u4e8c\u6b21\u590d\u6742\u5ea6\u4e0e\u8fb9\u7f18\u52a0\u901f\u5668\u7684\u5185\u5b58\u548c\u8ba1\u7b97\u6a21\u5f0f\u5b58\u5728\u67b6\u6784\u4e0d\u5339\u914d\u95ee\u9898\u3002", "method": "\u5728\u73b0\u4ee3NPU\u4e0a\u5bf9\u5404\u79cd\u56e0\u679c\u63a8\u7406\u7b97\u5b50\u8fdb\u884c\u5168\u9762\u7684\u6027\u80fd\u5206\u6790\uff0c\u57fa\u51c6\u6d4b\u8bd5\u6807\u51c6\u4e8c\u6b21\u6ce8\u610f\u529b\u4e0e\u51e0\u79cd\u6b21\u4e8c\u6b21\u66ff\u4ee3\u65b9\u6cd5\uff08\u5305\u62ec\u7ed3\u6784\u5316\u72b6\u6001\u7a7a\u95f4\u548c\u7ebf\u6027\u6ce8\u610f\u529b\u6a21\u578b\uff09\u3002", "result": "\u5206\u6790\u663e\u793a\uff0c\u6b21\u4e8c\u6b21\u65b9\u6cd5\u5177\u6709\u66f4\u597d\u7684\u53ef\u6269\u5c55\u6027\uff0c\u4f46\u5728NPU\u7684\u4e13\u7528\u6267\u884c\u5355\u5143\u4e0a\u5f15\u5165\u4e86\u72ec\u7279\u7684\u8ba1\u7b97\u74f6\u9888\u3002\u4e8c\u6b21\u6ce8\u610f\u529b\u5728\u957f\u4e0a\u4e0b\u6587\u4e0b\u4e25\u91cd\u53d7\u5185\u5b58\u9650\u5236\uff0c\u7f13\u5b58\u6548\u7387\u4f4e\u4e0b\uff0c\u6d41\u6c34\u7ebf\u505c\u987f\u8d85\u8fc795%\uff1b\u800c\u6b21\u4e8c\u6b21\u6a21\u578b\u53ef\u80fd\u5728\u53ef\u7f16\u7a0b\u5411\u91cf\u6838\u5fc3\u4e0a\u53d7\u8ba1\u7b97\u9650\u5236\u3002", "conclusion": "\u8fd9\u4e9b\u53d1\u73b0\u4e3a\u786c\u4ef6\u611f\u77e5\u6a21\u578b\u548c\u4f18\u5316\u7b56\u7565\u7684\u534f\u540c\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u5173\u952e\u89c1\u89e3\uff0c\u4ee5\u5b9e\u73b0\u8bbe\u5907\u7aef\u7684\u957f\u4e0a\u4e0b\u6587AI\u63a8\u7406\u3002"}}
