{"id": "2508.04833", "categories": ["cs.DC", "E.4, C.2.4"], "pdf": "https://arxiv.org/pdf/2508.04833", "abs": "https://arxiv.org/abs/2508.04833", "authors": ["Nicolas Nicolaou", "Onyeka Obi", "Aayush Rajasekaran", "Alejandro Bergasov", "Aleksandr Bezobchuk", "Kishori M. Konwar", "Michael Meier", "Santiago Paiva", "Har Preet Singh", "Swarnabha Sinha"], "title": "OPTIMUMP2P: Fast and Reliable Gossiping in P2P Networks", "comment": null, "summary": "Gossip algorithms are pivotal in the dissemination of information within\ndecentralized systems. Consequently, numerous gossip libraries have been\ndeveloped and widely utilized especially in blockchain protocols for the\npropagation of blocks and transactions. A well-established library is libp2p,\nwhich provides two gossip algorithms: floodsup and gossibsup. These algorithms\nenable the delivery of published messages to a set of peers. In this work we\naim to enhance the performance and reliability of libp2p by introducing\nOPTIMUMP2P, a novel gossip algorithm that leverages the capabilities of Random\nLinear Network Coding (RLNC) to expedite the dissemination of information in a\npeer-to-peer (P2P) network while ensuring reliable delivery, even in the\npresence of malicious actors capable of corrupting the transmitted data.\nPreliminary research from the Ethereum Foundation has demonstrated the use of\nRLNC in the significant improvement in the block propagation time [14]. Here we\npresent extensive evaluation results both in simulation and real-world\nenvironments that demonstrate the performance gains of OPTIMUMP2P over the\nGossipsub protocol.", "AI": {"tldr": "OPTIMUMP2P\u662f\u4e00\u79cd\u65b0\u578b\u7684gossip\u7b97\u6cd5\uff0c\u5229\u7528RLNC\u6280\u672f\u63d0\u5347P2P\u7f51\u7edc\u4e2d\u4fe1\u606f\u4f20\u64ad\u7684\u6027\u80fd\u548c\u53ef\u9760\u6027\uff0c\u4f18\u4e8e\u73b0\u6709\u7684Gossipsub\u534f\u8bae\u3002", "motivation": "\u63d0\u5347libp2p\u7684\u6027\u80fd\u548c\u53ef\u9760\u6027\uff0c\u7279\u522b\u662f\u5728\u5b58\u5728\u6076\u610f\u884c\u4e3a\u8005\u65f6\u786e\u4fdd\u4fe1\u606f\u53ef\u9760\u4f20\u8f93\u3002", "method": "\u5f15\u5165\u57fa\u4e8eRLNC\u7684OPTIMUMP2P\u7b97\u6cd5\uff0c\u4f18\u5316\u4fe1\u606f\u4f20\u64ad\u901f\u5ea6\u548c\u53ef\u9760\u6027\u3002", "result": "\u5728\u4eff\u771f\u548c\u5b9e\u9645\u73af\u5883\u4e2d\u9a8c\u8bc1\u4e86OPTIMUMP2P\u4f18\u4e8eGossipsub\u534f\u8bae\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "OPTIMUMP2P\u901a\u8fc7RLNC\u6280\u672f\u663e\u8457\u63d0\u5347\u4e86P2P\u7f51\u7edc\u4e2d\u7684\u4fe1\u606f\u4f20\u64ad\u6548\u7387\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2508.04870", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2508.04870", "abs": "https://arxiv.org/abs/2508.04870", "authors": ["Khaled Jawhar", "Evangelos Kranakis"], "title": "Linear Search for Capturing an Oblivious Mobile Target in the Sender/Receiver Model", "comment": null, "summary": "We consider linear search for capturing an oblivious moving target by two\nautonomous robots with different communicating abilities. Both robots can\ncommunicate Face-to-Face (F2F) when co-located but in addition one robot is a\nSender (can also send messages wirelessly) and the other also a Receiver (can\nalso receive messages wirelessly). This is known as Sender/Receiver (S/R, for\nshort) communication model. The robots can move with max speed $1$. The moving\ntarget starts at distance $d$ from the origin and can move either with speed\n$v<1$ away from the origin in the ``away'' model or with speed $v \\geq 0$\ntoward the origin in the ``toward'' model. We assume that the direction of\nmotion of the target (i.e., whether it is the away or toward model) is known to\nthe robots in advance. To capture the target the two robots must be co-located\nwith it.\n  We design new linear search algorithms and analyze the competitive ratio of\nthe time required to capture the target. The approach takes into account\nvarious scenarios related to what the robots know about the search environment\n(e.g., starting distance or speed of the mobile, away or toward model, or a\ncombination thereof). Our study contributes to understanding how asymmetric\ncommunication affects the competitive ratio of linear search.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u4e24\u4e2a\u5177\u6709\u4e0d\u540c\u901a\u4fe1\u80fd\u529b\u7684\u81ea\u4e3b\u673a\u5668\u4eba\u5982\u4f55\u901a\u8fc7\u7ebf\u6027\u641c\u7d22\u6355\u83b7\u4e00\u4e2a\u79fb\u52a8\u76ee\u6807\uff0c\u5206\u6790\u4e86\u4e0d\u540c\u901a\u4fe1\u6a21\u578b\u5bf9\u7ade\u4e89\u6bd4\u7684\u5f71\u54cd\u3002", "motivation": "\u63a2\u8ba8\u4e0d\u5bf9\u79f0\u901a\u4fe1\u80fd\u529b\uff08Sender/Receiver\u6a21\u578b\uff09\u5982\u4f55\u5f71\u54cd\u7ebf\u6027\u641c\u7d22\u7684\u6548\u7387\uff0c\u7279\u522b\u662f\u5728\u76ee\u6807\u79fb\u52a8\u901f\u5ea6\u3001\u65b9\u5411\u548c\u8d77\u59cb\u8ddd\u79bb\u5df2\u77e5\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u8bbe\u8ba1\u4e86\u65b0\u7684\u7ebf\u6027\u641c\u7d22\u7b97\u6cd5\uff0c\u8003\u8651\u4e86\u76ee\u6807\u79fb\u52a8\u7684\u4e24\u79cd\u6a21\u578b\uff08away\u548ctoward\uff09\uff0c\u5e76\u5206\u6790\u4e86\u4e0d\u540c\u73af\u5883\u4fe1\u606f\uff08\u5982\u8d77\u59cb\u8ddd\u79bb\u3001\u901f\u5ea6\uff09\u5bf9\u7b97\u6cd5\u7684\u5f71\u54cd\u3002", "result": "\u901a\u8fc7\u7b97\u6cd5\u8bbe\u8ba1\u548c\u5206\u6790\uff0c\u5f97\u51fa\u4e86\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u6355\u83b7\u76ee\u6807\u6240\u9700\u65f6\u95f4\u7684\u7ade\u4e89\u6bd4\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u4e0d\u5bf9\u79f0\u901a\u4fe1\u80fd\u529b\u5bf9\u7ebf\u6027\u641c\u7d22\u7684\u7ade\u4e89\u6bd4\u6709\u663e\u8457\u5f71\u54cd\uff0c\u4e3a\u76f8\u5173\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u89e3\u548c\u7b97\u6cd5\u652f\u6301\u3002"}}
{"id": "2508.04944", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2508.04944", "abs": "https://arxiv.org/abs/2508.04944", "authors": ["Craig Barnes", "Kyle Burton", "Michael S. Fitzsimons", "Hara Prasad Juvvala", "Brienna Larrick", "Christopher Meyer", "Pauline Ribeyre", "Ao Liu", "Clint Malson", "Noah Metoki-Shlubsky", "Andrii Prokhorenkov", "Jawad Qureshi", "Radhika Reddy", "L. Philip Schumm", "Mingfei Shao", "Trevar Simmons", "Alexander VanTol", "Peter Vassilatos", "Aarti Venkat", "Robert L. Grossman"], "title": "Managing, Analyzing and Sharing Research Data with Gen3 Data Commons", "comment": "20 pages, 9 figures", "summary": "Gen3 is an open-source data platform for building data commons. A data\ncommons is a cloud-based data platform for managing, analyzing, and sharing\ndata with a research community. Gen3 has been used to build over a dozen data\ncommons that in aggregate contain over 28 PB of data and 64 million FAIR data\nobjects. To set up a Gen3 data commons, you first define a data model. Gen3\nthen autogenerates 1) a data portal for searching and exploring data in the\ncommons; 2) a data portal for submitting data to the commons; and 3) FAIR APIs\nfor accessing the data programmatically. Gen3 is built over a small number of\nstandards-based software services, which are designed to support current and\nfuture Gen3 components so that Gen3 can interoperate with other data platforms\nand data ecosystems.", "AI": {"tldr": "Gen3\u662f\u4e00\u4e2a\u5f00\u6e90\u6570\u636e\u5e73\u53f0\uff0c\u7528\u4e8e\u6784\u5efa\u6570\u636e\u5171\u4eab\u7a7a\u95f4\uff0c\u652f\u6301\u7ba1\u7406\u3001\u5206\u6790\u548c\u5171\u4eab\u7814\u7a76\u6570\u636e\u3002", "motivation": "\u4e3a\u7814\u7a76\u793e\u533a\u63d0\u4f9b\u4e00\u4e2a\u4e91\u6570\u636e\u5e73\u53f0\uff0c\u4fc3\u8fdb\u6570\u636e\u7ba1\u7406\u548c\u5171\u4eab\u3002", "method": "\u901a\u8fc7\u5b9a\u4e49\u6570\u636e\u6a21\u578b\u81ea\u52a8\u751f\u6210\u6570\u636e\u95e8\u6237\u548cFAIR API\u3002", "result": "\u5df2\u652f\u6301\u6784\u5efa\u591a\u4e2a\u6570\u636e\u5171\u4eab\u7a7a\u95f4\uff0c\u7ba1\u7406\u8d85\u8fc728 PB\u6570\u636e\u548c6400\u4e07FAIR\u6570\u636e\u5bf9\u8c61\u3002", "conclusion": "Gen3\u57fa\u4e8e\u6807\u51c6\u5316\u670d\u52a1\u8bbe\u8ba1\uff0c\u652f\u6301\u4e0e\u5176\u4ed6\u6570\u636e\u5e73\u53f0\u548c\u751f\u6001\u7cfb\u7edf\u7684\u4e92\u64cd\u4f5c\u6027\u3002"}}
{"id": "2508.04953", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.04953", "abs": "https://arxiv.org/abs/2508.04953", "authors": ["Song Bian", "Saurabh Agarwal", "Md. Tareq Mahmood", "Shivaram Venkataraman"], "title": "Tesserae: Scalable Placement Policies for Deep Learning Workloads", "comment": "16 pages, 18 figures", "summary": "Training deep learning (DL) models has become a dominant workload in\ndata-centers and improving resource utilization is a key goal of DL cluster\nschedulers. In order to do this, schedulers typically incorporate placement\npolicies that govern where jobs are placed on the cluster. Existing placement\npolicies are either designed as ad-hoc heuristics or incorporated as\nconstraints within a complex optimization problem and thus either suffer from\nsuboptimal performance or poor scalability. Our key insight is that many\nplacement constraints can be formulated as graph matching problems and based on\nthat we design novel placement policies for minimizing job migration overheads\nand job packing. We integrate these policies into Tesserae and describe how our\ndesign leads to a scalable and effective GPU cluster scheduler. Our\nexperimental results show that Tesserae improves average JCT by up to 1.62x and\nthe Makespan by up to 1.15x compared with the existing schedulers.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u5339\u914d\u7684GPU\u96c6\u7fa4\u8c03\u5ea6\u5668Tesserae\uff0c\u901a\u8fc7\u4f18\u5316\u4efb\u52a1\u653e\u7f6e\u7b56\u7565\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8d44\u6e90\u5229\u7528\u7387\u548c\u8c03\u5ea6\u6548\u7387\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u8bad\u7ec3\u5df2\u6210\u4e3a\u6570\u636e\u4e2d\u5fc3\u7684\u4e3b\u8981\u8d1f\u8f7d\uff0c\u73b0\u6709\u8c03\u5ea6\u5668\u7684\u4efb\u52a1\u653e\u7f6e\u7b56\u7565\u8981\u4e48\u6027\u80fd\u4e0d\u4f73\uff0c\u8981\u4e48\u6269\u5c55\u6027\u5dee\uff0c\u4e9f\u9700\u6539\u8fdb\u3002", "method": "\u5c06\u4efb\u52a1\u653e\u7f6e\u7ea6\u675f\u5efa\u6a21\u4e3a\u56fe\u5339\u914d\u95ee\u9898\uff0c\u8bbe\u8ba1\u65b0\u7684\u653e\u7f6e\u7b56\u7565\u4ee5\u51cf\u5c11\u4efb\u52a1\u8fc1\u79fb\u5f00\u9500\u5e76\u4f18\u5316\u4efb\u52a1\u6253\u5305\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cTesserae\u5c06\u5e73\u5747\u4f5c\u4e1a\u5b8c\u6210\u65f6\u95f4\uff08JCT\uff09\u63d0\u5347\u81f31.62\u500d\uff0c\u603b\u5b8c\u6210\u65f6\u95f4\uff08Makespan\uff09\u63d0\u5347\u81f31.15\u500d\u3002", "conclusion": "Tesserae\u901a\u8fc7\u56fe\u5339\u914d\u65b9\u6cd5\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684GPU\u96c6\u7fa4\u8c03\u5ea6\u3002"}}
{"id": "2508.04829", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2508.04829", "abs": "https://arxiv.org/abs/2508.04829", "authors": ["Devora Chait-Roth", "Kedar S. Namjoshi", "Thomas Wies"], "title": "Consistent Updates for Scalable Microservices", "comment": null, "summary": "Online services are commonly implemented with a scalable microservice\narchitecture, where isomorphic worker processes service client requests,\nrecording persistent state in a backend data store. To maintain service, any\nmodifications to the service functionality must be made on the fly -- i.e., as\nthe service continues to process client requests -- but doing so is\nchallenging. The central difficulty is that of avoiding potential\ninconsistencies caused by ''mixed mode'' operation, where workers of current\nand new versions are concurrently active and interact via the data store. Some\nupdate methods avoid mixed mode altogether, but only at the cost of substantial\ninefficiency -- by doubling resources (memory and compute), or by halving\nthroughput. The alternative is a so-called ''rolling'' update, which is\nuncontrolled and runs the risk of serious service failures arising from\ninconsistent mixed-mode behavior.\n  In this paper, we present the first algorithms that guarantee consistency for\nmixed mode updates. The algorithms rely on semantic properties of service\nactions, such as commutativity. We show that semantic awareness is required, by\nproving that any semantically oblivious, mixed-mode update method cannot avoid\ninconsistencies. Ideally, it should appear to every client that a service\nupdate takes effect atomically; this ensures that a client is not exposed to\ninconsistent mixed-mode behavior. We introduce a framework that formalizes this\nintuition and develop foundational theory for reasoning about the consistency\nof mixed-mode updates, applying that theory to derive the new algorithms and\nestablish their correctness.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u9996\u4e2a\u4fdd\u8bc1\u6df7\u5408\u6a21\u5f0f\u66f4\u65b0\u4e00\u81f4\u6027\u7684\u7b97\u6cd5\uff0c\u5229\u7528\u670d\u52a1\u884c\u4e3a\u7684\u8bed\u4e49\u5c5e\u6027\uff08\u5982\u4ea4\u6362\u6027\uff09\u907f\u514d\u4e0d\u4e00\u81f4\u6027\u3002", "motivation": "\u5728\u7ebf\u670d\u52a1\u901a\u5e38\u91c7\u7528\u53ef\u6269\u5c55\u7684\u5fae\u670d\u52a1\u67b6\u6784\uff0c\u4f46\u52a8\u6001\u4fee\u6539\u529f\u80fd\u65f6\uff0c\u65b0\u65e7\u7248\u672c\u5de5\u4f5c\u8fdb\u7a0b\u7684\u6df7\u5408\u64cd\u4f5c\u53ef\u80fd\u5bfc\u81f4\u4e0d\u4e00\u81f4\u6027\u3002\u73b0\u6709\u65b9\u6cd5\u8981\u4e48\u6548\u7387\u4f4e\u4e0b\uff0c\u8981\u4e48\u65e0\u6cd5\u907f\u514d\u4e0d\u4e00\u81f4\u6027\u3002", "method": "\u901a\u8fc7\u5206\u6790\u670d\u52a1\u884c\u4e3a\u7684\u8bed\u4e49\u5c5e\u6027\uff08\u5982\u4ea4\u6362\u6027\uff09\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u6846\u67b6\u548c\u7406\u8bba\uff0c\u7528\u4e8e\u63a8\u5bfc\u65b0\u7684\u7b97\u6cd5\u5e76\u8bc1\u660e\u5176\u6b63\u786e\u6027\u3002", "result": "\u8bc1\u660e\u4e86\u8bed\u4e49\u611f\u77e5\u662f\u907f\u514d\u4e0d\u4e00\u81f4\u6027\u7684\u5fc5\u8981\u6761\u4ef6\uff0c\u5e76\u63d0\u51fa\u4e86\u9996\u4e2a\u4fdd\u8bc1\u4e00\u81f4\u6027\u7684\u6df7\u5408\u6a21\u5f0f\u66f4\u65b0\u7b97\u6cd5\u3002", "conclusion": "\u8bed\u4e49\u611f\u77e5\u7684\u7b97\u6cd5\u80fd\u591f\u6709\u6548\u907f\u514d\u6df7\u5408\u6a21\u5f0f\u66f4\u65b0\u7684\u4e0d\u4e00\u81f4\u6027\uff0c\u4e3a\u5728\u7ebf\u670d\u52a1\u7684\u52a8\u6001\u66f4\u65b0\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u7528\u65b9\u6cd5\u3002"}}
{"id": "2508.05266", "categories": ["cs.AR", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.05266", "abs": "https://arxiv.org/abs/2508.05266", "authors": ["Jiazheng Zhang", "Cheng Liu", "Huawei Li"], "title": "Understanding and Mitigating Errors of LLM-Generated RTL Code", "comment": "14 pages, 26 figures", "summary": "Despite the promising potential of large language model (LLM) based\nregister-transfer-level (RTL) code generation, the overall success rate remains\nunsatisfactory. Errors arise from various factors, with limited understanding\nof specific failure causes hindering improvement. To address this, we conduct a\ncomprehensive error analysis and manual categorization. Our findings reveal\nthat most errors stem not from LLM reasoning limitations, but from insufficient\nRTL programming knowledge, poor understanding of circuit concepts, ambiguous\ndesign descriptions, or misinterpretation of complex multimodal inputs.\nLeveraging in-context learning, we propose targeted error correction\ntechniques. Specifically, we construct a domain-specific knowledge base and\nemploy retrieval-augmented generation (RAG) to supply necessary RTL knowledge.\nTo mitigate ambiguity errors, we introduce design description rules and\nimplement a rule-checking mechanism. For multimodal misinterpretation, we\nintegrate external tools to convert inputs into LLM-compatible meta-formats.\nFor remaining errors, we adopt an iterative debugging loop (simulation-error\nlocalization-correction). Integrating these techniques into an LLM-based\nframework significantly improves performance. We incorporate these error\ncorrection techniques into a foundational LLM-based RTL code generation\nframework, resulting in significantly improved performance. Experimental\nresults show that our enhanced framework achieves 91.0\\% accuracy on the\nVerilogEval benchmark, surpassing the baseline code generation approach by\n32.7\\%, demonstrating the effectiveness of our methods.", "AI": {"tldr": "\u8bba\u6587\u901a\u8fc7\u5206\u6790LLM\u5728RTL\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u9519\u8bef\u539f\u56e0\uff0c\u63d0\u51fa\u9488\u5bf9\u6027\u4fee\u6b63\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u751f\u6210\u51c6\u786e\u7387\u3002", "motivation": "\u5c3d\u7ba1LLM\u5728RTL\u4ee3\u7801\u751f\u6210\u4e2d\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u6210\u529f\u7387\u4ecd\u4e0d\u7406\u60f3\uff0c\u7f3a\u4e4f\u5bf9\u5177\u4f53\u5931\u8d25\u539f\u56e0\u7684\u7406\u89e3\u963b\u788d\u4e86\u6539\u8fdb\u3002", "method": "\u901a\u8fc7\u9519\u8bef\u5206\u6790\u548c\u5206\u7c7b\uff0c\u63d0\u51fa\u57fa\u4e8e\u4e0a\u4e0b\u6587\u5b66\u4e60\u7684\u4fee\u6b63\u6280\u672f\uff0c\u5305\u62ec\u6784\u5efa\u9886\u57df\u77e5\u8bc6\u5e93\u3001\u5f15\u5165\u8bbe\u8ba1\u89c4\u5219\u548c\u8fed\u4ee3\u8c03\u8bd5\u3002", "result": "\u589e\u5f3a\u540e\u7684\u6846\u67b6\u5728VerilogEval\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u523091.0%\u51c6\u786e\u7387\uff0c\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u63d0\u534732.7%\u3002", "conclusion": "\u9488\u5bf9\u6027\u4fee\u6b63\u6280\u672f\u6709\u6548\u89e3\u51b3\u4e86LLM\u5728RTL\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u4e3b\u8981\u9519\u8bef\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002"}}
{"id": "2508.05020", "categories": ["cs.DC", "cs.CE", "cs.MS"], "pdf": "https://arxiv.org/pdf/2508.05020", "abs": "https://arxiv.org/abs/2508.05020", "authors": ["Anjiang Wei", "Hang Song", "Mert Hidayetoglu", "Elliott Slaughter", "Sanjiva K. Lele", "Alex Aiken"], "title": "Task-Based Programming for Adaptive Mesh Refinement in Compressible Flow Simulations", "comment": null, "summary": "High-order solvers for compressible flows are vital in scientific\napplications. Adaptive mesh refinement (AMR) is a key technique for reducing\ncomputational cost by concentrating resolution in regions of interest. In this\nwork, we develop an AMR-based numerical solver using Regent, a high-level\nprogramming language for the Legion programming model. We address several\nchallenges associated with implementing AMR in Regent. These include dynamic\ndata structures for patch refinement/coarsening, mesh validity enforcement, and\nreducing task launch overhead via task fusion. Experimental results show that\ntask fusion achieves 18x speedup, while automated GPU kernel generation via\nsimple annotations yields 9.7x speedup for the targeted kernel. We demonstrate\nour approach through simulations of two canonical compressible flow problems\ngoverned by the Euler equations.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e\u81ea\u9002\u5e94\u7f51\u683c\u7ec6\u5316\uff08AMR\uff09\u7684\u9ad8\u9636\u6c42\u89e3\u5668\uff0c\u89e3\u51b3\u4e86\u5728Regent\u4e2d\u5b9e\u73b0AMR\u7684\u6311\u6218\uff0c\u5305\u62ec\u52a8\u6001\u6570\u636e\u7ed3\u6784\u3001\u7f51\u683c\u6709\u6548\u6027\u5f3a\u5236\u548c\u4efb\u52a1\u878d\u5408\u3002\u5b9e\u9a8c\u663e\u793a\u4efb\u52a1\u878d\u5408\u5e26\u676518\u500d\u52a0\u901f\uff0cGPU\u5185\u6838\u751f\u6210\u5e26\u67659.7\u500d\u52a0\u901f\u3002", "motivation": "\u4e3a\u79d1\u5b66\u5e94\u7528\u4e2d\u7684\u53ef\u538b\u7f29\u6d41\u63d0\u4f9b\u9ad8\u6548\u7684\u9ad8\u9636\u6c42\u89e3\u5668\uff0c\u540c\u65f6\u901a\u8fc7AMR\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "method": "\u4f7f\u7528Regent\u7f16\u7a0b\u8bed\u8a00\u5f00\u53d1AMR\u6c42\u89e3\u5668\uff0c\u89e3\u51b3\u52a8\u6001\u6570\u636e\u7ed3\u6784\u3001\u7f51\u683c\u6709\u6548\u6027\u5f3a\u5236\u548c\u4efb\u52a1\u878d\u5408\u7b49\u6311\u6218\u3002", "result": "\u4efb\u52a1\u878d\u5408\u5b9e\u73b018\u500d\u52a0\u901f\uff0cGPU\u5185\u6838\u751f\u6210\u5b9e\u73b09.7\u500d\u52a0\u901f\u3002", "conclusion": "\u901a\u8fc7\u4e24\u4e2a\u6b27\u62c9\u65b9\u7a0b\u63a7\u5236\u7684\u53ef\u538b\u7f29\u6d41\u95ee\u9898\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2508.05354", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2508.05354", "abs": "https://arxiv.org/abs/2508.05354", "authors": ["Michael Rogenmoser", "Angelo Garofalo", "Luca Benini"], "title": "relOBI: A Reliable Low-latency Interconnect for Tightly-Coupled On-chip Communication", "comment": "2 pages extended abstract, accepted at IIRW 2025", "summary": "On-chip communication is a critical element of modern systems-on-chip (SoCs),\nallowing processor cores to interact with memory and peripherals. Interconnects\nrequire special care in radiation-heavy environments, as any soft error within\nthe SoC interconnect is likely to cause a functional failure of the whole SoC.\nThis work proposes relOBI, an extension to Open Bus Interface (OBI) combining\ntriple modular redundancy (TMR) for critical handshake signals with error\ncorrection codes (ECC) protection on other signals for complete reliability.\nImplementing and testing a fully reliable crossbar shows improved reliability\nto injected faults from a vulnerability of 34.85 % to 0 % compared to a\nreference design, with an area increase of 2.6x and 1.4x timing impact. The\narea overhead is 1.8x lower than that reported in the literature for\nfine-grained triplication and voting.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3arelOBI\u7684\u6269\u5c55\u65b9\u6848\uff0c\u7ed3\u5408\u4e09\u6a21\u5197\u4f59\uff08TMR\uff09\u548c\u7ea0\u9519\u7801\uff08ECC\uff09\uff0c\u663e\u8457\u63d0\u5347\u4e86SoC\u4e92\u8fde\u7684\u53ef\u9760\u6027\uff0c\u5c06\u6545\u969c\u7387\u4ece34.85%\u964d\u81f30%\u3002", "motivation": "\u5728\u8f90\u5c04\u5bc6\u96c6\u73af\u5883\u4e2d\uff0cSoC\u4e92\u8fde\u7684\u8f6f\u9519\u8bef\u53ef\u80fd\u5bfc\u81f4\u6574\u4e2a\u7cfb\u7edf\u5931\u6548\uff0c\u56e0\u6b64\u9700\u8981\u9ad8\u53ef\u9760\u6027\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u6269\u5c55Open Bus Interface\uff08OBI\uff09\uff0c\u7ed3\u5408TMR\u548cECC\u6280\u672f\uff0c\u5bf9\u5173\u952e\u63e1\u624b\u4fe1\u53f7\u548c\u5176\u4ed6\u4fe1\u53f7\u5206\u522b\u8fdb\u884c\u4fdd\u62a4\u3002", "result": "\u6d4b\u8bd5\u663e\u793a\uff0c\u5b8c\u5168\u53ef\u9760\u7684\u4ea4\u53c9\u5f00\u5173\u8bbe\u8ba1\u5c06\u6545\u969c\u7387\u964d\u81f30%\uff0c\u9762\u79ef\u589e\u52a02.6\u500d\uff0c\u65f6\u5e8f\u5f71\u54cd1.4\u500d\uff0c\u9762\u79ef\u5f00\u9500\u6bd4\u6587\u732e\u4e2d\u7684\u7ec6\u7c92\u5ea6\u4e09\u91cd\u5316\u65b9\u6848\u4f4e1.8\u500d\u3002", "conclusion": "relOBI\u65b9\u6848\u5728\u4fdd\u8bc1\u9ad8\u53ef\u9760\u6027\u7684\u540c\u65f6\uff0c\u4f18\u5316\u4e86\u9762\u79ef\u5f00\u9500\uff0c\u9002\u7528\u4e8e\u8f90\u5c04\u5bc6\u96c6\u73af\u5883\u4e2d\u7684SoC\u8bbe\u8ba1\u3002"}}
{"id": "2508.05029", "categories": ["cs.DC", "cs.DB", "H.2.4"], "pdf": "https://arxiv.org/pdf/2508.05029", "abs": "https://arxiv.org/abs/2508.05029", "authors": ["Felipe Arambur\u00fa", "William Malpica", "Kaouther Abrougui", "Amin Aramoon", "Romulo Auccapuclla", "Claude Brisson", "Matthijs Brobbel", "Colby Farrell", "Pradeep Garigipati", "Joost Hoozemans", "Supun Kamburugamuve", "Akhil Nair", "Alexander Ocsa", "Johan Peltenburg", "Rub\u00e9n Quesada L\u00f3pez", "Deepak Sihag", "Ahmet Uyar", "Dhruv Vats", "Michael Wendt", "Jignesh M. Patel", "Rodrigo Arambur\u00fa"], "title": "Theseus: A Distributed and Scalable GPU-Accelerated Query Processing Platform Optimized for Efficient Data Movement", "comment": "6 Pages,6 Figures", "summary": "Online analytical processing of queries on datasets in the many-terabyte\nrange is only possible with costly distributed computing systems. To decrease\nthe cost and increase the throughput, systems can leverage accelerators such as\nGPUs, which are now ubiquitous in the compute infrastructure. This introduces\nmany challenges, the majority of which are related to when, where, and how to\nbest move data around the system. We present Theseus -- a production-ready\nenterprise-scale distributed accelerator-native query engine designed to\nbalance data movement, memory utilization, and computation in an\naccelerator-based system context. Specialized asynchronous control mechanisms\nare tightly coupled to the hardware resources for the purpose of network\ncommunication, data pre-loading, data spilling across memories and storage, and\nGPU compute tasks. The memory subsystem contains a mechanism for fixed-size\npage-locked host memory allocations to increase throughput and reduce memory\nfragmentation. For the TPC-H benchmarks at scale factors ranging from 1k to 30k\non cloud infrastructure, Theseus outperforms Databricks Photon by up to\n$4\\times$ at cost parity. Theseus is capable of processing all queries of the\nTPC-H and TPC-DS benchmarks at scale factor 100k (100 TB scale) with as few as\n2 DGX A100 640GB nodes.", "AI": {"tldr": "Theseus\u662f\u4e00\u4e2a\u5206\u5e03\u5f0f\u52a0\u901f\u5668\u539f\u751f\u67e5\u8be2\u5f15\u64ce\uff0c\u65e8\u5728\u4f18\u5316\u6570\u636e\u79fb\u52a8\u3001\u5185\u5b58\u5229\u7528\u548c\u8ba1\u7b97\uff0c\u663e\u8457\u63d0\u5347\u5927\u89c4\u6a21\u6570\u636e\u5206\u6790\u6027\u80fd\u3002", "motivation": "\u964d\u4f4e\u5927\u89c4\u6a21\u6570\u636e\u5206\u6790\u7684\u6210\u672c\u5e76\u63d0\u9ad8\u541e\u5410\u91cf\uff0c\u5229\u7528GPU\u7b49\u52a0\u901f\u5668\u4f18\u5316\u67e5\u8be2\u5904\u7406\u3002", "method": "\u91c7\u7528\u5f02\u6b65\u63a7\u5236\u673a\u5236\u3001\u56fa\u5b9a\u5927\u5c0f\u9875\u9501\u5b9a\u4e3b\u673a\u5185\u5b58\u5206\u914d\u7b49\u6280\u672f\uff0c\u5e73\u8861\u6570\u636e\u79fb\u52a8\u3001\u5185\u5b58\u548c\u8ba1\u7b97\u3002", "result": "\u5728TPC-H\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cTheseus\u6027\u80fd\u4f18\u4e8eDatabricks Photon\u8fbe4\u500d\uff0c\u4e14\u80fd\u4ee52\u4e2aDGX A100\u8282\u70b9\u5904\u7406100TB\u89c4\u6a21\u6570\u636e\u3002", "conclusion": "Theseus\u5c55\u793a\u4e86\u5728\u5206\u5e03\u5f0f\u52a0\u901f\u5668\u73af\u5883\u4e2d\u9ad8\u6548\u5904\u7406\u5927\u89c4\u6a21\u6570\u636e\u5206\u6790\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.05370", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2508.05370", "abs": "https://arxiv.org/abs/2508.05370", "authors": ["Sumit Kumar", "Arjun Temura", "Naman Sharma", "Ramanjeet Singh", "Meet Dadhania", "Praveen Tammana", "Satananda Burla", "Abed Mohammad Kamaluddin", "Rinku Shah"], "title": "Simulating LLM training workloads for heterogeneous compute and network infrastructure", "comment": null, "summary": "The growing demand for large-scale GPU clusters in distributed model training\npresents a significant barrier to innovation, particularly in model\noptimization, performance tuning, and system-level enhancements. To address\nthis challenge, LLM training simulators are employed to estimate training time\nand guide design decisions. However, the state-of-the-art LLM training\nsimulators assume homogeneous compute and network infrastructure. In practice,\ndevice heterogeneity is inevitable due to resource sharing in cloud\nenvironments, frequent shifts in device generations, and inherent intra-chip\ninterconnect heterogeneity. To address the gap between state-of-the-art and\npractical requirements, we propose the design of a heterogeneity-aware\ndistributed LLM simulator capable of predicting training time while enabling\nabstractions to specify custom configurations for device groups and\ndevice-to-parallelism mapping. We present the design requirements and\nchallenges in building a heterogeneity-aware distributed ML training simulator,\nand design components such as non-uniform workload partitioning. Our initial\nsimulation results demonstrate the impact of heterogeneity on the model\ncomputation and communication time.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5f02\u6784\u611f\u77e5\u7684\u5206\u5e03\u5f0fLLM\u6a21\u62df\u5668\uff0c\u7528\u4e8e\u9884\u6d4b\u8bad\u7ec3\u65f6\u95f4\u5e76\u652f\u6301\u81ea\u5b9a\u4e49\u8bbe\u5907\u914d\u7f6e\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6a21\u62df\u5668\u5047\u8bbe\u540c\u8d28\u57fa\u7840\u8bbe\u65bd\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u5927\u89c4\u6a21GPU\u96c6\u7fa4\u5728\u5206\u5e03\u5f0f\u6a21\u578b\u8bad\u7ec3\u4e2d\u7684\u9700\u6c42\u589e\u957f\uff0c\u4f46\u73b0\u6709LLM\u8bad\u7ec3\u6a21\u62df\u5668\u5047\u8bbe\u57fa\u7840\u8bbe\u65bd\u540c\u8d28\uff0c\u65e0\u6cd5\u5e94\u5bf9\u5b9e\u8df5\u4e2d\u8bbe\u5907\u5f02\u6784\u6027\u5e26\u6765\u7684\u6311\u6218\u3002", "method": "\u8bbe\u8ba1\u4e86\u5f02\u6784\u611f\u77e5\u7684\u5206\u5e03\u5f0fLLM\u6a21\u62df\u5668\uff0c\u652f\u6301\u81ea\u5b9a\u4e49\u8bbe\u5907\u7ec4\u914d\u7f6e\u548c\u975e\u5747\u5300\u5de5\u4f5c\u8d1f\u8f7d\u5206\u533a\uff0c\u4ee5\u66f4\u51c6\u786e\u5730\u9884\u6d4b\u8bad\u7ec3\u65f6\u95f4\u3002", "result": "\u521d\u6b65\u6a21\u62df\u7ed3\u679c\u663e\u793a\u5f02\u6784\u6027\u5bf9\u6a21\u578b\u8ba1\u7b97\u548c\u901a\u4fe1\u65f6\u95f4\u6709\u663e\u8457\u5f71\u54cd\u3002", "conclusion": "\u5f02\u6784\u611f\u77e5\u6a21\u62df\u5668\u586b\u8865\u4e86\u73b0\u6709\u6280\u672f\u4e0e\u5b9e\u9645\u9700\u6c42\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u4e3a\u5206\u5e03\u5f0f\u8bad\u7ec3\u4f18\u5316\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002"}}
{"id": "2508.05511", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2508.05511", "abs": "https://arxiv.org/abs/2508.05511", "authors": ["Rasman Mubtasim Swargo", "Engin Arslan", "Md Arifuzzaman"], "title": "Adaptive Parallel Downloader for Large Genomic Datasets", "comment": null, "summary": "Modern next-generation sequencing (NGS) projects routinely generate terabytes\nof data, which researchers commonly download from public repositories such as\nSRA or ENA. Existing download tools often employ static concurrency settings,\nleading to inefficient bandwidth utilization and prolonged download times due\nto their inability to adapt to dynamic network conditions. We introduce\nFastBioDL, a parallel file downloader designed for large biological datasets,\nfeaturing an adaptive concurrency controller. FastBioDL frames the download\nprocess as an online optimization problem, utilizing a utility function and\ngradient descent to adjust the number of concurrent socket streams in real-time\ndynamically. This approach maximizes download throughput while minimizing\nresource overhead. Comprehensive evaluations on public genomic datasets\ndemonstrate that FastBioDL achieves up to $4x$ speedup over state-of-the-art\ntools. Moreover, in high-speed network experiments, its adaptive design was up\nto $2.1x$ faster than existing tools. By intelligently optimizing standard HTTP\nor FTP downloads on the client side, FastBioDL provides a robust and efficient\nsolution for large-scale genomic data acquisition, democratizing\nhigh-performance data retrieval for researchers without requiring specialized\ncommercial software or protocols.", "AI": {"tldr": "FastBioDL\u662f\u4e00\u79cd\u81ea\u9002\u5e94\u5e76\u884c\u6587\u4ef6\u4e0b\u8f7d\u5de5\u5177\uff0c\u7528\u4e8e\u5927\u578b\u751f\u7269\u6570\u636e\u96c6\uff0c\u901a\u8fc7\u5b9e\u65f6\u8c03\u6574\u5e76\u53d1\u6d41\u6570\u91cf\uff0c\u663e\u8457\u63d0\u5347\u4e0b\u8f7d\u901f\u5ea6\u3002", "motivation": "\u73b0\u6709\u4e0b\u8f7d\u5de5\u5177\u56e0\u9759\u6001\u5e76\u53d1\u8bbe\u7f6e\u65e0\u6cd5\u9002\u5e94\u52a8\u6001\u7f51\u7edc\u6761\u4ef6\uff0c\u5bfc\u81f4\u5e26\u5bbd\u5229\u7528\u4f4e\u6548\u548c\u4e0b\u8f7d\u65f6\u95f4\u957f\u3002", "method": "FastBioDL\u5c06\u4e0b\u8f7d\u8fc7\u7a0b\u5efa\u6a21\u4e3a\u5728\u7ebf\u4f18\u5316\u95ee\u9898\uff0c\u4f7f\u7528\u6548\u7528\u51fd\u6570\u548c\u68af\u5ea6\u4e0b\u964d\u52a8\u6001\u8c03\u6574\u5e76\u53d1\u6d41\u6570\u91cf\u3002", "result": "\u5728\u516c\u5171\u57fa\u56e0\u7ec4\u6570\u636e\u96c6\u4e0a\uff0cFastBioDL\u6bd4\u73b0\u6709\u5de5\u5177\u5feb4\u500d\uff0c\u9ad8\u901f\u7f51\u7edc\u4e0b\u5feb2.1\u500d\u3002", "conclusion": "FastBioDL\u4e3a\u5927\u89c4\u6a21\u57fa\u56e0\u7ec4\u6570\u636e\u83b7\u53d6\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u65e0\u9700\u4e13\u4e1a\u5546\u4e1a\u8f6f\u4ef6\u3002"}}
{"id": "2508.05546", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2508.05546", "abs": "https://arxiv.org/abs/2508.05546", "authors": ["Rasman Mubtasim Swargo", "Engin Arslan", "Md Arifuzzaman"], "title": "Modular Architecture for High-Performance and Low Overhead Data Transfers", "comment": null, "summary": "High-performance applications necessitate rapid and dependable transfer of\nmassive datasets across geographically dispersed locations. Traditional file\ntransfer tools often suffer from resource underutilization and instability\nbecause of fixed configurations or monolithic optimization methods. We propose\nAutoMDT, a novel modular data transfer architecture that employs a deep\nreinforcement learning based agent to simultaneously optimize concurrency\nlevels for read, network, and write operations. Our solution incorporates a\nlightweight network-system simulator, enabling offline training of a Proximal\nPolicy Optimization (PPO) agent in approximately 45 minutes on average, thereby\novercoming the impracticality of lengthy online training in production\nnetworks. AutoMDT's modular design decouples I/O and network tasks, allowing\nthe agent to capture complex buffer dynamics precisely and to adapt quickly to\nchanging system and network conditions. Evaluations on production-grade\ntestbeds show that AutoMDT achieves up to 8x faster convergence and a 68%\nreduction in transfer completion times compared with state-of-the-art\nsolutions.", "AI": {"tldr": "AutoMDT\u662f\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u6a21\u5757\u5316\u6570\u636e\u4f20\u8f93\u67b6\u6784\uff0c\u901a\u8fc7\u4f18\u5316\u5e76\u53d1\u7ea7\u522b\u663e\u8457\u63d0\u5347\u6570\u636e\u4f20\u8f93\u6027\u80fd\u3002", "motivation": "\u9ad8\u6027\u80fd\u5e94\u7528\u9700\u8981\u5feb\u901f\u53ef\u9760\u5730\u4f20\u8f93\u5927\u89c4\u6a21\u6570\u636e\uff0c\u4f46\u4f20\u7edf\u5de5\u5177\u56e0\u56fa\u5b9a\u914d\u7f6e\u6216\u5355\u4e00\u4f18\u5316\u65b9\u6cd5\u5bfc\u81f4\u8d44\u6e90\u5229\u7528\u4e0d\u8db3\u548c\u4e0d\u7a33\u5b9a\u3002", "method": "\u91c7\u7528\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08PPO\uff09\u4ee3\u7406\u4f18\u5316\u5e76\u53d1\u7ea7\u522b\uff0c\u7ed3\u5408\u8f7b\u91cf\u7ea7\u7f51\u7edc\u7cfb\u7edf\u6a21\u62df\u5668\u8fdb\u884c\u79bb\u7ebf\u8bad\u7ec3\u3002", "result": "\u5728\u6d4b\u8bd5\u4e2d\uff0cAutoMDT\u6bd4\u73b0\u6709\u65b9\u6848\u5feb8\u500d\u6536\u655b\uff0c\u4f20\u8f93\u5b8c\u6210\u65f6\u95f4\u51cf\u5c1168%\u3002", "conclusion": "AutoMDT\u901a\u8fc7\u6a21\u5757\u5316\u8bbe\u8ba1\u548c\u79bb\u7ebf\u8bad\u7ec3\uff0c\u9ad8\u6548\u9002\u5e94\u52a8\u6001\u6761\u4ef6\uff0c\u663e\u8457\u63d0\u5347\u6570\u636e\u4f20\u8f93\u6027\u80fd\u3002"}}
