<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 2]
- [cs.DC](#cs.DC) [Total: 5]
- [cs.AR](#cs.AR) [Total: 1]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Relational Hoare Logic for High-Level Synthesis of Hardware Accelerators](https://arxiv.org/abs/2601.09217)
*Izumi Tanaka,Ken Sakayori,Shinya Takamaeda-Yamazaki,Naoki Kobayashi*

Main category: cs.PL

TL;DR: 提出基于关系Hoare逻辑的形式化翻译框架，自动优化HLS程序中的内存访问模式，通过插入片上缓冲和流处理替换非顺序处理，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有HLS工具的内存自动优化对编程风格敏感且缺乏透明度，需要大量手动调优，难以设计高效的内存系统。

Method: 基于关系Hoare逻辑的形式化翻译框架，识别朴素HLS程序中的复杂内存访问模式，自动插入片上缓冲以强制线性访问外部内存，并用流处理替换非顺序处理。

Result: 原型翻译器结合商用HLS编译器和真实FPGA板进行实验，展示了显著的性能提升。

Conclusion: 提出的形式化框架能够实现稳健透明的程序转换，有效解决HLS中内存系统设计的自动化优化问题。

Abstract: High-level synthesis (HLS) is a powerful tool for developing efficient hardware accelerators that rely on specialized memory systems to achieve sufficient on-chip data reuse and off-chip bandwidth utilization. However, even with HLS, designing such systems still requires careful manual tuning, as automatic optimizations provided by existing tools are highly sensitive to programming style and often lack transparency. To address these issues, we present a formal translation framework based on relational Hoare logic, which enables robust and transparent transformations. Our method recognizes complex memory access patterns in naïve HLS programs and automatically transforms them by inserting on-chip buffers to enforce linear access to off-chip memory, and by replacing non-sequential processing with stream processing, while preserving program semantics. Experiments using our prototype translator, combined with an off-the-shelf HLS compiler and a real FPGA board, have demonstrated significant performance improvements.

</details>


### [2] [MLIR-Forge: A Modular Framework for Language Smiths](https://arxiv.org/abs/2601.09583)
*Berke Ates,Philipp Schaad,Timo Schneider,Alexandru Calotoiu,Torsten Hoefler*

Main category: cs.PL

TL;DR: MLIR-Forge是一个基于MLIR的随机程序生成框架，旨在简化针对编译器中间表示（IR）的专用程序生成器的开发，通过分离语言特定组件和可重用程序创建逻辑来降低复杂性。


<details>
  <summary>Details</summary>
Motivation: 领域特定语言（DSL）使用高级中间表示（IR）进行优化，但测试这些IR很复杂。随机程序生成器是测试编译器的有效工具，但为编译器IR开发专用程序生成器既困难又耗时。

Method: MLIR-Forge将生成过程分解为语言特定的基础构建块和可重用的程序创建逻辑，利用MLIR的灵活性简化专用程序生成器的创建。语言特定组件可以使用一组通用工具定义。

Result: 使用MLIR-Forge为MLIR内置方言、WebAssembly和数据中心程序表示DaCe生成程序，每个仅需不到一周的开发时间。通过差分测试发现了9个MLIR、15个WebAssembly和774个DaCe相关的bug组。

Conclusion: MLIR-Forge显著降低了为编译器IR创建随机程序生成器的复杂性，有效支持差分测试和模糊测试，能够高效发现编译器实现中的bug。

Abstract: Optimizing compilers are essential for the efficient and correct execution of software across various scientific fields. Domain-specific languages (DSL) typically use higher level intermediate representations (IR) in their compiler pipelines for domain-specific optimizations. As these IRs add to complexity, it is crucial to test them thoroughly. Random program generators have proven to be an effective tool to test compilers through differential and fuzz testing. However, developing specialized program generators for compiler IRs is not straightforward and demands considerable resources. We introduce MLIR-Forge, a novel random program generator framework that leverages the flexibility of MLIR, aiming to simplify the creation of specialized program generators. MLIR-Forge achieves this by splitting the generation process into fundamental building blocks that are language specific, and reusable program creation logic that constructs random programs from these building blocks. This hides complexity and furthermore, even the language specific components can be defined using a set of common tools. We demonstrate MLIR-Forge's capabilities by generating MLIR with built-in dialects, WebAssembly, and a data-centric program representation, DaCe -- requiring less than a week of development time in total for each of them. Using the generated programs we conduct differential testing and find 9 MLIR, 15 WebAssembly, and 774 DaCe groups of bugs with the corresponding program generators, after running them until the rate of new bugs stagnates.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [3] [A Machine Learning Approach Towards Runtime Optimisation of Matrix Multiplication](https://arxiv.org/abs/2601.09114)
*Yufan Xia,Marco De La Pierre,Amanda S. Barnard,Giuseppe Maria Junior Barca*

Main category: cs.DC

TL;DR: 使用机器学习动态选择最优线程数以优化GEMM性能，在两种HPC架构上实现25-40%加速


<details>
  <summary>Details</summary>
Motivation: 现代多核共享内存系统复杂，难以确定最小化多线程GEMM运行时的线程数，需要智能优化方法

Method: 构建ADSALA软件库，使用机器学习模型实时根据训练数据为给定GEMM任务自动选择最优线程数

Result: 在Intel Cascade Lake和AMD Zen 3两种HPC节点架构上测试，内存使用100MB内的GEMM相比传统BLAS实现获得25-40%加速

Conclusion: 机器学习方法能有效优化GEMM性能，证明ADSALA概念可行，为线性代数库优化提供新方向

Abstract: The GEneral Matrix Multiplication (GEMM) is one of the essential algorithms in scientific computing. Single-thread GEMM implementations are well-optimised with techniques like blocking and autotuning. However, due to the complexity of modern multi-core shared memory systems, it is challenging to determine the number of threads that minimises the multi-thread GEMM runtime. We present a proof-of-concept approach to building an Architecture and Data-Structure Aware Linear Algebra (ADSALA) software library that uses machine learning to optimise the runtime performance of BLAS routines. More specifically, our method uses a machine learning model on-the-fly to automatically select the optimal number of threads for a given GEMM task based on the collected training data. Test results on two different HPC node architectures, one based on a two-socket Intel Cascade Lake and the other on a two-socket AMD Zen 3, revealed a 25 to 40 per cent speedup compared to traditional GEMM implementations in BLAS when using GEMM of memory usage within 100 MB.

</details>


### [4] [Transaction-Driven Dynamic Reconfiguration for Certificate-Based Payment Systems](https://arxiv.org/abs/2601.09146)
*Lingkang Shangguan*

Main category: cs.DC

TL;DR: 提出基于拜占庭一致性广播的交易驱动动态重配置协议PDCC，通过避免全局交易排序实现高性能支付系统


<details>
  <summary>Details</summary>
Motivation: 现代支付系统需要高性能的动态重配置能力，同时避免传统全局交易排序带来的性能瓶颈

Method: 基于拜占庭一致性广播，结合用户nonce的交易排序和周期性系统范围共识机制，设计PDCC协议实现平滑重配置

Result: PDCC能够在不影响原系统性能的情况下实现平滑的重配置过程

Conclusion: 交易驱动的动态重配置协议为现代支付系统提供了高性能的重配置解决方案，避免了全局排序的开销

Abstract: We present a transaction-driven dynamic reconfiguration protocol in Modern payment systems based on Byzantine Consistent Broadcast which can achieve high performance by avoiding global transaction ordering. We demonstrate the fundamental paradigm of modern payment systems, which combines user nonce based transactions ordering with periodic system-wide consensus mechanisms. Building on this foundation, we design PDCC(Payment Dynamic Config Change), which can lead a smooth reconfiguration process without impacting the original system's performance.

</details>


### [5] [Optimizing View Change for Byzantine Fault Tolerance in Parallel Consensus](https://arxiv.org/abs/2601.09184)
*Yifei Xie,Btissam Er-Rahmadi,Xiao Chen,Tiejun Ma,Jane Hillston*

Main category: cs.DC

TL;DR: 提出基于混合整数规划的视图变更优化模型，通过优化领导者选择和追随者重新分配来提升并行BFT协议性能


<details>
  <summary>Details</summary>
Motivation: 现有并行BFT协议的视图变更机制采用被动盲选领导者，经常选择不可用或慢速节点作为领导者，导致性能下降，需要优化领导者选择策略

Method: 提出基于混合整数规划的视图变更优化模型，采用分解方法处理高效子问题并改进Benders割，基于分解结果设计迭代备份领导者选择算法

Result: 在微软Azure云环境中实验表明，VCO驱动的并行BFT在正常运行和故障条件下均优于现有配置方法，且随着网络规模增大效果更显著

Conclusion: VCO模型能有效优化并行BFT的视图变更过程，为高性能并行BFT系统提供合适解决方案

Abstract: The parallel Byzantine Fault Tolerant (BFT) protocol is viewed as a promising solution to address the consensus scalability issue of the permissioned blockchain. One of the main challenges in parallel BFT is the view change process that happens when the leader node fails, which can lead to performance bottlenecks. Existing parallel BFT protocols typically rely on passive view change mechanisms with blind leader rotation. Such approaches frequently select unavailable or slow nodes as leaders, resulting in degraded performance. To address these challenges, we propose a View Change Optimization (VCO) model based on mixed integer programming that optimizes leader selection and follower reassignment across parallel committees by considering communication delays and failure scenarios. We applied a decomposition method with efficient subproblems and improved benders cuts to solve the VCO model. Leveraging the results of improved decomposition solution method, we propose an efficient iterative backup leader selection algorithm as views proceed. By performing experiments in Microsoft Azure cloud environments, we demonstrate that the VCO-driven parallel BFT outperforms existing configuration methods under both normal operation and faulty condition. The results show that the VCO model is effective as network size increases, making it a suitable solution for high-performance parallel BFT systems.

</details>


### [6] [LatencyPrism: Online Non-intrusive Latency Sculpting for SLO-Guaranteed LLM Inference](https://arxiv.org/abs/2601.09258)
*Du Yin,Jiayi Ren,Xiayu Sun,Tianyao Zhou,Haizhu Zhou,Ruiyan Ma,Danyang Zhang*

Main category: cs.DC

TL;DR: LatencyPrism是一个零侵入、多平台的延迟雕刻系统，用于实时监控和分析LLM推理延迟，无需代码修改或服务重启，能区分正常延迟变化与异常，准确率达0.98 F1-score。


<details>
  <summary>Details</summary>
Motivation: LLM推理延迟直接影响用户体验和运营成本，分布式推理环境的软件框架和硬件架构多样性，加上动态工作负载，使得延迟分析变得困难。现有AI分析方法要么需要侵入性设计（需要服务重启或暂停），要么硬件绑定无法适应异构环境，不适合实时生产分析。

Method: LatencyPrism采用零侵入多平台延迟雕刻系统，无需代码修改或服务重启。系统能够分解推理延迟跨流水线，主动预警推理延迟异常，并保证SLO遵守。支持低开销的批级别实时监控，毫秒级触发警报。

Result: 已在数千个XPU上部署超过六个月，能够以毫秒级触发警报进行实时监控。系统能区分工作负载驱动的延迟变化与指示潜在问题的异常，F1-score达到0.98。通过广泛实验和根本原因分析验证了系统能力。

Conclusion: LatencyPrism是首个零侵入多平台延迟雕刻系统，成功解决了分布式LLM推理环境中的延迟监控和分析挑战，无需修改代码或重启服务，在真实生产环境中表现出色。

Abstract: LLM inference latency critically determines user experience and operational costs, directly impacting throughput under SLO constraints. Even brief latency spikes degrade service quality despite acceptable average performance. However, distributed inference environments featuring diverse software frameworks and XPU architectures combined with dynamic workloads make latency analysis challenging. Constrained by intrusive designs that necessitate service restarts or even suspension, and by hardware-bound implementations that fail to adapt to heterogeneous inference environments, existing AI profiling methods are often inadequate for real-time production analysis.
  We present LatencyPrism, the first zero-intrusion multi-platform latency sculpting system. It aims to break down the inference latency across pipeline, proactively alert on inference latency anomalies, and guarantee adherence to SLOs, all without requiring code modifications or service restarts. LatencyPrism has been deployed across thousands of XPUs for over six months. It enables low-overhead real-time monitoring at batch level with alerts triggered in milliseconds. This approach distinguishes between workload-driven latency variations and anomalies indicating underlying issues with an F1-score of 0.98. We also conduct extensive experiments and investigations into root cause analysis to demonstrate LatencyPrism's capability.

</details>


### [7] [High-Performance Serverless Computing: A Systematic Literature Review on Serverless for HPC, AI, and Big Data](https://arxiv.org/abs/2601.09334)
*Valerio Besozzi,Matteo Della Bartola,Patrizio Dazzi,Marco Danelutto*

Main category: cs.DC

TL;DR: 本文对2018-2025年间122篇研究论文进行系统性文献综述，探讨无服务器计算在云计算、高性能计算及混合环境中处理计算密集型应用的应用，提出了包含8个研究方向、9个用例领域的分类体系。


<details>
  <summary>Details</summary>
Motivation: 随着高性能计算、人工智能和大数据等计算密集型应用的广泛部署，云计算与高性能计算基础设施正在融合。云提供商开始集成高性能计算能力（如硬件加速器、高速互连），而高性能计算社区也开始探索云原生范式以提高可扩展性、弹性和资源利用率。在此背景下，无服务器计算成为处理高度动态、并行和分布式工作负载的有前景执行模型。

Method: 采用系统性文献综述方法，分析了2018年至2025年初期间发表的122篇研究论文。从这些文献中提取关键信息，提出了包含八个主要研究方向、九个目标用例领域的分类体系，并分析了近期的发表趋势和作者合作网络。

Result: 研究展示了无服务器计算在云计算、高性能计算和混合环境中处理并行计算密集型应用的日益增长的兴趣和相互联系。提出了一个全面的分类框架，为研究人员和实践者提供了清晰的路线图，突出了该新兴研究领域的发展趋势和合作模式。

Conclusion: 这项工作为新手研究人员和经验丰富的实践者提供了有价值的基础，指导下一代无服务器解决方案的开发，以支持并行计算密集型应用。无服务器计算在融合云和高性能计算环境中展现出巨大潜力，能够有效处理动态、并行和分布式工作负载。

Abstract: The widespread deployment of large-scale, compute-intensive applications such as high-performance computing, artificial intelligence, and big data is leading to convergence between cloud and high-performance computing infrastructures. Cloud providers are increasingly integrating high-performance computing capabilities in their infrastructures, such as hardware accelerators and high-speed interconnects, while researchers in the high-performance computing community are starting to explore cloud-native paradigms to improve scalability, elasticity, and resource utilization. In this context, serverless computing emerges as a promising execution model to efficiently handle highly dynamic, parallel, and distributed workloads. This paper presents a comprehensive systematic literature review of 122 research articles published between 2018 and early 2025, exploring the use of the serverless paradigm to develop, deploy, and orchestrate compute-intensive applications across cloud, high-performance computing, and hybrid environments. From these, a taxonomy comprising eight primary research directions and nine targeted use case domains is proposed, alongside an analysis of recent publication trends and collaboration networks among authors, highlighting the growing interest and interconnections within this emerging research field. Overall, this work aims to offer a valuable foundation for both new researchers and experienced practitioners, guiding the development of next-generation serverless solutions for parallel compute-intensive applications.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [8] [Annotated PIM Bibliography](https://arxiv.org/abs/2601.09002)
*Peter M. Kogge*

Main category: cs.AR

TL;DR: 本文提供了一份关于内存处理（PIM）技术的注释书目，涵盖其60多年的发展历史，旨在补充即将发表的文章。


<details>
  <summary>Details</summary>
Motivation: 虽然内存处理（PIM）及相关技术最近被视为"革命性新技术"，但实际上许多相关技术可以追溯到60多年前。本文旨在提供一个全面的注释书目，覆盖整个时间范围，以纠正人们对PIM技术历史发展的误解。

Method: 作者通过整理和注释PIM技术相关的文献资料，创建了一个按时间顺序组织的注释书目。该书目涵盖了PIM、CIM、LIM、IMC、NMC等不同术语下的相关技术发展。

Result: 提供了一个全面的PIM技术历史发展注释书目，展示了该技术从60多年前至今的演变过程，揭示了PIM并非全新概念而是有深厚历史渊源。

Conclusion: 内存处理技术有着悠久的历史渊源，而非最近才出现的革命性创新。通过提供全面的历史视角，本文有助于更准确地理解PIM技术的发展脉络和演变过程。

Abstract: Processing in Memory (PIM) and similar terms such as Compute In Memory (CIM), Logic in Memory (LIM), In Memory Computing (IMC), and Near Memory Computing (NMC) have gained attention recently as a potentially ``revolutionary new'' technique. The truth, however, is that many examples of the technology go back over 60 years. This document attempts to provide an annotated bibliography of PIM technology that attempts to cover the whole time-frame, and is organized to augment a forth-coming article.

</details>
