<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 4]
- [cs.DC](#cs.DC) [Total: 11]
- [cs.AR](#cs.AR) [Total: 2]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [LOOPRAG: Enhancing Loop Transformation Optimization with Retrieval-Augmented Large Language Models](https://arxiv.org/abs/2512.15766)
*Yijie Zhi,Yayu Cao,Jianhua Dai,Xiaoyang Han,Jingwen Pu,Qingran Wu,Sheng Cheng,Ming Cai*

Main category: cs.PL

TL;DR: LOOPRAG：基于检索增强生成的循环优化框架，利用参数驱动方法生成多样化合法示例，通过循环感知检索算法和反馈迭代机制指导LLM进行有效的循环变换优化。


<details>
  <summary>Details</summary>
Motivation: 传统循环变换优化复杂且难以找到最优组合，现有LLM在循环优化中常出错或效果不佳，错失性能提升机会，需要新方法指导LLM进行有效循环优化。

Method: 提出LOOPRAG框架：1) 参数驱动方法利用循环属性触发变换生成多样化合法示例；2) 基于循环特征的检索算法平衡相似性与多样性；3) 反馈迭代机制结合编译、测试和性能结果指导LLM；4) 通过变异、覆盖和差分测试进行等价性检查。

Result: 在PolyBench、TSVC和LORE基准测试中，相比基础编译器分别获得最高11.20×、14.34×和9.29×加速，相比基础LLM分别获得最高11.97×、5.61×和11.59×加速。

Conclusion: LOOPRAG通过检索增强生成和反馈迭代机制有效解决了LLM在循环优化中的局限性，显著提升了循环变换优化的性能，为代码优化提供了新方向。

Abstract: Loop transformations are semantics-preserving optimization techniques, widely used to maximize objectives such as parallelism. Despite decades of research, applying the optimal composition of loop transformations remains challenging due to inherent complexities, including cost modeling for optimization objectives. Recent studies have explored the potential of Large Language Models (LLMs) for code optimization. However, our key observation is that LLMs often struggle with effective loop transformation optimization, frequently leading to errors or suboptimal optimization, thereby missing opportunities for performance improvements. To bridge this gap, we propose LOOPRAG, a novel retrieval-augmented generation framework designed to guide LLMs in performing effective loop optimization on Static Control Part. We introduce a parameter-driven method to harness loop properties, which trigger various loop transformations, and generate diverse yet legal example codes serving as a demonstration source. To effectively obtain the most informative demonstrations, we propose a loop-aware algorithm based on loop features, which balances similarity and diversity for code retrieval. To enhance correct and efficient code generation, we introduce a feedback-based iterative mechanism that incorporates compilation, testing and performance results as feedback to guide LLMs. Each optimized code undergoes mutation, coverage and differential testing for equivalence checking. We evaluate LOOPRAG on PolyBench, TSVC and LORE benchmark suites, and compare it against compilers (GCC-Graphite, Clang-Polly, Perspective and ICX) and representative LLMs (DeepSeek and GPT-4). The results demonstrate average speedups over base compilers of up to 11.20$\times$, 14.34$\times$, and 9.29$\times$ for PolyBench, TSVC, and LORE, respectively, and speedups over base LLMs of up to 11.97$\times$, 5.61$\times$, and 11.59$\times$.

</details>


### [2] [Automated Formalization of Probabilistic Requirements from Structured Natural Language](https://arxiv.org/abs/2512.15788)
*Anastasia Mavridou,Marie Farrell,Gricel Vázquez,Tom Pressburger,Timothy E. Wang,Radu Calinescu,Michael Fisher*

Main category: cs.PL

TL;DR: 扩展NASA的FRET工具，支持用结构化自然语言编写概率需求，并自动转换为概率时序逻辑公式，便于自主自适应系统的形式化分析。


<details>
  <summary>Details</summary>
Motivation: 自主自适应系统开发面临概率需求规范困难，现有形式化方法对开发者不友好且易出错，需要更实用的解决方案。

Method: 扩展FRET的结构化自然语言，支持概率需求规范；开发自动转换方法，将需求转换为概率时序逻辑公式；建立验证框架确保公式正确性。

Result: 开发了扩展的FRET工具，能够自动将结构化自然语言概率需求转换为正确的概率时序逻辑公式，提高了形式化分析的实用性和可靠性。

Conclusion: 该方法使自主自适应系统的概率需求规范更加实用和可靠，降低了形式化分析的门槛和错误率。

Abstract: Integrating autonomous and adaptive behavior into software-intensive systems presents significant challenges for software development, as uncertainties in the environment or decision-making processes must be explicitly captured. These challenges are amplified in safety- and mission-critical systems, which must undergo rigorous scrutiny during design and development. Key among these challenges is the difficulty of specifying requirements that use probabilistic constructs to capture the uncertainty affecting these systems. To enable formal analysis, such requirements must be expressed in precise mathematical notations such as probabilistic logics. However, expecting developers to write requirements directly in complex formalisms is unrealistic and highly error-prone. We extend the structured natural language used by NASA's Formal Requirement Elicitation Tool (FRET) with support for the specification of unambiguous and correct probabilistic requirements, and develop an automated approach for translating these requirements into logical formulas. We propose and develop a formal, compositional, and automated approach for translating structured natural-language requirements into formulas in probabilistic temporal logic. To increase trust in our formalizations, we provide assurance that the generated formulas are well-formed and conform to the intended semantics through an automated validation framework and a formal proof. The extended FRET tool enables developers to specify probabilistic requirements in structured natural language, and to automatically translate them into probabilistic temporal logic, making the formal analysis of autonomous and adaptive systems more practical and less error-prone.

</details>


### [3] [A Neurosymbolic Approach to Loop Invariant Generation via Weakest Precondition Reasoning](https://arxiv.org/abs/2512.15816)
*Daragh King,Vasileios Koutavas,Laura Kovacs*

Main category: cs.PL

TL;DR: NeuroInv是一个神经符号方法，通过结合LLM的神经推理和基于反例的符号修复，实现了99.5%的循环不变式生成成功率。


<details>
  <summary>Details</summary>
Motivation: 现有的基于LLM的循环不变式生成方法缺乏可靠的结构化方法，且很少参考现有的程序验证理论。循环不变式生成仍然是自动化程序验证的关键瓶颈。

Method: NeuroInv包含两个关键模块：(1) 神经推理模块：利用LLM和霍尔逻辑，通过后向链式最弱前置条件推理来推导和精化候选不变式；(2) 验证引导的符号模块：使用OpenJML的反例迭代修复不变式。

Result: 在150个Java程序的综合基准测试中，NeuroInv达到了99.5%的成功率，显著优于其他评估方法。在包含10个更大规模多循环程序（平均每个程序7个循环）的困难基准测试中，NeuroInv也表现出色，证明其可以扩展到更复杂的验证场景。

Conclusion: NeuroInv通过神经符号方法成功解决了循环不变式生成问题，将LLM的能力与传统的程序验证理论相结合，为自动化程序验证提供了有效的解决方案。

Abstract: Loop invariant generation remains a critical bottleneck in automated program verification. Recent work has begun to explore the use of Large Language Models (LLMs) in this area, yet these approaches tend to lack a reliable and structured methodology, with little reference to existing program verification theory. This paper presents NeuroInv, a neurosymbolic approach to loop invariant generation. NeuroInv comprises two key modules: (1) a neural reasoning module that leverages LLMs and Hoare logic to derive and refine candidate invariants via backward-chaining weakest precondition reasoning, and (2) a verification-guided symbolic module that iteratively repairs invariants using counterexamples from OpenJML. We evaluate NeuroInv on a comprehensive benchmark of 150 Java programs, encompassing single and multiple (sequential) loops, multiple arrays, random branching, and noisy code segments. NeuroInv achieves a $99.5\%$ success rate, substantially outperforming the other evaluated approaches. Additionally, we introduce a hard benchmark of $10$ larger multi-loop programs (with an average of $7$ loops each); NeuroInv's performance in this setting demonstrates that it can scale to more complex verification scenarios.

</details>


### [4] [Optimizing Agentic Language Model Inference via Speculative Tool Calls](https://arxiv.org/abs/2512.15834)
*Daniel Nichols,Prajwal Singhania,Charles Jekel,Abhinav Bhatele,Harshitha Menon*

Main category: cs.PL

TL;DR: 提出针对语言模型工具调用性能瓶颈的系统优化方案，通过推测工具调用和保持序列驻留来提升推理吞吐量


<details>
  <summary>Details</summary>
Motivation: 语言模型越来越依赖外部工具（文件搜索、代码执行、API调用等），这些工具虽然增强了模型能力，但也在推理过程中引入了性能瓶颈，影响推理吞吐量

Method: 采用推测工具调用和强制序列在推理引擎中保持驻留的系统优化方法，减少开销；提出理论分析指导最佳推测配置，并建议新的"工具缓存"API端点

Result: 优化方案使LM代理推理的吞吐量提升数百个token/秒

Conclusion: 通过系统优化解决工具调用带来的性能瓶颈是可行的，提出的优化方法和API建议能够显著提升语言模型代理的推理效率

Abstract: Language models (LMs) are becoming increasingly dependent on external tools. LM-based agentic frameworks frequently interact with their environment via such tools to search files, run code, call APIs, etc. Further, modern reasoning-based LMs use tools such as web search and Python code execution to enhance their reasoning capabilities. While tools greatly improve the capabilities of LMs, they also introduce performance bottlenecks during the inference process. In this paper, we introduce novel systems optimizations to address such performance bottlenecks by speculating tool calls and forcing sequences to remain resident in the inference engine to minimize overheads. Our optimizations lead to throughput improvements of several hundred tokens per second when hosting inference for LM agents. We provide a theoretical analysis of our algorithms to provide insights into speculation configurations that will yield the best performance. Further, we recommend a new "tool cache" API endpoint to enable LM providers to easily adopt these optimizations.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [5] [LOG.io: Unified Rollback Recovery and Data Lineage Capture for Distributed Data Pipelines](https://arxiv.org/abs/2512.16038)
*Eric Simon,Renato B. Hoffmann,Lucas Alf,Dalvan Griebler*

Main category: cs.DC

TL;DR: LOG.io 是一个用于分布式数据流水线的回滚恢复和细粒度数据血缘捕获系统，采用基于日志的恢复协议，支持非确定性操作符和外部系统交互，在特定条件下（存在慢速操作符、中等事件吞吐量）恢复性能优于ABS协议。


<details>
  <summary>Details</summary>
Motivation: 分布式数据流水线需要可靠的故障恢复机制，现有解决方案如Flink的ABS协议在特定场景下存在性能限制。需要一种支持非确定性操作符、外部系统交互和自定义代码的通用恢复方案，同时实现细粒度数据血缘跟踪。

Method: 采用基于日志的回滚恢复协议，支持非阻塞恢复（失败操作符独立恢复不影响其他操作符），支持数据并行化和动态扩缩容。在SAP Data Intelligence系统中实现并与ABS协议进行对比实验。

Result: 当流水线存在慢速操作符且事件吞吐量中等（如每100ms一个事件）时，LOG.io在正常处理时性能与ABS相当，在恢复时优于ABS。其他情况下ABS表现更好，但数据并行化能显著降低LOG.io开销而ABS无改善。数据血缘捕获开销在所有实验中低于1.5%。

Conclusion: LOG.io为分布式数据流水线提供了有效的回滚恢复和细粒度数据血缘捕获方案，在特定场景下恢复性能优于现有方案，且数据血缘跟踪开销可忽略不计。

Abstract: This paper introduces LOG.io, a comprehensive solution designed for correct rollback recovery and fine-grain data lineage capture in distributed data pipelines. It is tailored for serverless scalable architectures and uses a log-based rollback recovery protocol. LOG.io supports a general programming model, accommodating non-deterministic operators, interactions with external systems, and arbitrary custom code. It is non-blocking, allowing failed operators to recover independently without interrupting other active operators, thereby leveraging data parallelization, and it facilitates dynamic scaling of operators during pipeline execution. Performance evaluations, conducted within the SAP Data Intelligence system, compare LOG.io with the Asynchronous Barrier Snapshotting (ABS) protocol, originally implemented in Flink. Our experiments show that when there are straggler operators in a data pipeline and the throughput of events is moderate (e.g., 1 event every 100 ms), LOG.io performs as well as ABS during normal processing and outperforms ABS during recovery. Otherwise, ABS performs better than LOG.io for both normal processing and recovery. However, we show that in these cases, data parallelization can largely reduce the overhead of LOG.io while ABS does not improve. Finally, we show that the overhead of data lineage capture, at the granularity of the event and between any two operators in a pipeline, is marginal, with less than 1.5% in all our experiments.

</details>


### [6] [MultiPath Transfer Engine: Breaking GPU and Host-Memory Bandwidth Bottlenecks in LLM Services](https://arxiv.org/abs/2512.16056)
*Lingfeng Tang,Daoping Zhang,Junjie Chen,Peihao Huang,Feng Jin,Chengguang Xu,Yuxin Chen,Feiqiang Sun,Guo Chen*

Main category: cs.DC

TL;DR: MMA通过多路径内存访问技术，利用服务器内未充分利用的带宽，显著提升GPU与主机内存间的数据传输性能，从而改善LLM服务的首令牌时间和模型切换延迟。


<details>
  <summary>Details</summary>
Motivation: PCIe带宽已成为大语言模型性能的关键瓶颈，特别是在前缀缓存获取和模型切换场景中。虽然理论上可以在GPU和主机内存之间进行多路径数据传输，但PCIe和NVLink等异构协议限制了带宽只能达到单个PCIe链路的水平，导致服务器内带宽未充分利用。

Method: 提出多路径内存访问（MMA）方案，首次实现GPU与主机内存间的高效多路径数据传输。通过动态库注入实现无缝部署，使LLM应用无需代码修改即可受益于MMA技术。

Result: 在测试平台中，MMA显著提升GPU与内存间的数据传输带宽，峰值带宽达到245GB/s，相比原生单路径带宽实现4.62倍加速。端到端评估显示，MMA将LLM服务的首令牌时间减少1.14-2.38倍，并将vLLM睡眠模式下的模型切换延迟降低1.12-2.48倍。

Conclusion: MMA通过有效利用服务器内未充分利用的带宽，成功解决了PCIe带宽瓶颈问题，显著提升了大语言模型服务的性能，特别是在首令牌时间和模型切换延迟方面取得了实质性改进。

Abstract: The limited bandwidth of PCIe has emerged as the critical bottleneck for large language model (LLM) performance, such as prefix cache fetching and model switching. Although intra-server multipath data transfer between GPU and host memory is theoretically possible, heterogeneous protocols such as PCIe and NVLink currently limit the bandwidth between host memory and GPUs to that of a single PICe link. This limitation resuals in underutilized intra-server bandwidth. To address this issue, we propose Multipath Memory Access (MMA), a scheme that, to the best of our knowledge, is the first to enalbe efficient multipath data transfer between GPU and host memory. MMA supports seamless deployment via dynamic library injection, enabling LLM applications to benefit from MMA without requiring any code modification. In our testbed, MMA significantly improves the data transfer bandwidth between the GPU and memory, achieving a peak bandwidth of 245 GB/s-representing a 4.62x speedup compared to the natice single-path bandwidth. End-to-end evaluations demonstrate that MMA reduces the time-to-first-token (TTFT) for LLM serving by 1.14x to 2.38x and decreases model-switching latency in vLLM's sleep mode by 1.12x to 2.48x.

</details>


### [7] [Twinning for Space-Air-Ground-Sea Integrated Networks: Beyond Conventional Digital Twin Towards Goal-Oriented Semantic Twin](https://arxiv.org/abs/2512.16058)
*Yifei Qiu,Tianle Liao,Xin Jin,Shaohua Wu,Dusit Niyato,Qinyu Zhang*

Main category: cs.DC

TL;DR: 本文提出了一种面向目标的语义孪生（GOST）框架，用于解决空间-空中-地面-海洋一体化网络（SAGSIN）中传统数字孪生面临的高计算开销、模型同步延迟和跨系统语义鸿沟等问题。


<details>
  <summary>Details</summary>
Motivation: 传统数字孪生技术在SAGSIN环境中面临根本性限制，包括计算开销过高、模型同步延迟和跨系统语义鸿沟。需要一种新的孪生框架来支持实时态势感知和智能运维维护。

Method: 提出面向目标的语义孪生（GOST）框架，通过三层结构实现：基于知识的语义层、数据驱动的语义层和面向目标的原则层。该框架优先考虑"效用"而非"保真度"，利用语义技术和面向目标原则构建轻量级、任务特定的表示。

Result: 在远程卫星-无人机网络的协同跟踪任务案例研究中，GOST在感知数据时效性和协同跟踪性能方面显著优于传统数字孪生。

Conclusion: GOST作为一种变革性的孪生范式，通过语义技术和面向目标原则解决了SAGSIN中传统数字孪生的局限性，为6G系统的发展提供了指导方向。

Abstract: A space-air-ground-sea integrated network (SAGSIN) has emerged as a cornerstone of 6G systems, establishing a unified global architecture by integrating multi-domain network resources. Motivated by the demand for real-time situational awareness and intelligent operational maintenance, digital twin (DT) technology was initially regarded as a promising solution, owing to its capability to create virtual replicas and emulate physical system behaviors. However, in the context of SAGSIN, the high-fidelity, full-scale modeling paradigm inherent to conventional DTs encounters fundamental limitations, including prohibitive computational overhead, delayed model synchronization, and cross-system semantic gaps. To address these limitations, this survey paper proposes a novel twinning framework: goal-oriented semantic twin (GOST). Unlike DTs that pursue physical mirroring, GOST prioritizes ``utility'' over ``fidelity,'' leveraging semantic technologies and goal-oriented principles to construct lightweight, task-specific representations. This paper systematically articulates the GOST framework through three layers: knowledge-based semantics, data-driven semantics, and goal-oriented principles. Furthermore, we provide a comprehensive tutorial on constructing GOST by detailing its core enabling technologies and introduce a multidimensional evaluation framework for GOST. We present a case study targeting collaborative tracking tasks in remote satellite-UAV networks, demonstrating that GOST significantly outperforms conventional DTs in timeliness of perceptual data and collaborative tracking. Finally, we outline research directions, establishing GOST as a transformative twinning paradigm to guide the development of SAGSIN.

</details>


### [8] [Cold-Start Anti-Patterns and Refactorings in Serverless Systems: An Empirical Study](https://arxiv.org/abs/2512.16066)
*Syed Salauddin Mohammad Tariq,Foyzul Hassan,Amiangshu Bosu,Probir Roy*

Main category: cs.DC

TL;DR: 该论文针对无服务器计算中的冷启动延迟问题，从开发者可见的设计角度进行研究，提出了SCABENCH基准测试和INITSCOPE分析框架，显著提升了冷启动问题的诊断效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 无服务器计算简化了部署和扩展，但冷启动延迟仍然是主要的性能瓶颈。现有研究将缓解措施视为黑盒优化，而本文从开发者可见的设计问题角度研究冷启动，通过分析81个开源无服务器系统的问题报告来深入理解该问题。

Method: 1. 分析81个开源无服务器系统的裁决问题报告，推导出初始化反模式、修复策略和诊断挑战的分类体系；2. 基于这些洞察，开发SCABENCH可重现基准测试；3. 设计INITSCOPE轻量级分析框架，将加载的代码与执行的代码关联起来。

Result: 在SCABENCH上，INITSCOPE相比现有工具将定位准确率提高了40%，诊断工作量减少了64%。开发者研究显示任务准确率更高且诊断速度更快。研究结果为无服务器设计中的冷启动缓解提供了基于证据的性能感知实践。

Conclusion: 该研究通过将冷启动视为开发者可见的设计问题，提出了系统化的分析方法和工具，显著改善了冷启动问题的诊断效率和准确性，推动了无服务器计算中基于证据的性能感知实践发展。

Abstract: Serverless computing simplifies deployment and scaling, yet cold-start latency remains a major performance bottleneck. Unlike prior work that treats mitigation as a black-box optimization, we study cold starts as a developer-visible design problem. From 81 adjudicated issue reports across open-source serverless systems, we derive taxonomies of initialization anti-patterns, remediation strategies, and diagnostic challenges spanning design, packaging, and runtime layers. Building on these insights, we introduce SCABENCH, a reproducible benchmark, and INITSCOPE, a lightweight analysis framework linking what code is loaded with what is executed. On SCABENCH, INITSCOPE improved localization accuracy by up to 40% and reduced diagnostic effort by 64% compared with prior tools, while a developer study showed higher task accuracy and faster diagnosis. Together, these results advance evidence-driven, performance-aware practices for cold-start mitigation in serverless design. Availability: The research artifact is publicly accessible for future studies and improvements.

</details>


### [9] [An Online Fragmentation-Aware Scheduler for Managing GPU-Sharing Workloads on Multi-Instance GPUs](https://arxiv.org/abs/2512.16099)
*Hsu-Tzu Ting,Jerry Chou,Ming-Hung Chen,I-Hsin Chung*

Main category: cs.DC

TL;DR: 提出在线调度框架，通过条件负载均衡、动态分区和作业迁移，解决NVIDIA MIG GPU共享中的资源争用和碎片化问题，提升系统效率35%


<details>
  <summary>Details</summary>
Motivation: 现代GPU工作负载需要高效资源共享，但NVIDIA MIG虽然提供硬件级GPU分区，仍面临PCIe带宽等共享组件的资源争用问题，以及由于MIG配置有限导致的GPU碎片化问题（包括空间不连续性和刚性配置约束）

Method: 提出在线调度框架，集成条件负载均衡、动态分区和作业迁移技术，动态调整作业放置以减少争用，并重新组织GPU分配以应对内部和外部碎片化

Result: 实验结果显示，当应用所有技术时，makespan（完成时间）最高可提升35%，显著改善系统效率

Conclusion: 提出的在线调度框架能有效解决MIG GPU共享中的资源争用和碎片化问题，显著提升GPU资源利用率和系统性能

Abstract: Modern GPU workloads increasingly demand efficient resource sharing, as many jobs do not require the full capacity of a GPU. Among sharing techniques, NVIDIA's Multi-Instance GPU (MIG) offers strong resource isolation by enabling hardware-level GPU partitioning. However, leveraging MIG effectively introduces new challenges. First, resource contention persists due to shared components such as PCIe bandwidth. Second, GPU fragmentation becomes a critical issue, which is different from prior fine-grained GPU sharing work due to MIG's limited number of valid MIG configurations. Fragmentation arises not only from spatial discontinuity but also from rigid profile placement constraints, especially after job arrivals and terminations. To address these issues, we propose an online scheduling framework that integrates conditional load balancing, dynamic partitioning, and job migration. Our approach dynamically adapts job placement to minimize contention and reorganizes GPU allocations to combat both internal and external fragmentation. Experimental results show that our method significantly improves system efficiency. When all techniques are applied, the makespan improves by up to 35%.

</details>


### [10] [Staggered Batch Scheduling: Co-optimizing Time-to-First-Token and Throughput for High-Efficiency LLM Inference](https://arxiv.org/abs/2512.16134)
*Jian Tian,Shuailong Li,Yang Cao,Wenbo Cui,Minghan Zhu,Wenkang Wu,Jianming Zhang,Yanpeng Wang,Zhiwen Xiao,Zhenyu Hou,Dou Shen*

Main category: cs.DC

TL;DR: 提出Staggered Batch Scheduling (SBS)机制，通过缓冲请求形成最优执行批次，解决DP+EP架构中的调度问题，降低TTFT 30%-40%，提升吞吐量15%-20%


<details>
  <summary>Details</summary>
Motivation: 随着LLM服务向复杂分布式架构（特别是P/D分离的大规模DP+EP范式）演进，传统调度方法无法应对高内部同步成本。立即请求调度会导致严重的引擎内排队和并行化气泡，降低首次令牌时间(TTFT)。

Method: 提出Staggered Batch Scheduling (SBS)机制，通过故意缓冲请求形成最优执行批次，实现时间解耦消除内部排队气泡。同时利用缓冲创建的调度窗口，引入负载感知全局分配策略，平衡DP单元在Prefill和Decode阶段的计算负载。

Result: 在生产H800集群上部署服务Deepseek-V3，相比最先进的立即调度基线，系统将TTFT降低30%-40%，吞吐量提升15%-20%。

Conclusion: Staggered Batch Scheduling机制有效解决了DP+EP架构中的调度挑战，通过缓冲和负载感知调度显著改善了LLM服务的响应时间和吞吐性能。

Abstract: The evolution of Large Language Model (LLM) serving towards complex, distributed architectures--specifically the P/D-separated, large-scale DP+EP paradigm--introduces distinct scheduling challenges. Unlike traditional deployments where schedulers can treat instances as black boxes, DP+EP architectures exhibit high internal synchronization costs. We identify that immediate request dispatching in such systems leads to severe in-engine queuing and parallelization bubbles, degrading Time-to-First-Token (TTFT). To address this, we propose Staggered Batch Scheduling (SBS), a mechanism that deliberately buffers requests to form optimal execution batches. This temporal decoupling eliminates internal queuing bubbles without compromising throughput. Furthermore, leveraging the scheduling window created by buffering, we introduce a Load-Aware Global Allocation strategy that balances computational load across DP units for both Prefill and Decode phases. Deployed on a production H800 cluster serving Deepseek-V3, our system reduces TTFT by 30%-40% and improves throughput by 15%-20% compared to state-of-the-art immediate scheduling baselines.

</details>


### [11] [Lotus: Optimizing Disaggregated Transactions with Disaggregated Locks](https://arxiv.org/abs/2512.16136)
*Zhisheng Hu,Pengfei Zuo,Junliang Hu,Yizou Chen,Yingjia Wang,Ming-Chang Yang*

Main category: cs.DC

TL;DR: Lotus提出了一种在分解内存架构上实现锁分解的分布式事务系统，通过将锁管理从内存节点转移到计算节点，解决了RDMA网络接口卡的性能瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 现有分解内存上的分布式事务系统中，内存节点的RDMA网络接口卡成为主要性能瓶颈，这源于大量用于锁操作的单边原子操作，限制了系统的可扩展性。

Method: 1. 锁分解：将锁从数据中分离，所有锁操作在计算节点上执行；2. 应用感知的锁管理机制：利用OLTP工作负载的局部性进行锁分片，同时保持负载均衡；3. 锁优先事务协议：将锁定阶段作为读写事务执行的第一步；4. 无锁重建恢复机制：将锁视为临时状态，避免在计算节点故障时重建锁。

Result: 实验结果显示，与现有最先进的分解内存事务系统相比，Lotus将事务吞吐量提高了最多2.1倍，并将延迟降低了最多49.4%。

Conclusion: Lotus通过锁分解和创新的锁管理机制，有效解决了分解内存系统中RDMA网络接口卡的瓶颈问题，显著提升了分布式事务系统的性能和可扩展性。

Abstract: Disaggregated memory (DM) separates compute and memory resources, allowing flexible scaling to achieve high resource utilization. To ensure atomic and consistent data access on DM, distributed transaction systems have been adapted, where compute nodes (CNs) rely on one-sided RDMA operations to access remote data in memory nodes (MNs). However, we observe that in existing transaction systems, the RDMA network interface cards at MNs become a primary performance bottleneck. This bottleneck arises from the high volume of one-sided atomic operations used for locks, which hinders the system's ability to scale efficiently.
  To address this issue, this paper presents Lotus, a scalable distributed transaction system with lock disaggregation on DM. The key innovation of Lotus is to disaggregate locks from data and execute all locks on CNs, thus eliminating the bottleneck at MN RNICs. To achieve efficient lock management on CNs, Lotus employs an application-aware lock management mechanism that leverages the locality of the OLTP workloads to shard locks while maintaining load balance. To ensure consistent transaction processing with lock disaggregation, Lotus introduces a lock-first transaction protocol, which separates the locking phase as the first step in each read-write transaction execution. This protocol allows the system to determine the success of lock acquisitions early and proactively abort conflicting transactions, improving overall efficiency. To tolerate lock loss during CN failures, Lotus employs a lock-rebuild-free recovery mechanism that treats locks as ephemeral and avoids their reconstruction, ensuring lightweight recovery for CN failures. Experimental results demonstrate that Lotus improves transaction throughput by up to 2.1$\times$ and reduces latency by up to 49.4% compared to state-of-the-art transaction systems on DM.

</details>


### [12] [FlexKV: Flexible Index Offloading for Memory-Disaggregated Key-Value Store](https://arxiv.org/abs/2512.16148)
*Zhisheng Hu,Jiacheng Shen,Ming-Chang Yang*

Main category: cs.DC

TL;DR: FlexKV是一个内存解耦的键值存储系统，通过索引代理技术将索引动态卸载到计算节点，解决了现有系统对单边原子操作的过度依赖和计算侧缓存效率受限的问题，实现了高达2.94倍的吞吐量提升和85.2%的延迟降低。


<details>
  <summary>Details</summary>
Motivation: 内存解耦架构将CPU和内存解耦为独立资源池以提高利用率，但现有的内存解耦键值存储存在性能问题：1）索引处理过度依赖单边原子操作；2）计算侧缓存效率受限。这些问题导致性能不佳。

Method: FlexKV提出索引代理技术，将索引动态卸载到计算节点，利用其强大CPU加速索引处理。具体解决三个挑战：1）基于等级感知的热度检测算法平衡索引负载；2）两级计算节点内存优化方案高效利用内存；3）RPC聚合的缓存管理机制降低缓存一致性开销。

Result: 实验结果显示，与最先进的内存解耦键值存储相比，FlexKV将吞吐量提升高达2.94倍，延迟降低高达85.2%。

Conclusion: FlexKV通过索引代理技术有效解决了内存解耦键值存储的性能瓶颈，显著提升了系统性能，为内存解耦架构下的高效数据管理提供了新思路。

Abstract: Disaggregated memory (DM) is a promising data center architecture that decouples CPU and memory into independent resource pools to improve resource utilization. Building on DM, memory-disaggregated key-value (KV) stores are adopted to efficiently manage remote data. Unfortunately, existing approaches suffer from poor performance due to two critical issues: 1) the overdependence on one-sided atomic operations in index processing, and 2) the constrained efficiency in compute-side caches. To address these issues, we propose FlexKV, a memory-disaggregated KV store with index proxying. Our key idea is to dynamically offload the index to compute nodes, leveraging their powerful CPUs to accelerate index processing and maintain high-performance compute-side caches. Three challenges have to be addressed to enable efficient index proxying on DM, i.e., the load imbalance across compute nodes, the limited memory of compute nodes, and the expensive cache coherence overhead. FlexKV proposes: 1) a rank-aware hotness detection algorithm to continuously balance index load across compute nodes, 2) a two-level CN memory optimization scheme to efficiently utilize compute node memory, and 3) an RPC-aggregated cache management mechanism to reduce cache coherence overhead. The experimental results show that FlexKV improves throughput by up to 2.94$\times$ and reduces latency by up to 85.2%, compared with the state-of-the-art memory-disaggregated KV stores.

</details>


### [13] [AI4EOSC: a Federated Cloud Platform for Artificial Intelligence in Scientific Research](https://arxiv.org/abs/2512.16455)
*Ignacio Heredia,Álvaro López García,Germán Moltó,Amanda Calatrava,Valentin Kozlov,Alessandro Costantini,Viet Tran,Mario David,Daniel San Martín,Marcin Płóciennik,Marta Obregón Ruiz,Saúl Fernandez,Judith Sáinz-Pardo Díaz,Miguel Caballer,Caterina Alarcón Marín,Stefan Dlugolinsky,Martin Šeleng,Lisana Berberi,Khadijeh Alibabaei,Borja Esteban Sanchis,Pedro Castro,Giacinto Donvito,Diego Aguirre,Sergio Langarita,Vicente Rodriguez,Leonhard Duda,Andrés Heredia Canales,Susana Rebolledo Ruiz,João Machado,Giang Nguyen,Fernando Aguilar Gómez,Jaime Díez*

Main category: cs.DC

TL;DR: 本文介绍了一个专为科学AI工作负载设计的联邦计算平台，提供覆盖完整机器学习生命周期的集成服务，支持模型开发、训练和部署，并注重可重复性和可定制性。


<details>
  <summary>Details</summary>
Motivation: 为科学AI工作负载提供可重复部署的联邦计算平台，实现对物理分布式电子基础设施的一致透明访问，降低外部社区采用门槛。

Method: 通过全面的服务目录构建联邦计算平台，提供交互式开发环境、GPU资源、注释工具、实验跟踪、联邦学习支持，以及覆盖云连续体的多种部署选项。

Result: 平台能够提供完整的机器学习生命周期管理，集成多种AI模型提供商、数据集和存储资源，支持AI模型的可追溯性和可重复性，并与更广泛的机器学习生态系统交互。

Conclusion: 该联邦计算平台成功为科学AI工作负载提供了全面支持，通过可定制化设计降低了外部社区的采用门槛，实现了分布式基础设施的透明访问和完整机器学习生命周期管理。

Abstract: In this paper, we describe a federated compute platform dedicated to support Artificial Intelligence in scientific workloads. Putting the effort into reproducible deployments, it delivers consistent, transparent access to a federation of physically distributed e-Infrastructures. Through a comprehensive service catalogue, the platform is able to offer an integrated user experience covering the full Machine Learning lifecycle, including model development (with dedicated interactive development environments), training (with GPU resources, annotation tools, experiment tracking, and federated learning support) and deployment (covering a wide range of deployment options all along the Cloud Continuum). The platform also provides tools for traceability and reproducibility of AI models, integrates with different Artificial Intelligence model providers, datasets and storage resources, allowing users to interact with the broader Machine Learning ecosystem. Finally, it is easily customizable to lower the adoption barrier by external communities.

</details>


### [14] [Efficient CPU-GPU Collaborative Inference for MoE-based LLMs on Memory-Limited Systems](https://arxiv.org/abs/2512.16473)
*En-Ming Huang,Li-Shang Lin,Chun-Yi Lee*

Main category: cs.DC

TL;DR: 提出了一种CPU-GPU协作推理框架，通过GPU专家缓存机制减少数据传输，利用CPU多线程处理缓存未命中，在消费级硬件上提升MoE模型推理性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型计算需求高，MoE模型虽能选择性激活参数减少计算，但仍需大量内存超出消费级GPU容量。传统卸载方法在CPU和GPU间传输权重会引入延迟，限制推理性能。

Method: 提出CPU-GPU协作推理框架：1) GPU上实现专家缓存机制，减少数据传输需求；2) 计算卸载到CPU，利用CPU多线程优化处理缓存未命中；3) 最大化消费级系统硬件利用率。

Result: 评估显示该框架能提升性能，证明了CPU-GPU协作在单请求推理场景下最大化硬件利用的潜力。框架实现已开源。

Conclusion: CPU-GPU协作推理框架通过专家缓存和计算卸载，有效解决了MoE模型在消费级硬件上的部署挑战，实现了更快的推理速度和更好的硬件利用率。

Abstract: Large Language Models (LLMs) have achieved impressive results across various tasks, yet their high computational demands pose deployment challenges, especially on consumer-grade hardware. Mixture of Experts (MoE) models provide an efficient solution through selective activation of parameter subsets, which reduces computation requirements. Despite this efficiency, state-of-the-art MoE models still require substantial memory beyond typical consumer GPU capacities. Traditional offloading methods that transfer model weights between CPU and GPU introduce latency, limiting inference performance. This paper presents a novel CPU-GPU collaborative inference framework that incorporates an expert caching mechanism on the GPU to reduce data transfer requirements and enable faster inference through cache hits. Computations are offloaded to CPU for efficient cache miss handling, which benefits from CPU multithreading optimizations. The evaluations of our framework demonstrate performance improvements and highlight the potential of CPU-GPU collaboration to maximize hardware utilization for single-request inference scenarios on consumer-grade systems. The implementation of our framework is available at https://github.com/elsa-lab/MoE-CPU-GPU-Collaborative-Inference.

</details>


### [15] [Delay-Aware Multi-Stage Edge Server Upgrade with Budget Constraint](https://arxiv.org/abs/2512.16792)
*Endar Suprih Wihidayat,Sieteng Soh,Kwan-Wu Chin,Duc-son Pham*

Main category: cs.DC

TL;DR: 提出多阶段边缘服务器升级（M-ESU）规划问题，通过混合整数线性规划和启发式算法优化边缘计算系统的长期升级决策，在预算约束下最大化满足延迟要求的任务数量。


<details>
  <summary>Details</summary>
Motivation: 现有的多接入边缘计算（MEC）系统需要长期规划以适应任务增长、延迟要求变严格等变化。传统方法缺乏考虑多阶段升级决策，无法在预算约束下优化长期性能。

Method: 提出M-ESU框架，包含两个关键决策：1）部署新服务器或升级现有服务器；2）任务卸载优化以最大化满足延迟要求的任务数量。提供两种解决方案：混合整数线性规划（MILP）模型用于小型网络，高效的启发式算法（M-ESU/H）用于大规模网络。

Result: 对于小型网络，M-ESU/H算法解与最优解差距在1.25%以内，运行速度快几个数量级。对于大型网络，相比仅考虑服务器部署或优先部署/升级的三种替代启发式方案，M-ESU/H在相同预算和需求增长条件下任务满意度提升高达21.57%。

Conclusion: M-ESU框架为MEC系统的长期规划提供了有效解决方案，M-ESU/H算法在保持接近最优性能的同时具有良好可扩展性，适用于实际大规模边缘计算系统。

Abstract: In this paper, the Multi-stage Edge Server Upgrade (M-ESU) is proposed as a new network planning problem, involving the upgrading of an existing multi-access edge computing (MEC) system through multiple stages (e.g., over several years). More precisely, the problem considers two key decisions: (i) whether to deploy additional edge servers or upgrade those already installed, and (ii) how tasks should be offloaded so that the average number of tasks that meet their delay requirement is maximized. The framework specifically involves: (i) deployment of new servers combined with capacity upgrades for existing servers, and (ii) the optimal task offloading to maximize the average number of tasks with a delay requirement. It also considers the following constraints: (i) budget per stage, (ii) server deployment and upgrade cost (in $) and cost depreciation rate, (iii) computation resource of servers, (iv) number of tasks and their growth rate (in %), and (v) the increase in task sizes and stricter delay requirements over time. We present two solutions: a Mixed Integer Linear Programming (MILP) model and an efficient heuristic algorithm (M-ESU/H). MILP yields the optimal solution for small networks, whereas M-ESU/H is used in large-scale networks. For small networks, the simulation results show that the solution computed by M-ESU/H is within 1.25% of the optimal solution while running several orders of magnitude faster. For large networks, M-ESU/H is compared against three alternative heuristic solutions that consider only server deployment, or giving priority to server deployment or upgrade. Our experiments show that M-ESU/H yields up to 21.57% improvement in task satisfaction under identical budget and demand growth conditions, confirming its scalability and practical value for long-term MEC systems.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [16] [Workload Characterization for Branch Predictability](https://arxiv.org/abs/2512.15827)
*FNU Vikas,Paul Gratz,Daniel Jiménez*

Main category: cs.AR

TL;DR: 该论文提出了一种分支预测的工作负载特征化方法，引入分支工作集大小和分支可预测性两个新指标，用于分析现代分支预测器的准确性。


<details>
  <summary>Details</summary>
Motivation: 条件分支预测是支持指令级并行提取的关键技术，准确的预测器能减少错误路径上的指令执行，提升性能和降低能耗。然而，需要更好的方法来分析和理解不同工作负载对分支预测准确性的影响。

Method: 提出工作负载特征化方法，定义分支工作集为最常出现的分支上下文（分支地址、全局历史、局部历史的三元组）。通过分析分支工作集的大小和可预测性，研究其与现代分支预测器准确性的关系。对2,451个工作负载轨迹进行了特征化分析。

Result: 分支工作集大小和分支可预测性与现代分支预测方案（如TAGE和感知器）的误预测率高度相关。将工作负载轨迹分为7个分支工作集大小类别和9个可预测性类别，提供了关于预测准确性来源和现代分支预测器偏好的工作负载类别的深入见解。

Conclusion: 提出的工作负载特征化方法和两个新指标能有效分析分支预测准确性，为理解现代分支预测器在不同工作负载下的表现提供了系统框架，有助于优化预测器设计和性能评估。

Abstract: Conditional branch prediction predicts the likely direction of a conditional branch instruction to support ILP extraction. Branch prediction is a pattern recognition problem that learns mappings between a context to the branch outcome. An accurate predictor reduces the number of instructions executed on the wrong path resulting in an improvement of performance and energy consumption. In this paper, we present a workload characterization methodology for branch prediction. We propose two new workload-driven branch prediction accuracy identifiers -- branch working set size and branch predictability. These parameters are highly correlated with misprediction rates of modern branch prediction schemes (e.g. TAGE and perceptron). We define the branch working set of a trace as a group of most frequently occurring branch contexts, i.e. the 3-part tuple of branch address, and associated global and local history. We analyze the branch working set's size and predictability on a per-trace basis to study its relationship with a modern branch predictor's accuracy. We have characterized 2,451 workload traces into seven branch working set size and nine predictability categories after analyzing their branch behavior. We present further insights into the source of prediction accuracy and favored workload categories for modern branch predictors.

</details>


### [17] [Full System Architecture Modeling for Wearable Egocentric Contextual AI](https://arxiv.org/abs/2512.16045)
*Vincent T. Lee,Tanfer Alan,Sung Kim,Ecenur Ustun,Amr Suleiman,Ajit Krisshna,Tim Balbekov,Armin Alaghi,Richard Newcombe*

Main category: cs.AR

TL;DR: 论文介绍了为下一代情境AI设计的可穿戴系统Aria2的完整架构，强调端到端系统建模的重要性，因为系统功耗没有单一主导组件，需要全局优化以避免瓶颈。


<details>
  <summary>Details</summary>
Motivation: 下一代以人为本的计算需要全天候、空间感知的可穿戴设备来捕获自我中心视觉和功能原语，以构建用户个人情境。结合生成式AI，这将解锁强大的情境AI个人助手。然而，设计这样的系统面临复杂性和严格的功耗限制挑战。

Method: 提出了首个完整的可穿戴情境AI系统（Aria2）架构视图，通过系统建模和设计空间探索过程，展示了端到端完整系统模型的重要性。

Result: 研究表明，这类系统的端到端完整系统模型至关重要，因为没有单一组件或类别主导系统功耗。这意味着需要在完整系统背景下进行长期设计决策和功耗优化，以避免因其他系统瓶颈或瓶颈变化而受限。

Conclusion: 通过系统建模获得的经验和见解对于最终实现全天候可穿戴情境AI系统至关重要，强调了全局系统视角在功耗优化中的关键作用。

Abstract: The next generation of human-oriented computing will require always-on, spatially-aware wearable devices to capture egocentric vision and functional primitives (e.g., Where am I? What am I looking at?, etc.). These devices will sense an egocentric view of the world around us to observe all human-relevant signals across space and time to construct and maintain a user's personal context. This personal context, combined with advanced generative AI, will unlock a powerful new generation of contextual AI personal assistants and applications. However, designing a wearable system to support contextual AI is a daunting task because of the system's complexity and stringent power constraints due to weight and battery restrictions. To understand how to guide design for such systems, this work provides the first complete system architecture view of one such wearable contextual AI system (Aria2), along with the lessons we have learned through the system modeling and design space exploration process. We show that an end-to-end full system model view of such systems is vitally important, as no single component or category overwhelmingly dominates system power. This means long-range design decisions and power optimizations need to be made in the full system context to avoid running into limits caused by other system bottlenecks (i.e., Amdahl's law as applied to power) or as bottlenecks change. Finally, we reflect on lessons and insights for the road ahead, which will be important toward eventually enabling all-day, wearable, contextual AI systems.

</details>
