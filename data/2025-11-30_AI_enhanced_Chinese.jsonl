{"id": "2511.20780", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.20780", "abs": "https://arxiv.org/abs/2511.20780", "authors": ["Alison Silva", "Gustavo Callou"], "title": "Assessing Redundancy Strategies to Improve Availability in Virtualized System Architectures", "comment": null, "summary": "Cloud-based storage platforms are becoming more common in both academic and business settings due to their flexible access to data and support for collaborative functionalities. As reliability becomes a vital requirement, particularly for organizations looking for alternatives to public cloud services, assessing the dependability of these systems is crucial. This paper presents a methodology for analyzing the availability of a file server (Nextcloud) hosted in a private cloud environment using Apache CloudStack. The analysis is based on a modeling approach through Stochastic Petri Nets (SPNs) that allows the evaluation of different redundancy strategies to enhance the availability of such systems. Four architectural configurations were modeled, including the baseline, host-level redundancy, virtual machine (VM) redundancy, and a combination of both. The results show that redundancy at both the host and VM levels significantly improves availability and reduces expected downtime. The proposed approach provides a method to evaluate the availability of a private cloud and support infrastructure design decisions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u968f\u673aPetri\u7f51\u7684\u65b9\u6cd5\u6765\u5206\u6790\u79c1\u6709\u4e91\u73af\u5883\u4e2dNextcloud\u6587\u4ef6\u670d\u52a1\u5668\u7684\u53ef\u7528\u6027\uff0c\u8bc4\u4f30\u4e86\u56db\u79cd\u5197\u4f59\u7b56\u7565\u5bf9\u7cfb\u7edf\u53ef\u7528\u6027\u7684\u5f71\u54cd\u3002", "motivation": "\u968f\u7740\u4e91\u5b58\u50a8\u5e73\u53f0\u5728\u5b66\u672f\u548c\u5546\u4e1a\u73af\u5883\u4e2d\u7684\u666e\u53ca\uff0c\u53ef\u9760\u6027\u6210\u4e3a\u5173\u952e\u9700\u6c42\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u5bfb\u6c42\u516c\u5171\u4e91\u66ff\u4ee3\u65b9\u6848\u7684\u7ec4\u7ec7\u3002\u8bc4\u4f30\u8fd9\u4e9b\u7cfb\u7edf\u7684\u53ef\u9760\u6027\u81f3\u5173\u91cd\u8981\u3002", "method": "\u4f7f\u7528\u968f\u673aPetri\u7f51(SPNs)\u5efa\u6a21\u65b9\u6cd5\uff0c\u5728Apache CloudStack\u6258\u7ba1\u7684\u79c1\u6709\u4e91\u73af\u5883\u4e2d\u5206\u6790Nextcloud\u6587\u4ef6\u670d\u52a1\u5668\u7684\u53ef\u7528\u6027\uff0c\u8bc4\u4f30\u4e86\u56db\u79cd\u67b6\u6784\u914d\u7f6e\uff1a\u57fa\u7ebf\u3001\u4e3b\u673a\u7ea7\u5197\u4f59\u3001\u865a\u62df\u673a\u5197\u4f59\u4ee5\u53ca\u4e24\u8005\u7ec4\u5408\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u5728\u4e3b\u673a\u548c\u865a\u62df\u673a\u7ea7\u522b\u540c\u65f6\u5b9e\u65bd\u5197\u4f59\u7b56\u7565\u80fd\u663e\u8457\u63d0\u9ad8\u53ef\u7528\u6027\u5e76\u51cf\u5c11\u9884\u671f\u505c\u673a\u65f6\u95f4\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u4e3a\u8bc4\u4f30\u79c1\u6709\u4e91\u53ef\u7528\u6027\u548c\u652f\u6301\u57fa\u7840\u8bbe\u65bd\u8bbe\u8ba1\u51b3\u7b56\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2511.20834", "categories": ["cs.DC", "cs.AR", "cs.LG", "cs.PF"], "pdf": "https://arxiv.org/pdf/2511.20834", "abs": "https://arxiv.org/abs/2511.20834", "authors": ["Dionysios Adamopoulos", "Anastasia Poulopoulou", "Georgios Goumas", "Christina Giannoula"], "title": "Accelerating Sparse Convolutions in Voxel-Based Point Cloud Networks", "comment": null, "summary": "Sparse Convolution (SpC) powers 3D point cloud networks widely used in autonomous driving and AR/VR. SpC builds a kernel map that stores mappings between input voxel coordinates, output coordinates, and weight offsets, then uses this map to compute feature vectors for output coordinates. Our work identifies three key properties of voxel coordinates: they are integer-valued, bounded within a limited spatial range, and geometrically continuous-neighboring voxels on the same object surface are highly likely to exist at small spatial offsets from each other. Prior SpC engines do not fully exploit these properties and suffer from high pre-processing and post-processing overheads during kernel map construction. To address this, we design Spira, the first voxel-property-aware SpC engine for GPUs. Spira proposes: (i) a high-performance one-shot search algorithm that builds the kernel map with no preprocessing and high memory locality, (ii) an effective packed-native processing scheme that accesses packed voxel coordinates at low cost, (iii) a flexible dual-dataflow execution mechanism that efficiently computes output feature vectors by adapting to layer characteristics, and (iv) a network-wide parallelization strategy that builds kernel maps for all SpC layers concurrently at network start. Our evaluation shows that Spira significantly outperforms prior SpC engines by 1.71x on average and up to 2.31x for end-to-end inference, and by 2.13x on average and up to 3.32x for layer-wise execution across diverse layer configurations.", "AI": {"tldr": "Spira\u662f\u4e00\u4e2a\u57fa\u4e8eGPU\u7684\u7a00\u758f\u5377\u79ef\u5f15\u64ce\uff0c\u901a\u8fc7\u5229\u7528\u4f53\u7d20\u5750\u6807\u7684\u6574\u6570\u6027\u3001\u6709\u754c\u6027\u548c\u51e0\u4f55\u8fde\u7eed\u6027\u7b49\u7279\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e863D\u70b9\u4e91\u7f51\u7edc\u7684\u5904\u7406\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u7a00\u758f\u5377\u79ef\u5f15\u64ce\u672a\u80fd\u5145\u5206\u5229\u7528\u4f53\u7d20\u5750\u6807\u7684\u4e09\u4e2a\u5173\u952e\u7279\u6027\uff08\u6574\u6570\u503c\u3001\u6709\u9650\u7a7a\u95f4\u8303\u56f4\u3001\u51e0\u4f55\u8fde\u7eed\u6027\uff09\uff0c\u5bfc\u81f4\u5728\u6838\u6620\u5c04\u6784\u5efa\u8fc7\u7a0b\u4e2d\u5b58\u5728\u9ad8\u9884\u5904\u7406\u548c\u540e\u5904\u7406\u5f00\u9500\u3002", "method": "Spira\u8bbe\u8ba1\u4e86\u56db\u79cd\u5173\u952e\u6280\u672f\uff1a(1) \u4e00\u6b21\u6027\u641c\u7d22\u7b97\u6cd5\u6784\u5efa\u6838\u6620\u5c04\uff1b(2) \u6253\u5305\u539f\u751f\u5904\u7406\u65b9\u6848\u4f4e\u6210\u672c\u8bbf\u95ee\u4f53\u7d20\u5750\u6807\uff1b(3) \u53cc\u6570\u636e\u6d41\u6267\u884c\u673a\u5236\u9002\u5e94\u4e0d\u540c\u5c42\u7279\u6027\uff1b(4) \u7f51\u7edc\u7ea7\u5e76\u884c\u5316\u7b56\u7565\u540c\u65f6\u6784\u5efa\u6240\u6709\u5c42\u7684\u6838\u6620\u5c04\u3002", "result": "Spira\u5728\u7aef\u5230\u7aef\u63a8\u7406\u4e2d\u5e73\u5747\u6027\u80fd\u63d0\u53471.71\u500d\uff08\u6700\u9ad82.31\u500d\uff09\uff0c\u5728\u9010\u5c42\u6267\u884c\u4e2d\u5e73\u5747\u63d0\u53472.13\u500d\uff08\u6700\u9ad83.32\u500d\uff09\u3002", "conclusion": "Spira\u901a\u8fc7\u5145\u5206\u5229\u7528\u4f53\u7d20\u5750\u6807\u7684\u7279\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7a00\u758f\u5377\u79ef\u5728GPU\u4e0a\u7684\u6267\u884c\u6548\u7387\uff0c\u4e3a3D\u70b9\u4e91\u5904\u7406\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2511.20975", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.20975", "abs": "https://arxiv.org/abs/2511.20975", "authors": ["Yinwei Dai", "Zhuofu Chen", "Anand Iyer", "Ravi Netravali"], "title": "Aragog: Just-in-Time Model Routing for Scalable Serving of Agentic Workflows", "comment": null, "summary": "Agentic workflows have emerged as a powerful paradigm for solving complex, multi-stage tasks, but serving them at scale is computationally expensive given the many LLM inferences that each request must pass through. Configuration selection, or the cost-aware assignment of workflow agents to specific LLMs, can reduce these costs, but existing approaches bind configuration decisions before request execution, making them ill-suited for the heterogeneous and lengthy execution of workflows. Specifically, system loads can fluctuate rapidly and substantially during a request's lifetime, causing fixed configurations to quickly become suboptimal. We present Aragog, a system that progressively adapts a request's configuration throughout its execution to match runtime dynamics. To make this practical despite the massive space of workflow configurations, Aragog decouples the problem into two core elements -- a one-time routing step that identifies all accuracy-preserving configurations, and a cheap per-stage scheduler that selects among them using up-to-date system observations -- and introduces novel strategies to accelerate each. Across diverse workflows and model families, Aragog increases maximum serving throughput by 50.0--217.0\\% and reduces median latency by 32.5--78.9\\% at peak request rates, while maintaining accuracy comparable to the most expensive configurations.", "AI": {"tldr": "Aragog\u662f\u4e00\u4e2a\u52a8\u6001\u914d\u7f6e\u9009\u62e9\u7cfb\u7edf\uff0c\u80fd\u591f\u5728\u5de5\u4f5c\u6d41\u6267\u884c\u8fc7\u7a0b\u4e2d\u6839\u636e\u8fd0\u884c\u65f6\u52a8\u6001\u8c03\u6574LLM\u914d\u7f6e\uff0c\u4ee5\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u5e76\u63d0\u9ad8\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u4ee3\u7406\u5de5\u4f5c\u6d41\u914d\u7f6e\u9009\u62e9\u65b9\u6cd5\u5728\u8bf7\u6c42\u6267\u884c\u524d\u5c31\u56fa\u5b9a\u914d\u7f6e\uff0c\u65e0\u6cd5\u9002\u5e94\u7cfb\u7edf\u8d1f\u8f7d\u7684\u5feb\u901f\u53d8\u5316\uff0c\u5bfc\u81f4\u914d\u7f6e\u5f88\u5feb\u53d8\u5f97\u4e0d\u4f18\u5316\u3002", "method": "Aragog\u5c06\u95ee\u9898\u89e3\u8026\u4e3a\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a\u4e00\u6b21\u6027\u8def\u7531\u6b65\u9aa4\u8bc6\u522b\u6240\u6709\u4fdd\u6301\u7cbe\u5ea6\u7684\u914d\u7f6e\uff0c\u4ee5\u53ca\u57fa\u4e8e\u6700\u65b0\u7cfb\u7edf\u89c2\u5bdf\u7684\u5ec9\u4ef7\u6bcf\u9636\u6bb5\u8c03\u5ea6\u5668\u3002", "result": "\u5728\u591a\u6837\u5316\u5de5\u4f5c\u6d41\u548c\u6a21\u578b\u5bb6\u65cf\u4e2d\uff0cAragog\u5c06\u6700\u5927\u670d\u52a1\u541e\u5410\u91cf\u63d0\u9ad8\u4e8650.0-217.0%\uff0c\u5728\u5cf0\u503c\u8bf7\u6c42\u7387\u4e0b\u5c06\u4e2d\u4f4d\u5ef6\u8fdf\u964d\u4f4e\u4e8632.5-78.9%\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u6700\u6602\u8d35\u914d\u7f6e\u76f8\u5f53\u7684\u7cbe\u5ea6\u3002", "conclusion": "Aragog\u901a\u8fc7\u52a8\u6001\u914d\u7f6e\u8c03\u6574\u6709\u6548\u89e3\u51b3\u4e86\u4ee3\u7406\u5de5\u4f5c\u6d41\u670d\u52a1\u4e2d\u7684\u6210\u672c\u6548\u7387\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u6027\u80fd\u3002"}}
{"id": "2511.20982", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.20982", "abs": "https://arxiv.org/abs/2511.20982", "authors": ["Junhan Liao", "Minxian Xu", "Wanyi Zheng", "Yan Wang", "Kejiang Ye", "Rajkumar Buyya", "Chengzhong Xu"], "title": "A Dynamic PD-Disaggregation Architecture for Maximizing Goodput in LLM Inference Serving", "comment": "14 pages", "summary": "To meet strict Service-Level Objectives (SLOs),contemporary Large Language Models (LLMs) decouple the prefill and decoding stages and place them on separate GPUs to mitigate the distinct bottlenecks inherent to each phase. However, the heterogeneity of LLM workloads causes producerconsumer imbalance between the two instance types in such disaggregated architecture. To address this problem, we propose DOPD (Dynamic Optimal Prefill/Decoding), a dynamic LLM inference system that adjusts instance allocations to achieve an optimal prefill-to-decoding (P/D) ratio based on real-time load monitoring. Combined with an appropriate request-scheduling policy, DOPD effectively resolves imbalances between prefill and decoding instances and mitigates resource allocation mismatches due to mixed-length requests under high concurrency. Experimental evaluations show that, compared with vLLM and DistServe (representative aggregation-based and disaggregationbased approaches), DOPD improves overall system goodput by up to 1.5X, decreases P90 time-to-first-token (TTFT) by up to 67.5%, and decreases P90 time-per-output-token (TPOT) by up to 22.8%. Furthermore, our dynamic P/D adjustment technique performs proactive reconfiguration based on historical load, achieving over 99% SLOs attainment while using less additional resources.", "AI": {"tldr": "DOPD\u662f\u4e00\u4e2a\u52a8\u6001LLM\u63a8\u7406\u7cfb\u7edf\uff0c\u901a\u8fc7\u5b9e\u65f6\u76d1\u63a7\u8d1f\u8f7d\u52a8\u6001\u8c03\u6574\u9884\u586b\u5145\u548c\u89e3\u7801\u5b9e\u4f8b\u5206\u914d\u6bd4\u4f8b\uff0c\u89e3\u51b3\u5f02\u6784\u5de5\u4f5c\u8d1f\u8f7d\u4e0b\u7684\u751f\u4ea7\u8005-\u6d88\u8d39\u8005\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u7cfb\u7edf\u541e\u5410\u91cf\u548c\u54cd\u5e94\u6027\u80fd\u3002", "motivation": "\u5f53\u4ee3LLM\u5c06\u9884\u586b\u5145\u548c\u89e3\u7801\u9636\u6bb5\u89e3\u8026\u5230\u4e0d\u540cGPU\u4e0a\u4ee5\u7f13\u89e3\u5404\u81ea\u74f6\u9888\uff0c\u4f46\u5f02\u6784\u5de5\u4f5c\u8d1f\u8f7d\u5bfc\u81f4\u8fd9\u79cd\u5206\u79bb\u67b6\u6784\u4e2d\u4e24\u79cd\u5b9e\u4f8b\u7c7b\u578b\u95f4\u7684\u751f\u4ea7\u8005-\u6d88\u8d39\u8005\u4e0d\u5e73\u8861\u3002", "method": "\u63d0\u51faDOPD\u7cfb\u7edf\uff0c\u57fa\u4e8e\u5b9e\u65f6\u8d1f\u8f7d\u76d1\u63a7\u52a8\u6001\u8c03\u6574\u5b9e\u4f8b\u5206\u914d\u4ee5\u8fbe\u5230\u6700\u4f18\u9884\u586b\u5145/\u89e3\u7801\u6bd4\u4f8b\uff0c\u7ed3\u5408\u9002\u5f53\u7684\u8bf7\u6c42\u8c03\u5ea6\u7b56\u7565\u89e3\u51b3\u5b9e\u4f8b\u4e0d\u5e73\u8861\u548c\u8d44\u6e90\u5206\u914d\u4e0d\u5339\u914d\u95ee\u9898\u3002", "result": "\u76f8\u6bd4vLLM\u548cDistServe\uff0cDOPD\u5c06\u7cfb\u7edf\u541e\u5410\u91cf\u63d0\u53471.5\u500d\uff0cP90\u9996token\u65f6\u95f4\u964d\u4f4e67.5%\uff0cP90\u6bcf\u8f93\u51fatoken\u65f6\u95f4\u964d\u4f4e22.8%\u3002", "conclusion": "\u52a8\u6001P/D\u8c03\u6574\u6280\u672f\u57fa\u4e8e\u5386\u53f2\u8d1f\u8f7d\u8fdb\u884c\u4e3b\u52a8\u91cd\u914d\u7f6e\uff0c\u5728\u4f7f\u7528\u66f4\u5c11\u989d\u5916\u8d44\u6e90\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u8d85\u8fc799%\u7684SLO\u8fbe\u6210\u7387\u3002"}}
{"id": "2511.20782", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2511.20782", "abs": "https://arxiv.org/abs/2511.20782", "authors": ["Russel Arbore", "Alvin Cheung", "Max Willsey"], "title": "Optimism in Equality Saturation", "comment": null, "summary": "Equality saturation is a technique for program optimization based on non-destructive rewriting and a form of program analysis called e-class analysis. The current form of e-class analysis is pessimistic and therefore ineffective at analyzing cyclic programs, such as those in SSA form. We propose an abstract interpretation algorithm that can precisely analyze cycles during equality saturation. This results in a unified algorithm for optimistic analysis and non-destructive rewriting. We instantiate this approach on a prototype abstract interpreter for SSA programs using a new semantics of SSA. Our prototype can analyze simple example programs more precisely than clang and gcc.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u62bd\u8c61\u89e3\u91ca\u7684\u4e50\u89c2\u5206\u6790\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u7b49\u5f0f\u9971\u548c\u8fc7\u7a0b\u4e2d\u7cbe\u786e\u5206\u6790\u5faa\u73af\u7a0b\u5e8f\uff0c\u7279\u522b\u662f\u5728SSA\u5f62\u5f0f\u4e0b\u7684\u7a0b\u5e8f\u3002", "motivation": "\u73b0\u6709\u7684e-class\u5206\u6790\u5bf9\u5faa\u73af\u7a0b\u5e8f\u7684\u5206\u6790\u662f\u60b2\u89c2\u7684\uff0c\u5728\u5206\u6790SSA\u5f62\u5f0f\u7b49\u5faa\u73af\u7a0b\u5e8f\u65f6\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u5f00\u53d1\u4e86\u57fa\u4e8e\u62bd\u8c61\u89e3\u91ca\u7684\u7b97\u6cd5\uff0c\u5728\u7b49\u5f0f\u9971\u548c\u8fc7\u7a0b\u4e2d\u7cbe\u786e\u5206\u6790\u5faa\u73af\uff0c\u4f7f\u7528\u65b0\u7684SSA\u8bed\u4e49\u6784\u5efa\u539f\u578b\u62bd\u8c61\u89e3\u91ca\u5668\u3002", "result": "\u539f\u578b\u7cfb\u7edf\u80fd\u591f\u6bd4clang\u548cgcc\u66f4\u7cbe\u786e\u5730\u5206\u6790\u7b80\u5355\u793a\u4f8b\u7a0b\u5e8f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u4e50\u89c2\u5206\u6790\u548c\u975e\u7834\u574f\u6027\u91cd\u5199\u7684\u7edf\u4e00\u7b97\u6cd5\uff0c\u63d0\u9ad8\u4e86\u5faa\u73af\u7a0b\u5e8f\u5206\u6790\u7684\u7cbe\u786e\u6027\u3002"}}
{"id": "2511.21232", "categories": ["cs.AR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2511.21232", "abs": "https://arxiv.org/abs/2511.21232", "authors": ["Muhammed Yildirim", "Ozcan Ozturk"], "title": "RISC-V Based TinyML Accelerator for Depthwise Separable Convolutions in Edge AI", "comment": "13 pages, 7 tables, 14 figures", "summary": "The increasing demand for on-device intelligence in Edge AI and TinyML applications requires the efficient execution of modern Convolutional Neural Networks (CNNs). While lightweight architectures like MobileNetV2 employ Depthwise Separable Convolutions (DSC) to reduce computational complexity, their multi-stage design introduces a critical performance bottleneck inherent to layer-by-layer execution: the high energy and latency cost of transferring intermediate feature maps to either large on-chip buffers or off-chip DRAM. To address this memory wall, this paper introduces a novel hardware accelerator architecture that utilizes a fused pixel-wise dataflow. Implemented as a Custom Function Unit (CFU) for a RISC-V processor, our architecture eliminates the need for intermediate buffers entirely, reducing the data movement up to 87\\% compared to conventional layer-by-layer execution. It computes a single output pixel to completion across all DSC stages-expansion, depthwise convolution, and projection-by streaming data through a tightly-coupled pipeline without writing to memory. Evaluated on a Xilinx Artix-7 FPGA, our design achieves a speedup of up to 59.3x over the baseline software execution on the RISC-V core. Furthermore, ASIC synthesis projects a compact 0.284 mm$^2$ footprint with 910 mW power at 2 GHz in 28 nm, and a 1.20 mm$^2$ footprint with 233 mW power at 300 MHz in 40 nm. This work confirms the feasibility of a zero-buffer dataflow within a TinyML resource envelope, offering a novel and effective strategy for overcoming the memory wall in edge AI accelerators.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u878d\u5408\u50cf\u7d20\u6570\u636e\u6d41\u786c\u4ef6\u52a0\u901f\u5668\uff0c\u6d88\u9664\u6df1\u5ea6\u53ef\u5206\u79bb\u5377\u79ef\u4e2d\u95f4\u7f13\u51b2\u533a\uff0c\u51cf\u5c1187%\u6570\u636e\u4f20\u8f93\uff0c\u5728FPGA\u4e0a\u5b9e\u73b059.3\u500d\u52a0\u901f\u3002", "motivation": "\u89e3\u51b3\u8fb9\u7f18AI\u548cTinyML\u5e94\u7528\u4e2d\u6df1\u5ea6\u53ef\u5206\u79bb\u5377\u79ef\u9010\u5c42\u6267\u884c\u65f6\u4e2d\u95f4\u7279\u5f81\u56fe\u4f20\u8f93\u5e26\u6765\u7684\u9ad8\u80fd\u8017\u548c\u5ef6\u8fdf\u95ee\u9898\uff0c\u7a81\u7834\u5185\u5b58\u5899\u9650\u5236\u3002", "method": "\u8bbe\u8ba1\u57fa\u4e8eRISC-V\u5904\u7406\u5668\u7684\u5b9a\u5236\u529f\u80fd\u5355\u5143\uff0c\u91c7\u7528\u878d\u5408\u50cf\u7d20\u6570\u636e\u6d41\uff0c\u5c06\u6269\u5c55\u3001\u6df1\u5ea6\u5377\u79ef\u548c\u6295\u5f71\u9636\u6bb5\u6d41\u6c34\u7ebf\u5316\u5904\u7406\u5355\u4e2a\u8f93\u51fa\u50cf\u7d20\uff0c\u65e0\u9700\u4e2d\u95f4\u7f13\u51b2\u533a\u3002", "result": "\u5728Xilinx Artix-7 FPGA\u4e0a\u5b9e\u73b059.3\u500d\u52a0\u901f\uff0cASIC\u5408\u6210\u663e\u793a28nm\u5de5\u827a\u4e0b0.284mm\u00b2\u9762\u79ef\u3001910mW\u529f\u8017\uff0c40nm\u5de5\u827a\u4e0b1.20mm\u00b2\u9762\u79ef\u3001233mW\u529f\u8017\u3002", "conclusion": "\u8bc1\u5b9e\u4e86\u5728TinyML\u8d44\u6e90\u7ea6\u675f\u4e0b\u5b9e\u73b0\u96f6\u7f13\u51b2\u533a\u6570\u636e\u6d41\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u8fb9\u7f18AI\u52a0\u901f\u5668\u514b\u670d\u5185\u5b58\u5899\u63d0\u4f9b\u4e86\u6709\u6548\u7b56\u7565\u3002"}}
{"id": "2511.21018", "categories": ["cs.DC", "cs.AR"], "pdf": "https://arxiv.org/pdf/2511.21018", "abs": "https://arxiv.org/abs/2511.21018", "authors": ["Antonis Psistakis"], "title": "Handling of Memory Page Faults during Virtual-Address RDMA", "comment": "Antonis Psistakis, Master of Science (MSc) Thesis 2019. The abstract and text were lightly revised in 2025 to comply with arXiv formatting guidelines", "summary": "Nowadays, avoiding system calls during cluster communication (e.g., in Data Centers and High Performance Computing) in modern high-speed interconnection networks has become a necessity, due to the high overhead of multiple data copies between kernel and user space. User-level zero-copy Remote Direct Memory Access (RDMA) technologies address this problem by improving performance and reducing system energy consumption. However, traditional RDMA engines cannot tolerate page faults and therefore use various techniques to avoid them.\n  State-of-the-art RDMA approaches typically rely on pinning address spaces or multiple pages per application. This method introduces long-term disadvantages due to increased programming complexity (pinning and unpinning buffers), limits on how much memory can be pinned, and inefficient memory utilization. In addition, pinning does not fully prevent page faults because modern operating systems apply internal optimization mechanisms, such as Transparent Huge Pages (THP), which are enabled by default in Linux.\n  This thesis implements a page-fault handling mechanism integrated with the DMA engine of the ExaNeSt project. Faults are detected by the ARM System Memory Management Unit (SMMU) and resolved through a hardware-software solution that can request retransmission when needed. This mechanism required modifications to the Linux SMMU driver, the development of a new software library, changes to the DMA engine hardware, and adjustments to the DMA scheduling logic. Experiments were conducted on the Quad-FPGA Daughter Board (QFDB) of ExaNeSt, which uses Xilinx Zynq UltraScale+ MPSoCs.\n  Finally, we evaluate our mechanism and compare it against alternatives such as pinning and pre-faulting, and discuss the advantages of our approach.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5b9e\u73b0\u4e86\u4e00\u79cd\u4e0eDMA\u5f15\u64ce\u96c6\u6210\u7684\u9875\u9762\u9519\u8bef\u5904\u7406\u673a\u5236\uff0c\u901a\u8fc7SMMU\u68c0\u6d4b\u6545\u969c\u5e76\u91c7\u7528\u786c\u4ef6-\u8f6f\u4ef6\u89e3\u51b3\u65b9\u6848\u8fdb\u884c\u89e3\u51b3\uff0c\u907f\u514d\u4e86\u4f20\u7edfRDMA\u6280\u672f\u4e2d\u5185\u5b58\u56fa\u5b9a\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u4f20\u7edfRDMA\u6280\u672f\u65e0\u6cd5\u5bb9\u5fcd\u9875\u9762\u9519\u8bef\uff0c\u901a\u5e38\u91c7\u7528\u5185\u5b58\u56fa\u5b9a\u6280\u672f\uff0c\u4f46\u8fd9\u5e26\u6765\u4e86\u7f16\u7a0b\u590d\u6742\u6027\u3001\u5185\u5b58\u4f7f\u7528\u9650\u5236\u548c\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\u3002\u73b0\u4ee3\u64cd\u4f5c\u7cfb\u7edf\u5982Linux\u9ed8\u8ba4\u542f\u7528\u7684\u900f\u660e\u5927\u9875\u7b49\u4f18\u5316\u673a\u5236\u4f7f\u5f97\u56fa\u5b9a\u65e0\u6cd5\u5b8c\u5168\u9632\u6b62\u9875\u9762\u9519\u8bef\u3002", "method": "\u5728ExaNeSt\u9879\u76ee\u7684DMA\u5f15\u64ce\u4e2d\u96c6\u6210\u9875\u9762\u9519\u8bef\u5904\u7406\u673a\u5236\uff0c\u901a\u8fc7ARM SMMU\u68c0\u6d4b\u6545\u969c\uff0c\u91c7\u7528\u786c\u4ef6-\u8f6f\u4ef6\u89e3\u51b3\u65b9\u6848\u8bf7\u6c42\u91cd\u4f20\u3002\u9700\u8981\u4fee\u6539Linux SMMU\u9a71\u52a8\u3001\u5f00\u53d1\u65b0\u8f6f\u4ef6\u5e93\u3001\u8c03\u6574DMA\u5f15\u64ce\u786c\u4ef6\u548c\u8c03\u5ea6\u903b\u8f91\u3002", "result": "\u5728ExaNeSt\u7684QFDB\u5e73\u53f0\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\u8bc4\u4f30\uff0c\u6bd4\u8f83\u4e86\u8be5\u673a\u5236\u4e0e\u5185\u5b58\u56fa\u5b9a\u548c\u9884\u9519\u8bef\u5904\u7406\u7b49\u66ff\u4ee3\u65b9\u6848\u7684\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684\u9875\u9762\u9519\u8bef\u5904\u7406\u673a\u5236\u76f8\u6bd4\u4f20\u7edf\u5185\u5b58\u56fa\u5b9a\u65b9\u6cd5\u5177\u6709\u4f18\u52bf\uff0c\u80fd\u591f\u66f4\u597d\u5730\u5904\u7406\u9875\u9762\u9519\u8bef\u95ee\u9898\u3002"}}
{"id": "2511.21209", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2511.21209", "abs": "https://arxiv.org/abs/2511.21209", "authors": ["Yee-Jian Tan", "Andreas Nuyts", "Dominique Devriese"], "title": "Towards Computational UIP in Cubical Agda", "comment": null, "summary": "Some advantages of Cubical Type Theory, as implemented by Cubical Agda, over intensional Martin-L\u00f6f Type Theory include Quotient Inductive Types (QITs), which exist as instances of Higher Inductive Types, and functional extensionality, which is provable in Cubical Type Theory. However, HoTT features an infinite hierarchy of equalities that may become unwieldy in formalisations. Fortunately, QITs and functional extensionality are both preserved even if the equality levels of Cubical Type Theory are truncated to only homotopical Sets (h-Sets). In other words, removing the univalence axiom from Cubical Type Theory and instead postulating a conflicting axiom: the Uniqueness of Identity Proofs (UIP) postulate. Since univalence is proved in Cubical Type Theory from the so-called Glue Types, therefore, it is known that one can first remove the Glue Types (thus removing univalence) and then set-truncate all equalities (essentially assuming UIP), \u00e0 la XTT. The result is a \"h-Set Cubical Type Theory\" that retains features such as functional extensionality and QITs.\n  However, in Cubical Agda, there are currently only two unsatisfying ways to achieve h-Set Cubical Type Theory. The first is to give up on the canonicity of the theory and simply postulate the UIP axiom, while the second way is to use a standard result stating \"type formers preserve h-levels\" to manually prove UIP for every defined type. The latter is, however, laborious work best suited for an automatic implementation by the proof assistant. In this project, we analyse formulations of UIP and detail their computation rules for Cubical Agda, and evaluate their suitability for implementation. We also implement a variant of Cubical Agda without Glue, which is already compatible with postulated UIP, in anticipation of a future implementation of UIP in Cubical Agda.", "AI": {"tldr": "\u672c\u6587\u63a2\u8ba8\u4e86\u5728Cubical Agda\u4e2d\u5b9e\u73b0h-Set\u7acb\u65b9\u7c7b\u578b\u8bba\u7684\u65b9\u6cd5\uff0c\u5206\u6790\u4e86UIP\u7684\u4e0d\u540c\u8868\u8ff0\u5f62\u5f0f\u53ca\u5176\u8ba1\u7b97\u89c4\u5219\uff0c\u5e76\u5b9e\u73b0\u4e86\u4e00\u4e2a\u65e0Glue\u7684Cubical Agda\u53d8\u4f53\u3002", "motivation": "\u7acb\u65b9\u7c7b\u578b\u7406\u8bba\u5177\u6709\u5546\u5f52\u7eb3\u7c7b\u578b\u548c\u51fd\u6570\u5916\u5ef6\u6027\u7b49\u4f18\u52bf\uff0c\u4f46\u9ad8\u9636\u7b49\u5f0f\u5c42\u6b21\u53ef\u80fd\u4f7f\u5f62\u5f0f\u5316\u53d8\u5f97\u590d\u6742\u3002\u867d\u7136\u622a\u65ad\u5230h-Set\u5c42\u6b21\u53ef\u4ee5\u4fdd\u7559\u8fd9\u4e9b\u7279\u6027\uff0c\u4f46\u76ee\u524d\u5728Cubical Agda\u4e2d\u5b9e\u73b0h-Set\u7acb\u65b9\u7c7b\u578b\u8bba\u7684\u65b9\u6cd5\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u5206\u6790UIP\u7684\u4e0d\u540c\u8868\u8ff0\u5f62\u5f0f\u53ca\u5176\u8ba1\u7b97\u89c4\u5219\uff0c\u8bc4\u4f30\u5b83\u4eec\u5728Cubical Agda\u4e2d\u5b9e\u73b0\u7684\u9002\u7528\u6027\uff0c\u5e76\u5b9e\u73b0\u4e00\u4e2a\u65e0Glue\u7684Cubical Agda\u53d8\u4f53\u3002", "result": "\u63d0\u51fa\u4e86UIP\u7684\u591a\u79cd\u8868\u8ff0\u5f62\u5f0f\u53ca\u5176\u8ba1\u7b97\u89c4\u5219\uff0c\u5e76\u6210\u529f\u5b9e\u73b0\u4e86\u4e00\u4e2a\u4e0e\u5047\u8bbeUIP\u517c\u5bb9\u7684\u65e0Glue Cubical Agda\u53d8\u4f53\u3002", "conclusion": "\u4e3aCubical Agda\u4e2d\u672a\u6765\u5b9e\u73b0UIP\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u73b0\u8def\u5f84\uff0c\u4f7fh-Set\u7acb\u65b9\u7c7b\u578b\u8bba\u80fd\u591f\u66f4\u6709\u6548\u5730\u5728\u8bc1\u660e\u52a9\u624b\u4e2d\u5b9e\u73b0\u3002"}}
{"id": "2511.21346", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.21346", "abs": "https://arxiv.org/abs/2511.21346", "authors": ["Mohamed Shahawy", "Julien de Castelnau", "Paolo Ienne"], "title": "Bombyx: OpenCilk Compilation for FPGA Hardware Acceleration", "comment": null, "summary": "Task-level parallelism (TLP) is a widely used approach in software where independent tasks are dynamically created and scheduled at runtime. Recent systems have explored architectural support for TLP on field-programmable gate arrays (FPGAs), often leveraging high-level synthesis (HLS) to create processing elements (PEs). In this paper, we present Bombyx, a compiler toolchain that lowers OpenCilk programs into a Cilk-1-inspired intermediate representation, enabling efficient mapping of CPU-oriented TLP applications to spatial architectures on FPGAs. Unlike OpenCilk's implicit task model, which requires costly context switching in hardware, Cilk-1 adopts explicit continuation-passing - a model that better aligns with the streaming nature of FPGAs. Bombyx supports multiple compilation targets: one is an OpenCilk-compatible runtime for executing Cilk-1-style code using the OpenCilk backend, and another is a synthesizable PE generator designed for HLS tools like Vitis HLS. Additionally, we introduce a decoupled access-execute optimization that enables automatic generation of high-performance PEs, improving memory-compute overlap and overall throughput.", "AI": {"tldr": "Bombyx\u662f\u4e00\u4e2a\u7f16\u8bd1\u5668\u5de5\u5177\u94fe\uff0c\u5c06OpenCilk\u7a0b\u5e8f\u8f6c\u6362\u4e3aCilk-1\u98ce\u683c\u7684\u4e2d\u95f4\u8868\u793a\uff0c\u4f7fCPU\u5bfc\u5411\u7684\u4efb\u52a1\u7ea7\u5e76\u884c\u5e94\u7528\u80fd\u9ad8\u6548\u6620\u5c04\u5230FPGA\u7a7a\u95f4\u67b6\u6784\u4e0a\u3002", "motivation": "\u73b0\u6709\u7cfb\u7edf\u5728FPGA\u4e0a\u652f\u6301\u4efb\u52a1\u7ea7\u5e76\u884c\u65f6\uff0cOpenCilk\u7684\u9690\u5f0f\u4efb\u52a1\u6a21\u578b\u9700\u8981\u6602\u8d35\u7684\u786c\u4ef6\u4e0a\u4e0b\u6587\u5207\u6362\uff0c\u800cCilk-1\u7684\u663e\u5f0f\u5ef6\u7eed\u4f20\u9012\u6a21\u578b\u66f4\u9002\u5408FPGA\u7684\u6d41\u5f0f\u7279\u6027\u3002", "method": "\u5f00\u53d1Bombyx\u7f16\u8bd1\u5668\u5de5\u5177\u94fe\uff0c\u652f\u6301\u591a\u79cd\u7f16\u8bd1\u76ee\u6807\uff1aOpenCilk\u517c\u5bb9\u8fd0\u884c\u65f6\u548c\u53ef\u7efc\u5408\u7684PE\u751f\u6210\u5668\uff0c\u5e76\u5f15\u5165\u89e3\u8026\u8bbf\u95ee-\u6267\u884c\u4f18\u5316\u6765\u63d0\u5347\u5185\u5b58-\u8ba1\u7b97\u91cd\u53e0\u3002", "result": "\u80fd\u591f\u81ea\u52a8\u751f\u6210\u9ad8\u6027\u80fd\u5904\u7406\u5355\u5143\uff0c\u6539\u5584\u5185\u5b58-\u8ba1\u7b97\u91cd\u53e0\u548c\u6574\u4f53\u541e\u5410\u91cf\u3002", "conclusion": "Bombyx\u6210\u529f\u5c06CPU\u5bfc\u5411\u7684\u4efb\u52a1\u7ea7\u5e76\u884c\u5e94\u7528\u6620\u5c04\u5230FPGA\u7a7a\u95f4\u67b6\u6784\uff0c\u901a\u8fc7\u663e\u5f0f\u5ef6\u7eed\u4f20\u9012\u6a21\u578b\u548c\u89e3\u8026\u4f18\u5316\u5b9e\u73b0\u4e86\u9ad8\u6548\u6267\u884c\u3002"}}
{"id": "2511.21413", "categories": ["cs.DC", "cs.AI", "cs.DB", "cs.PF"], "pdf": "https://arxiv.org/pdf/2511.21413", "abs": "https://arxiv.org/abs/2511.21413", "authors": ["Tim Trappen", "Robert Ke\u00dfler", "Roland Pabel", "Viktor Achter", "Stefan Wesner"], "title": "Automated Dynamic AI Inference Scaling on HPC-Infrastructure: Integrating Kubernetes, Slurm and vLLM", "comment": "6 pages, 3 figures", "summary": "Due to rising demands for Artificial Inteligence (AI) inference, especially in higher education, novel solutions utilising existing infrastructure are emerging. The utilisation of High-Performance Computing (HPC) has become a prevalent approach for the implementation of such solutions. However, the classical operating model of HPC does not adapt well to the requirements of synchronous, user-facing dynamic AI application workloads. In this paper, we propose our solution that serves LLMs by integrating vLLM, Slurm and Kubernetes on the supercomputer \\textit{RAMSES}. The initial benchmark indicates that the proposed architecture scales efficiently for 100, 500 and 1000 concurrent requests, incurring only an overhead of approximately 500 ms in terms of end-to-end latency.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u8d85\u7ea7\u8ba1\u7b97\u673aRAMSES\u4e0a\u96c6\u6210vLLM\u3001Slurm\u548cKubernetes\u6765\u670d\u52a1LLM\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u9ad8\u6548\u5904\u7406100-1000\u4e2a\u5e76\u53d1\u8bf7\u6c42\uff0c\u5ef6\u8fdf\u5f00\u9500\u4ec5\u7ea6500ms\u3002", "motivation": "\u7531\u4e8eAI\u63a8\u7406\u9700\u6c42\u589e\u957f\uff0c\u7279\u522b\u662f\u5728\u9ad8\u7b49\u6559\u80b2\u9886\u57df\uff0c\u9700\u8981\u5229\u7528\u73b0\u6709\u57fa\u7840\u8bbe\u65bd\u7684\u65b0\u89e3\u51b3\u65b9\u6848\u3002\u4f20\u7edfHPC\u64cd\u4f5c\u6a21\u578b\u4e0d\u9002\u5408\u540c\u6b65\u3001\u9762\u5411\u7528\u6237\u7684\u52a8\u6001AI\u5e94\u7528\u5de5\u4f5c\u8d1f\u8f7d\u3002", "method": "\u5728\u8d85\u7ea7\u8ba1\u7b97\u673aRAMSES\u4e0a\u96c6\u6210vLLM\u3001Slurm\u548cKubernetes\u6765\u670d\u52a1\u5927\u8bed\u8a00\u6a21\u578b(LLM)\u3002", "result": "\u521d\u6b65\u57fa\u51c6\u6d4b\u8bd5\u8868\u660e\uff0c\u8be5\u67b6\u6784\u5728100\u3001500\u548c1000\u4e2a\u5e76\u53d1\u8bf7\u6c42\u4e0b\u90fd\u80fd\u9ad8\u6548\u6269\u5c55\uff0c\u7aef\u5230\u7aef\u5ef6\u8fdf\u4ec5\u589e\u52a0\u7ea6500\u6beb\u79d2\u7684\u5f00\u9500\u3002", "conclusion": "\u8be5\u96c6\u6210\u65b9\u6848\u6210\u529f\u89e3\u51b3\u4e86\u4f20\u7edfHPC\u5728AI\u63a8\u7406\u5e94\u7528\u4e2d\u7684\u9002\u5e94\u6027\u6311\u6218\uff0c\u4e3a\u9ad8\u7b49\u6559\u80b2\u9886\u57df\u7684AI\u670d\u52a1\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u6280\u672f\u8def\u5f84\u3002"}}
{"id": "2511.21509", "categories": ["cs.PL", "cs.SC", "cs.SE"], "pdf": "https://arxiv.org/pdf/2511.21509", "abs": "https://arxiv.org/abs/2511.21509", "authors": ["Dirk Beyer", "Gidon Ernst", "Martin Jon\u00e1\u0161", "Marian Lingsch-Rosenfeld"], "title": "SV-LIB 1.0: A Standard Exchange Format for Software-Verification Tasks", "comment": null, "summary": "In the past two decades, significant research and development effort went into the development of verification tools for individual languages, such asC, C++, and Java. Many of the used verification approaches are in fact language-agnostic and it would be beneficial for the technology transfer to allow for using the implementations also for other programming and modeling languages. To address the problem, we propose SV-LIB, an exchange format and intermediate language for software-verification tasks, including programs, specifications, and verification witnesses. SV-LIBis based on well-known concepts from imperative programming languages and uses SMT-LIB to represent expressions and sorts used in the program. This makes it easy to parse and to build into existing infrastructure, since many verification tools are based on SMT solvers already. Furthermore, SV-LIBdefines a witness format for both correct and incorrect SV-LIB programs, together with means for specifying witness-validation tasks. This makes it possible both to implement independent witness validators and to reuse some verifiers also as validators for witnesses. This paper presents version 1.0 of the SV-LIBformat, including its design goals, the syntax, and informal semantics. Formal semantics and further extensions to concurrency are planned for future versions.", "AI": {"tldr": "SV-LIB\u662f\u4e00\u4e2a\u7528\u4e8e\u8f6f\u4ef6\u9a8c\u8bc1\u4efb\u52a1\u7684\u4ea4\u6362\u683c\u5f0f\u548c\u4e2d\u95f4\u8bed\u8a00\uff0c\u57fa\u4e8e\u547d\u4ee4\u5f0f\u7f16\u7a0b\u8bed\u8a00\u6982\u5ff5\uff0c\u4f7f\u7528SMT-LIB\u8868\u793a\u8868\u8fbe\u5f0f\u548c\u7c7b\u578b\uff0c\u652f\u6301\u9a8c\u8bc1\u89c1\u8bc1\u683c\u5f0f\u3002", "motivation": "\u89e3\u51b3\u4e0d\u540c\u7f16\u7a0b\u8bed\u8a00\u9a8c\u8bc1\u5de5\u5177\u4e4b\u95f4\u7684\u4e92\u64cd\u4f5c\u6027\u95ee\u9898\uff0c\u4f7f\u8bed\u8a00\u65e0\u5173\u7684\u9a8c\u8bc1\u65b9\u6cd5\u80fd\u591f\u8de8\u8bed\u8a00\u590d\u7528\uff0c\u4fc3\u8fdb\u6280\u672f\u8f6c\u79fb\u3002", "method": "\u8bbe\u8ba1\u57fa\u4e8e\u547d\u4ee4\u5f0f\u7f16\u7a0b\u8bed\u8a00\u6982\u5ff5\u548cSMT-LIB\u7684\u4e2d\u95f4\u8bed\u8a00\u683c\u5f0f\uff0c\u5b9a\u4e49\u7a0b\u5e8f\u3001\u89c4\u8303\u548c\u9a8c\u8bc1\u89c1\u8bc1\u7684\u8868\u793a\u65b9\u6cd5\uff0c\u652f\u6301\u6b63\u786e\u548c\u9519\u8bef\u7a0b\u5e8f\u7684\u89c1\u8bc1\u683c\u5f0f\u3002", "result": "\u63d0\u51fa\u4e86SV-LIB 1.0\u7248\u672c\u683c\u5f0f\uff0c\u5305\u62ec\u8bbe\u8ba1\u76ee\u6807\u3001\u8bed\u6cd5\u548c\u975e\u5f62\u5f0f\u5316\u8bed\u4e49\uff0c\u652f\u6301\u72ec\u7acb\u89c1\u8bc1\u9a8c\u8bc1\u5668\u5f00\u53d1\u548c\u9a8c\u8bc1\u5668\u590d\u7528\u3002", "conclusion": "SV-LIB\u4e3a\u8f6f\u4ef6\u9a8c\u8bc1\u5de5\u5177\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u4ea4\u6362\u683c\u5f0f\uff0c\u672a\u6765\u8ba1\u5212\u589e\u52a0\u5f62\u5f0f\u5316\u8bed\u4e49\u548c\u5e76\u53d1\u6269\u5c55\u3002"}}
{"id": "2511.21451", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.21451", "abs": "https://arxiv.org/abs/2511.21451", "authors": ["Flurin Arquint", "Oscar Casta\u00f1eda", "Gian Marti", "Christoph Studer"], "title": "A Jammer-Resilient 2.87 mm$^2$ 1.28 MS/s 310 mW Multi-Antenna Synchronization ASIC in 65 nm", "comment": "Presented at the 2025 IEEE European Solid-State Electronics Research Conference (ESSERC)", "summary": "We present the first ASIC implementation of jammer-resilient multi-antenna time synchronization. The ASIC implements a recent algorithm that mitigates jamming attacks on synchronization signals using multi-antenna processing. Our design supports synchronization between a single-antenna transmitter and a 16-antenna receiver while mitigating smart jammers with up to two transmit antennas. The fabricated 65 nm ASIC has a core area of 2.87 mm$^2$, consumes a power of 310 mW, and supports a sampling rate of 1.28 mega-samples per second (MS/s).", "AI": {"tldr": "\u9996\u4e2a\u6297\u5e72\u6270\u591a\u5929\u7ebf\u65f6\u95f4\u540c\u6b65ASIC\u5b9e\u73b0\uff0c\u652f\u6301\u5355\u5929\u7ebf\u53d1\u5c04\u5668\u4e0e16\u5929\u7ebf\u63a5\u6536\u5668\u540c\u6b65\uff0c\u53ef\u62b5\u5fa1\u6700\u591a2\u5929\u7ebf\u667a\u80fd\u5e72\u6270\u5668\u3002", "motivation": "\u89e3\u51b3\u65e0\u7ebf\u901a\u4fe1\u7cfb\u7edf\u4e2d\u65f6\u95f4\u540c\u6b65\u4fe1\u53f7\u6613\u53d7\u5e72\u6270\u653b\u51fb\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u9488\u5bf9\u667a\u80fd\u5e72\u6270\u5668\u7684\u5a01\u80c1\u3002", "method": "\u91c7\u7528\u591a\u5929\u7ebf\u5904\u7406\u7b97\u6cd5\uff0c\u572865nm\u5de5\u827a\u4e0b\u5b9e\u73b0ASIC\u8bbe\u8ba1\uff0c\u652f\u63011.28 MS/s\u91c7\u6837\u7387\u3002", "result": "\u82af\u7247\u6838\u5fc3\u9762\u79ef2.87 mm\u00b2\uff0c\u529f\u8017310 mW\uff0c\u6210\u529f\u5b9e\u73b0\u6297\u5e72\u6270\u65f6\u95f4\u540c\u6b65\u529f\u80fd\u3002", "conclusion": "\u8be5ASIC\u8bc1\u660e\u4e86\u591a\u5929\u7ebf\u5904\u7406\u5728\u786c\u4ef6\u5c42\u9762\u5b9e\u73b0\u6297\u5e72\u6270\u65f6\u95f4\u540c\u6b65\u7684\u53ef\u884c\u6027\uff0c\u4e3a\u5b89\u5168\u65e0\u7ebf\u901a\u4fe1\u7cfb\u7edf\u63d0\u4f9b\u4e86\u786c\u4ef6\u57fa\u7840\u3002"}}
{"id": "2511.21431", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.21431", "abs": "https://arxiv.org/abs/2511.21431", "authors": ["Lu Zhao", "Rong Shi", "Shaoqing Zhang", "Yueqiang Chen", "Baoguo He", "Hongfeng Sun", "Ziqing Yin", "Shangchao Su", "Zhiyan Cui", "Liang Dong", "Xiyuan Li", "Lingbin Wang", "Jianwei He", "Jiesong Ma", "Weikang Huang", "Jianglei Tong", "Dongdong Gao", "Jian Zhang", "Hong Tian"], "title": "MemFine: Memory-Aware Fine-Grained Scheduling for MoE Training", "comment": null, "summary": "The training of large-scale Mixture of Experts (MoE) models faces a critical memory bottleneck due to severe load imbalance caused by dynamic token routing. This imbalance leads to memory overflow on GPUs with limited capacity, constraining model scalability. Existing load balancing methods, which cap expert capacity, compromise model accuracy and fail on memory-constrained hardware. To address this, we propose MemFine, a memory-aware fine-grained scheduling framework for MoE training. MemFine decomposes the token distribution and expert computation into manageable chunks and employs a chunked recomputation strategy, dynamically optimized through a theoretical memory model to balance memory efficiency and throughput. Experiments demonstrate that MemFine reduces activation memory by 48.03% and improves throughput by 4.42% compared to full recomputation-based baselines, enabling stable large-scale MoE training on memory-limited GPUs.", "AI": {"tldr": "MemFine\u662f\u4e00\u4e2a\u5185\u5b58\u611f\u77e5\u7684\u7ec6\u7c92\u5ea6\u8c03\u5ea6\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5757\u91cd\u8ba1\u7b97\u7b56\u7565\u89e3\u51b3MoE\u8bad\u7ec3\u4e2d\u7684\u5185\u5b58\u74f6\u9888\u95ee\u9898\uff0c\u5728\u5185\u5b58\u53d7\u9650\u7684GPU\u4e0a\u5b9e\u73b0\u7a33\u5b9a\u7684\u5927\u89c4\u6a21MoE\u8bad\u7ec3\u3002", "motivation": "\u5927\u89c4\u6a21MoE\u6a21\u578b\u8bad\u7ec3\u9762\u4e34\u4e25\u91cd\u7684\u5185\u5b58\u74f6\u9888\uff0c\u52a8\u6001token\u8def\u7531\u5bfc\u81f4\u7684\u8d1f\u8f7d\u4e0d\u5e73\u8861\u4f1a\u9020\u6210GPU\u5185\u5b58\u6ea2\u51fa\uff0c\u9650\u5236\u4e86\u6a21\u578b\u7684\u53ef\u6269\u5c55\u6027\u3002\u73b0\u6709\u8d1f\u8f7d\u5747\u8861\u65b9\u6cd5\u4f1a\u727a\u7272\u6a21\u578b\u7cbe\u5ea6\uff0c\u4e14\u5728\u5185\u5b58\u53d7\u9650\u786c\u4ef6\u4e0a\u5931\u6548\u3002", "method": "\u5c06token\u5206\u5e03\u548c\u4e13\u5bb6\u8ba1\u7b97\u5206\u89e3\u4e3a\u53ef\u7ba1\u7406\u7684\u5757\uff0c\u91c7\u7528\u5206\u5757\u91cd\u8ba1\u7b97\u7b56\u7565\uff0c\u901a\u8fc7\u7406\u8bba\u5185\u5b58\u6a21\u578b\u52a8\u6001\u4f18\u5316\u5185\u5b58\u6548\u7387\u548c\u541e\u5410\u91cf\u4e4b\u95f4\u7684\u5e73\u8861\u3002", "result": "\u76f8\u6bd4\u57fa\u4e8e\u5b8c\u5168\u91cd\u8ba1\u7b97\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0cMemFine\u51cf\u5c11\u4e8648.03%\u7684\u6fc0\u6d3b\u5185\u5b58\uff0c\u63d0\u9ad8\u4e864.42%\u7684\u541e\u5410\u91cf\u3002", "conclusion": "MemFine\u80fd\u591f\u5728\u5185\u5b58\u53d7\u9650\u7684GPU\u4e0a\u5b9e\u73b0\u7a33\u5b9a\u7684\u5927\u89c4\u6a21MoE\u8bad\u7ec3\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5185\u5b58\u74f6\u9888\u95ee\u9898\u3002"}}
{"id": "2511.21461", "categories": ["cs.AR", "eess.SP"], "pdf": "https://arxiv.org/pdf/2511.21461", "abs": "https://arxiv.org/abs/2511.21461", "authors": ["Jonas Elmiger", "Fabian Stuber", "Oscar Casta\u00f1eda", "Gian Marti", "Christoph Studer"], "title": "A 0.32 mm$^2$ 100 Mb/s 223 mW ASIC in 22FDX for Joint Jammer Mitigation, Channel Estimation, and SIMO Data Detection", "comment": "Presented at the 2025 IEEE European Solid-State Electronics Research Conference (ESSERC)", "summary": "We present the first single-input multiple-output (SIMO) receiver ASIC that jointly performs jammer mitigation, channel estimation, and data detection. The ASIC implements a recent algorithm called siMultaneous mitigAtion, Estimation, and Detection (MAED). MAED mitigates smart jammers via spatial filtering using a nonlinear optimization problem that unifies jammer estimation and nulling, channel estimation, and data detection to achieve state-of-the-art error-rate performance under jamming. The design supports eight receive antennas and enables mitigation of smart jammers as well as of barrage jammers. The ASIC is fabricated in 22 nm FD-SOI, has a core area of 0.32 mm$^2$, and achieves a throughput of 100 Mb/s at 223 mW, thus delivering 3$\\times$ higher per-user throughput and 4.5$\\times$ higher area efficiency than the state-of-the-art jammer-resilient detector.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u9996\u4e2a\u5355\u8f93\u5165\u591a\u8f93\u51fa(SIMO)\u63a5\u6536\u5668ASIC\uff0c\u8054\u5408\u6267\u884c\u5e72\u6270\u6291\u5236\u3001\u4fe1\u9053\u4f30\u8ba1\u548c\u6570\u636e\u68c0\u6d4b\uff0c\u91c7\u7528MAED\u7b97\u6cd5\uff0c\u572822nm FD-SOI\u5de5\u827a\u4e0b\u5b9e\u73b00.32mm\u00b2\u6838\u5fc3\u9762\u79ef\uff0c\u541e\u5410\u91cf100Mb/s\uff0c\u529f\u8017223mW\u3002", "motivation": "\u73b0\u6709\u7cfb\u7edf\u5728\u9762\u5bf9\u667a\u80fd\u5e72\u6270\u5668\u548c\u626b\u9891\u5e72\u6270\u5668\u65f6\u6027\u80fd\u53d7\u9650\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u540c\u65f6\u5904\u7406\u5e72\u6270\u6291\u5236\u3001\u4fe1\u9053\u4f30\u8ba1\u548c\u6570\u636e\u68c0\u6d4b\u7684\u96c6\u6210\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528MAED\u7b97\u6cd5\uff0c\u901a\u8fc7\u975e\u7ebf\u6027\u4f18\u5316\u95ee\u9898\u7edf\u4e00\u5e72\u6270\u5668\u4f30\u8ba1\u4e0e\u6291\u5236\u3001\u4fe1\u9053\u4f30\u8ba1\u548c\u6570\u636e\u68c0\u6d4b\uff0c\u4f7f\u7528\u7a7a\u95f4\u6ee4\u6ce2\u6280\u672f\u6765\u6291\u5236\u667a\u80fd\u5e72\u6270\u5668\u3002", "result": "\u652f\u63018\u4e2a\u63a5\u6536\u5929\u7ebf\uff0c\u572822nm FD-SOI\u5de5\u827a\u4e0b\u5b9e\u73b00.32mm\u00b2\u6838\u5fc3\u9762\u79ef\uff0c\u541e\u5410\u91cf100Mb/s\uff0c\u529f\u8017223mW\uff0c\u76f8\u6bd4\u73b0\u6709\u6297\u5e72\u6270\u68c0\u6d4b\u5668\uff0c\u6bcf\u7528\u6237\u541e\u5410\u91cf\u63d0\u9ad83\u500d\uff0c\u9762\u79ef\u6548\u7387\u63d0\u9ad84.5\u500d\u3002", "conclusion": "\u8be5ASIC\u8bbe\u8ba1\u6210\u529f\u5b9e\u73b0\u4e86\u8054\u5408\u5e72\u6270\u6291\u5236\u3001\u4fe1\u9053\u4f30\u8ba1\u548c\u6570\u636e\u68c0\u6d4b\u7684\u529f\u80fd\uff0c\u5728\u6297\u5e72\u6270\u6027\u80fd\u548c\u7cfb\u7edf\u6548\u7387\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002"}}
{"id": "2511.21535", "categories": ["cs.DC", "cs.PF"], "pdf": "https://arxiv.org/pdf/2511.21535", "abs": "https://arxiv.org/abs/2511.21535", "authors": ["Morteza Sadeghi"], "title": "Modeling the Effect of Data Redundancy on Speedup in MLFMA Near-Field Computation", "comment": null, "summary": "The near-field (P2P) operator in the Multilevel Fast Multipole Algorithm (MLFMA) is a performance bottleneck on GPUs due to poor memory locality. This work introduces data redundancy to improve spatial locality by reducing memory access dispersion. For validation of results, we propose an analytical model based on a Locality metric that combines data volume and access dispersion to predict speedup trends without hardware-specific profiling. The approach is validated on two MLFMA-based applications: an electromagnetic solver (DBIM-MLFMA) with regular structure, and a stellar dynamics code (PhotoNs-2.0) with irregular particle distribution. Results show up to 7X kernel speedup due to improved cache behavior. However, increased data volume raises overheads in data restructuring, limiting end-to-end application speedup to 1.04X. While the model cannot precisely predict absolute speedups, it reliably captures performance trends across different problem sizes and densities. The technique is injectable into existing implementations with minimal code changes. This work demonstrates that data redundancy can enhance GPU performance for P2P operator, provided locality gains outweigh data movement costs.", "AI": {"tldr": "\u901a\u8fc7\u5f15\u5165\u6570\u636e\u5197\u4f59\u6539\u5584MLFMA\u4e2d\u8fd1\u573a\u7b97\u5b50\u7684\u5185\u5b58\u5c40\u90e8\u6027\uff0c\u5728GPU\u4e0a\u5b9e\u73b0\u6700\u9ad87\u500d\u7684\u5185\u6838\u52a0\u901f\uff0c\u4f46\u53d7\u9650\u4e8e\u6570\u636e\u91cd\u7ec4\u5f00\u9500\uff0c\u7aef\u5230\u7aef\u5e94\u7528\u52a0\u901f\u4ec5\u4e3a1.04\u500d\u3002", "motivation": "MLFMA\u4e2d\u7684\u8fd1\u573a\u7b97\u5b50\u5728GPU\u4e0a\u7531\u4e8e\u5185\u5b58\u5c40\u90e8\u6027\u5dee\u6210\u4e3a\u6027\u80fd\u74f6\u9888\uff0c\u9700\u8981\u6539\u8fdb\u5185\u5b58\u8bbf\u95ee\u6a21\u5f0f\u3002", "method": "\u5f15\u5165\u6570\u636e\u5197\u4f59\u51cf\u5c11\u5185\u5b58\u8bbf\u95ee\u5206\u6563\uff0c\u63d0\u51fa\u57fa\u4e8e\u5c40\u90e8\u6027\u5ea6\u91cf\u7684\u5206\u6790\u6a21\u578b\u9884\u6d4b\u6027\u80fd\u8d8b\u52bf\uff0c\u5e76\u5728\u4e24\u4e2aMLFMA\u5e94\u7528\u4e2d\u9a8c\u8bc1\u3002", "result": "\u5185\u6838\u52a0\u901f\u6700\u9ad8\u8fbe7\u500d\uff0c\u4f46\u7aef\u5230\u7aef\u5e94\u7528\u52a0\u901f\u4ec51.04\u500d\uff0c\u6a21\u578b\u80fd\u53ef\u9760\u9884\u6d4b\u4e0d\u540c\u95ee\u9898\u89c4\u6a21\u548c\u5bc6\u5ea6\u4e0b\u7684\u6027\u80fd\u8d8b\u52bf\u3002", "conclusion": "\u6570\u636e\u5197\u4f59\u53ef\u63d0\u5347GPU\u4e0aP2P\u7b97\u5b50\u6027\u80fd\uff0c\u4f46\u9700\u6743\u8861\u5c40\u90e8\u6027\u6536\u76ca\u4e0e\u6570\u636e\u79fb\u52a8\u6210\u672c\uff0c\u8be5\u6280\u672f\u53ef\u6700\u5c0f\u5316\u4ee3\u7801\u4fee\u6539\u6ce8\u5165\u73b0\u6709\u5b9e\u73b0\u3002"}}
{"id": "2511.21549", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2511.21549", "abs": "https://arxiv.org/abs/2511.21549", "authors": ["Jason Yik", "Walter Gallego Gomez", "Andrew Cheng", "Benedetto Leto", "Alessandro Pierro", "Noah Pacik-Nelson", "Korneel Van den Berghe", "Vittorio Fra", "Andreea Danielescu", "Gianvito Urgese", "Vijay Janapa Reddi"], "title": "Modeling and Optimizing Performance Bottlenecks for Neuromorphic Accelerators", "comment": null, "summary": "Neuromorphic accelerators offer promising platforms for machine learning (ML) inference by leveraging event-driven, spatially-expanded architectures that naturally exploit unstructured sparsity through co-located memory and compute. However, their unique architectural characteristics create performance dynamics that differ fundamentally from conventional accelerators. Existing workload optimization approaches for neuromorphic accelerators rely on aggregate network-wide sparsity and operation counting, but the extent to which these metrics actually improve deployed performance remains unknown. This paper presents the first comprehensive performance bound and bottleneck analysis of neuromorphic accelerators, revealing the shortcomings of the conventional metrics and offering an understanding of what facets matter for workload performance. We present both theoretical analytical modeling and extensive empirical characterization of three real neuromorphic accelerators: Brainchip AKD1000, Synsense Speck, and Intel Loihi 2. From these, we establish three distinct accelerator bottleneck states, memory-bound, compute-bound, and traffic-bound, and identify which workload configuration features are likely to exhibit these bottleneck states. We synthesize all of our insights into the floorline performance model, a visual model that identifies performance bounds and informs how to optimize a given workload, based on its position on the model. Finally, we present an optimization methodology that combines sparsity-aware training with floorline-informed partitioning. Our methodology achieves substantial performance improvements at iso-accuracy: up to 3.86x runtime improvement and 3.38x energy reduction compared to prior manually-tuned configurations.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u5bf9\u795e\u7ecf\u5f62\u6001\u52a0\u901f\u5668\u8fdb\u884c\u5168\u9762\u7684\u6027\u80fd\u8fb9\u754c\u548c\u74f6\u9888\u5206\u6790\uff0c\u63ed\u793a\u4e86\u4f20\u7edf\u6307\u6807\uff08\u5982\u7f51\u7edc\u7ea7\u7a00\u758f\u6027\u548c\u64cd\u4f5c\u8ba1\u6570\uff09\u7684\u4e0d\u8db3\uff0c\u63d0\u51fa\u4e86floorline\u6027\u80fd\u6a21\u578b\u6765\u8bc6\u522b\u6027\u80fd\u8fb9\u754c\uff0c\u5e76\u5f00\u53d1\u4e86\u7ed3\u5408\u7a00\u758f\u611f\u77e5\u8bad\u7ec3\u548cfloorline\u6307\u5bfc\u5206\u533a\u7684\u4f18\u5316\u65b9\u6cd5\uff0c\u5728\u4fdd\u6301\u51c6\u786e\u7387\u7684\u540c\u65f6\u5b9e\u73b0\u4e863.86\u500d\u8fd0\u884c\u65f6\u95f4\u63d0\u5347\u548c3.38\u500d\u80fd\u8017\u964d\u4f4e\u3002", "motivation": "\u795e\u7ecf\u5f62\u6001\u52a0\u901f\u5668\u5229\u7528\u4e8b\u4ef6\u9a71\u52a8\u3001\u7a7a\u95f4\u6269\u5c55\u67b6\u6784\u81ea\u7136\u5229\u7528\u975e\u7ed3\u6784\u5316\u7a00\u758f\u6027\uff0c\u4f46\u5176\u72ec\u7279\u7684\u67b6\u6784\u7279\u6027\u4ea7\u751f\u4e86\u4e0e\u4f20\u7edf\u52a0\u901f\u5668\u6839\u672c\u4e0d\u540c\u7684\u6027\u80fd\u52a8\u6001\u3002\u73b0\u6709\u4f18\u5316\u65b9\u6cd5\u4f9d\u8d56\u805a\u5408\u7f51\u7edc\u7ea7\u7a00\u758f\u6027\u548c\u64cd\u4f5c\u8ba1\u6570\uff0c\u4f46\u8fd9\u4e9b\u6307\u6807\u5bf9\u5b9e\u9645\u90e8\u7f72\u6027\u80fd\u7684\u6539\u5584\u7a0b\u5ea6\u672a\u77e5\u3002", "method": "\u5bf9\u4e09\u79cd\u771f\u5b9e\u795e\u7ecf\u5f62\u6001\u52a0\u901f\u5668\uff08Brainchip AKD1000\u3001Synsense Speck\u3001Intel Loihi 2\uff09\u8fdb\u884c\u7406\u8bba\u5206\u6790\u5efa\u6a21\u548c\u5e7f\u6cdb\u5b9e\u8bc1\u8868\u5f81\uff0c\u5efa\u7acb\u4e09\u79cd\u74f6\u9888\u72b6\u6001\uff08\u5185\u5b58\u53d7\u9650\u3001\u8ba1\u7b97\u53d7\u9650\u3001\u6d41\u91cf\u53d7\u9650\uff09\uff0c\u63d0\u51fafloorline\u6027\u80fd\u6a21\u578b\u8bc6\u522b\u6027\u80fd\u8fb9\u754c\uff0c\u5f00\u53d1\u7ed3\u5408\u7a00\u758f\u611f\u77e5\u8bad\u7ec3\u548cfloorline\u6307\u5bfc\u5206\u533a\u7684\u4f18\u5316\u65b9\u6cd5\u3002", "result": "\u8bc6\u522b\u4e86\u4e09\u79cd\u4e0d\u540c\u7684\u52a0\u901f\u5668\u74f6\u9888\u72b6\u6001\uff0c\u5efa\u7acb\u4e86floorline\u6027\u80fd\u6a21\u578b\u6765\u8bc6\u522b\u6027\u80fd\u8fb9\u754c\uff0c\u4f18\u5316\u65b9\u6cd5\u5728\u4fdd\u6301\u51c6\u786e\u7387\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u6700\u9ad83.86\u500d\u7684\u8fd0\u884c\u65f6\u95f4\u63d0\u5347\u548c3.38\u500d\u7684\u80fd\u8017\u964d\u4f4e\u3002", "conclusion": "\u4f20\u7edf\u57fa\u4e8e\u7f51\u7edc\u7ea7\u7a00\u758f\u6027\u548c\u64cd\u4f5c\u8ba1\u6570\u7684\u6307\u6807\u4e0d\u8db3\u4ee5\u4f18\u5316\u795e\u7ecf\u5f62\u6001\u52a0\u901f\u5668\u6027\u80fd\uff0cfloorline\u6027\u80fd\u6a21\u578b\u80fd\u6709\u6548\u8bc6\u522b\u6027\u80fd\u8fb9\u754c\u548c\u74f6\u9888\uff0c\u7ed3\u5408\u7a00\u758f\u611f\u77e5\u8bad\u7ec3\u7684\u5206\u533a\u4f18\u5316\u65b9\u6cd5\u80fd\u663e\u8457\u63d0\u5347\u6027\u80fd\u6548\u7387\u3002"}}
{"id": "2511.21612", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2511.21612", "abs": "https://arxiv.org/abs/2511.21612", "authors": ["Shahir Abdullah", "Syed Rohit Zaman"], "title": "Diagonal Scaling: A Multi-Dimensional Resource Model and Optimization Framework for Distributed Databases", "comment": null, "summary": "Modern cloud databases present scaling as a binary decision: scale-out by adding nodes or scale-up by increasing per-node resources. This one-dimensional view is limiting because database performance, cost, and coordination overhead emerge from the joint interaction of horizontal elasticity and per-node CPU, memory, network bandwidth, and storage IOPS. As a result, systems often overreact to load spikes, underreact to memory pressure, or oscillate between suboptimal states. We introduce the Scaling Plane, a two-dimensional model in which each distributed database configuration is represented as a point (H, V), with H denoting node count and V a vector of resources. Over this plane, we define smooth approximations of latency, throughput, coordination overhead, and monetary cost, providing a unified view of performance trade-offs. We show analytically and empirically that optimal scaling trajectories frequently lie along diagonal paths: sequences of joint horizontal and vertical adjustments that simultaneously exploit cluster parallelism and per-node improvements. To compute such actions, we propose DIAGONALSCALE, a discrete local-search algorithm that evaluates horizontal, vertical, and diagonal moves in the Scaling Plane and selects the configuration minimizing a multi-objective function subject to SLA constraints. Using synthetic surfaces, microbenchmarks, and experiments on distributed SQL and KV systems, we demonstrate that diagonal scaling reduces p95 latency by up to 40 percent, lowers cost-per-query by up to 37 percent, and reduces rebalancing by 2 to 5 times compared to horizontal-only and vertical-only autoscaling. Our results highlight the need for multi-dimensional scaling models and provide a foundation for next-generation autoscaling in cloud database systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e8c\u7ef4\u6269\u5c55\u5e73\u9762\u6a21\u578b\uff0c\u5c06\u6c34\u5e73\u6269\u5c55\u548c\u5782\u76f4\u6269\u5c55\u7ed3\u5408\u8003\u8651\uff0c\u901a\u8fc7DIAGONALSCALE\u7b97\u6cd5\u8ba1\u7b97\u6700\u4f18\u7684\u659c\u5411\u6269\u5c55\u8def\u5f84\uff0c\u76f8\u6bd4\u4f20\u7edf\u5355\u4e00\u7ef4\u5ea6\u7684\u6269\u5c55\u65b9\u5f0f\u80fd\u663e\u8457\u964d\u4f4e\u5ef6\u8fdf\u548c\u6210\u672c\u3002", "motivation": "\u73b0\u4ee3\u4e91\u6570\u636e\u5e93\u5c06\u6269\u5c55\u89c6\u4e3a\u4e8c\u5143\u51b3\u7b56\uff1a\u6c34\u5e73\u6269\u5c55\uff08\u589e\u52a0\u8282\u70b9\uff09\u6216\u5782\u76f4\u6269\u5c55\uff08\u589e\u52a0\u5355\u8282\u70b9\u8d44\u6e90\uff09\u3002\u8fd9\u79cd\u4e00\u7ef4\u89c6\u56fe\u9650\u5236\u4e86\u6027\u80fd\u4f18\u5316\uff0c\u56e0\u4e3a\u6570\u636e\u5e93\u6027\u80fd\u3001\u6210\u672c\u548c\u534f\u8c03\u5f00\u9500\u662f\u6c34\u5e73\u5f39\u6027\u548c\u5355\u8282\u70b9\u8d44\u6e90\u5171\u540c\u4f5c\u7528\u7684\u7ed3\u679c\u3002", "method": "\u5f15\u5165\u6269\u5c55\u5e73\u9762\u4e8c\u7ef4\u6a21\u578b\uff0c\u5c06\u5206\u5e03\u5f0f\u6570\u636e\u5e93\u914d\u7f6e\u8868\u793a\u4e3a\u70b9(H,V)\uff0c\u5176\u4e2dH\u662f\u8282\u70b9\u6570\uff0cV\u662f\u8d44\u6e90\u5411\u91cf\u3002\u63d0\u51faDIAGONALSCALE\u7b97\u6cd5\uff0c\u901a\u8fc7\u5c40\u90e8\u641c\u7d22\u8bc4\u4f30\u6c34\u5e73\u3001\u5782\u76f4\u548c\u659c\u5411\u79fb\u52a8\uff0c\u9009\u62e9\u6700\u5c0f\u5316\u591a\u76ee\u6807\u51fd\u6570\u7684\u914d\u7f6e\u3002", "result": "\u659c\u5411\u6269\u5c55\u76f8\u6bd4\u4ec5\u6c34\u5e73\u6216\u4ec5\u5782\u76f4\u7684\u81ea\u52a8\u6269\u5c55\uff0c\u80fd\u5c06p95\u5ef6\u8fdf\u964d\u4f4e\u9ad8\u8fbe40%\uff0c\u6bcf\u67e5\u8be2\u6210\u672c\u964d\u4f4e\u9ad8\u8fbe37%\uff0c\u91cd\u5e73\u8861\u6b21\u6570\u51cf\u5c112-5\u500d\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u591a\u7ef4\u6269\u5c55\u6a21\u578b\u7684\u5fc5\u8981\u6027\uff0c\u4e3a\u4e0b\u4e00\u4ee3\u4e91\u6570\u636e\u5e93\u7cfb\u7edf\u7684\u81ea\u52a8\u6269\u5c55\u63d0\u4f9b\u4e86\u57fa\u7840\u3002"}}
{"id": "2511.21661", "categories": ["cs.DC", "cs.DB"], "pdf": "https://arxiv.org/pdf/2511.21661", "abs": "https://arxiv.org/abs/2511.21661", "authors": ["Beth Plale", "Neelesh Karthikeyan", "Isuru Gamage", "Joe Stubbs", "Sachith Withana"], "title": "AI/ML Model Cards in Edge AI Cyberinfrastructure: towards Agentic AI", "comment": null, "summary": "AI/ML model cards can contain a benchmarked evaluation of an AI/ML model against intended use but a one time assessment during model training does not get at how and where a model is actually used over its lifetime. Through Patra Model Cards embedded in the ICICLE AI Institute software ecosystem we study model cards as dynamic objects. The study reported here assesses the benefits and tradeoffs of adopting the Model Context Protocol (MCP) as an interface to the Patra Model Card server. Quantitative assessment shows the overhead of MCP as compared to a REST interface. The core question however is of active sessions enabled by MCP; this is a qualitative question of fit and use in the context of dynamic model cards that we address as well.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8bc4\u4f30\u4e86\u5728Patra\u6a21\u578b\u5361\u670d\u52a1\u5668\u4e2d\u4f7f\u7528\u6a21\u578b\u4e0a\u4e0b\u6587\u534f\u8bae(MCP)\u4f5c\u4e3a\u63a5\u53e3\u7684\u6548\u76ca\u4e0e\u6743\u8861\uff0c\u5305\u62ec\u5b9a\u91cf\u6027\u80fd\u5f00\u9500\u548c\u5b9a\u6027\u4f7f\u7528\u9002\u914d\u6027\u5206\u6790\u3002", "motivation": "\u4f20\u7edfAI/ML\u6a21\u578b\u5361\u5728\u8bad\u7ec3\u65f6\u7684\u4e00\u6b21\u6027\u8bc4\u4f30\u65e0\u6cd5\u53cd\u6620\u6a21\u578b\u5728\u5b9e\u9645\u4f7f\u7528\u4e2d\u7684\u52a8\u6001\u8868\u73b0\uff0c\u9700\u8981\u7814\u7a76\u6a21\u578b\u5361\u4f5c\u4e3a\u52a8\u6001\u5bf9\u8c61\u7684\u4f7f\u7528\u65b9\u5f0f\u3002", "method": "\u901a\u8fc7\u5d4c\u5165ICICLE AI\u7814\u7a76\u6240\u8f6f\u4ef6\u751f\u6001\u7cfb\u7edf\u7684Patra\u6a21\u578b\u5361\uff0c\u7814\u7a76MCP\u4f5c\u4e3a\u63a5\u53e3\u4e0eREST\u63a5\u53e3\u7684\u6027\u80fd\u5bf9\u6bd4\uff0c\u5e76\u5206\u6790MCP\u652f\u6301\u7684\u6d3b\u8dc3\u4f1a\u8bdd\u4f7f\u7528\u60c5\u51b5\u3002", "result": "\u5b9a\u91cf\u8bc4\u4f30\u663e\u793aMCP\u76f8\u6bd4REST\u63a5\u53e3\u5b58\u5728\u6027\u80fd\u5f00\u9500\uff0c\u4f46\u6838\u5fc3\u4ef7\u503c\u5728\u4e8e\u652f\u6301\u52a8\u6001\u6a21\u578b\u5361\u7684\u6d3b\u8dc3\u4f1a\u8bdd\u529f\u80fd\u3002", "conclusion": "MCP\u4f5c\u4e3a\u63a5\u53e3\u5728\u52a8\u6001\u6a21\u578b\u5361\u573a\u666f\u4e2d\u5177\u6709\u4f7f\u7528\u9002\u914d\u6027\u4f18\u52bf\uff0c\u5c3d\u7ba1\u5b58\u5728\u6027\u80fd\u5f00\u9500\uff0c\u4f46\u652f\u6301\u6d3b\u8dc3\u4f1a\u8bdd\u529f\u80fd\u662f\u5176\u6838\u5fc3\u4ef7\u503c\u3002"}}
