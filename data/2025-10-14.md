<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 12]
- [cs.DC](#cs.DC) [Total: 14]
- [cs.AR](#cs.AR) [Total: 5]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Herb.jl: A Unifying Program Synthesis Library](https://arxiv.org/abs/2510.09726)
*Tilman Hinnerichs,Reuben Gardos Reid,Jaap de Jong,Bart Swinkels,Pamela Wochner,Nicolae Filat,Tudor Magurescu,Issa Hanou,Sebastijan Dumancic*

Main category: cs.PL

TL;DR: Herb.jl是一个用Julia编程语言编写的统一程序合成库，旨在模块化合成算法，便于重用和扩展现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的程序合成工具虽然众多，但重用和混合先前开发的方法既繁琐又耗时。

Method: 将底层合成算法模块化为可通信和完全可扩展的子组件，允许直接重用这些模块。

Result: 展示了三个常见用例：实现简单问题和语法并解决、用少量代码实现先前开发的合成器、以及针对基准运行合成器。

Conclusion: Herb.jl通过模块化方法解决了程序合成中方法重用困难的问题，提高了开发效率。

Abstract: Program synthesis -- the automatic generation of code given a specification
-- is one of the most fundamental tasks in artificial intelligence (AI) and
many programmers' dream. Numerous synthesizers have been developed to tackle
program synthesis, manifesting different ideas to approach the exponentially
growing program space. While numerous smart program synthesis tools exist,
reusing and remixing previously developed methods is tedious and
time-consuming. We propose Herb.jl, a unifying program synthesis library
written in the Julia programming language, to address these issues. Since
current methods rely on similar building blocks, we aim to modularize the
underlying synthesis algorithm into communicating and fully extendable
sub-compartments, allowing for straightforward reapplication of these modules.
To demonstrate the benefits of using Herb.jl, we show three common use cases:
1. how to implement a simple problem and grammar, and how to solve it, 2. how
to implement a previously developed synthesizer with just a few lines of code,
and 3. how to run a synthesizer against a benchmark.

</details>


### [2] [ACT: Automatically Generating Compiler Backends from Tensor Accelerator ISA Descriptions](https://arxiv.org/abs/2510.09932)
*Devansh Jain,Akash Pardeshi,Marco Frigo,Krut Patel,Kaustubh Khulbe,Jai Arora,Charith Mendis*

Main category: cs.PL

TL;DR: ACT是一个自动生成张量加速器编译器后端的工具，仅需指令集架构描述即可生成高性能代码，解决了新型张量加速器缺乏编译器支持的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的张量加速器缺乏编译器后端支持，且设计迭代速度快，手动开发编译器后端成本高、周期长，阻碍了新型加速器的采用和软件开发。

Method: ACT通过形式化定义编译器后端生成问题，支持用户可编程内存和复杂参数化指令，采用参数化等式饱和指令选择和基于约束规划的内存分配方法。

Result: 为三个工业界和学术界的加速器平台生成了编译器后端，生成的代码性能达到或超过手工优化内核库，同时保持低编译开销。

Conclusion: ACT能够自动、高效地生成张量加速器的编译器后端，证明其生成的后端是正确且完整的，有助于加速新型张量加速器的软件开发生态建设。

Abstract: Tensor compilers play a key role in enabling high-performance implementations
of deep learning workloads. These compilers rely on existing CPU and GPU code
generation backends to generate device-specific code. Recently, many tensor
accelerators (neural processing units) have been proposed to further accelerate
these workloads. Compared to commodity hardware, however, most of the proposed
tensor accelerators do not have compiler backends with code generation support.
Moreover, the accelerator designs are subject to fast iteration cycles, making
it difficult to manually develop compiler backends similar to commodity
hardware platforms. Therefore, to increase adoption and enable faster software
development cycles for novel tensor accelerator designs, we need to make the
compiler backend construction process more agile.
  To address this gap, we introduce ACT, a compiler backend generator that
automatically generates compiler backends for tensor accelerators, given just
the instruction set architecture (ISA) descriptions. We first formally specify
the compiler backend generation problem that introduces a novel specification
for describing tensor accelerator ISAs. Next, we design ACT such that it
supports user-programmable memories and complex parameterized instructions that
are prevalent in tensor accelerators. ACT uses a novel parameterized equality
saturation-based instruction selection phase and a constraint programming-based
memory allocation phase. We prove that compiler backends generated by ACT are
sound and complete. Finally, we generate compiler backends for three
accelerator platforms from industry and academia, and show that they match or
outperform code written using hand-optimized kernel libraries while maintaining
low compilation overheads.

</details>


### [3] [End-to-end Compositional Verification of Program Safety through Verified and Verifying Compilation](https://arxiv.org/abs/2510.10015)
*Jinhua Wu,Yuting Wang,Liukun Yu,Linglong Meng*

Main category: cs.PL

TL;DR: 提出了基于开放标签转移系统的开放安全性概念，用于解决现代安全编程语言（如Rust）中混合安全与不安全模块时的端到端安全验证问题。


<details>
  <summary>Details</summary>
Motivation: 现代安全编程语言需要混合安全和不安全模块来实现所有功能，因此需要模块化的安全性定义，能够在目标级别组合，并与验证编译协作确保端到端安全。

Method: 基于开放标签转移系统定义开放安全性概念，支持模块化组合和验证组合编译，可推广到部分安全性，并将验证编译的正确性形式化为从部分安全性推导出完全安全性。

Result: 开发了所有权语言Owlang的验证编译器，并在由Owlang和C实现的哈希映射上评估了组合安全验证方法。

Conclusion: 开放安全性框架能够结合验证编译和验证编译，支持异构模块的单独安全验证和在目标级别的安全结果组合。

Abstract: Program safety (i.e., absence of undefined behaviors) is critical for correct
operation of computer systems. It is usually verified at the source level
(e.g., by separation logics) and preserved to the target by verified compilers
(e.g., CompCert), thereby achieving end-to-end verification of safety. However,
modern safe programming languages like Rust pose new problems in achieving
end-to-end safety. Because not all functionalities can be implemented in the
safe language, mixing safe and unsafe modules is needed. Therefore, verified
compilation must preserve a modular notion of safety which can be composed at
the target level. Furthermore, certain classes of errors (e.g., memory errors)
are automatically excluded by verifying compilation (e.g., borrow checking) for
modules written in safe languages. As a result, verified compilation needs to
cooperate with verifying compilation to ensure end-to-end safety.
  To address the above problems, we propose a modular and generic definition of
safety called open safety based on program semantics described as open labeled
transition systems (LTS). Open safety is composable at the boundary of modules
and can be modularly preserved by verified compositional compilation. Those
properties enable separate verification of safety for heterogeneous modules and
composition of the safety results at the target level. Open safety can be
generalized to partial safety (i.e., only a certain class of errors can occur).
By this we formalized the correctness of verifying compilation as derivation of
total safety from partial safety. We demonstrate how our framework can combine
verified and verifying compilation by developing a verified compiler for an
ownership language (called Owlang) inspired by Rust. We evaluate our approach
on the compositional safety verification using a hash map implemented by Owlang
and C.

</details>


### [4] [LOOPerSet: A Large-Scale Dataset for Data-Driven Polyhedral Compiler Optimization](https://arxiv.org/abs/2510.10209)
*Massinissa Merouani,Afif Boudaoud,Riyadh Baghdadi*

Main category: cs.PL

TL;DR: LOOPerSet是一个包含2800万个标记数据点的新公共数据集，用于机器学习驱动的编译器优化研究，特别是多面体模型中的性能预测。


<details>
  <summary>Details</summary>
Motivation: 机器学习在编译器优化中的应用受到大规模公开性能数据集稀缺的限制，这迫使研究人员进行昂贵的数据生成，减缓了创新并阻碍了可重现研究。

Method: 从22万个独特的合成生成的多面体程序中提取2800万个数据点，每个数据点将程序和复杂的语义保持转换序列（如融合、倾斜、平铺和并行化）映射到真实性能测量（执行时间）。

Result: 创建了LOOPerSet数据集，其规模和多样性使其成为训练和评估学习成本模型、基准测试新模型架构以及探索自动化多面体调度前沿的宝贵资源。

Conclusion: 该数据集在宽松许可下发布，旨在促进可重现研究并降低数据驱动编译器优化的入门门槛。

Abstract: The advancement of machine learning for compiler optimization, particularly
within the polyhedral model, is constrained by the scarcity of large-scale,
public performance datasets. This data bottleneck forces researchers to
undertake costly data generation campaigns, slowing down innovation and
hindering reproducible research learned code optimization. To address this gap,
we introduce LOOPerSet, a new public dataset containing 28 million labeled data
points derived from 220,000 unique, synthetically generated polyhedral
programs. Each data point maps a program and a complex sequence of
semantics-preserving transformations (such as fusion, skewing, tiling, and
parallelism)to a ground truth performance measurement (execution time). The
scale and diversity of LOOPerSet make it a valuable resource for training and
evaluating learned cost models, benchmarking new model architectures, and
exploring the frontiers of automated polyhedral scheduling. The dataset is
released under a permissive license to foster reproducible research and lower
the barrier to entry for data-driven compiler optimization.

</details>


### [5] [Learning to Guarantee Type Correctness in Code Generation through Type-Guided Program Synthesis](https://arxiv.org/abs/2510.10216)
*Zhechong Huang,Zhao Zhang,Ruyi Ji,Tingxuan Xia,Qihao Zhu,Qinxiang Cao,Zeyu Sun,Yingfei Xiong*

Main category: cs.PL

TL;DR: TyFlow通过将类型推理内化到代码生成中，提出基于合成决策序列的新型代码表示方法，在消除类型错误的同时显著提升功能正确性。


<details>
  <summary>Details</summary>
Motivation: 传统方法如约束解码只能外部拒绝不可类型化的代码，但模型本身未能有效学习类型推理，限制了整体性能。

Method: 提出类型引导的程序合成系统，保持类型推导树与合成推导树之间的同构关系，创建基于合成决策序列而非传统文本标记序列的代码表示。

Result: TyFlow不仅消除了类型错误，还显著提高了功能正确性。

Conclusion: 将语言模型与类型系统内部对齐对于提升代码生成性能至关重要。

Abstract: Language models have shown remarkable proficiency in code generation;
nevertheless, ensuring type correctness remains a challenge. Although
traditional methods, such as constrained decoding, alleviate this problem by
externally rejecting untypable code, the model itself does not effectively
learn type reasoning internally, which ultimately limits its overall
performance. This paper introduces TyFlow, a novel system that internalizes
type reasoning within code generation to guide the model to learn the type
system. The core of our approach is a novel type-guided program synthesis
system that maintains an isomorphism between type derivation trees and
synthesis derivation trees, enabling a new code representation based on
synthesis decision sequences rather than traditional text-based token
sequences. By offloading the complexity of type system learning to the
representation itself, models can redirect their computational resources toward
higher-level program semantics. Our evaluation shows that TyFlow not only
eliminates type errors but also significantly improves functional correctness,
highlighting the importance of aligning LMs with type systems internally.

</details>


### [6] [Old is Gold: Optimizing Single-threaded Applications with Exgen-Malloc](https://arxiv.org/abs/2510.10219)
*Ruihao Li,Lizy K. John,Neeraja J. Yadwadkar*

Main category: cs.PL

TL;DR: Exgen-Malloc是一个专为单线程应用设计的内存分配器，通过简化元数据和控制流，在SPEC CPU2017、redis-benchmark和mimalloc-bench测试中分别实现了1.17x、1.10x和1.93x的速度提升，以及6.2%、0.1%和25.2%的内存节省。


<details>
  <summary>Details</summary>
Motivation: 现代内存分配器为多线程环境优化，但复杂的元数据和控制逻辑带来了显著开销。在单线程场景中，这些开销可以避免，从而提升分配效率和降低内存消耗。

Method: 设计专为单线程的Exgen-Malloc，采用集中式堆、单一空闲块列表、平衡的内存提交和重定位策略，并借鉴现代多线程分配器的设计原则。

Result: 在两个Intel Xeon平台上测试，Exgen-Malloc在SPEC CPU2017、redis-benchmark和mimalloc-bench上分别比dlmalloc快1.17x、1.10x和1.93x，比mimalloc节省6.2%、0.1%和25.2%内存。

Conclusion: 专为单线程应用优化的内存分配器能够显著提升性能并减少内存消耗，证明在特定场景下简化设计的有效性。

Abstract: Memory allocators hide beneath nearly every application stack, yet their
performance footprint extends far beyond their code size. Even small
inefficiencies in the allocators ripple through caches and the rest of the
memory hierarchy, collectively imposing what operators often call a "datacenter
tax". At hyperscale, even a 1% improvement in allocator efficiency can unlock
millions of dollars in savings and measurable reductions in datacenter energy
consumption. Modern memory allocators are designed to optimize allocation speed
and memory fragmentation in multi-threaded environments, relying on complex
metadata and control logic to achieve high performance. However, the overhead
introduced by this complexity prompts a reevaluation of allocator design.
Notably, such overhead can be avoided in single-threaded scenarios, which
continue to be widely used across diverse application domains.
  In this paper, we introduce Exgen-Malloc, a memory allocator purpose-built
for single-threaded applications. By specializing for single-threaded
execution, Exgen-Malloc eliminates unnecessary metadata, simplifies the control
flow, thereby reducing overhead and improving allocation efficiency. Its core
design features include a centralized heap, a single free-block list, and a
balanced strategy for memory commitment and relocation. Additionally,
Exgen-Malloc incorporates design principles in modern multi-threaded
allocators, which do not exist in legacy single-threaded allocators such as
dlmalloc. We evaluate Exgen-Malloc on two Intel Xeon platforms. Across both
systems, Exgen-Malloc achieves a speedup of 1.17x, 1.10x, and 1.93x over
dlmalloc on SPEC CPU2017, redis-benchmark, and mimalloc-bench, respectively. In
addition to performance, Exgen-Malloc achieves 6.2%, 0.1%, and 25.2% memory
savings over mimalloc on SPEC CPU2017, redis-benchmark, and mimalloc-bench,
respectively.

</details>


### [7] [A Trace-based Approach for Code Safety Analysis](https://arxiv.org/abs/2510.10410)
*Hui Xu*

Main category: cs.PL

TL;DR: 本文系统分析了Rust语言中的不安全代码和未定义行为，建立了理解框架并总结了Rust代码的健全性标准，为安全封装提供了可操作指导。


<details>
  <summary>Details</summary>
Motivation: Rust作为内存安全编程语言虽然禁止未定义行为，但不安全代码仍然是关键问题。通过审查Rust的安全设计和分析实际项目，需要建立系统性框架来理解不安全代码和未定义行为。

Method: 通过审查Rust的安全设计并分析真实世界的Rust项目，建立理解不安全代码和未定义行为的系统性框架，总结Rust代码的健全性标准。

Result: 建立了理解不安全代码和未定义行为的系统性框架，总结了Rust代码的健全性标准，并推导出实现安全封装的可操作指导。

Conclusion: 该研究为Rust开发者提供了理解不安全代码的系统性方法和实现安全封装的具体指导，有助于提高Rust代码的可靠性和安全性。

Abstract: Rust is a memory-safe programming language that disallows undefined behavior.
Its safety guarantees have been extensively examined by the community through
empirical studies, which has led to its remarkable success. However, unsafe
code remains a critical concern in Rust. By reviewing the safety design of Rust
and analyzing real-world Rust projects, this paper establishes a systematic
framework for understanding unsafe code and undefined behavior, and summarizes
the soundness criteria for Rust code. It further derives actionable guidance
for achieving sound encapsulation.

</details>


### [8] [ECO: Enhanced Code Optimization via Performance-Aware Prompting for Code-LLMs](https://arxiv.org/abs/2510.10517)
*Su-Hyeon Kim,Joonghyuk Hahn,Sooyoung Cha,Yo-Sub Han*

Main category: cs.PL

TL;DR: ECO是一个性能感知的代码优化提示框架，通过分析性能瓶颈和优化原理，指导代码LLM生成更高效的代码，无需微调即可实现高达7.81倍的加速。


<details>
  <summary>Details</summary>
Motivation: 现有的基于慢快代码对的优化方法往往导致表面模式模仿而非真正的性能推理，无法揭示性能提升的因果因素。

Method: ECO框架首先从参考代码对中提取运行时优化指令(ROI)，然后并行使用符号顾问生成瓶颈诊断和ROI检索器检索相关优化指令，最后组合成性能感知提示来指导代码LLM。

Result: ECO提示显著提高了代码LLM生成高效代码的能力，实现了高达7.81倍的加速，同时最小化正确性损失。

Conclusion: ECO是一个模型无关、无需微调的代码优化框架，通过性能感知提示有效指导代码LLM进行深度性能推理而非表面模式模仿。

Abstract: Code runtime optimization-the task of rewriting a given code to a faster
one-remains challenging, as it requires reasoning about performance trade-offs
involving algorithmic and structural choices. Recent approaches employ
code-LLMs with slow-fast code pairs provided as optimization guidance, but such
pair-based methods obscure the causal factors of performance gains and often
lead to superficial pattern imitation rather than genuine performance
reasoning. We introduce ECO, a performance-aware prompting framework for code
optimization. ECO first distills runtime optimization instructions (ROIs) from
reference slow-fast code pairs; Each ROI describes root causes of inefficiency
and the rationales that drive performance improvements. For a given input code,
ECO in parallel employs (i) a symbolic advisor to produce a bottleneck
diagnosis tailored to the code, and (ii) an ROI retriever to return related
ROIs. These two outputs are then composed into a performance-aware prompt,
providing actionable guidance for code-LLMs. ECO's prompts are model-agnostic,
require no fine-tuning, and can be easily prepended to any code-LLM prompt. Our
empirical studies highlight that ECO prompting significantly improves
code-LLMs' ability to generate efficient code, achieving speedups of up to
7.81x while minimizing correctness loss.

</details>


### [9] [A Verified High-Performance Composable Object Library for Remote Direct Memory Access (Extended Version)](https://arxiv.org/abs/2510.10531)
*Guillaume Ambal,George Hodgkins,Mark Madler,Gregory Chockler,Brijesh Dongol,Joseph Izraelevitz,Azalea Raad,Viktor Vafeiadis*

Main category: cs.PL

TL;DR: LOCO是一个经过形式化验证的RDMA多节点对象库，填补了共享内存和分布式系统编程之间的空白，提供高性能且易于验证的正确性保证。


<details>
  <summary>Details</summary>
Motivation: RDMA虽然提供低延迟高吞吐的网络性能，但其弱内存模型难以在实际中使用，且最近才被形式化，需要简化编程模型并确保正确性。

Method: 开发LOCO库构建多节点对象，利用RDMA的强局部性和弱一致性特性；创建Mowgli模块化声明式验证框架，独立于内存一致性模型，并实例化到RDMA内存模型。

Result: LOCO对象性能与定制RDMA系统相当，但编程模型更简单，适合形式化正确性证明；Mowgli框架成功验证了LOCO库的正确性。

Conclusion: LOCO为RDMA提供了易于使用且可验证的多节点对象编程抽象，Mowgli框架为多节点对象验证提供了灵活的方法。

Abstract: Remote Direct Memory Access (RDMA) is a memory technology that allows remote
devices to directly write to and read from each other's memory, bypassing
components such as the CPU and operating system. This enables low-latency
high-throughput networking, as required for many modern data centres, HPC
applications and AI/ML workloads. However, baseline RDMA comprises a highly
permissive weak memory model that is difficult to use in practice and has only
recently been formalised. In this paper, we introduce the Library of Composable
Objects (LOCO), a formally verified library for building multi-node objects on
RDMA, filling the gap between shared memory and distributed system programming.
LOCO objects are well-encapsulated and take advantage of the strong locality
and the weak consistency characteristics of RDMA. They have performance
comparable to custom RDMA systems (e.g. distributed maps), but with a far
simpler programming model amenable to formal proofs of correctness. To support
verification, we develop a novel modular declarative verification framework,
called Mowgli, that is flexible enough to model multinode objects and is
independent of a memory consistency model. We instantiate Mowgli with the RDMA
memory model, and use it to verify correctness of LOCO libraries.

</details>


### [10] [Abstract String Domain Defined with Word Equations as a Reduced Product (Extended Version)](https://arxiv.org/abs/2510.11007)
*Antonina Nepeivoda,Ilya Afanasyev*

Main category: cs.PL

TL;DR: 提出了一个字符串区间抽象域，通过字方程和字不等式来表征字符串边界，构建了基于长度非递增态射的抽象字符串对象格结构，并应用于JavaScript字符串程序分析。


<details>
  <summary>Details</summary>
Motivation: 为了解决JavaScript等语言中字符串操作程序的分析问题，需要一种能够精确表征字符串边界和约束的抽象域。

Method: 定义字符串区间抽象域，使用字方程表示下界、字不等式表示上界；构建基于长度非递增态射的字符串属性半格上的简化积；设计多种约简策略形成格结构；定义基本抽象字符串操作以最小化计算开销。

Result: 构建了字符串对象域的格结构，实现了对JavaScript字符串操作程序的有效分析。

Conclusion: 该字符串区间抽象域能够有效分析字符串操作程序的属性，为程序验证提供理论基础。

Abstract: We introduce a string-interval abstract domain, where string intervals are
characterized by systems of word equations (encoding lower bounds on string
values) and word disequalities (encoding upper bounds). Building upon the
lattice structure of string intervals, we define an abstract string object as a
reduced product on a string property semilattice, determined by
length-non-increasing morphisms. We consider several reduction strategies for
abstract string objects and show that upon these strategies the string object
domain forms a lattice. We define basic abstract string operations on the
domain, aiming to minimize computational overheads on the reduction, and show
how the domain can be used to analyse properties of JavaScript string
manipulating programs.

</details>


### [11] [HUGR: A Quantum-Classical Intermediate Representation](https://arxiv.org/abs/2510.11420)
*Mark Koch,Agustín Borgna,Seyon Sivarajah,Alan Lawrence,Alec Edgington,Douglas Wilson,Craig Roy,Luca Mondada,Lukas Heidemann,Ross Duncan*

Main category: cs.PL

TL;DR: 提出了HUGR：一种基于图的新型中间表示，用于混合量子-经典程序，具有高表达性和可扩展性，支持多级抽象编译和安全性保证。


<details>
  <summary>Details</summary>
Motivation: 为了捕捉近量子计算设备的能力和新兴量子编程范式的抽象，需要一个机器友好且支持强大编译技术的中间表示。

Method: 设计基于图的中间表示HUGR，受MLIR启发，支持多级抽象、严格静态类型和线性量子类型等安全保证。

Result: 开发了完整的HUGR规范和开源参考实现，支持模式匹配编译技术和平滑的抽象层级转换。

Conclusion: HUGR为量子编译工具提供了安全、可扩展的基础设施，能够适应不断发展的量子计算设备和编程范式。

Abstract: We introduce the Hierarchical Unified Graph Representation (HUGR): a novel
graph based intermediate representation for mixed quantum-classical programs.
HUGR's design features high expressivity and extensibility to capture the
capabilities of near-term and forthcoming quantum computing devices, as well as
new and evolving abstractions from novel quantum programming paradigms. The
graph based structure is machine-friendly and supports powerful pattern
matching based compilation techniques. Inspired by MLIR, HUGR's extensibility
further allows compilation tooling to reason about programs at multiple levels
of abstraction, lowering smoothly between them. Safety guarantees in the
structure including strict, static typing and linear quantum types allow rapid
development of compilation tooling without fear of program invalidation. A full
specification of HUGR and reference implementation are open-source and
available online.

</details>


### [12] [(Dis)Proving Spectre Security with Speculation-Passing Style](https://arxiv.org/abs/2510.11573)
*Santiago Arranz-Olmos,Gilles Barthe,Lionel Blatter,Xingyu Xie,Zhiyuan Zhang*

Main category: cs.PL

TL;DR: 本文提出了Speculation-Passing Style (SPS)程序转换方法，将推测性常数时间(SCT)验证简化为常数时间(CT)验证，为SCT工具提供了形式化基础。


<details>
  <summary>Details</summary>
Motivation: 现有推测性常数时间(SCT)验证工具通常是从常数时间(CT)工具提升而来，但这种提升缺乏精确的定义和形式化分析。本文旨在填补这一空白，为这些提升提供形式化基础。

Method: 引入Speculation-Passing Style (SPS)程序转换方法，通过为程序添加攻击者控制的预测输入并修改程序来遵循这些预测，从而将SCT验证转化为CT验证。

Result: SPS转换是完备的，即程序是SCT当且仅当其SPS转换是CT。通过与EasyCrypt、BINSEC和ctgrind三种CT验证工具结合，在Spectre-v1基准测试中验证了方法的有效性。

Conclusion: SPS方法为SCT验证提供了形式化基础，能够利用现有CT验证工具证明SCT属性，并可扩展到其他Spectre变体和泄漏模型。

Abstract: Constant-time (CT) verification tools are commonly used for detecting
potential side-channel vulnerabilities in cryptographic libraries. Recently, a
new class of tools, called speculative constant-time (SCT) tools, has also been
used for detecting potential Spectre vulnerabilities. In many cases, these SCT
tools have emerged as liftings of CT tools. However, these liftings are seldom
defined precisely and are almost never analyzed formally. The goal of this
paper is to address this gap, by developing formal foundations for these
liftings, and to demonstrate that these foundations can yield practical
benefits.
  Concretely, we introduce a program transformation, coined Speculation-Passing
Style (SPS), for reducing SCT verification to CT verification. Essentially, the
transformation instruments the program with a new input that corresponds to
attacker-controlled predictions and modifies the program to follow them. This
approach is sound and complete, in the sense that a program is SCT if and only
if its SPS transform is CT. Thus, we can leverage existing CT verification
tools to prove SCT; we illustrate this by combining SPS with three standard
methodologies for CT verification, namely reducing it to non-interference,
assertion safety and dynamic taint analysis. We realize these combinations with
three existing tools, EasyCrypt, BINSEC, and ctgrind, and we evaluate them on
Kocher's benchmarks for Spectre-v1. Our results focus on Spectre-v1 in the
standard CT leakage model; however, we also discuss applications of our method
to other variants of Spectre and other leakage models.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [13] [THEAS: Efficient Power Management in Multi-Core CPUs via Cache-Aware Resource Scheduling](https://arxiv.org/abs/2510.09847)
*Said Muhammad,Lahlou Laaziz,Nadjia Kara,Phat Tan Nguyen,Timothy Murphy*

Main category: cs.DC

TL;DR: 提出THEAS算法用于动态调整资源级别，在异构系统中平衡性能与功耗，特别适用于工作负载波动大的场景。


<details>
  <summary>Details</summary>
Motivation: 在异构系统中，工作负载特征分布不均，需要动态资源调整来提高能效同时保持必要的计算资源。

Method: 部署THEAS算法，通过动态适应资源级别来平衡性能和功耗，并与CFS、EAS、HeteroSched和Utility-Based Scheduling等调度技术进行比较分析。

Result: THEAS算法在适应性、核心选择标准、性能扩展、缓存感知、开销和实时适用性等方面与其他调度方案进行了比较。

Conclusion: THEAS算法能够在异构系统中有效平衡性能和功耗，适用于各种实时应用场景。

Abstract: The dynamic adaptation of resource levels enables the system to enhance
energy efficiency while maintaining the necessary computational resources,
particularly in scenarios where workloads fluctuate significantly over time.
The proposed approach can play a crucial role in heterogeneous systems where
workload characteristics are not uniformly distributed, such as non-pinning
tasks. The deployed THEAS algorithm in this research work ensures a balance
between performance and power consumption, making it suitable for a wide range
of real-time applications. A comparative analysis of the proposed THEAS
algorithm with well-known scheduling techniques such as Completely Fair
Scheduler (CFS), Energy-Aware Scheduling (EAS), Heterogeneous Scheduling
(HeteroSched), and Utility-Based Scheduling is presented in Table III. Each
scheme is compared based on adaptability, core selection criteria, performance
scaling, cache awareness, overhead, and real-time suitability.

</details>


### [14] [QONNECT: A QoS-Aware Orchestration System for Distributed Kubernetes Clusters](https://arxiv.org/abs/2510.09851)
*Haci Ismail Aslan,Syed Muhammad Mahmudul Haque,Joel Witzke,Odej Kao*

Main category: cs.DC

TL;DR: QONNECT是一个供应商无关的编排框架，支持在异构Kubernetes和K3s集群上进行声明式、QoS驱动的应用部署，解决了标准Kubernetes调度器无法满足用户定义目标的问题。


<details>
  <summary>Details</summary>
Motivation: 现代应用越来越多地跨越云、雾和边缘环境，需要能够适应不同部署环境并满足QoS要求的编排系统。标准Kubernetes调度器无法考虑用户定义的目标，如能效、成本优化和全局性能，导致操作员需要手动进行集群级部署决策。

Method: QONNECT采用分布式架构，包括中央知识库、Raft复制的资源领导代理和每个集群中的轻量级资源代理。用户通过基于YAML的接口指定高级QoS目标，系统将其转换为具体的放置和迁移操作。

Result: 在包含多达九个云-雾-边缘集群的联合测试平台上使用Istio Bookinfo微服务应用进行评估，系统展示了动态、策略驱动的微服务放置、自动故障转移、QoS合规的重调度以及节点故障后的领导者重新选举，所有这些都无需手动干预。

Conclusion: 通过弥合声明式部署模型和操作QoS目标之间的差距，QONNECT将云-边缘连续体转变为一个统一的、自优化的平台。

Abstract: Modern applications increasingly span across cloud, fog, and edge
environments, demanding orchestration systems that can adapt to diverse
deployment contexts while meeting Quality-of-Service (QoS) requirements.
Standard Kubernetes schedulers do not account for user-defined objectives such
as energy efficiency, cost optimization, and global performance, often leaving
operators to make manual, cluster-by-cluster placement decisions. To address
this need, we present QONNECT, a vendor-agnostic orchestration framework that
enables declarative, QoS-driven application deployment across heterogeneous
Kubernetes and K3s clusters. QONNECT introduces a distributed architecture
composed of a central Knowledge Base, Raft-replicated Resource Lead Agents, and
lightweight Resource Agents in each cluster. Through a minimal YAML-based
interface, users specify high-level QoS goals, which the system translates into
concrete placement and migration actions. Our implementation is evaluated on a
federated testbed of up to nine cloud-fog-edge clusters using the Istio
Bookinfo microservice application. The system demonstrates dynamic,
policy-driven microservice placement, automated failover, QoS-compliant
rescheduling, and leader re-election after node failure, all without manual
intervention. By bridging the gap between declarative deployment models and
operational QoS goals, QONNECT transforms the cloud-edge continuum into a
unified, self-optimizing platform.

</details>


### [15] [FedMon: Federated eBPF Monitoring for Distributed Anomaly Detection in Multi-Cluster Cloud Environments](https://arxiv.org/abs/2510.10126)
*Sehar Zehra,Hassan Jamil Syed,Ummay Faseeha*

Main category: cs.DC

TL;DR: FedMon是一个基于联邦学习和eBPF的多集群异常检测框架，通过轻量级eBPF代理收集系统调用和网络事件，仅共享模型更新，在三个Kubernetes集群中实现了94%精确率、91%召回率和0.92 F1分数，同时比集中式方法减少60%带宽使用。


<details>
  <summary>Details</summary>
Motivation: 解决Kubernetes多集群部署中可扩展且保护隐私的异常检测需求。现有eBPF监控器仅限于单集群，而集中式方法面临带宽、隐私和异构性挑战。

Method: 结合联邦学习(FL)和eBPF技术，轻量级eBPF代理捕获系统调用和网络事件，提取本地统计和序列特征，仅与全局服务器共享模型更新。使用变分自编码器(VAE)和孤立森林的混合检测引擎进行时间模式建模和异常检测。

Result: 在三个Kubernetes集群部署中，达到94%精确率、91%召回率和0.92 F1分数，相比集中式基线减少60%带宽使用。

Conclusion: FedMon提高了准确性、可扩展性和隐私保护，为大规模多租户云原生环境提供了有效防御方案。

Abstract: Kubernetes multi-cluster deployments demand scalable and privacy-preserving
anomaly detection. Existing eBPF-based monitors provide low-overhead system and
network visibility but are limited to single clusters, while centralized
approaches incur bandwidth, privacy, and heterogeneity challenges. We propose
FedMon, a federated eBPF framework that unifies kernel-level telemetry with
federated learning (FL) for cross-cluster anomaly detection. Lightweight eBPF
agents capture syscalls and network events, extract local statistical and
sequence features, and share only model updates with a global server. A hybrid
detection engine combining Variational Autoencoders (VAEs) with Isolation
Forests enables both temporal pattern modeling and outlier detection. Deployed
across three Kubernetes clusters, FedMon achieves 94% precision, 91% recall,
and an F1-score of 0.92, while cutting bandwidth usage by 60% relative to
centralized baselines. Results demonstrate that FedMon enhances accuracy,
scalability, and privacy, providing an effective defense for large-scale,
multi-tenant cloud-native environments.

</details>


### [16] [Proactive and Reactive Autoscaling Techniques for Edge Computing](https://arxiv.org/abs/2510.10166)
*Suhrid Gupta,Muhammed Tawfiqul Islam,Rajkumar Buyya*

Main category: cs.DC

TL;DR: 本章概述了边缘计算架构及其在满足SLA要求方面的挑战，介绍了现有的自动扩展算法及其优缺点。


<details>
  <summary>Details</summary>
Motivation: 边缘计算通过微服务架构实现计算资源去中心化，需要低延迟来满足严格的SLA要求，但现有的自动扩展算法存在性能问题和配置复杂性。

Method: 提供边缘计算架构的简要概述，介绍SLA概念，并综述边缘计算环境中用于满足SLA的算法研究。

Result: 分析了边缘计算在资源扩展方面的使用、优势和挑战，以及现有算法在满足SLA方面的表现。

Conclusion: 混合云和边缘环境对于确保SLA合规性至关重要，但需要改进现有的自动扩展算法以解决性能和配置复杂性问题。

Abstract: Edge computing allows for the decentralization of computing resources. This
decentralization is achieved through implementing microservice architectures,
which require low latencies to meet stringent service level agreements (SLA)
such as performance, reliability, and availability metrics. While cloud
computing offers the large data storage and computation resources necessary to
handle peak demands, a hybrid cloud and edge environment is required to ensure
SLA compliance. Several auto-scaling algorithms have been proposed to try to
achieve these compliance challenges, but they suffer from performance issues
and configuration complexity. This chapter provides a brief overview of edge
computing architecture, its uses, benefits, and challenges for resource
scaling. We then introduce Service Level Agreements, and existing research on
devising algorithms used in edge computing environments to meet these
agreements, along with their benefits and drawbacks.

</details>


### [17] [SP-MoE: Speculative Decoding and Prefetching for Accelerating MoE-based Model Inference](https://arxiv.org/abs/2510.10302)
*Liangkun Chen,Zijian Wen,Tian Wu,Xiaoxi Zhang,Chuan Wu*

Main category: cs.DC

TL;DR: SP-MoE是一个结合推测解码和专家混合模型的推理加速框架，通过推测性专家预取、截止层策略和流水线运行时来优化GPU内存和带宽使用。


<details>
  <summary>Details</summary>
Motivation: MoE架构与推测解码结合会加剧GPU内存压力和CPU-GPU带宽竞争，现有MoE卸载系统未能解决这一瓶颈。

Method: 提出推测性专家预取、基于经验配置和分析延迟模型的截止层策略，以及异步预取线程和批处理I/O的流水线运行时。

Result: 在多样化数据集、环境和MoE模型上，SP-MoE相比最先进方法实现了1.07-3.5倍的TPOT加速。

Conclusion: SP-MoE是首个SD感知的专家卸载和计算通信流水线框架，有效解决了MoE与推测解码结合时的性能瓶颈。

Abstract: The Mixture-of-Experts (MoE) architecture has been widely adopted in large
language models (LLMs) to reduce computation cost through model sparsity.
Employing speculative decoding (SD) can further accelerate MoE inference by
drafting multiple tokens per step and verifying them in parallel. However,
combining MoE with SD inflates GPU memory and aggravates CPU-GPU bandwidth
contention during multi-token verification. Existing MoE offloading systems are
SD-agnostic and do not address this bottleneck. We present SP-MoE, the first
SD-aware expert-offloading and compute-communication pipelining framework.
SP-MoE introduces: (1) speculative expert prefetching that exploits structural
correspondence between the draft and target models to prefetch likely experts
ahead of verification; (2) a cutoff-layer policy that bounds per-layer prefetch
depth based on empirical profiles and an analytical latency model, guaranteeing
just-in-time availability without overfetch; and (3) a pipelined runtime with
asynchronous prefetch threads and batched I/O to hide loading latency.
Extensive experiments demonstrate that SP-MoE achieves a 1.07-3.5 times TPOT
speedup over state-of-the-art methods across diverse datasets, environments,
and MoE-based models.

</details>


### [18] [FLAMMABLE: A Multi-Model Federated Learning Framework with Multi-Model Engagement and Adaptive Batch Sizes](https://arxiv.org/abs/2510.10380)
*Shouxu Lin,Zimeng Pan,Yuhang Yao,Haeyoung Noh,Pei Zhang,Carlee Joe-Wong*

Main category: cs.DC

TL;DR: FLAMMABLE是一个多模型联邦学习框架，通过智能调整客户端批量大小和选择合适模型来优化训练效率，相比基线方法在时间效率和模型精度上均有显著提升。


<details>
  <summary>Details</summary>
Motivation: 多模型联邦学习面临数据异构、系统异构以及模型间异构的挑战，现有单模型联邦学习方法无法有效应对这些复杂问题，需要专门的多模型训练框架。

Method: 提出FLAMMABLE框架，智能调整客户端批量大小，根据客户端系统能力选择合适模型进行训练，并开发了首个多模型联邦学习基准平台。

Result: 在多个数据集和模型上的评估显示，FLAMMABLE将时间效率提升了1.1-10.0倍，最终模型精度提高了1.3-5.4%。

Conclusion: FLAMMABLE有效解决了多模型联邦学习中的异构性挑战，显著提升了训练效率和模型性能，为未来可复现的多模型联邦学习研究奠定了基础。

Abstract: Multi-Model Federated Learning (MMFL) is an emerging direction in Federated
Learning (FL) where multiple models are trained in parallel, generally on
various datasets. Optimizing the models' accuracies and training times in the
MMFL setting requires adapting to data and system heterogeneity across clients
as in single-model FL; these challenges are amplified in the MMFL setting due
to additional heterogeneity across models. Neither existing solutions nor
na\"ive extensions of single-model FL frameworks efficiently address these
challenges. To bridge this gap, we propose FLAMMABLE, a comprehensive MMFL
training framework. FLAMMABLE optimizes model training by intelligently
adapting client batch sizes while engaging them to train multiple carefully
chosen models, depending on their system capabilities, in each training round.
To evaluate FLAMMABLE, we develop the first benchmark platform for the MMFL
setting, which may enable future reproducible MMFL research. Extensive
evaluations on multiple datasets and models show that FLAMMABLE boosts the MMFL
time-to-accuracy performance by 1.1$\sim$10.0$\times$ while improving the final
model accuracy by 1.3$\sim$5.4\% compared to several known baselines.

</details>


### [19] [DCP: Addressing Input Dynamism In Long-Context Training via Dynamic Context Parallelism](https://arxiv.org/abs/2510.10620)
*Chenyu Jiang,Zhenkun Cai,Ye Tian,Zhen Jia,Yida Wang,Chuan Wu*

Main category: cs.DC

TL;DR: DCP是一个动态上下文并行训练框架，通过细粒度数据块划分和计算映射来适应不同序列特征，减少通信开销并改善计算平衡。


<details>
  <summary>Details</summary>
Motivation: 现有上下文并行方法使用静态并行配置，无法适应训练数据中序列长度和注意力模式的动态变化，导致不必要的通信开销和计算不平衡。

Method: 引入细粒度的数据块划分和计算分区，实现数据和计算块到设备的灵活映射，动态适应不同序列特征。

Result: 在因果掩码下注意力计算加速1.19x~2.45x，稀疏注意力模式下加速2.15x~3.77x；端到端训练在因果掩码下加速0.94x~1.16x，稀疏掩码下加速1.00x~1.46x。

Conclusion: DCP通过动态上下文并行有效解决了现有方法的通信和计算平衡问题，显著提升了长上下文训练效率。

Abstract: Context parallelism has emerged as a key technique to support long-context
training, a growing trend in generative AI for modern large models. However,
existing context parallel methods rely on static parallelization configurations
that overlook the dynamic nature of training data, specifically, the
variability in sequence lengths and token relationships (i.e., attention
patterns) across samples. As a result, these methods often suffer from
unnecessary communication overhead and imbalanced computation. In this paper,
we present DCP, a dynamic context parallel training framework that introduces
fine-grained blockwise partitioning of both data and computation. By enabling
flexible mapping of data and computation blocks to devices, DCP can adapt to
varying sequence characteristics, effectively reducing communication and
improving memory and computation balance. Micro-benchmarks demonstrate that DCP
accelerates attention by 1.19x~2.45x under causal masks and 2.15x~3.77x under
sparse attention patterns. Additionally, we observe up to 0.94x~1.16x
end-to-end training speed-up for causal masks, and 1.00x~1.46x for sparse
masks.

</details>


### [20] [CPU-Limits kill Performance: Time to rethink Resource Control](https://arxiv.org/abs/2510.10747)
*Chirag Shetty,Sarthak Chakraborty,Hubertus Franke,Larisa Shwartz,Chandra Narayanaswami,Indranil Gupta,Saurabh Jha*

Main category: cs.DC

TL;DR: 本文质疑CPU限制的必要性，认为对于延迟敏感应用应完全避免使用CPU限制，这需要重新思考自动扩展和计费模式。


<details>
  <summary>Details</summary>
Motivation: 传统观点认为CPU限制对云原生应用至关重要，但实际经验表明CPU限制会损害应用性能并增加成本，这与学术研究和行业最佳实践相矛盾。

Method: 通过实证证据分析CPU限制对延迟敏感应用的影响，并探讨特定场景下CPU限制的合理使用方式。

Result: 研究发现CPU限制对延迟敏感应用弊大于利，应该完全避免使用，但背景作业等特定场景仍可受益。

Conclusion: 需要从根本上重新思考自动扩展和计费范式，为CPU限制的使用提供更细致的指导原则。

Abstract: Research in compute resource management for cloud-native applications is
dominated by the problem of setting optimal CPU limits -- a fundamental OS
mechanism that strictly restricts a container's CPU usage to its specified
CPU-limits . Rightsizing and autoscaling works have innovated on
allocation/scaling policies assuming the ubiquity and necessity of CPU-limits .
We question this. Practical experiences of cloud users indicate that CPU-limits
harms application performance and costs more than it helps. These observations
are in contradiction to the conventional wisdom presented in both academic
research and industry best practices. We argue that this indiscriminate
adoption of CPU-limits is driven by erroneous beliefs that CPU-limits is
essential for operational and safety purposes. We provide empirical evidence
making a case for eschewing CPU-limits completely from latency-sensitive
applications. This prompts a fundamental rethinking of auto-scaling and billing
paradigms and opens new research avenues. Finally, we highlight specific
scenarios where CPU-limits can be beneficial if used in a well-reasoned way
(e.g. background jobs).

</details>


### [21] [Fair Kernel-Lock-Free Claim/Release Protocol for Shared Object Access in Cooperatively Scheduled Runtimes](https://arxiv.org/abs/2510.10818)
*Kevin Chalmers,Jan Bækgaard Pedersen*

Main category: cs.DC

TL;DR: 提出首个无自旋、无内核锁的互斥锁，与用户模式调度器协作，使用CSP/FDR形式化验证为FIFO公平且线性化。


<details>
  <summary>Details</summary>
Motivation: 为ProcessJ语言设计声明/释放协议，管理共享进程间通信通道的竞争，确保对共享资源的控制和公平访问。

Method: 使用无锁队列来停放等待获取共享对象访问权限的进程，协议按照队列插入顺序处理声明请求。

Result: 构建了协议的CSP模型和互斥锁规范，通过FDR验证协议行为符合锁定互斥锁的要求。

Conclusion: 开发了可重用的公平性预言机和基于稳定性的证明方法，适用于各种协程运行时设计。

Abstract: We present the first spin-free, kernel-lock-free mutex that cooperates with
user-mode schedulers and is formally proven FIFO-fair and linearizable using
CSP/FDR. Our fairness oracle and stability-based proof method are reusable
across coroutine runtime designs. We designed the claim/release protocol for a
process-oriented language -- ProcessJ -- to manage the race for claiming shared
inter-process communication channels. Internally, we use a lock-free queue to
park waiting processes for gaining access to a shared object, such as exclusive
access to a shared channel to read from or write to. The queue ensures control
and fairness for processes wishing to access a shared resource, as the protocol
handles claim requests in the order they are inserted into the queue. We
produce CSP models of our protocol and a mutex specification, demonstrating
with FDR that our protocol behaves as a locking mutex.

</details>


### [22] [FIDRS: A Novel Framework for Integrated Distributed Reliable Systems](https://arxiv.org/abs/2510.10833)
*Mehdi Zekriyapanah Gashti*

Main category: cs.DC

TL;DR: 提出FIDRS框架，通过异构分布式数据库和RMSD算法提高集成系统的性能、效率和可靠性


<details>
  <summary>Details</summary>
Motivation: 改进现有集成分布式系统的性能和可靠性问题，解决响应时间慢和可靠性不足的缺陷

Method: 使用异构分布式数据库技术提高响应速度，采用RMSD算法减少大数据环境下的响应时间

Result: 新框架成功提高了系统效率、性能和可靠性，解决了之前框架的一些问题

Conclusion: FIDRS框架有效提升了集成分布式系统的整体表现，在性能和可靠性方面都有显著改进

Abstract: In this paper we represent a new framework for integrated distributed and
reliable systems. In the proposed framework we have used three parts to
increase Satisfaction and Performance of this framework. At first we analyze
previous frameworks related to integrated systems, then represent new proposed
framework in order to improving previous framework, and we discuss its
different phases. Finally we compare the results of simulation of the new
framework with previous ones. In FIDRS framework, the technique of
heterogeneous distributed data base is used to improve Performance and speed in
responding to users and in this way we can improve dependability and
reliability of framework simultaneously. In extraction phase of the new
framework we have used RMSD algorithm that decreases responding time in big
database. Finally by using FDIRS framework we succeeded to increase Efficiency,
Performance and reliability of integrated systems and remove some of previous
frameworks problems.

</details>


### [23] [A Decentralized Microservice Scheduling Approach Using Service Mesh in Cloud-Edge Systems](https://arxiv.org/abs/2510.11189)
*Yangyang Wen,Paul Townend,Per-Olov Östberg,Abel Souza,Clément Courageux-Sudan*

Main category: cs.DC

TL;DR: 提出利用服务网格边车代理作为去中心化调度器的新架构方向，以解决云边环境中传统集中式调度的延迟和扩展性问题。


<details>
  <summary>Details</summary>
Motivation: 随着微服务系统在云边环境中扩展，传统集中式调度机制面临延迟、协调开销和容错性等挑战，需要新的解决方案。

Method: 将轻量级、自主的调度逻辑嵌入到每个服务网格边车代理中，实现本地化调度决策，无需集中控制。

Result: 初步结果显示该架构在响应时间和延迟方面具有良好的扩展潜力，特别是在不同请求速率下。

Conclusion: 本文提出了一个系统级架构方向，并提供了初步证据支持其在大型云原生环境中的可扩展性潜力。

Abstract: As microservice-based systems scale across the cloud-edge continuum,
traditional centralized scheduling mechanisms increasingly struggle with
latency, coordination overhead, and fault tolerance. This paper presents a new
architectural direction: leveraging service mesh sidecar proxies as
decentralized, in-situ schedulers to enable scalable, low-latency coordination
in large-scale, cloud-native environments. We propose embedding lightweight,
autonomous scheduling logic into each sidecar, allowing scheduling decisions to
be made locally without centralized control. This approach leverages the
growing maturity of service mesh infrastructures, which support programmable
distributed traffic management. We describe the design of such an architecture
and present initial results demonstrating its scalability potential in terms of
response time and latency under varying request rates. Rather than delivering a
finalized scheduling algorithm, this paper presents a system-level
architectural direction and preliminary evidence to support its scalability
potential.

</details>


### [24] [An Explorative Study on Distributed Computing Techniques in Training and Inference of Large Language Models](https://arxiv.org/abs/2510.11211)
*Sheikh Azizul Hakim,Saem Hasan*

Main category: cs.DC

TL;DR: 该论文从两个角度探讨了分布式计算技术在大型语言模型(LLM)中的应用：一是研究如何在消费级计算机上运行大型模型，二是对三种最先进的LLM服务技术进行对比研究。


<details>
  <summary>Details</summary>
Motivation: 当前拥有数十亿参数的LLM过于庞大，单个计算节点难以进行训练、微调或推理，因此需要引入分布式计算技术来有效利用这些模型。

Method: 1. 研究使LLM民主化的技术，即在消费级计算机上运行大型模型的方法，并对现有系统进行了基于元启发式的新颖改进。2. 对三种最先进的LLM服务技术进行对比研究。

Result: 论文探索了分布式计算在LLM中的应用，包括在资源受限环境下的部署方案和不同服务技术的性能比较。

Conclusion: 分布式计算技术对于解决LLM规模过大、难以在单个节点上运行的问题至关重要，需要从模型部署和服务效率两个维度进行优化。

Abstract: Large language models (LLM) are advanced AI systems trained on extensive
textual data, leveraging deep learning techniques to understand and generate
human-like language. Today's LLMs with billions of parameters are so huge that
hardly any single computing node can train, fine-tune, or infer from them.
Therefore, several distributed computing techniques are being introduced in the
literature to properly utilize LLMs. We have explored the application of
distributed computing techniques in LLMs from two angles.
  \begin{itemize}
  \item We study the techniques that democratize the LLM, that is, how large
models can be run on consumer-grade computers. Here, we also implement a novel
metaheuristics-based modification to an existing system.
  \item We perform a comparative study on three state-of-the-art LLM serving
techniques. \end{itemize}

</details>


### [25] [An Asynchronous Many-Task Algorithm for Unstructured $S_{N}$ Transport on Shared Memory Systems](https://arxiv.org/abs/2510.11513)
*Alex Elwood,Tom Deakin,Justin Lovegrove,Chris Nelson*

Main category: cs.DC

TL;DR: 本文分析了非结构化网格上离散纵标法传输求解器的性能瓶颈，提出了一种新的异步多任务算法来提高计算性能。


<details>
  <summary>Details</summary>
Motivation: 非结构化网格上的离散纵标法传输求解器由于复杂的数据依赖关系、内存访问模式和高维域特性，在多核架构上难以有效扩展。

Method: 首先分析现有传输求解器在共享内存并行化方案中的性能瓶颈，然后提出一种新的异步多任务（AMT）算法用于共享内存并行。

Result: 新算法相比现有方法在计算性能上有所提升，并评估了性能改进的原因。

Conclusion: 异步多任务算法能够有效解决非结构化网格传输求解器在多核架构上的扩展性问题，提高计算性能。

Abstract: Discrete ordinates $S_N$ transport solvers on unstructured meshes pose a
challenge to scale due to complex data dependencies, memory access patterns and
a high-dimensional domain. In this paper, we review the performance bottlenecks
within the shared memory parallelization scheme of an existing transport solver
on modern many-core architectures with high core counts. With this analysis, we
then survey the performance of this solver across a variety of compute
hardware. We then present a new Asynchronous Many-Task (AMT) algorithm for
shared memory parallelism, present results showing an increase in computational
performance over the existing method, and evaluate why performance is improved.

</details>


### [26] [A Fast-Converging Decentralized Approach to the Weighted Minimum Vertex Cover Problem](https://arxiv.org/abs/2510.11697)
*Matteo Mordacchini,Emanuele Carlini,Patrizio Dazzi*

Main category: cs.DC

TL;DR: 提出了一种完全去中心化的最小加权顶点覆盖计算协议，仅使用本地知识和邻居通信，在真实和合成图上验证了其竞争性的解决方案质量和低通信开销。


<details>
  <summary>Details</summary>
Motivation: 最小加权顶点覆盖是NP难问题，在网络监控和资源放置等应用中具有基础性作用，需要去中心化解决方案以避免集中协调。

Method: 设计完全去中心化协议，每个节点仅基于本地知识和邻居通信做出决策，具有自适应性和通信效率。

Result: 在真实世界和合成图上的评估显示，与集中式和去中心化基线相比，该协议在解决方案质量上具有竞争力，同时显著降低了通信开销。

Conclusion: 证明了在去中心化环境中计算最小加权顶点覆盖的可行性，为实际应用提供了高效解决方案。

Abstract: We address the problem of computing a Minimum Weighted Vertex Cover (MWVC) in
a decentralized network. MWVC, a classical NP-hard problem, is foundational in
applications such as network monitoring and resource placement. We propose a
fully decentralized protocol where each node makes decisions using only local
knowledge and communicates with its neighbors. The method is adaptive,
communication-efficient, and avoids centralized coordination. We evaluate the
protocol on real-world and synthetic graphs, comparing it to both centralized
and decentralized baselines. Our results demonstrate competitive solution
quality with reduced communication overhead, highlighting the feasibility of
MWVC computation in decentralized environments.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [27] [ISAAC: Intelligent, Scalable, Agile, and Accelerated CPU Verification via LLM-aided FPGA Parallelism](https://arxiv.org/abs/2510.10225)
*Jialin Sun,Yuchen Hu,Dean You,Yushu Du,Hui Wang,Xinwei Fang,Weiwei Shan,Nan Guan,Zhe Jiang*

Main category: cs.AR

TL;DR: ISAAC是一个基于大语言模型的CPU验证框架，通过多代理激励生成和FPGA并行仿真，显著提高了验证效率并发现了新的bug。


<details>
  <summary>Details</summary>
Motivation: 传统CPU验证方法存在瓶颈：前端激励生成缺乏微架构意识导致测试质量低，后端仿真基础设施即使使用FPGA加速也面临长运行测试和有限可见性问题。

Method: 提出ISAAC全栈框架：前端使用注入微架构知识和历史bug模式的多代理激励引擎；后端采用轻量级前向快照机制和解耦协同仿真架构，实现单个ISS驱动多个DUT并行运行。

Result: 在成熟CPU上验证，相比软件RTL仿真获得高达17,536倍的加速，并检测到多个先前未知的bug。

Conclusion: ISAAC通过消除长尾测试瓶颈和利用FPGA并行性，显著提升了仿真吞吐量，为CPU验证提供了高效解决方案。

Abstract: Functional verification is a critical bottleneck in integrated circuit
development, with CPU verification being especially time-intensive and
labour-consuming. Industrial practice relies on differential testing for CPU
verification, yet faces bottlenecks at nearly each stage of the framework
pipeline: front-end stimulus generation lacks micro-architectural awareness,
yielding low-quality and redundant tests that impede coverage closure and miss
corner cases. Meanwhile, back-end simulation infrastructure, even with FPGA
acceleration, often stalls on long-running tests and offers limited visibility,
delaying feedback and prolonging the debugging cycle. Here, we present ISAAC, a
full-stack, Large Language Model (LLM)-aided CPU verification framework with
FPGA parallelism, from bug categorisation and stimulus generation to simulation
infrastructure. To do so, we presented a multi-agent stimulus engine in ISAAC's
front-end, infused with micro-architectural knowledge and historical bug
patterns, generating highly targeted tests that rapidly achieve coverage goals
and capture elusive corner cases. In ISAAC's back-end, we introduce a
lightweight forward-snapshot mechanism and a decoupled co-simulation
architecture between the Instruction Set Simulator (ISS) and the Design Under
Test (DUT), enabling a single ISS to drive multiple DUTs in parallel. By
eliminating long-tail test bottlenecks and exploiting FPGA parallelism, the
simulation throughput is significantly improved. As a demonstration, we used
ISAAC to verify a mature CPU that has undergone multiple successful tape-outs.
Results show up to 17,536x speed-up over software RTL simulation, while
detecting several previously unknown bugs, two of which are reported in this
paper.

</details>


### [28] [ADiP: Adaptive Precision Systolic Array for Matrix Multiplication Acceleration](https://arxiv.org/abs/2510.10623)
*Ahmed J. Abdelmaksoud,Cristian Sestito,Shiwei Wang,Themis Prodromakis*

Main category: cs.AR

TL;DR: ADiP是一种新型自适应精度脉动阵列架构，专门为高效矩阵乘法加速而设计，支持多种计算模式和精度配置，在Transformer工作负载上实现了显著的延迟和能效改进。


<details>
  <summary>Details</summary>
Motivation: Transformer模型在现代AI中占据核心地位，但其矩阵乘法运算具有巨大的内存和计算需求。量化技术可以减少内存使用，但需要可重构架构来动态调整精度以实现高效计算。

Method: 提出ADiP架构，包含NxN自适应精度处理单元和共享累加器，支持对称单矩阵乘法和非对称多矩阵乘法，能够适应8bit×8bit、8bit×4bit、8bit×2bit等不同精度配置。

Result: 在22nm商用技术下，ADiP实现了高达4倍的计算吞吐量提升。在GPT-2 Medium、BERT Large和BitNet-1.58B模型上，延迟改进最高达53.6%，BitNet-1.58B MHA工作负载的能效改进最高达24.4%。64×64规模下峰值吞吐量分别为8.192 TOPS、16.384 TOPS和32.768 TOPS。

Conclusion: ADiP架构通过自适应精度和多种计算模式，有效提升了矩阵乘法的计算效率和能效，特别适合现代Transformer模型的加速需求。

Abstract: Transformers are at the core of modern AI nowadays. They rely heavily on
matrix multiplication and require efficient acceleration due to their
substantial memory and computational requirements. Quantization plays a vital
role in reducing memory usage, and can be exploited for computations by
designing reconfigurable architectures that enhance matrix multiplication by
dynamically adjusting the precision. This paper proposes ADiP, a novel
adaptive-precision systolic array architecture designed for efficient matrix
multiplication acceleration.The proposed architecture consists of NxN
adaptive-precision processing elements (PEs) and shared accumulators. ADiP
supports multiple computation modes, including symmetric single-matrix
multiplication as well as asymmetric multi-matrix multiplication with a shared
input matrix, thereby improving data-reuse and PE utilization. In addition,
ADiP maximizes the computational density by adapting to different precisions,
such as 8bitx8bit, 8bitx4bit, and 8bitx2bit. Analytical models are developed
for ADiP architecture, including latency and throughput for versatile
architecture configurations. A comprehensive hardware design space exploration
is demonstrated using 22nm commercial technology, achieving up to a 4x higher
computational throughput. Furthermore, ADiP is evaluated on different
transformer workloads from GPT-2 Medium, BERT Large, and BitNet-1.58B models,
delivering latency improvement up to 53.6%, and energy improvement up to 24.4%
for BitNet-1.58B MHA workloads. At a 64x64 size with 4096 PEs, ADiP achieves a
peak throughput of 8.192 TOPS, 16.384 TOPS, and 32.768 TOPS for 8bitx8bit,
8bitx4bit, and 8bitx2bit operations, respectively.

</details>


### [29] [Bhasha-Rupantarika: Algorithm-Hardware Co-design approach for Multilingual Neural Machine Translation](https://arxiv.org/abs/2510.10676)
*Mukul Lokhande,Tanushree Dewangan,Mohd Sharik Mansoori,Tejas Chaudhari,Akarsh J.,Damayanti Lokhande,Adam Teman,Santosh Kumar Vishvakarma*

Main category: cs.AR

TL;DR: Bhasha-Rupantarika是一个通过算法-硬件协同设计的高效多语言翻译系统，专门针对资源受限环境优化。该系统采用亚字节精度量化（FP8/INT8/INT4/FP4），在FPGA上部署实现了4.1倍模型压缩和4.2倍推理加速。


<details>
  <summary>Details</summary>
Motivation: 针对资源受限的物联网设备，需要开发轻量级、高效的多语言翻译系统，实现实时部署和低功耗运行。

Method: 采用算法-硬件协同设计方法，研究亚字节精度量化（FP8、INT8、INT4、FP4），在FPGA加速器上部署多语言翻译模型。

Result: FP4量化实现4.1倍模型压缩和4.2倍推理加速，吞吐量提升至66 tokens/s（4.8倍提升）。FPGA部署减少1.96倍LUTs和1.65倍FFs，相比OPU和HPTA分别提升2.2倍和4.6倍吞吐量。

Conclusion: 该研究证明了超低精度量化在资源受限设备上的可行性，为可部署的多语言AI系统提供了基于量化感知翻译和硬件效率的可行解决方案。

Abstract: This paper introduces Bhasha-Rupantarika, a light and efficient multilingual
translation system tailored through algorithm-hardware codesign for
resource-limited settings. The method investigates model deployment at
sub-octet precision levels (FP8, INT8, INT4, and FP4), with experimental
results indicating a 4.1x reduction in model size (FP4) and a 4.2x speedup in
inference speed, which correlates with an increased throughput of 66 tokens/s
(improvement by 4.8x). This underscores the importance of ultra-low precision
quantization for real-time deployment in IoT devices using FPGA accelerators,
achieving performance on par with expectations. Our evaluation covers
bidirectional translation between Indian and international languages,
showcasing its adaptability in low-resource linguistic contexts. The FPGA
deployment demonstrated a 1.96x reduction in LUTs and a 1.65x decrease in FFs,
resulting in a 2.2x enhancement in throughput compared to OPU and a 4.6x
enhancement compared to HPTA. Overall, the evaluation provides a viable
solution based on quantisation-aware translation along with hardware efficiency
suitable for deployable multilingual AI systems. The entire codes
[https://github.com/mukullokhande99/Bhasha-Rupantarika/] and dataset for
reproducibility are publicly available, facilitating rapid integration and
further development by researchers.

</details>


### [30] [FeNOMS: Enhancing Open Modification Spectral Library Search with In-Storage Processing on Ferroelectric NAND (FeNAND) Flash](https://arxiv.org/abs/2510.10872)
*Sumukh Pinge,Ashkan Moradifirouzabadi,Keming Fan,Prasanna Venkatesan Ravindran,Tanvir H. Pantha,Po-Kai Hsu,Zheyu Li,Weihong Xu,Zihan Xia,Flavio Ponzina,Winston Chern,Taeyoung Song,Priyankka Ravikumar,Mengkun Tian,Lance Fernandes,Huy Tran,Hari Jayasankar,Hang Chen,Chinsung Park,Amrit Garlapati,Kijoon Kim,Jongho Woo,Suhwan Lim,Kwangsoo Kim,Wanki Kim,Daewon Ha,Duygu Kuzum,Shimeng Yu,Sourav Dutta,Asif Khan,Tajana Rosing,Mingu Kang*

Main category: cs.AR

TL;DR: 提出了一种基于3D铁电NAND和超维计算的存储内计算架构，用于高效质谱数据搜索，相比现有3D NAND方法实现43倍加速和21倍能效提升。


<details>
  <summary>Details</summary>
Motivation: 质谱数据快速增长至数百TB规模，传统处理器难以高效处理大规模库搜索，存储内计算成为有前景的替代方案。

Method: 结合3D铁电NAND结构和超维计算，采用双边界近似匹配距离度量，在FeNAND结构中并行化向量计算。

Result: 实现了43倍速度提升和21倍能效提升，同时保持可比的准确度。

Conclusion: 该架构成功解决了NAND结构在存储内计算中的吞吐量限制，为大规模质谱数据搜索提供了高效解决方案。

Abstract: The rapid expansion of mass spectrometry (MS) data, now exceeding hundreds of
terabytes, poses significant challenges for efficient, large-scale library
search - a critical component for drug discovery. Traditional processors
struggle to handle this data volume efficiently, making in-storage computing
(ISP) a promising alternative. This work introduces an ISP architecture
leveraging a 3D Ferroelectric NAND (FeNAND) structure, providing significantly
higher density, faster speeds, and lower voltage requirements compared to
traditional NAND flash. Despite its superior density, the NAND structure has
not been widely utilized in ISP applications due to limited throughput
associated with row-by-row reads from serially connected cells. To overcome
these limitations, we integrate hyperdimensional computing (HDC), a
brain-inspired paradigm that enables highly parallel processing with simple
operations and strong error tolerance. By combining HDC with the proposed
dual-bound approximate matching (D-BAM) distance metric, tailored to the FeNAND
structure, we parallelize vector computations to enable efficient MS spectral
library search, achieving 43x speedup and 21x higher energy efficiency over
state-of-the-art 3D NAND methods, while maintaining comparable accuracy.

</details>


### [31] [Efficient In-Memory Acceleration of Sparse Block Diagonal LLMs](https://arxiv.org/abs/2510.11192)
*João Paulo Cardoso de Lima,Marc Dietrich,Jeronimo Castrillon,Asif Ali Khan*

Main category: cs.AR

TL;DR: 提出了一个自动化框架，通过新颖的映射和调度策略来加速稀疏大语言模型在内存计算加速器上的推理，利用块对角稀疏性提高阵列利用率50%以上。


<details>
  <summary>Details</summary>
Motivation: 解决稀疏大语言模型在传统冯·诺依曼架构上推理成本高的问题，以及在内存计算架构上直接映射稀疏矩阵导致的阵列利用率低和计算效率下降的问题。

Method: 开发自动化框架，采用新颖的映射和调度策略，利用块对角稀疏性来优化稀疏矩阵在内存计算阵列上的布局。

Result: 内存计算阵列利用率提高超过50%，内存占用和浮点运算次数均减少4倍以上。

Conclusion: 该框架有效解决了稀疏大语言模型在内存计算加速器上的效率问题，显著提升了推理性能。

Abstract: Structured sparsity enables deploying large language models (LLMs) on
resource-constrained systems. Approaches like dense-to-sparse fine-tuning are
particularly compelling, achieving remarkable structured sparsity by reducing
the model size by over 6.7x, while still maintaining acceptable accuracy.
Despite this reduction, LLM inference, especially the decode stage being
inherently memory-bound, is extremely expensive on conventional Von-Neumann
architectures. Compute-in-memory (CIM) architectures mitigate this by
performing computations directly in memory, and when paired with sparse LLMs,
enable storing and computing the entire model in memory, eliminating the data
movement on the off-chip bus and improving efficiency. Nonetheless, naively
mapping sparse matrices onto CIM arrays leads to poor array utilization and
diminished computational efficiency. In this paper, we present an automated
framework with novel mapping and scheduling strategies to accelerate sparse LLM
inference on CIM accelerators. By exploiting block-diagonal sparsity, our
approach improves CIM array utilization by over 50%, achieving more than 4x
reduction in both memory footprint and the number of required floating-point
operations.

</details>
