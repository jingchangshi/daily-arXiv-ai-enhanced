{"id": "2512.06443", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06443", "abs": "https://arxiv.org/abs/2512.06443", "authors": ["Xiangyu Li", "Chengyu Yin", "Weijun Wang", "Jianyu Wei", "Ting Cao", "Yunxin Liu"], "title": "Vec-LUT: Vector Table Lookup for Parallel Ultra-Low-Bit LLM Inference on Edge Devices", "comment": "Preprint", "summary": "Large language models (LLMs) are increasingly deployed on edge devices. To meet strict resource constraints, real-world deployment has pushed LLM quantization from 8-bit to 4-bit, 2-bit, and now 1.58-bit. Combined with lookup table (LUT)-based inference, CPUs run these ultra-low-bit LLMs even faster than NPUs, opening new opportunities for ubiquitous on-device intelligence.\n  However, this paper identifies that LUT-based inference underutilizes memory bandwidth during parallel inference, which is required for prefilling, test-time scaling, and other multi-token scenarios. The root cause is the scalar LUT paradigm, which performs repetitive and non-contiguous memory accesses for each token.\n  To solve the issue, we propose vector LUT, a new lookup paradigm that constructs a unified LUT across parallel tokens, and performs a single $1 \\rightarrow N$ lookup per index. To realize it efficiently, we further introduce (1) Vector LUT-Centric Tensor Layout, and (2) Cache-Aware Streamed Lookup techniques. Evaluations on 5 edge devices across 3 LLMs show that Vec-LUT outperforms state-of-the-art baselines by up to $4.2\\times$. Our implementation is integrated into llama.cpp. The code is available at https://github.com/Cipherxzc/vlut.cpp.", "AI": {"tldr": "\u63d0\u51fa\u5411\u91cf\u67e5\u627e\u8868(Vec-LUT)\u65b9\u6cd5\uff0c\u89e3\u51b3\u6807\u91cfLUT\u5728\u5e76\u884c\u63a8\u7406\u4e2d\u5185\u5b58\u5e26\u5bbd\u5229\u7528\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u5355\u6b211\u2192N\u67e5\u627e\uff0c\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u6027\u80fd\u63d0\u5347\u6700\u9ad8\u8fbe4.2\u500d\u3002", "motivation": "\u968f\u7740LLM\u5728\u8fb9\u7f18\u8bbe\u5907\u90e8\u7f72\u9700\u6c42\u589e\u52a0\uff0c\u91cf\u5316\u6280\u672f\u5df2\u53d1\u5c55\u52301.58\u4f4d\u8d85\u4f4e\u6bd4\u7279\u3002\u867d\u7136\u57fa\u4e8e\u67e5\u627e\u8868(LUT)\u7684\u63a8\u7406\u4f7fCPU\u8fd0\u884c\u901f\u5ea6\u8d85\u8fc7NPU\uff0c\u4f46\u6807\u91cfLUT\u8303\u5f0f\u5728\u5e76\u884c\u63a8\u7406\uff08\u5982\u9884\u586b\u5145\u3001\u6d4b\u8bd5\u65f6\u7f29\u653e\u7b49\u573a\u666f\uff09\u4e2d\u5b58\u5728\u5185\u5b58\u5e26\u5bbd\u5229\u7528\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u6839\u6e90\u5728\u4e8e\u6bcf\u4e2atoken\u90fd\u9700\u8981\u91cd\u590d\u4e14\u975e\u8fde\u7eed\u7684\u5185\u5b58\u8bbf\u95ee\u3002", "method": "\u63d0\u51fa\u5411\u91cf\u67e5\u627e\u8868(Vec-LUT)\u65b0\u8303\u5f0f\uff1a1) \u6784\u5efa\u8de8\u5e76\u884ctoken\u7684\u7edf\u4e00LUT\uff0c\u5b9e\u73b0\u5355\u6b211\u2192N\u67e5\u627e\uff1b2) \u5411\u91cfLUT\u4e2d\u5fc3\u5f20\u91cf\u5e03\u5c40\uff1b3) \u7f13\u5b58\u611f\u77e5\u6d41\u5f0f\u67e5\u627e\u6280\u672f\u3002\u8fd9\u4e9b\u6280\u672f\u4f18\u5316\u4e86\u5185\u5b58\u8bbf\u95ee\u6a21\u5f0f\uff0c\u63d0\u9ad8\u4e86\u5e76\u884c\u63a8\u7406\u6548\u7387\u3002", "result": "\u57285\u79cd\u8fb9\u7f18\u8bbe\u5907\u548c3\u4e2aLLM\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cVec-LUT\u6bd4\u73b0\u6709\u6700\u4f18\u65b9\u6cd5\u6027\u80fd\u63d0\u5347\u6700\u9ad8\u8fbe4.2\u500d\u3002\u8be5\u65b9\u6cd5\u5df2\u96c6\u6210\u5230llama.cpp\u4e2d\uff0c\u4ee3\u7801\u5f00\u6e90\u3002", "conclusion": "\u5411\u91cf\u67e5\u627e\u8868\u8303\u5f0f\u6709\u6548\u89e3\u51b3\u4e86\u6807\u91cfLUT\u5728\u5e76\u884c\u63a8\u7406\u4e2d\u7684\u5185\u5b58\u5e26\u5bbd\u74f6\u9888\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8d85\u4f4e\u6bd4\u7279LLM\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u63a8\u7406\u6548\u7387\uff0c\u4e3a\u65e0\u5904\u4e0d\u5728\u7684\u8bbe\u5907\u7aef\u667a\u80fd\u5f00\u8f9f\u4e86\u65b0\u673a\u4f1a\u3002"}}
{"id": "2512.06784", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.06784", "abs": "https://arxiv.org/abs/2512.06784", "authors": ["Long Shi", "Bingyan Ou", "Kang Wei", "Weihao Zhu", "Zhe Wang", "Zhiyong Chen"], "title": "Stable-MoE: Lyapunov-based Token Routing for Distributed Mixture-of-Experts Training over Edge Networks", "comment": null, "summary": "The sparse activation mechanism of mixture of experts (MoE) model empowers edge intelligence with enhanced training efficiency and reduced computational resource consumption. However, traditional token routing in distributed MoE training faces significant challenges in resource-constrained edge networks characterized by heterogeneous computing capabilities and stochastic token arrivals, which inevitably suffer from workload backlog, resource inefficiency, and performance degradation. To address this issue, we propose a novel Lyapunov-based token routing framework for distributed MoE training over resource-heterogeneous edge networks, termed Stable-MoE. Specifically, we formulate a stochastic optimization problem to maximize both system throughput and gating consistency via optimizing the token routing strategy and computational resource allocation, while ensuring long-term stability of both token and energy queues at the edge devices. Using the Lyapunov optimization, we transform the intractable long-term optimization problem into tractable per-slot subproblems by enabling online decision-making of token routing and computation frequency utilization without the knowledge of future system states. Experimental results on the SVHN and CIFAR-100 datasets demonstrate that Stable-MoE outperforms the baselines with at least 40% and 5% gains in system throughput and test accuracy, respectively.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8eLyapunov\u4f18\u5316\u7684\u5206\u5e03\u5f0fMoE\u8bad\u7ec3\u4ee4\u724c\u8def\u7531\u6846\u67b6Stable-MoE\uff0c\u89e3\u51b3\u8fb9\u7f18\u7f51\u7edc\u4e2d\u5f02\u6784\u8ba1\u7b97\u80fd\u529b\u548c\u968f\u673a\u4ee4\u724c\u5230\u8fbe\u5e26\u6765\u7684\u8d1f\u8f7d\u79ef\u538b\u3001\u8d44\u6e90\u6548\u7387\u4f4e\u4e0b\u548c\u6027\u80fd\u4e0b\u964d\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u5206\u5e03\u5f0fMoE\u8bad\u7ec3\u4e2d\u7684\u4ee4\u724c\u8def\u7531\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u7f51\u7edc\u4e2d\u9762\u4e34\u6311\u6218\uff0c\u5305\u62ec\u5f02\u6784\u8ba1\u7b97\u80fd\u529b\u3001\u968f\u673a\u4ee4\u724c\u5230\u8fbe\u5bfc\u81f4\u7684\u8d1f\u8f7d\u79ef\u538b\u3001\u8d44\u6e90\u6548\u7387\u4f4e\u4e0b\u548c\u6027\u80fd\u4e0b\u964d\u95ee\u9898\u3002", "method": "\u63d0\u51faLyapunov-based\u4ee4\u724c\u8def\u7531\u6846\u67b6Stable-MoE\uff0c\u901a\u8fc7\u4f18\u5316\u4ee4\u724c\u8def\u7531\u7b56\u7565\u548c\u8ba1\u7b97\u8d44\u6e90\u5206\u914d\u6765\u6700\u5927\u5316\u7cfb\u7edf\u541e\u5410\u91cf\u548c\u95e8\u63a7\u4e00\u81f4\u6027\uff0c\u540c\u65f6\u786e\u4fdd\u8fb9\u7f18\u8bbe\u5907\u4ee4\u724c\u548c\u80fd\u91cf\u961f\u5217\u7684\u957f\u671f\u7a33\u5b9a\u6027\u3002\u4f7f\u7528Lyapunov\u4f18\u5316\u5c06\u957f\u671f\u4f18\u5316\u95ee\u9898\u8f6c\u5316\u4e3a\u53ef\u5904\u7406\u7684\u6bcf\u65f6\u9699\u5b50\u95ee\u9898\uff0c\u5b9e\u73b0\u65e0\u9700\u672a\u6765\u7cfb\u7edf\u72b6\u6001\u4fe1\u606f\u7684\u5728\u7ebf\u51b3\u7b56\u3002", "result": "\u5728SVHN\u548cCIFAR-100\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cStable-MoE\u5728\u7cfb\u7edf\u541e\u5410\u91cf\u548c\u6d4b\u8bd5\u51c6\u786e\u7387\u65b9\u9762\u5206\u522b\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u81f3\u5c11\u63d0\u534740%\u548c5%\u3002", "conclusion": "Stable-MoE\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u8d44\u6e90\u5f02\u6784\u8fb9\u7f18\u7f51\u7edc\u4e2d\u5206\u5e03\u5f0fMoE\u8bad\u7ec3\u7684\u4ee4\u724c\u8def\u7531\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u6027\u80fd\u548c\u8bad\u7ec3\u6548\u7387\u3002"}}
{"id": "2512.06800", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.06800", "abs": "https://arxiv.org/abs/2512.06800", "authors": ["Deepa Gurung", "S M Zia Ur Rashid", "Zain ul Abdeen", "Suman Rath"], "title": "Cloud Revolution: Tracing the Origins and Rise of Cloud Computing", "comment": null, "summary": "The history behind the development of cloud computing is more than several decades of technological progress in the fields of virtualization, distributed systems, and high-speed networking, but its current application is much broader than the underlying technologies that made it possible. This paper reexamines the historical evolution of the field, including the initial ideas of resource sharing and utility-based computing approaches and the development of hyperscale data centers and modern globally federated cloud ecosystems. We also analyze the technological and economic forces and point to the way cloud platforms altered the organizational computing habits, decreasing the entrance-level to the data-intensive and computation-heavy apps. The study also takes into account the ongoing limitations which have come with the large-scale adoption of clouds which include exposure to security due to the weaknesses in configuration, particular establishment regulations, and structural reliance on the single vendors. Lastly, we address some of the new trends that are transforming the cloud environment, including the convergence of edge and cloud infrastructure, the increased prominence of AI-optimised architectures and the initial adoption of quantum computing services. Collectively, the developments above describe an emerging but quickly changing paradigm with its future direction being determined by a strike of balancing between scalability, openness, and trust.", "AI": {"tldr": "\u672c\u6587\u91cd\u65b0\u5ba1\u89c6\u4e86\u4e91\u8ba1\u7b97\u7684\u5386\u53f2\u6f14\u53d8\uff0c\u4ece\u865a\u62df\u5316\u3001\u5206\u5e03\u5f0f\u7cfb\u7edf\u7b49\u6280\u672f\u57fa\u7840\u5230\u73b0\u4ee3\u5168\u7403\u4e91\u751f\u6001\u7cfb\u7edf\uff0c\u5206\u6790\u4e86\u6280\u672f\u7ecf\u6d4e\u9a71\u52a8\u529b\u3001\u7ec4\u7ec7\u53d8\u9769\u5f71\u54cd\u3001\u5f53\u524d\u5c40\u9650\u6027\u548c\u65b0\u5174\u8d8b\u52bf\u3002", "motivation": "\u91cd\u65b0\u5ba1\u89c6\u4e91\u8ba1\u7b97\u6570\u5341\u5e74\u53d1\u5c55\u5386\u7a0b\uff0c\u7406\u89e3\u5176\u4ece\u6280\u672f\u57fa\u7840\u5230\u5e7f\u6cdb\u5e94\u7528\u7684\u6f14\u53d8\uff0c\u5206\u6790\u6280\u672f\u7ecf\u6d4e\u9a71\u52a8\u529b\u5982\u4f55\u6539\u53d8\u7ec4\u7ec7\u8ba1\u7b97\u4e60\u60ef\uff0c\u5e76\u8bc6\u522b\u5927\u89c4\u6a21\u91c7\u7528\u5e26\u6765\u7684\u5c40\u9650\u6027\u548c\u65b0\u5174\u8d8b\u52bf\u3002", "method": "\u5386\u53f2\u56de\u987e\u4e0e\u7efc\u5408\u5206\u6790\uff1a\u901a\u8fc7\u91cd\u65b0\u5ba1\u89c6\u4e91\u8ba1\u7b97\u7684\u5386\u53f2\u6f14\u53d8\uff0c\u5305\u62ec\u8d44\u6e90\u5171\u4eab\u3001\u6548\u7528\u8ba1\u7b97\u7b49\u521d\u59cb\u7406\u5ff5\uff0c\u5206\u6790\u6280\u672f\u7ecf\u6d4e\u529b\u91cf\uff0c\u8bc4\u4f30\u7ec4\u7ec7\u8ba1\u7b97\u4e60\u60ef\u53d8\u5316\uff0c\u8bc6\u522b\u5f53\u524d\u5c40\u9650\u6027\u548c\u65b0\u5174\u8d8b\u52bf\u3002", "result": "\u4e91\u8ba1\u7b97\u5df2\u4ece\u865a\u62df\u5316\u7b49\u6280\u672f\u53d1\u5c55\u4e3a\u5168\u7403\u4e91\u751f\u6001\u7cfb\u7edf\uff0c\u964d\u4f4e\u4e86\u6570\u636e\u5bc6\u96c6\u548c\u8ba1\u7b97\u5bc6\u96c6\u578b\u5e94\u7528\u7684\u95e8\u69db\uff0c\u4f46\u9762\u4e34\u5b89\u5168\u914d\u7f6e\u6f0f\u6d1e\u3001\u76d1\u7ba1\u5408\u89c4\u548c\u4f9b\u5e94\u5546\u4f9d\u8d56\u7b49\u5c40\u9650\u6027\uff0c\u8fb9\u7f18\u8ba1\u7b97\u3001AI\u4f18\u5316\u67b6\u6784\u548c\u91cf\u5b50\u8ba1\u7b97\u670d\u52a1\u6b63\u5728\u91cd\u5851\u4e91\u73af\u5883\u3002", "conclusion": "\u4e91\u8ba1\u7b97\u662f\u4e00\u4e2a\u5feb\u901f\u6f14\u53d8\u7684\u8303\u5f0f\uff0c\u5176\u672a\u6765\u53d1\u5c55\u65b9\u5411\u9700\u8981\u5728\u53ef\u6269\u5c55\u6027\u3001\u5f00\u653e\u6027\u548c\u53ef\u4fe1\u5ea6\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u65b0\u5174\u8d8b\u52bf\u5c06\u7ee7\u7eed\u91cd\u5851\u4e91\u73af\u5883\u3002"}}
{"id": "2512.07009", "categories": ["cs.DC", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.07009", "abs": "https://arxiv.org/abs/2512.07009", "authors": ["Saeid Ghafouri", "Yuming Ding", "Katerine Diaz Chito", "Jes\u00fas Martinez del Rinc\u00f3n", "Niamh O'Connell", "Hans Vandierendonck"], "title": "Optimizing video analytics inference pipelines: a case study", "comment": "Accepted to the IEEE/ACM International Conference on Big Data Computing, Applications and Technologies (BDCAT 2025)", "summary": "Cost-effective and scalable video analytics are essential for precision livestock monitoring, where high-resolution footage and near-real-time monitoring needs from commercial farms generates substantial computational workloads. This paper presents a comprehensive case study on optimizing a poultry welfare monitoring system through system-level improvements across detection, tracking, clustering, and behavioral analysis modules. We introduce a set of optimizations, including multi-level parallelization, Optimizing code with substituting CPU code with GPU-accelerated code, vectorized clustering, and memory-efficient post-processing. Evaluated on real-world farm video footage, these changes deliver up to a 2x speedup across pipelines without compromising model accuracy. Our findings highlight practical strategies for building high-throughput, low-latency video inference systems that reduce infrastructure demands in agricultural and smart sensing deployments as well as other large-scale video analytics applications.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u7cfb\u7edf\u7ea7\u4f18\u5316\uff08\u591a\u7ea7\u5e76\u884c\u5316\u3001GPU\u52a0\u901f\u3001\u5411\u91cf\u5316\u805a\u7c7b\u3001\u5185\u5b58\u9ad8\u6548\u540e\u5904\u7406\uff09\u5b9e\u73b0\u4e86\u5bb6\u79bd\u798f\u5229\u76d1\u63a7\u7cfb\u7edf2\u500d\u52a0\u901f\uff0c\u4e3a\u519c\u4e1a\u89c6\u9891\u5206\u6790\u63d0\u4f9b\u5b9e\u7528\u7b56\u7565", "motivation": "\u5546\u4e1a\u517b\u6b96\u573a\u7684\u9ad8\u5206\u8fa8\u7387\u89c6\u9891\u548c\u8fd1\u5b9e\u65f6\u76d1\u63a7\u9700\u6c42\u4ea7\u751f\u5de8\u5927\u8ba1\u7b97\u8d1f\u8f7d\uff0c\u9700\u8981\u6210\u672c\u6548\u76ca\u9ad8\u4e14\u53ef\u6269\u5c55\u7684\u89c6\u9891\u5206\u6790\u89e3\u51b3\u65b9\u6848", "method": "\u91c7\u7528\u7cfb\u7edf\u7ea7\u6539\u8fdb\uff0c\u5305\u62ec\u591a\u7ea7\u5e76\u884c\u5316\u3001\u7528GPU\u52a0\u901f\u4ee3\u7801\u66ff\u4ee3CPU\u4ee3\u7801\u3001\u5411\u91cf\u5316\u805a\u7c7b\u548c\u5185\u5b58\u9ad8\u6548\u540e\u5904\u7406\uff0c\u4f18\u5316\u68c0\u6d4b\u3001\u8ddf\u8e2a\u3001\u805a\u7c7b\u548c\u884c\u4e3a\u5206\u6790\u6a21\u5757", "result": "\u5728\u771f\u5b9e\u519c\u573a\u89c6\u9891\u6570\u636e\u4e0a\u8bc4\u4f30\uff0c\u4f18\u5316\u540e\u7684\u7cfb\u7edf\u5728\u4fdd\u6301\u6a21\u578b\u7cbe\u5ea6\u7684\u524d\u63d0\u4e0b\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8fbe2\u500d\u7684\u7ba1\u9053\u52a0\u901f", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u6784\u5efa\u9ad8\u541e\u5410\u91cf\u3001\u4f4e\u5ef6\u8fdf\u7684\u89c6\u9891\u63a8\u7406\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u7b56\u7565\uff0c\u53ef\u964d\u4f4e\u519c\u4e1a\u548c\u667a\u80fd\u4f20\u611f\u90e8\u7f72\u4ee5\u53ca\u5176\u4ed6\u5927\u89c4\u6a21\u89c6\u9891\u5206\u6790\u5e94\u7528\u7684\u57fa\u7840\u8bbe\u65bd\u9700\u6c42"}}
{"id": "2512.06442", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2512.06442", "abs": "https://arxiv.org/abs/2512.06442", "authors": ["Xuanyu Peng", "Dominic Kennedy", "Yuyou Fan", "Ben Greenman", "John Regehr", "Loris D'Antoni"], "title": "Nice to Meet You: Synthesizing Practical MLIR Abstract Transformers", "comment": null, "summary": "Static analyses play a fundamental role during compilation: they discover facts that are true in all executions of the code being compiled, and then these facts are used to justify optimizations and diagnostics. Each static analysis is based on a collection of abstract transformers that provide abstract semantics for the concrete instructions that make up a program. It can be challenging to implement abstract transformers that are sound, precise, and efficient, and in fact both LLVM and GCC have suffered from miscompilations caused by unsound abstract transformers. Moreover, even after more than 20 years of development, LLVM lacks abstract transformers for hundreds of instructions in its intermediate representation (IR). We developed NiceToMeetYou, a program synthesis framework for abstract transformers that are aimed at the kinds of non-relational integer abstract domains that are heavily used by today's production compilers. It exploits a simple but novel technique for breaking the synthesis problem into parts: each of our transformers is the meet of a collection of simpler, sound transformers that are synthesized such that each new piece fills a gap in the precision of the final transformer. Our design point is bulk automation: no sketches are required. Transformers are verified by lowering to a previously created SMT dialect of MLIR. Each of our synthesized transformers is provably sound and some (17 percent) are more precise than those provided by LLVM.", "AI": {"tldr": "NiceToMeetYou\u662f\u4e00\u4e2a\u7a0b\u5e8f\u5408\u6210\u6846\u67b6\uff0c\u7528\u4e8e\u4e3a\u7f16\u8bd1\u5668\u4e2d\u7684\u6574\u6570\u62bd\u8c61\u57df\u81ea\u52a8\u751f\u6210\u62bd\u8c61\u8f6c\u6362\u5668\uff0c\u901a\u8fc7\u5206\u89e3\u5408\u6210\u95ee\u9898\u5e76\u9a8c\u8bc1\u6b63\u786e\u6027\uff0c\u63d0\u9ad8\u4e86\u8f6c\u6362\u5668\u7684\u7cbe\u5ea6\u548c\u8986\u76d6\u7387\u3002", "motivation": "\u9759\u6001\u5206\u6790\u5728\u7f16\u8bd1\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5b9e\u73b0\u6b63\u786e\u3001\u7cbe\u786e\u4e14\u9ad8\u6548\u7684\u62bd\u8c61\u8f6c\u6362\u5668\u5177\u6709\u6311\u6218\u6027\u3002LLVM\u548cGCC\u90fd\u66fe\u56e0\u4e0d\u6b63\u786e\u7684\u62bd\u8c61\u8f6c\u6362\u5668\u5bfc\u81f4\u9519\u8bef\u7f16\u8bd1\uff0c\u4e14LLVM\u4ecd\u6709\u6570\u767e\u6761\u6307\u4ee4\u7f3a\u4e4f\u62bd\u8c61\u8f6c\u6362\u5668\uff0c\u9700\u8981\u81ea\u52a8\u5316\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5f00\u53d1\u4e86NiceToMeetYou\u6846\u67b6\uff0c\u91c7\u7528\u5206\u89e3\u5408\u6210\u6280\u672f\uff1a\u5c06\u6bcf\u4e2a\u62bd\u8c61\u8f6c\u6362\u5668\u5206\u89e3\u4e3a\u591a\u4e2a\u66f4\u7b80\u5355\u7684\u8f6c\u6362\u5668\u7684\u4ea4\u96c6\uff0c\u6bcf\u4e2a\u65b0\u7ec4\u4ef6\u586b\u8865\u6700\u7ec8\u8f6c\u6362\u5668\u7684\u7cbe\u5ea6\u7f3a\u53e3\u3002\u65e0\u9700\u4eba\u5de5\u8349\u56fe\uff0c\u901a\u8fc7\u5c06\u9a8c\u8bc1\u964d\u4f4e\u5230MLIR\u7684SMT\u65b9\u8a00\u6765\u9a8c\u8bc1\u8f6c\u6362\u5668\u3002", "result": "\u5408\u6210\u7684\u8f6c\u6362\u5668\u88ab\u8bc1\u660e\u662f\u6b63\u786e\u7684\uff0c\u5176\u4e2d17%\u6bd4LLVM\u63d0\u4f9b\u7684\u8f6c\u6362\u5668\u66f4\u7cbe\u786e\uff0c\u5b9e\u73b0\u4e86\u6279\u91cf\u81ea\u52a8\u5316\u751f\u6210\uff0c\u89e3\u51b3\u4e86LLVM\u4e2d\u62bd\u8c61\u8f6c\u6362\u5668\u8986\u76d6\u7387\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "conclusion": "NiceToMeetYou\u6846\u67b6\u6210\u529f\u5b9e\u73b0\u4e86\u62bd\u8c61\u8f6c\u6362\u5668\u7684\u81ea\u52a8\u5316\u5408\u6210\uff0c\u63d0\u9ad8\u4e86\u7cbe\u5ea6\u548c\u8986\u76d6\u7387\uff0c\u4e3a\u7f16\u8bd1\u5668\u9759\u6001\u5206\u6790\u63d0\u4f9b\u4e86\u53ef\u9760\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.06093", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2512.06093", "abs": "https://arxiv.org/abs/2512.06093", "authors": ["Boyu Li", "Zongwei Zhu", "Yi Xiong", "Qianyue Cao", "Jiawei Geng", "Xiaonan Zhang", "Xi Li"], "title": "Compass: Mapping Space Exploration for Multi-Chiplet Accelerators Targeting LLM Inference Serving Workloads", "comment": null, "summary": "Large Language Models (LLMs) impose massive computational demands, driving the need for scalable multi-chiplet accelerators. However, existing mapping space exploration efforts for such accelerators primarily focus on traditional CNN/Transformer workloads and fail to adequately support the dynamic behaviors of mixed request types and variable sequence lengths in real-world LLM inference serving. To bridge this gap, we first propose a computation execution graph-based mapping encoding scheme that decouples micro-batches and layers, enabling fine-grained execution control on heterogeneous chiplets and flexibly representing various parallelism strategies. Second, building upon this scheme, we develop the Compass framework, which integrates an evaluation engine and a genetic algorithm-based mapping generation engine to achieve efficient mapping search. Compared to state-of-the-art works, our solution achieves an average EDP reduction of 63.12%.", "AI": {"tldr": "\u63d0\u51faCompass\u6846\u67b6\uff0c\u901a\u8fc7\u7ec6\u7c92\u5ea6\u6620\u5c04\u7f16\u7801\u548c\u9057\u4f20\u7b97\u6cd5\u641c\u7d22\uff0c\u4f18\u5316LLM\u63a8\u7406\u5728\u591a\u82af\u7247\u52a0\u901f\u5668\u4e0a\u7684\u90e8\u7f72\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5e73\u5747EDP\u964d\u4f4e63.12%", "motivation": "\u73b0\u6709\u6620\u5c04\u7a7a\u95f4\u63a2\u7d22\u4e3b\u8981\u9488\u5bf9\u4f20\u7edfCNN/Transformer\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u65e0\u6cd5\u5145\u5206\u652f\u6301\u771f\u5b9eLLM\u63a8\u7406\u670d\u52a1\u4e2d\u6df7\u5408\u8bf7\u6c42\u7c7b\u578b\u548c\u53ef\u53d8\u5e8f\u5217\u957f\u5ea6\u7684\u52a8\u6001\u884c\u4e3a", "method": "1) \u63d0\u51fa\u57fa\u4e8e\u8ba1\u7b97\u6267\u884c\u56fe\u7684\u6620\u5c04\u7f16\u7801\u65b9\u6848\uff0c\u89e3\u8026\u5fae\u6279\u6b21\u548c\u5c42\uff0c\u5b9e\u73b0\u5f02\u6784\u82af\u7247\u4e0a\u7684\u7ec6\u7c92\u5ea6\u6267\u884c\u63a7\u5236\uff1b2) \u5f00\u53d1Compass\u6846\u67b6\uff0c\u96c6\u6210\u8bc4\u4f30\u5f15\u64ce\u548c\u57fa\u4e8e\u9057\u4f20\u7b97\u6cd5\u7684\u6620\u5c04\u751f\u6210\u5f15\u64ce", "result": "\u76f8\u6bd4\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5e73\u5747EDP\uff08\u80fd\u91cf\u5ef6\u8fdf\u79ef\uff09\u964d\u4f4e63.12%", "conclusion": "Compass\u6846\u67b6\u901a\u8fc7\u7ec6\u7c92\u5ea6\u6620\u5c04\u7f16\u7801\u548c\u9ad8\u6548\u641c\u7d22\u7b97\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86LLM\u63a8\u7406\u5728\u591a\u82af\u7247\u52a0\u901f\u5668\u4e0a\u7684\u52a8\u6001\u90e8\u7f72\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u80fd\u6548"}}
{"id": "2512.07189", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.07189", "abs": "https://arxiv.org/abs/2512.07189", "authors": ["Jiahao Zhang", "Minghui Xu", "Hechuan Guo", "Xiuzhen Cheng"], "title": "PIR-DSN: A Decentralized Storage Network Supporting Private Information Retrieval", "comment": "Accepted by IEEE INFOCOM 2026", "summary": "Decentralized Storage Networks (DSNs) are emerging as a foundational infrastructure for Web 3.0, offering global peer-to-peer storage. However, a critical vulnerability persists: user privacy during file retrieval remains largely unaddressed, risking the exposure of sensitive information. To overcome this, we introduce PIR-DSN, the first DSN protocol to integrate Private Information Retrieval (PIR) for both single and multi-server settings. Our key innovations include a novel secure mapping method that transforms sparse file identifiers into compact integer indexes, enabling both public verifiability of file operations and efficient private retrieval. Furthermore, PIR-DSN guarantees Byzantine-robust private retrieval through file replication across multiple miners. We implement and rigorously evaluate PIR-DSN against three prominent industrial DSN systems. Experimental results demonstrate that PIR-DSN achieves comparable overhead for file upload and deletion. While PIR inherently introduces an additional computational cost leading to higher retrieval latency, PIR-DSN maintains comparable throughput. These findings underscore PIR-DSN's practical viability for privacy-sensitive applications within DSN environments.", "AI": {"tldr": "PIR-DSN\uff1a\u9996\u4e2a\u5728\u53bb\u4e2d\u5fc3\u5316\u5b58\u50a8\u7f51\u7edc\u4e2d\u96c6\u6210\u79c1\u6709\u4fe1\u606f\u68c0\u7d22\uff08PIR\uff09\u7684\u534f\u8bae\uff0c\u901a\u8fc7\u5b89\u5168\u6620\u5c04\u548c\u6587\u4ef6\u590d\u5236\u5b9e\u73b0\u9690\u79c1\u4fdd\u62a4\u7684\u6587\u4ef6\u68c0\u7d22", "motivation": "\u53bb\u4e2d\u5fc3\u5316\u5b58\u50a8\u7f51\u7edc\uff08DSNs\uff09\u4f5c\u4e3aWeb 3.0\u7684\u57fa\u7840\u8bbe\u65bd\uff0c\u5728\u6587\u4ef6\u68c0\u7d22\u8fc7\u7a0b\u4e2d\u5b58\u5728\u7528\u6237\u9690\u79c1\u6cc4\u9732\u7684\u98ce\u9669\uff0c\u654f\u611f\u4fe1\u606f\u53ef\u80fd\u88ab\u66b4\u9732\uff0c\u9700\u8981\u89e3\u51b3\u8fd9\u4e00\u5173\u952e\u6f0f\u6d1e", "method": "\u63d0\u51faPIR-DSN\u534f\u8bae\uff0c\u96c6\u6210\u79c1\u6709\u4fe1\u606f\u68c0\u7d22\uff08PIR\uff09\u6280\u672f\uff0c\u5305\u62ec\uff1a1\uff09\u65b0\u9896\u7684\u5b89\u5168\u6620\u5c04\u65b9\u6cd5\uff0c\u5c06\u7a00\u758f\u6587\u4ef6\u6807\u8bc6\u7b26\u8f6c\u6362\u4e3a\u7d27\u51d1\u6574\u6570\u7d22\u5f15\uff1b2\uff09\u652f\u6301\u516c\u5171\u53ef\u9a8c\u8bc1\u7684\u6587\u4ef6\u64cd\u4f5c\uff1b3\uff09\u901a\u8fc7\u8de8\u591a\u4e2a\u77ff\u5de5\u7684\u6587\u4ef6\u590d\u5236\u5b9e\u73b0\u62dc\u5360\u5ead\u9c81\u68d2\u7684\u79c1\u6709\u68c0\u7d22", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u8868\u660e\uff1a1\uff09\u6587\u4ef6\u4e0a\u4f20\u548c\u5220\u9664\u7684\u5f00\u9500\u4e0e\u73b0\u6709DSN\u7cfb\u7edf\u76f8\u5f53\uff1b2\uff09PIR\u5f15\u5165\u989d\u5916\u8ba1\u7b97\u6210\u672c\u5bfc\u81f4\u68c0\u7d22\u5ef6\u8fdf\u8f83\u9ad8\uff0c\u4f46\u541e\u5410\u91cf\u4fdd\u6301\u53ef\u6bd4\uff1b3\uff09\u5728\u4e09\u4e2a\u4e3b\u6d41\u5de5\u4e1aDSN\u7cfb\u7edf\u4e0a\u9a8c\u8bc1\u4e86\u53ef\u884c\u6027", "conclusion": "PIR-DSN\u4e3aDSN\u73af\u5883\u4e2d\u7684\u9690\u79c1\u654f\u611f\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9996\u6b21\u5b9e\u73b0\u4e86\u53bb\u4e2d\u5fc3\u5316\u5b58\u50a8\u7f51\u7edc\u4e2d\u7684\u79c1\u6709\u4fe1\u606f\u68c0\u7d22\uff0c\u5e73\u8861\u4e86\u9690\u79c1\u4fdd\u62a4\u4e0e\u7cfb\u7edf\u6027\u80fd"}}
{"id": "2512.07299", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2512.07299", "abs": "https://arxiv.org/abs/2512.07299", "authors": ["H\u00e5vard Rognebakke Krogstie", "Helge Bahmann", "Magnus Sj\u00e4lander", "Nico Reissmann"], "title": "PIP: Making Andersen's Points-to Analysis Sound and Practical for Incomplete C Programs", "comment": "11 pages, 10 figures. To be published in CGO 2026", "summary": "Compiling files individually lends itself well to parallelization, but forces the compiler to operate on incomplete programs. State-of-the-art points-to analyses guarantee sound solutions only for complete programs, requiring summary functions to describe any missing program parts. Summary functions are rarely available in production compilers, however, where soundness and efficiency are non-negotiable. This paper presents an Andersen-style points-to analysis that efficiently produces sound solutions for incomplete C programs. The analysis accomplishes soundness by tracking memory locations and pointers that are accessible from external modules, and efficiency by performing this tracking implicitly in the constraint graph. We show that implicit pointee tracking makes the constraint solver 15$\\times$ faster than any combination of five different state-of-the-art techniques using explicit pointee tracking. We also present the Prefer Implicit Pointees (PIP) technique that further reduces the use of explicit pointees. PIP gives an additional speedup of 1.9$\\times$, compared to the fastest solver configuration not benefiting from PIP. The precision of the analysis is evaluated in terms of an alias-analysis client, where it reduces the number of MayAlias-responses by 40% compared to LLVM's BasicAA pass alone. Finally, we show that the analysis is scalable in terms of memory, making it suitable for optimizing compilers in practice.", "AI": {"tldr": "\u9488\u5bf9\u4e0d\u5b8c\u6574C\u7a0b\u5e8f\u7684\u5b89\u5fb7\u68ee\u5f0f\u6307\u9488\u5206\u6790\uff0c\u901a\u8fc7\u9690\u5f0f\u6307\u9488\u76ee\u6807\u8ddf\u8e2a\u5b9e\u73b0\u9ad8\u6548\u4e14\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6bd4\u73b0\u6709\u6280\u672f\u5feb15\u500d\uff0c\u5185\u5b58\u53ef\u6269\u5c55\uff0c\u9002\u5408\u5b9e\u9645\u7f16\u8bd1\u5668\u4f18\u5316\u3002", "motivation": "\u73b0\u4ee3\u7f16\u8bd1\u5668\u901a\u5e38\u5355\u72ec\u7f16\u8bd1\u6587\u4ef6\u4ee5\u5b9e\u73b0\u5e76\u884c\u5316\uff0c\u4f46\u8fd9\u5bfc\u81f4\u7f16\u8bd1\u5668\u5fc5\u987b\u5728\u7a0b\u5e8f\u4e0d\u5b8c\u6574\u7684\u60c5\u51b5\u4e0b\u5de5\u4f5c\u3002\u73b0\u6709\u7684\u6307\u9488\u5206\u6790\u6280\u672f\u9700\u8981\u5b8c\u6574\u7684\u7a0b\u5e8f\u624d\u80fd\u4fdd\u8bc1\u53ef\u9760\u6027\uff0c\u800c\u751f\u4ea7\u7f16\u8bd1\u5668\u5f88\u5c11\u63d0\u4f9b\u6458\u8981\u51fd\u6570\uff0c\u540c\u65f6\u53ef\u9760\u6027\u548c\u6548\u7387\u53c8\u662f\u4e0d\u53ef\u59a5\u534f\u7684\u8981\u6c42\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u5b89\u5fb7\u68ee\u5f0f\u6307\u9488\u5206\u6790\u65b9\u6cd5\uff0c\u901a\u8fc7\u9690\u5f0f\u8ddf\u8e2a\u53ef\u4ece\u5916\u90e8\u6a21\u5757\u8bbf\u95ee\u7684\u5185\u5b58\u4f4d\u7f6e\u548c\u6307\u9488\u6765\u5b9e\u73b0\u53ef\u9760\u6027\u3002\u5f15\u5165\"\u504f\u597d\u9690\u5f0f\u6307\u9488\u76ee\u6807\"\uff08PIP\uff09\u6280\u672f\u8fdb\u4e00\u6b65\u51cf\u5c11\u663e\u5f0f\u6307\u9488\u76ee\u6807\u7684\u4f7f\u7528\uff0c\u5728\u7ea6\u675f\u56fe\u4e2d\u9690\u5f0f\u6267\u884c\u8ddf\u8e2a\u4ee5\u63d0\u9ad8\u6548\u7387\u3002", "result": "\u9690\u5f0f\u6307\u9488\u76ee\u6807\u8ddf\u8e2a\u4f7f\u7ea6\u675f\u6c42\u89e3\u5668\u6bd4\u4f7f\u7528\u663e\u5f0f\u8ddf\u8e2a\u7684\u4e94\u79cd\u6700\u5148\u8fdb\u6280\u672f\u7ec4\u5408\u5feb15\u500d\u3002PIP\u6280\u672f\u63d0\u4f9b\u989d\u59161.9\u500d\u52a0\u901f\u3002\u5728\u522b\u540d\u5206\u6790\u5ba2\u6237\u7aef\u8bc4\u4f30\u4e2d\uff0c\u76f8\u6bd4LLVM\u7684BasicAA\u5355\u72ec\u4f7f\u7528\uff0c\u51cf\u5c1140%\u7684MayAlias\u54cd\u5e94\u3002\u5206\u6790\u5728\u5185\u5b58\u65b9\u9762\u5177\u6709\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u4e0d\u5b8c\u6574C\u7a0b\u5e8f\u63d0\u4f9b\u4e86\u9ad8\u6548\u53ef\u9760\u7684\u6307\u9488\u5206\u6790\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u5408\u5b9e\u9645\u751f\u4ea7\u7f16\u8bd1\u5668\u4f7f\u7528\uff0c\u5728\u4fdd\u6301\u53ef\u9760\u6027\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u4e86\u5206\u6790\u6548\u7387\u3002"}}
{"id": "2512.06113", "categories": ["cs.AR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.06113", "abs": "https://arxiv.org/abs/2512.06113", "authors": ["Bin Xu", "Ayan Banerjee", "Sandeep Gupta"], "title": "Hardware Software Optimizations for Fast Model Recovery on Reconfigurable Architectures", "comment": null, "summary": "Model Recovery (MR) is a core primitive for physical AI and real-time digital twins, but GPUs often execute MR inefficiently due to iterative dependencies, kernel-launch overheads, underutilized memory bandwidth, and high data-movement latency. We present MERINDA, an FPGA-accelerated MR framework that restructures computation as a streaming dataflow pipeline. MERINDA exploits on-chip locality through BRAM tiling, fixed-point kernels, and the concurrent use of LUT fabric and carry-chain adders to expose fine-grained spatial parallelism while minimizing off-chip traffic. This hardware-aware formulation removes synchronization bottlenecks and sustains high throughput across the iterative updates in MR. On representative MR workloads, MERINDA delivers up to 6.3x fewer cycles than an FPGA-based LTC baseline, enabling real-time performance for time-critical physical systems.", "AI": {"tldr": "MERINDA\u662f\u4e00\u4e2a\u57fa\u4e8eFPGA\u52a0\u901f\u7684\u6a21\u578b\u6062\u590d\u6846\u67b6\uff0c\u901a\u8fc7\u6d41\u5f0f\u6570\u636e\u6d41\u7ba1\u9053\u91cd\u6784\u8ba1\u7b97\uff0c\u76f8\u6bd4FPGA\u57fa\u51c6\u5b9e\u73b0\u51cf\u5c116.3\u500d\u5468\u671f\u6570\uff0c\u4e3a\u65f6\u95f4\u5173\u952e\u7269\u7406\u7cfb\u7edf\u63d0\u4f9b\u5b9e\u65f6\u6027\u80fd\u3002", "motivation": "\u6a21\u578b\u6062\u590d\u662f\u7269\u7406AI\u548c\u5b9e\u65f6\u6570\u5b57\u5b6a\u751f\u7684\u6838\u5fc3\u539f\u8bed\uff0c\u4f46GPU\u5728\u6267\u884c\u65f6\u6548\u7387\u4f4e\u4e0b\uff0c\u5b58\u5728\u8fed\u4ee3\u4f9d\u8d56\u3001\u5185\u6838\u542f\u52a8\u5f00\u9500\u3001\u5185\u5b58\u5e26\u5bbd\u672a\u5145\u5206\u5229\u7528\u548c\u9ad8\u6570\u636e\u79fb\u52a8\u5ef6\u8fdf\u7b49\u95ee\u9898\u3002", "method": "\u5c06\u8ba1\u7b97\u91cd\u6784\u4e3a\u6d41\u5f0f\u6570\u636e\u6d41\u7ba1\u9053\uff0c\u5229\u7528BRAM\u5206\u5757\u3001\u5b9a\u70b9\u5185\u6838\u3001LUT\u7ed3\u6784\u548c\u8fdb\u4f4d\u94fe\u52a0\u6cd5\u5668\u7684\u5e76\u53d1\u4f7f\u7528\uff0c\u66b4\u9732\u7ec6\u7c92\u5ea6\u7a7a\u95f4\u5e76\u884c\u6027\uff0c\u540c\u65f6\u6700\u5c0f\u5316\u7247\u5916\u901a\u4fe1\u3002", "result": "\u5728\u4ee3\u8868\u6027\u6a21\u578b\u6062\u590d\u5de5\u4f5c\u8d1f\u8f7d\u4e0a\uff0cMERINDA\u76f8\u6bd4\u57fa\u4e8eFPGA\u7684LTC\u57fa\u51c6\u5b9e\u73b0\u51cf\u5c11\u4e86\u6700\u591a6.3\u500d\u7684\u5468\u671f\u6570\uff0c\u4e3a\u65f6\u95f4\u5173\u952e\u7269\u7406\u7cfb\u7edf\u5b9e\u73b0\u4e86\u5b9e\u65f6\u6027\u80fd\u3002", "conclusion": "MERINDA\u901a\u8fc7\u786c\u4ef6\u611f\u77e5\u7684\u6d41\u5f0f\u6570\u636e\u6d41\u8bbe\u8ba1\u89e3\u51b3\u4e86\u6a21\u578b\u6062\u590d\u4e2d\u7684\u6027\u80fd\u74f6\u9888\uff0c\u4e3a\u7269\u7406AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684FPGA\u52a0\u901f\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.07280", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.07280", "abs": "https://arxiv.org/abs/2512.07280", "authors": ["Hendrik Reiter", "Janick Edinger", "Martin Kabierski", "Agnes Koschmider", "Olaf Landsiedel", "Arvid Lepsien", "Xixi Lu", "Andrea Marrella", "Estefania Serral", "Stefan Schulte", "Florian Tschorsch", "Matthias Weidlich", "Wilhelm Hasselbring"], "title": "ContinuumConductor : Decentralized Process Mining on the Edge-Cloud Continuum", "comment": "Accepted at COMINDS workshop ICPM2025", "summary": "Process mining traditionally assumes centralized event data collection and analysis. However, modern Industrial Internet of Things systems increasingly operate over distributed, resource-constrained edge-cloud infrastructures. This paper proposes a structured approach for decentralizing process mining by enabling event data to be mined directly within the IoT systems edge-cloud continuum. We introduce ContinuumConductor a layered decision framework that guides when to perform process mining tasks such as preprocessing, correlation, and discovery centrally or decentrally. Thus, enabling privacy, responsive and resource-efficient process mining. For each step in the process mining pipeline, we analyze the trade-offs of decentralization versus centralization across these layers and propose decision criteria. We demonstrate ContinuumConductor at a real-world use-case of process optimazition in inland ports. Our contributions lay the foundation for computing-aware process mining in cyber-physical and IIoT systems.", "AI": {"tldr": "\u63d0\u51faContinuumConductor\u6846\u67b6\uff0c\u5728\u8fb9\u7f18-\u4e91\u8fde\u7eed\u4f53\u4e0a\u5b9e\u73b0\u53bb\u4e2d\u5fc3\u5316\u7684\u6d41\u7a0b\u6316\u6398\uff0c\u5e73\u8861\u9690\u79c1\u3001\u54cd\u5e94\u6027\u548c\u8d44\u6e90\u6548\u7387", "motivation": "\u4f20\u7edf\u6d41\u7a0b\u6316\u6398\u5047\u8bbe\u96c6\u4e2d\u5f0f\u4e8b\u4ef6\u6570\u636e\u6536\u96c6\u548c\u5206\u6790\uff0c\u4f46\u73b0\u4ee3\u5de5\u4e1a\u7269\u8054\u7f51\u7cfb\u7edf\u5728\u5206\u5e03\u5f0f\u3001\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18-\u4e91\u57fa\u7840\u8bbe\u65bd\u4e0a\u8fd0\u884c\uff0c\u9700\u8981\u65b0\u7684\u53bb\u4e2d\u5fc3\u5316\u65b9\u6cd5", "method": "\u63d0\u51faContinuumConductor\u5206\u5c42\u51b3\u7b56\u6846\u67b6\uff0c\u6307\u5bfc\u4f55\u65f6\u5728\u6d41\u7a0b\u6316\u6398\u7ba1\u9053\uff08\u9884\u5904\u7406\u3001\u5173\u8054\u3001\u53d1\u73b0\u7b49\u6b65\u9aa4\uff09\u4e2d\u6267\u884c\u96c6\u4e2d\u6216\u53bb\u4e2d\u5fc3\u5316\u64cd\u4f5c\uff0c\u5206\u6790\u5404\u5c42\u53bb\u4e2d\u5fc3\u5316\u4e0e\u96c6\u4e2d\u5316\u7684\u6743\u8861\u5e76\u63d0\u51fa\u51b3\u7b56\u6807\u51c6", "result": "\u5728\u5185\u6cb3\u6e2f\u53e3\u6d41\u7a0b\u4f18\u5316\u5b9e\u9645\u7528\u4f8b\u4e2d\u6f14\u793a\u4e86ContinuumConductor\u6846\u67b6\uff0c\u4e3a\u7f51\u7edc\u7269\u7406\u7cfb\u7edf\u548c\u5de5\u4e1a\u7269\u8054\u7f51\u7cfb\u7edf\u4e2d\u7684\u8ba1\u7b97\u611f\u77e5\u6d41\u7a0b\u6316\u6398\u5960\u5b9a\u57fa\u7840", "conclusion": "ContinuumConductor\u6846\u67b6\u4e3a\u5de5\u4e1a\u7269\u8054\u7f51\u8fb9\u7f18-\u4e91\u8fde\u7eed\u4f53\u4e0a\u7684\u53bb\u4e2d\u5fc3\u5316\u6d41\u7a0b\u6316\u6398\u63d0\u4f9b\u4e86\u7ed3\u6784\u5316\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u9690\u79c1\u4fdd\u62a4\u3001\u54cd\u5e94\u6027\u548c\u8d44\u6e90\u6548\u7387\u7684\u5e73\u8861"}}
{"id": "2512.07511", "categories": ["cs.PL", "cs.LO"], "pdf": "https://arxiv.org/pdf/2512.07511", "abs": "https://arxiv.org/abs/2512.07511", "authors": ["Zanzi Mihejevs", "Jules Hedges"], "title": "Canonical bidirectional typechecking", "comment": null, "summary": "We demonstrate that the checkable/synthesisable split in bidirectional typechecking coincides with existing dualities in polarised System L, also known as polarised $\u03bc\\tilde\u03bc$-calculus. Specifically, positive terms and negative coterms are checkable, and negative terms and positive coterms are synthesisable. This combines a standard formulation of bidirectional typechecking with Zeilberger's `cocontextual' variant. We extend this to ordinary `cartesian' System L using Mc Bride's co-de Bruijn formulation of scopes, and show that both can be combined in a linear-nonlinear style, where linear types are positive and cartesian types are negative. This yields a remarkable 3-way coincidence between the shifts of polarised System L, LNL calculi, and bidirectional calculi.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5c55\u793a\u4e86\u53cc\u5411\u7c7b\u578b\u68c0\u67e5\u4e2d\u7684\u53ef\u68c0\u67e5/\u53ef\u7efc\u5408\u5212\u5206\u4e0e\u6781\u5316\u7cfb\u7edfL\u4e2d\u7684\u5bf9\u5076\u6027\u4e00\u81f4\uff0c\u5efa\u7acb\u4e86\u6781\u5316\u03bc\u03bc\u0303\u6f14\u7b97\u3001LNL\u6f14\u7b97\u548c\u53cc\u5411\u6f14\u7b97\u4e4b\u95f4\u7684\u4e09\u5411\u5bf9\u5e94\u5173\u7cfb\u3002", "motivation": "\u63a2\u7d22\u53cc\u5411\u7c7b\u578b\u68c0\u67e5\u4e2d\u53ef\u68c0\u67e5\u4e0e\u53ef\u7efc\u5408\u7684\u5212\u5206\u4e0e\u6781\u5316\u7cfb\u7edfL\u4e2d\u5bf9\u5076\u6027\u4e4b\u95f4\u7684\u6df1\u5c42\u8054\u7cfb\uff0c\u5efa\u7acb\u4e0d\u540c\u8ba1\u7b97\u6a21\u578b\u4e4b\u95f4\u7684\u7edf\u4e00\u7406\u8bba\u6846\u67b6\u3002", "method": "\u7ed3\u5408\u6807\u51c6\u53cc\u5411\u7c7b\u578b\u68c0\u67e5\u4e0eZeilberger\u7684\"\u5171\u4e0a\u4e0b\u6587\"\u53d8\u4f53\uff0c\u6269\u5c55\u5230\u4f7f\u7528McBride\u7684\u5171\u5fb7\u5e03\u9c81\u56e0\u4f5c\u7528\u57df\u8868\u8ff0\u7684\u7b1b\u5361\u5c14\u7cfb\u7edfL\uff0c\u5e76\u5728\u7ebf\u6027-\u975e\u7ebf\u6027\u98ce\u683c\u4e2d\u7ec4\u5408\u4e24\u8005\u3002", "result": "\u8bc1\u660e\u4e86\u6781\u5316\u7cfb\u7edfL\u4e2d\u7684\u6b63\u9879\u548c\u8d1f\u4f59\u9879\u662f\u53ef\u68c0\u67e5\u7684\uff0c\u8d1f\u9879\u548c\u6b63\u4f59\u9879\u662f\u53ef\u7efc\u5408\u7684\uff0c\u5efa\u7acb\u4e86\u6781\u5316\u7cfb\u7edfL\u7684\u79fb\u4f4d\u3001LNL\u6f14\u7b97\u548c\u53cc\u5411\u6f14\u7b97\u4e4b\u95f4\u7684\u4e09\u5411\u5bf9\u5e94\u5173\u7cfb\u3002", "conclusion": "\u53cc\u5411\u7c7b\u578b\u68c0\u67e5\u7684\u5212\u5206\u4e0e\u6781\u5316\u7cfb\u7edfL\u7684\u5bf9\u5076\u6027\u672c\u8d28\u76f8\u540c\uff0c\u8fd9\u4e00\u53d1\u73b0\u4e3a\u7c7b\u578b\u7406\u8bba\u4e2d\u7684\u4e0d\u540c\u8ba1\u7b97\u6a21\u578b\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u6570\u5b66\u57fa\u7840\u3002"}}
{"id": "2512.06177", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2512.06177", "abs": "https://arxiv.org/abs/2512.06177", "authors": ["Jiahan Xie", "Evan Williams", "Adrian Sampson"], "title": "From PyTorch to Calyx: An Open-Source Compiler Toolchain for ML Accelerators", "comment": "5 pages, 3 figures", "summary": "We present an end-to-end open-source compiler toolchain that targets synthesizable SystemVerilog from ML models written in PyTorch. Our toolchain leverages the accelerator design language Allo, the hardware intermediate representation (IR) Calyx, and the CIRCT project under LLVM. We also implement a set of compiler passes for memory partitioning, enabling effective parallelism in memory-intensive ML workloads. Experimental results demonstrate that our compiler can effectively generate optimized FPGA-implementable hardware designs that perform reasonably well against closed-source industry-grade tools such as Vitis HLS.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u4ecePyTorch ML\u6a21\u578b\u5230\u53ef\u7efc\u5408SystemVerilog\u7684\u7aef\u5230\u7aef\u5f00\u6e90\u7f16\u8bd1\u5668\u5de5\u5177\u94fe\uff0c\u6027\u80fd\u53ef\u4e0eVitis HLS\u7b49\u95ed\u6e90\u5de5\u4e1a\u5de5\u5177\u76f8\u5ab2\u7f8e\u3002", "motivation": "\u4e3aML\u6a21\u578b\u786c\u4ef6\u52a0\u901f\u63d0\u4f9b\u5f00\u6e90\u89e3\u51b3\u65b9\u6848\uff0c\u66ff\u4ee3\u95ed\u6e90\u5de5\u4e1a\u5de5\u5177\u5982Vitis HLS\uff0c\u964d\u4f4e\u786c\u4ef6\u8bbe\u8ba1\u95e8\u69db\u5e76\u4fc3\u8fdb\u5f00\u6e90\u786c\u4ef6\u751f\u6001\u53d1\u5c55\u3002", "method": "\u57fa\u4e8eAllo\u52a0\u901f\u5668\u8bbe\u8ba1\u8bed\u8a00\u3001Calyx\u786c\u4ef6\u4e2d\u95f4\u8868\u793a\u548cLLVM\u7684CIRCT\u9879\u76ee\u6784\u5efa\u5de5\u5177\u94fe\uff0c\u5b9e\u73b0\u5185\u5b58\u5206\u533a\u7f16\u8bd1\u5668\u4f18\u5316\u4ee5\u63d0\u5347\u5185\u5b58\u5bc6\u96c6\u578bML\u5de5\u4f5c\u8d1f\u8f7d\u7684\u5e76\u884c\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u7f16\u8bd1\u5668\u80fd\u6709\u6548\u751f\u6210\u4f18\u5316\u7684FPGA\u53ef\u5b9e\u73b0\u7684\u786c\u4ef6\u8bbe\u8ba1\uff0c\u6027\u80fd\u4e0eVitis HLS\u7b49\u95ed\u6e90\u5de5\u4e1a\u7ea7\u5de5\u5177\u76f8\u5f53\u3002", "conclusion": "\u6210\u529f\u5f00\u53d1\u4e86\u4ecePyTorch\u5230SystemVerilog\u7684\u5b8c\u6574\u5f00\u6e90\u7f16\u8bd1\u5668\u5de5\u5177\u94fe\uff0c\u4e3aML\u786c\u4ef6\u52a0\u901f\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u5f00\u6e90\u66ff\u4ee3\u65b9\u6848\uff0c\u6027\u80fd\u8fbe\u5230\u5de5\u4e1a\u7ea7\u6807\u51c6\u3002"}}
{"id": "2512.07344", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.07344", "abs": "https://arxiv.org/abs/2512.07344", "authors": ["Shengyuan Ye", "Bei Ouyang", "Tianyi Qian", "Liekang Zeng", "Mu Yuan", "Xiaowen Chu", "Weijie Hong", "Xu Chen"], "title": "Venus: An Efficient Edge Memory-and-Retrieval System for VLM-based Online Video Understanding", "comment": "Accepted by IEEE International Conference on Computer Communications 2026", "summary": "Vision-language models (VLMs) have demonstrated impressive multimodal comprehension capabilities and are being deployed in an increasing number of online video understanding applications. While recent efforts extensively explore advancing VLMs' reasoning power in these cases, deployment constraints are overlooked, leading to overwhelming system overhead in real-world deployments. To address that, we propose Venus, an on-device memory-and-retrieval system for efficient online video understanding. Venus proposes an edge-cloud disaggregated architecture that sinks memory construction and keyframe retrieval from cloud to edge, operating in two stages. In the ingestion stage, Venus continuously processes streaming edge videos via scene segmentation and clustering, where the selected keyframes are embedded with a multimodal embedding model to build a hierarchical memory for efficient storage and retrieval. In the querying stage, Venus indexes incoming queries from memory, and employs a threshold-based progressive sampling algorithm for keyframe selection that enhances diversity and adaptively balances system cost and reasoning accuracy. Our extensive evaluation shows that Venus achieves a 15x-131x speedup in total response latency compared to state-of-the-art methods, enabling real-time responses within seconds while maintaining comparable or even superior reasoning accuracy.", "AI": {"tldr": "Venus\u662f\u4e00\u4e2a\u7528\u4e8e\u9ad8\u6548\u5728\u7ebf\u89c6\u9891\u7406\u89e3\u7684\u7aef\u4fa7\u5185\u5b58\u4e0e\u68c0\u7d22\u7cfb\u7edf\uff0c\u91c7\u7528\u8fb9\u4e91\u5206\u79bb\u67b6\u6784\uff0c\u901a\u8fc7\u5173\u952e\u5e27\u9009\u62e9\u548c\u6e10\u8fdb\u91c7\u6837\u7b97\u6cd5\uff0c\u5728\u4fdd\u6301\u63a8\u7406\u7cbe\u5ea6\u7684\u540c\u65f6\u5b9e\u73b015-131\u500d\u7684\u5ef6\u8fdf\u52a0\u901f\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5728\u7ebf\u89c6\u9891\u7406\u89e3\u5e94\u7528\u4e2d\u867d\u7136\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u591a\u6a21\u6001\u7406\u89e3\u80fd\u529b\uff0c\u4f46\u90e8\u7f72\u65f6\u5ffd\u7565\u4e86\u7cfb\u7edf\u5f00\u9500\u95ee\u9898\uff0c\u5bfc\u81f4\u5b9e\u9645\u90e8\u7f72\u4e2d\u4ea7\u751f\u8fc7\u9ad8\u7684\u7cfb\u7edf\u5f00\u9500\u3002", "method": "\u63d0\u51fa\u8fb9\u4e91\u5206\u79bb\u67b6\u6784\uff0c\u5c06\u5185\u5b58\u6784\u5efa\u548c\u5173\u952e\u5e27\u68c0\u7d22\u4ece\u4e91\u7aef\u4e0b\u6c89\u5230\u8fb9\u7f18\u3002\u5305\u542b\u4e24\u4e2a\u9636\u6bb5\uff1a1) \u6444\u5165\u9636\u6bb5\uff1a\u901a\u8fc7\u573a\u666f\u5206\u5272\u548c\u805a\u7c7b\u5904\u7406\u6d41\u5f0f\u8fb9\u7f18\u89c6\u9891\uff0c\u4f7f\u7528\u591a\u6a21\u6001\u5d4c\u5165\u6a21\u578b\u6784\u5efa\u5206\u5c42\u5185\u5b58\uff1b2) \u67e5\u8be2\u9636\u6bb5\uff1a\u7d22\u5f15\u67e5\u8be2\u5e76\u91c7\u7528\u57fa\u4e8e\u9608\u503c\u7684\u6e10\u8fdb\u91c7\u6837\u7b97\u6cd5\u8fdb\u884c\u5173\u952e\u5e27\u9009\u62e9\uff0c\u5e73\u8861\u7cfb\u7edf\u6210\u672c\u548c\u63a8\u7406\u7cbe\u5ea6\u3002", "result": "\u4e0e\u6700\u5148\u8fdb\u65b9\u6cd5\u76f8\u6bd4\uff0cVenus\u5b9e\u73b0\u4e8615-131\u500d\u7684\u603b\u54cd\u5e94\u5ef6\u8fdf\u52a0\u901f\uff0c\u80fd\u591f\u5728\u51e0\u79d2\u5185\u5b9e\u73b0\u5b9e\u65f6\u54cd\u5e94\uff0c\u540c\u65f6\u4fdd\u6301\u76f8\u5f53\u751a\u81f3\u66f4\u4f18\u7684\u63a8\u7406\u7cbe\u5ea6\u3002", "conclusion": "Venus\u7cfb\u7edf\u901a\u8fc7\u521b\u65b0\u7684\u8fb9\u4e91\u5206\u79bb\u67b6\u6784\u548c\u9ad8\u6548\u7684\u68c0\u7d22\u7b97\u6cd5\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u5728\u7ebf\u89c6\u9891\u7406\u89e3\u5e94\u7528\u4e2d\u7684\u90e8\u7f72\u5f00\u9500\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u5b9e\u65f6\u7684\u89c6\u9891\u7406\u89e3\u80fd\u529b\u3002"}}
{"id": "2512.06208", "categories": ["cs.AR", "cs.LG", "hep-ex"], "pdf": "https://arxiv.org/pdf/2512.06208", "abs": "https://arxiv.org/abs/2512.06208", "authors": ["Ho Fung Tsoi", "Dylan Rankin", "Vladimir Loncar", "Philip Harris"], "title": "SparsePixels: Efficient Convolution for Sparse Data on FPGAs", "comment": "Under review", "summary": "Inference of standard CNNs on FPGAs often incurs high latency and a long initiation interval due to the deep nested loops required to densely convolve every input pixel regardless of its feature value, especially when the image size is large. However, in some image data, input features can be spatially sparse, and semantic information may occupy only a small fraction of the input pixels. In this case most computation would be wasted on empty regions. In this work, we introduce SparsePixels, a framework for efficient convolution for spatially sparse image data on FPGAs, targeting fast inference applications in constrained environments with latency requirements of microseconds or below. Our approach implements a special class of CNNs that selectively retain and compute on a small subset of pixels that are active while ignoring the rest. We show that, for example, in a neutrino physics dataset for identifying neutrino interactions in LArTPC images that have around 4k input pixels but are naturally very sparse, a standard CNN with a compact size of 4k parameters incurs an inference latency of 48.665 $\u03bc$s on an FPGA, whereas a sparse CNN of the same base architecture computing on less than 1% of the input pixels results in a $\\times 73$ inference speedup to 0.665 $\u03bc$s, with resource utilization well within on-chip budgets, trading only a small percent-level performance loss. At least one-order-of magnitude speedups with comparable performance are also demonstrated in similar datasets with sparse image patterns. This work aims to benefit future algorithm developments for fast and efficient data readout in modern experiments such as the trigger and data acquisition systems at the CERN Large Hadron Collider. For easy adoption, we have developed a library to support building sparse CNNs with quantization-aware training, as well as an HLS implementation for FPGA deployment.", "AI": {"tldr": "SparsePixels\uff1a\u9488\u5bf9\u7a7a\u95f4\u7a00\u758f\u56fe\u50cf\u6570\u636e\u7684FPGA\u9ad8\u6548\u5377\u79ef\u6846\u67b6\uff0c\u901a\u8fc7\u4ec5\u8ba1\u7b97\u6d3b\u8dc3\u50cf\u7d20\u5b9e\u73b073\u500d\u63a8\u7406\u52a0\u901f", "motivation": "\u4f20\u7edfCNN\u5728FPGA\u4e0a\u63a8\u7406\u65f6\uff0c\u7531\u4e8e\u9700\u8981\u5bf9\u6240\u6709\u8f93\u5165\u50cf\u7d20\u8fdb\u884c\u5bc6\u96c6\u5377\u79ef\uff0c\u5bfc\u81f4\u9ad8\u5ef6\u8fdf\u548c\u957f\u542f\u52a8\u95f4\u9694\u3002\u8bb8\u591a\u56fe\u50cf\u6570\u636e\u5177\u6709\u7a7a\u95f4\u7a00\u758f\u6027\uff0c\u5927\u90e8\u5206\u8ba1\u7b97\u6d6a\u8d39\u5728\u7a7a\u533a\u57df\u4e0a\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u5fae\u79d2\u7ea7\u5ef6\u8fdf\u7684\u7ea6\u675f\u73af\u5883\u4e2d\u3002", "method": "\u63d0\u51faSparsePixels\u6846\u67b6\uff0c\u5b9e\u73b0\u7279\u6b8a\u7c7b\u522b\u7684CNN\uff0c\u4ec5\u9009\u62e9\u6027\u5730\u4fdd\u7559\u548c\u8ba1\u7b97\u6d3b\u8dc3\u50cf\u7d20\u5b50\u96c6\uff0c\u5ffd\u7565\u5176\u4f59\u50cf\u7d20\u3002\u5f00\u53d1\u652f\u6301\u7a00\u758fCNN\u6784\u5efa\u548c\u91cf\u5316\u611f\u77e5\u8bad\u7ec3\u7684\u5e93\uff0c\u4ee5\u53ca\u7528\u4e8eFPGA\u90e8\u7f72\u7684HLS\u5b9e\u73b0\u3002", "result": "\u5728\u4e2d\u5fae\u5b50\u7269\u7406\u6570\u636e\u96c6\u4e2d\uff0c\u6807\u51c6CNN\u63a8\u7406\u5ef6\u8fdf\u4e3a48.665\u03bcs\uff0c\u800c\u7a00\u758fCNN\u4ec5\u8ba1\u7b97\u4e0d\u52301%\u7684\u8f93\u5165\u50cf\u7d20\uff0c\u5b9e\u73b073\u500d\u52a0\u901f\u81f30.665\u03bcs\uff0c\u8d44\u6e90\u5229\u7528\u7387\u5728\u7247\u4e0a\u9884\u7b97\u5185\uff0c\u4ec5\u5e26\u6765\u5fae\u5c0f\u6027\u80fd\u635f\u5931\u3002\u5728\u7c7b\u4f3c\u7a00\u758f\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u4e5f\u5c55\u793a\u51fa\u81f3\u5c11\u4e00\u4e2a\u6570\u91cf\u7ea7\u7684\u52a0\u901f\u3002", "conclusion": "SparsePixels\u6846\u67b6\u4e3a\u7a7a\u95f4\u7a00\u758f\u56fe\u50cf\u6570\u636e\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684FPGA\u5377\u79ef\u89e3\u51b3\u65b9\u6848\uff0c\u663e\u8457\u964d\u4f4e\u63a8\u7406\u5ef6\u8fdf\uff0c\u9002\u7528\u4e8eCERN\u5927\u578b\u5f3a\u5b50\u5bf9\u649e\u673a\u7b49\u73b0\u4ee3\u5b9e\u9a8c\u4e2d\u7684\u89e6\u53d1\u548c\u6570\u636e\u91c7\u96c6\u7cfb\u7edf\u3002\u63d0\u4f9b\u4e86\u6613\u4e8e\u91c7\u7528\u7684\u5de5\u5177\u94fe\u652f\u6301\u3002"}}
{"id": "2512.07350", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.07350", "abs": "https://arxiv.org/abs/2512.07350", "authors": ["Zhiyuan Wu", "Shuai Wang", "Li Chen", "Kaihui Gao", "Dan Li", "Yanyu Ren", "Qiming Zhang", "Yong Wang"], "title": "Communication-Efficient Serving for Video Diffusion Models with Latent Parallelism", "comment": "19 pages", "summary": "Video diffusion models (VDMs) perform attention computation over the 3D spatio-temporal domain. Compared to large language models (LLMs) processing 1D sequences, their memory consumption scales cubically, necessitating parallel serving across multiple GPUs. Traditional parallelism strategies partition the computational graph, requiring frequent high-dimensional activation transfers that create severe communication bottlenecks. To tackle this issue, we exploit the local spatio-temporal dependencies inherent in the diffusion denoising process and propose Latent Parallelism (LP), the first parallelism strategy tailored for VDM serving. \\textcolor{black}{LP decomposes the global denoising problem into parallelizable sub-problems by dynamically rotating the partitioning dimensions (temporal, height, and width) within the compact latent space across diffusion timesteps, substantially reducing the communication overhead compared to prevailing parallelism strategies.} To ensure generation quality, we design a patch-aligned overlapping partition strategy that matches partition boundaries with visual patches and a position-aware latent reconstruction mechanism for smooth stitching. Experiments on three benchmarks demonstrate that LP reduces communication overhead by up to 97\\% over baseline methods while maintaining comparable generation quality. As a non-intrusive plug-in paradigm, LP can be seamlessly integrated with existing parallelism strategies, enabling efficient and scalable video generation services.", "AI": {"tldr": "\u63d0\u51faLatent Parallelism (LP)\u5e76\u884c\u7b56\u7565\uff0c\u901a\u8fc7\u52a8\u6001\u65cb\u8f6c\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7684\u5206\u533a\u7ef4\u5ea6\u6765\u51cf\u5c11\u89c6\u9891\u6269\u6563\u6a21\u578b\u670d\u52a1\u65f6\u7684\u901a\u4fe1\u5f00\u9500\uff0c\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u51cf\u5c11\u9ad8\u8fbe97%\u7684\u901a\u4fe1\u91cf\u3002", "motivation": "\u89c6\u9891\u6269\u6563\u6a21\u578b(VDMs)\u57283D\u65f6\u7a7a\u57df\u4e0a\u8fdb\u884c\u6ce8\u610f\u529b\u8ba1\u7b97\uff0c\u76f8\u6bd4\u5904\u74061D\u5e8f\u5217\u7684\u5927\u8bed\u8a00\u6a21\u578b(LLMs)\uff0c\u5176\u5185\u5b58\u6d88\u8017\u5448\u7acb\u65b9\u7ea7\u589e\u957f\uff0c\u9700\u8981\u8de8\u591a\u4e2aGPU\u5e76\u884c\u670d\u52a1\u3002\u4f20\u7edf\u7684\u5e76\u884c\u7b56\u7565\u5212\u5206\u8ba1\u7b97\u56fe\uff0c\u9700\u8981\u9891\u7e41\u7684\u9ad8\u7ef4\u6fc0\u6d3b\u4f20\u8f93\uff0c\u9020\u6210\u4e25\u91cd\u7684\u901a\u4fe1\u74f6\u9888\u3002", "method": "\u63d0\u51faLatent Parallelism (LP)\u5e76\u884c\u7b56\u7565\uff1a1) \u5229\u7528\u6269\u6563\u53bb\u566a\u8fc7\u7a0b\u4e2d\u56fa\u6709\u7684\u5c40\u90e8\u65f6\u7a7a\u4f9d\u8d56\u6027\uff1b2) \u5728\u6269\u6563\u65f6\u95f4\u6b65\u4e2d\u52a8\u6001\u65cb\u8f6c\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7684\u5206\u533a\u7ef4\u5ea6\uff08\u65f6\u95f4\u3001\u9ad8\u5ea6\u3001\u5bbd\u5ea6\uff09\uff0c\u5c06\u5168\u5c40\u53bb\u566a\u95ee\u9898\u5206\u89e3\u4e3a\u53ef\u5e76\u884c\u5b50\u95ee\u9898\uff1b3) \u8bbe\u8ba1\u8865\u4e01\u5bf9\u9f50\u7684\u91cd\u53e0\u5206\u533a\u7b56\u7565\uff0c\u4f7f\u5206\u533a\u8fb9\u754c\u4e0e\u89c6\u89c9\u8865\u4e01\u5339\u914d\uff1b4) \u91c7\u7528\u4f4d\u7f6e\u611f\u77e5\u7684\u6f5c\u5728\u91cd\u5efa\u673a\u5236\u5b9e\u73b0\u5e73\u6ed1\u62fc\u63a5\u3002", "result": "\u5728\u4e09\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\uff0cLP\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u51cf\u5c11\u901a\u4fe1\u5f00\u9500\u9ad8\u8fbe97%\uff0c\u540c\u65f6\u4fdd\u6301\u53ef\u6bd4\u7684\u751f\u6210\u8d28\u91cf\u3002LP\u4f5c\u4e3a\u975e\u4fb5\u5165\u5f0f\u63d2\u4ef6\u8303\u5f0f\uff0c\u53ef\u4ee5\u65e0\u7f1d\u96c6\u6210\u5230\u73b0\u6709\u5e76\u884c\u7b56\u7565\u4e2d\u3002", "conclusion": "LP\u662f\u9996\u4e2a\u9488\u5bf9VDM\u670d\u52a1\u5b9a\u5236\u7684\u5e76\u884c\u7b56\u7565\uff0c\u901a\u8fc7\u5229\u7528\u6269\u6563\u8fc7\u7a0b\u7684\u5c40\u90e8\u4f9d\u8d56\u6027\uff0c\u5728\u7d27\u51d1\u6f5c\u5728\u7a7a\u95f4\u4e2d\u52a8\u6001\u65cb\u8f6c\u5206\u533a\u7ef4\u5ea6\uff0c\u663e\u8457\u51cf\u5c11\u901a\u4fe1\u5f00\u9500\uff0c\u5b9e\u73b0\u9ad8\u6548\u53ef\u6269\u5c55\u7684\u89c6\u9891\u751f\u6210\u670d\u52a1\u3002"}}
{"id": "2512.06362", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2512.06362", "abs": "https://arxiv.org/abs/2512.06362", "authors": ["Junyi Yang", "Xinyu Luo", "Ye Ke", "Zheng Wang", "Hongyang Shang", "Shuai Dong", "Zhengnan Fu", "Xiaofeng Yang", "Hongjie Liu", "Arindam Basu"], "title": "A 33.6-136.2 TOPS/W Nonlinear Analog Computing-In-Memory Macro for Multi-bit LSTM Accelerator in 65 nm CMOS", "comment": null, "summary": "The energy efficiency of analog computing-in-memory (ACIM) accelerator for recurrent neural networks, particularly long short-term memory (LSTM) network, is limited by the high proportion of nonlinear (NL) operations typically executed digitally. To address this, we propose an LSTM accelerator incorporating an ACIM macro with reconfigurable (1-5 bit) nonlinear in-memory (NLIM) analog-to-digital converter (ADC) to compute NL activations directly in the analog domain using: 1) a dual 9T bitcell with decoupled read/write paths for signed inputs and ternary weight operations; 2) a read-word-line underdrive Cascode (RUDC) technique achieving 2.8X higher read-bitline dynamic range than single-transistor designs (1.4X better over conventional Cascode structure with 7X lower current variation); 3) a dual-supply 6T-SRAM array for efficient multi-bit weight operations and reducing both bitcell count (7.8X) and latency (4X) for 5-bit weight operations. We experimentally demonstrate 5-bit NLIM ADC for approximating NL activations in LSTM cells, achieving average error <1 LSB. Simulation confirms the robustness of NLIM ADC against temperature variations thanks to the replica bias strategy. Our design achieves 92.0% on-chip inference accuracy for a 12-class keyword-spotting task while demonstrating 2.2X higher system-level normalized energy efficiency and 1.6X better normalized area efficiency than state-of-the-art works. The results combine physical measurements of a macro unit-accounting for the majority of LSTM operations (99% linear and 80% nonlinear operations)-with simulations of the remaining components, including additional LSTM and fully connected layers.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u53ef\u91cd\u6784\u975e\u7ebf\u6027\u5185\u5b58ADC\u7684LSTM\u52a0\u901f\u5668\uff0c\u76f4\u63a5\u5728\u6a21\u62df\u57df\u8ba1\u7b97\u975e\u7ebf\u6027\u6fc0\u6d3b\uff0c\u663e\u8457\u63d0\u5347\u80fd\u6548\u548c\u9762\u79ef\u6548\u7387", "motivation": "\u4f20\u7edf\u6a21\u62df\u5185\u5b58\u8ba1\u7b97\u52a0\u901f\u5668\u5728\u5904\u7406LSTM\u7f51\u7edc\u65f6\uff0c\u7531\u4e8e\u5927\u91cf\u975e\u7ebf\u6027\u64cd\u4f5c\u9700\u8981\u6570\u5b57\u5904\u7406\uff0c\u9650\u5236\u4e86\u80fd\u6548\u63d0\u5347", "method": "\u91c7\u7528\u53ef\u91cd\u6784(1-5\u4f4d)\u975e\u7ebf\u6027\u5185\u5b58ADC\uff0c\u5305\u542b\uff1a1)\u53cc9T\u4f4d\u5355\u5143\u652f\u6301\u6709\u7b26\u53f7\u8f93\u5165\u548c\u4e09\u5143\u6743\u91cd\uff1b2)RUDC\u6280\u672f\u63d0\u5347\u8bfb\u53d6\u52a8\u6001\u8303\u56f4\uff1b3)\u53cc\u7535\u6e906T-SRAM\u9635\u5217\u4f18\u5316\u591a\u6bd4\u7279\u6743\u91cd\u64cd\u4f5c", "result": "5\u4f4dNLIM ADC\u5b9e\u73b0\u975e\u7ebf\u6027\u6fc0\u6d3b\u8fd1\u4f3c\uff0c\u5e73\u5747\u8bef\u5dee<1 LSB\uff1b\u572812\u7c7b\u5173\u952e\u8bcd\u68c0\u6d4b\u4efb\u52a1\u4e2d\u8fbe\u523092.0%\u7247\u4e0a\u63a8\u7406\u51c6\u786e\u7387\uff0c\u7cfb\u7edf\u7ea7\u80fd\u6548\u63d0\u53472.2\u500d\uff0c\u9762\u79ef\u6548\u7387\u63d0\u53471.6\u500d", "conclusion": "\u63d0\u51fa\u7684LSTM\u52a0\u901f\u5668\u901a\u8fc7\u6a21\u62df\u57df\u76f4\u63a5\u8ba1\u7b97\u975e\u7ebf\u6027\u64cd\u4f5c\uff0c\u663e\u8457\u63d0\u5347\u4e86\u80fd\u6548\u548c\u9762\u79ef\u6548\u7387\uff0c\u4e3aRNN\u52a0\u901f\u5668\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u601d\u8def"}}
{"id": "2512.07401", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.07401", "abs": "https://arxiv.org/abs/2512.07401", "authors": ["Sadaf Ehtesabi", "Manoar Hossain", "Tobias Kenter", "Andreas Krawinkel", "Holger Nitsche", "Lukas Ostermann", "Christian Plessl", "Heinrich Riebler", "Stefan Rohde", "Robert Schade", "Michael Schwarz", "Jens Simon", "Nils Winnwa", "Alex Wiens", "Xin Wu"], "title": "Otus Supercomputer", "comment": null, "summary": "Otus is a high-performance computing cluster that was launched in 2025 and is operated by the Paderborn Center for Parallel Computing (PC2) at Paderborn University in Germany. The system is part of the National High Performance Computing (NHR) initiative. Otus complements the previous supercomputer Noctua 2, offering approximately twice the computing power while retaining the three node types that were characteristic of Noctua 2: 1) CPU compute nodes with different memory capacities, 2) high-end GPU nodes, and 3) HPC-grade FPGA nodes. On the Top500 list, which ranks the 500 most powerful supercomputers in the world, Otus is in position 164 with the CPU partition and in position 255 with the GPU partition (June 2025). On the Green500 list, ranking the 500 most energy-efficient supercomputers in the world, Otus is in position 5 with the GPU partition (June 2025).\n  This article provides a comprehensive overview of the system in terms of its hardware, software, system integration, and its overall integration into the data center building to ensure energy-efficient operation. The article aims to provide unique insights for scientists using the system and for other centers operating HPC clusters. The article will be continuously updated to reflect the latest system setup and measurements.", "AI": {"tldr": "Otus\u662f\u5fb7\u56fd\u5e15\u5fb7\u535a\u6069\u5927\u5b66PC2\u4e2d\u5fc3\u4e8e2025\u5e74\u63a8\u51fa\u7684\u9ad8\u6027\u80fd\u8ba1\u7b97\u96c6\u7fa4\uff0c\u4f5c\u4e3aNHR\u8ba1\u5212\u7684\u4e00\u90e8\u5206\uff0c\u8865\u5145\u4e86Noctua 2\u7cfb\u7edf\uff0c\u63d0\u4f9b\u7ea6\u4e24\u500d\u8ba1\u7b97\u80fd\u529b\uff0c\u5728Top500\u4e2dCPU\u5206\u533a\u6392\u540d164\u4f4d\uff0cGPU\u5206\u533a\u6392\u540d255\u4f4d\uff0c\u5728Green500\u4e2dGPU\u5206\u533a\u6392\u540d\u7b2c5\u4f4d\u3002", "motivation": "\u672c\u6587\u65e8\u5728\u4e3aOtus\u8d85\u7ea7\u8ba1\u7b97\u673a\u63d0\u4f9b\u5168\u9762\u7684\u7cfb\u7edf\u6982\u8ff0\uff0c\u5305\u62ec\u786c\u4ef6\u3001\u8f6f\u4ef6\u3001\u7cfb\u7edf\u96c6\u6210\u548c\u80fd\u6548\u8bbe\u8ba1\uff0c\u4e3a\u4f7f\u7528\u8be5\u7cfb\u7edf\u7684\u79d1\u5b66\u5bb6\u548c\u8fd0\u8425HPC\u96c6\u7fa4\u7684\u5176\u4ed6\u4e2d\u5fc3\u63d0\u4f9b\u72ec\u7279\u89c1\u89e3\u3002", "method": "\u6587\u7ae0\u91c7\u7528\u7cfb\u7edf\u63cf\u8ff0\u6027\u65b9\u6cd5\uff0c\u8be6\u7ec6\u4ecb\u7ecd\u4e86Otus\u96c6\u7fa4\u7684\u786c\u4ef6\u914d\u7f6e\uff08\u5305\u62ec\u4e09\u79cd\u8282\u70b9\u7c7b\u578b\uff1aCPU\u8ba1\u7b97\u8282\u70b9\u3001\u9ad8\u7aefGPU\u8282\u70b9\u548cHPC\u7ea7FPGA\u8282\u70b9\uff09\u3001\u8f6f\u4ef6\u73af\u5883\u3001\u7cfb\u7edf\u96c6\u6210\u7b56\u7565\u4ee5\u53ca\u6570\u636e\u4e2d\u5fc3\u80fd\u6548\u4f18\u5316\u8bbe\u8ba1\u3002", "result": "Otus\u5728\u6027\u80fd\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff1aCPU\u5206\u533a\u5728Top500\u4e2d\u6392\u540d164\u4f4d\uff0cGPU\u5206\u533a\u6392\u540d255\u4f4d\uff1b\u5728\u80fd\u6548\u65b9\u9762\uff0cGPU\u5206\u533a\u5728Green500\u4e2d\u6392\u540d\u7b2c5\u4f4d\uff0c\u5c55\u73b0\u4e86\u5353\u8d8a\u7684\u80fd\u6548\u8868\u73b0\u3002", "conclusion": "Otus\u4f5c\u4e3aNoctua 2\u7684\u8865\u5145\u7cfb\u7edf\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u7ea6\u4e24\u500d\u7684\u8ba1\u7b97\u80fd\u529b\u63d0\u5347\uff0c\u540c\u65f6\u5728\u80fd\u6548\u65b9\u9762\u8fbe\u5230\u4e16\u754c\u9886\u5148\u6c34\u5e73\uff0c\u4e3a\u79d1\u5b66\u8ba1\u7b97\u63d0\u4f9b\u4e86\u9ad8\u6027\u80fd\u4e14\u8282\u80fd\u7684\u8ba1\u7b97\u5e73\u53f0\uff0c\u6587\u7ae0\u5c06\u6301\u7eed\u66f4\u65b0\u4ee5\u53cd\u6620\u6700\u65b0\u7684\u7cfb\u7edf\u914d\u7f6e\u548c\u6d4b\u91cf\u6570\u636e\u3002"}}
{"id": "2512.06537", "categories": ["cs.AR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.06537", "abs": "https://arxiv.org/abs/2512.06537", "authors": ["A. M. H. H. Alahakoon", "Hassaan Saadat", "Darshana Jayasinghe", "Sri Parameswaran"], "title": "Approximate Multiplier Induced Error Propagation in Deep Neural Networks", "comment": "7 pages, Submitted to Design and Automation Conference (DAC 2026)", "summary": "Deep Neural Networks (DNNs) rely heavily on dense arithmetic operations, motivating the use of Approximate Multipliers (AxMs) to reduce energy consumption in hardware accelerators. However, a rigorous mathematical characterization of how AxMs error distributions influence DNN accuracy remains underdeveloped. This work presents an analytical framework that connects the statistical error moments of an AxM to the induced distortion in General Matrix Multiplication (GEMM). Using the Frobenius norm of the resulting error matrix, we derive a closed form expression for practical DNN dimensions that demonstrates the distortion is predominantly governed by the multiplier mean error (bias). To evaluate this model in realistic settings, we incorporate controlled error injection into GEMM and convolution layers and examine its effect on ImageNet scale networks. The predicted distortion correlates strongly with the observed accuracy degradation, and an error configurable AxM case study implemented on an FPGA further confirms the analytical trends. By providing a lightweight alternative to behavioral or hardware level simulations, this framework enables rapid estimation of AxM impact on DNN inference quality.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5206\u6790\u6846\u67b6\uff0c\u5c06\u8fd1\u4f3c\u4e58\u6cd5\u5668\u7684\u7edf\u8ba1\u8bef\u5dee\u7279\u6027\u4e0eDNN\u7cbe\u5ea6\u635f\u5931\u5173\u8054\u8d77\u6765\uff0c\u53d1\u73b0\u8bef\u5dee\u5747\u503c\uff08\u504f\u7f6e\uff09\u662f\u5f71\u54cd\u7cbe\u5ea6\u7684\u4e3b\u8981\u56e0\u7d20\u3002", "motivation": "DNN\u4e25\u91cd\u4f9d\u8d56\u5bc6\u96c6\u7b97\u672f\u8fd0\u7b97\uff0c\u8fd1\u4f3c\u4e58\u6cd5\u5668\u53ef\u964d\u4f4e\u786c\u4ef6\u80fd\u8017\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u5176\u8bef\u5dee\u5206\u5e03\u5982\u4f55\u5f71\u54cdDNN\u7cbe\u5ea6\u7684\u4e25\u683c\u6570\u5b66\u5206\u6790\u3002", "method": "\u5f00\u53d1\u5206\u6790\u6846\u67b6\u8fde\u63a5AxM\u8bef\u5dee\u7edf\u8ba1\u77e9\u4e0eGEMM\u5931\u771f\uff0c\u4f7f\u7528Frobenius\u8303\u6570\u63a8\u5bfc\u95ed\u5f0f\u8868\u8fbe\u5f0f\uff0c\u901a\u8fc7\u53d7\u63a7\u8bef\u5dee\u6ce8\u5165\u5230GEMM\u548c\u5377\u79ef\u5c42\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u9884\u6d4b\u7684\u5931\u771f\u4e0e\u89c2\u6d4b\u7684\u7cbe\u5ea6\u4e0b\u964d\u5f3a\u76f8\u5173\uff0cFPGA\u4e0a\u53ef\u914d\u7f6e\u8bef\u5dee\u7684AxM\u6848\u4f8b\u7814\u7a76\u8bc1\u5b9e\u4e86\u5206\u6790\u8d8b\u52bf\uff0c\u8be5\u6846\u67b6\u4e3a\u786c\u4ef6\u6a21\u62df\u63d0\u4f9b\u4e86\u8f7b\u91cf\u7ea7\u66ff\u4ee3\u65b9\u6848\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u5feb\u901f\u8bc4\u4f30\u8fd1\u4f3c\u4e58\u6cd5\u5668\u5bf9DNN\u63a8\u7406\u8d28\u91cf\u7684\u5f71\u54cd\uff0c\u8bef\u5dee\u5747\u503c\u662f\u5f71\u54cd\u7cbe\u5ea6\u7684\u4e3b\u8981\u56e0\u7d20\uff0c\u4e3a\u786c\u4ef6\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u7406\u8bba\u6307\u5bfc\u3002"}}
{"id": "2512.07536", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.07536", "abs": "https://arxiv.org/abs/2512.07536", "authors": ["Yipeng Shen", "Zehan Zhu", "Yan Huang", "Changzhi Yan", "Cheng Zhuo", "Jinming Xu"], "title": "Bandwidth-Aware Network Topology Optimization for Decentralized Learning", "comment": "13 pages", "summary": "Network topology is critical for efficient parameter synchronization in distributed learning over networks. However, most existing studies do not account for bandwidth limitations in network topology design. In this paper, we propose a bandwidth-aware network topology optimization framework to maximize consensus speed under edge cardinality constraints. For heterogeneous bandwidth scenarios, we introduce a maximum bandwidth allocation strategy for the edges to ensure efficient communication among nodes. By reformulating the problem into an equivalent Mixed-Integer SDP problem, we leverage a computationally efficient ADMM-based method to obtain topologies that yield the maximum consensus speed. Within the ADMM substep, we adopt the conjugate gradient method to efficiently solve large-scale linear equations to achieve better scalability. Experimental results demonstrate that the resulting network topologies outperform the benchmark topologies in terms of consensus speed, and reduce the training time required for decentralized learning tasks on real-world datasets to achieve the target test accuracy, exhibiting speedups of more than $1.11\\times$ and $1.21\\times$ for homogeneous and heterogeneous bandwidth settings, respectively.", "AI": {"tldr": "\u63d0\u51fa\u5e26\u5bbd\u611f\u77e5\u7684\u7f51\u7edc\u62d3\u6251\u4f18\u5316\u6846\u67b6\uff0c\u5728\u8fb9\u6570\u7ea6\u675f\u4e0b\u6700\u5927\u5316\u5171\u8bc6\u901f\u5ea6\uff0c\u901a\u8fc7ADMM\u65b9\u6cd5\u6c42\u89e3\uff0c\u5b9e\u9a8c\u663e\u793a\u5728\u53bb\u4e2d\u5fc3\u5316\u5b66\u4e60\u4e2d\u663e\u8457\u52a0\u901f\u8bad\u7ec3", "motivation": "\u73b0\u6709\u7f51\u7edc\u62d3\u6251\u8bbe\u8ba1\u5927\u591a\u5ffd\u7565\u5e26\u5bbd\u9650\u5236\uff0c\u800c\u5e26\u5bbd\u5bf9\u5206\u5e03\u5f0f\u5b66\u4e60\u4e2d\u7684\u53c2\u6570\u540c\u6b65\u6548\u7387\u81f3\u5173\u91cd\u8981\u3002\u9700\u8981\u8bbe\u8ba1\u8003\u8651\u5e26\u5bbd\u7ea6\u675f\u7684\u62d3\u6251\u4f18\u5316\u65b9\u6cd5\u4ee5\u63d0\u9ad8\u5171\u8bc6\u901f\u5ea6\u3002", "method": "\u63d0\u51fa\u5e26\u5bbd\u611f\u77e5\u7f51\u7edc\u62d3\u6251\u4f18\u5316\u6846\u67b6\uff0c\u5c06\u95ee\u9898\u91cd\u6784\u4e3a\u6df7\u5408\u6574\u6570\u534a\u5b9a\u89c4\u5212\u95ee\u9898\uff0c\u91c7\u7528ADMM\u65b9\u6cd5\u9ad8\u6548\u6c42\u89e3\u3002\u5728ADMM\u5b50\u6b65\u9aa4\u4e2d\u4f7f\u7528\u5171\u8f6d\u68af\u5ea6\u6cd5\u89e3\u51b3\u5927\u89c4\u6a21\u7ebf\u6027\u65b9\u7a0b\u4ee5\u63d0\u9ad8\u53ef\u6269\u5c55\u6027\u3002\u9488\u5bf9\u5f02\u6784\u5e26\u5bbd\u573a\u666f\u5f15\u5165\u6700\u5927\u5e26\u5bbd\u5206\u914d\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u751f\u6210\u7684\u7f51\u7edc\u62d3\u6251\u5728\u5171\u8bc6\u901f\u5ea6\u4e0a\u4f18\u4e8e\u57fa\u51c6\u62d3\u6251\uff0c\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u53bb\u4e2d\u5fc3\u5316\u5b66\u4e60\u4efb\u52a1\u4e2d\uff0c\u4e3a\u8fbe\u5230\u76ee\u6807\u6d4b\u8bd5\u7cbe\u5ea6\uff0c\u540c\u6784\u548c\u5f02\u6784\u5e26\u5bbd\u8bbe\u7f6e\u4e0b\u7684\u8bad\u7ec3\u65f6\u95f4\u5206\u522b\u51cf\u5c11\u8d85\u8fc71.11\u500d\u548c1.21\u500d\u3002", "conclusion": "\u5e26\u5bbd\u611f\u77e5\u7684\u7f51\u7edc\u62d3\u6251\u4f18\u5316\u80fd\u663e\u8457\u63d0\u9ad8\u5206\u5e03\u5f0f\u5b66\u4e60\u7684\u53c2\u6570\u540c\u6b65\u6548\u7387\uff0cADMM\u65b9\u6cd5\u7ed3\u5408\u5171\u8f6d\u68af\u5ea6\u6cd5\u53ef\u6709\u6548\u89e3\u51b3\u5927\u89c4\u6a21\u62d3\u6251\u4f18\u5316\u95ee\u9898\uff0c\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5e26\u6765\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2512.06854", "categories": ["cs.AR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.06854", "abs": "https://arxiv.org/abs/2512.06854", "authors": ["Qijun Zhang", "Yao Lu", "Mengming Li", "Shang Liu", "Zhiyao Xie"], "title": "ArchPower: Dataset for Architecture-Level Power Modeling of Modern CPU Design", "comment": "Published in NeurIPS'25 Dataset and Benchmark Track", "summary": "Power is the primary design objective of large-scale integrated circuits (ICs), especially for complex modern processors (i.e., CPUs). Accurate CPU power evaluation requires designers to go through the whole time-consuming IC implementation process, easily taking months. At the early design stage (e.g., architecture-level), classical power models are notoriously inaccurate. Recently, ML-based architecture-level power models have been proposed to boost accuracy, but the data availability is a severe challenge. Currently, there is no open-source dataset for this important ML application. A typical dataset generation process involves correct CPU design implementation and repetitive execution of power simulation flows, requiring significant design expertise, engineering effort, and execution time. Even private in-house datasets often fail to reflect realistic CPU design scenarios. In this work, we propose ArchPower, the first open-source dataset for architecture-level processor power modeling. We go through complex and realistic design flows to collect the CPU architectural information as features and the ground-truth simulated power as labels. Our dataset includes 200 CPU data samples, collected from 25 different CPU configurations when executing 8 different workloads. There are more than 100 architectural features in each data sample, including both hardware and event parameters. The label of each sample provides fine-grained power information, including the total design power and the power for each of the 11 components. Each power value is further decomposed into four fine-grained power groups: combinational logic power, sequential logic power, memory power, and clock power. ArchPower is available at https://github.com/hkust-zhiyao/ArchPower.", "AI": {"tldr": "ArchPower\u662f\u9996\u4e2a\u7528\u4e8e\u67b6\u6784\u7ea7\u5904\u7406\u5668\u529f\u8017\u5efa\u6a21\u7684\u5f00\u6e90\u6570\u636e\u96c6\uff0c\u5305\u542b200\u4e2aCPU\u6570\u636e\u6837\u672c\uff0c\u6db5\u76d625\u79cdCPU\u914d\u7f6e\u548c8\u79cd\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u63d0\u4f9b\u8d85\u8fc7100\u4e2a\u67b6\u6784\u7279\u5f81\u548c\u7ec6\u7c92\u5ea6\u529f\u8017\u6807\u7b7e\u3002", "motivation": "\u5f53\u524dCPU\u529f\u8017\u8bc4\u4f30\u9700\u8981\u8017\u65f6\u7684IC\u5b9e\u73b0\u8fc7\u7a0b\uff0c\u65e9\u671f\u8bbe\u8ba1\u9636\u6bb5\u7684\u4f20\u7edf\u529f\u8017\u6a21\u578b\u4e0d\u51c6\u786e\uff0c\u800c\u57fa\u4e8eML\u7684\u67b6\u6784\u7ea7\u529f\u8017\u6a21\u578b\u7f3a\u4e4f\u5f00\u6e90\u6570\u636e\u96c6\u3002\u73b0\u6709\u6570\u636e\u96c6\u751f\u6210\u8fc7\u7a0b\u590d\u6742\u8017\u65f6\uff0c\u4e14\u5f80\u5f80\u4e0d\u80fd\u53cd\u6620\u771f\u5b9e\u7684CPU\u8bbe\u8ba1\u573a\u666f\u3002", "method": "\u901a\u8fc7\u590d\u6742\u4e14\u771f\u5b9e\u7684\u8bbe\u8ba1\u6d41\u7a0b\u6536\u96c6CPU\u67b6\u6784\u4fe1\u606f\u4f5c\u4e3a\u7279\u5f81\uff0c\u4f7f\u7528\u4eff\u771f\u529f\u8017\u4f5c\u4e3a\u771f\u5b9e\u6807\u7b7e\u3002\u6570\u636e\u96c6\u5305\u542b200\u4e2aCPU\u6570\u636e\u6837\u672c\uff0c\u6765\u81ea25\u79cd\u4e0d\u540cCPU\u914d\u7f6e\u6267\u884c8\u79cd\u5de5\u4f5c\u8d1f\u8f7d\u7684\u60c5\u51b5\uff0c\u6bcf\u4e2a\u6837\u672c\u6709\u8d85\u8fc7100\u4e2a\u67b6\u6784\u7279\u5f81\u3002", "result": "\u521b\u5efa\u4e86ArchPower\u5f00\u6e90\u6570\u636e\u96c6\uff0c\u63d0\u4f9b\u7ec6\u7c92\u5ea6\u529f\u8017\u4fe1\u606f\uff1a\u5305\u62ec\u603b\u8bbe\u8ba1\u529f\u8017\u548c11\u4e2a\u7ec4\u4ef6\u7684\u529f\u8017\uff0c\u6bcf\u4e2a\u529f\u8017\u503c\u8fdb\u4e00\u6b65\u5206\u89e3\u4e3a\u7ec4\u5408\u903b\u8f91\u529f\u8017\u3001\u65f6\u5e8f\u903b\u8f91\u529f\u8017\u3001\u5b58\u50a8\u5668\u529f\u8017\u548c\u65f6\u949f\u529f\u8017\u56db\u7c7b\u3002", "conclusion": "ArchPower\u586b\u8865\u4e86\u67b6\u6784\u7ea7\u5904\u7406\u5668\u529f\u8017\u5efa\u6a21\u9886\u57df\u5f00\u6e90\u6570\u636e\u96c6\u7684\u7a7a\u767d\uff0c\u4e3aML-based\u529f\u8017\u6a21\u578b\u7814\u7a76\u63d0\u4f9b\u4e86\u9ad8\u8d28\u91cf\u7684\u771f\u5b9e\u6570\u636e\u96c6\uff0c\u6709\u52a9\u4e8e\u63d0\u5347\u65e9\u671f\u8bbe\u8ba1\u9636\u6bb5\u7684\u529f\u8017\u8bc4\u4f30\u51c6\u786e\u6027\u3002"}}
{"id": "2512.07750", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.07750", "abs": "https://arxiv.org/abs/2512.07750", "authors": ["Roozbeh Bostandoost", "Pooria Namyar", "Siva Kesava Reddy Kakarla", "Ryan Beckett", "Santiago Segarra", "Eli Cortez", "Ankur Mallick", "Kevin Hsieh", "Rodrigo Fonseca", "Mohammad Hajiesmaili", "Behnaz Arzani"], "title": "A Performance Analyzer for a Public Cloud's ML-Augmented VM Allocator", "comment": null, "summary": "Many operational cloud systems use one or more machine learning models that help them achieve better efficiency and performance. But operators do not have tools to help them understand how each model and the interaction between them affect the end-to-end system performance. SANJESH is such a tool. SANJESH supports a diverse set of performance-related queries which we answer through a bi-level optimization. We invent novel mechanisms to solve this optimization more quickly. These techniques allow us to solve an optimization which prior work failed to solve even after $24$ hours.\n  As a proof of concept, we apply SANJESH to an example production system that uses multiple ML models to optimize virtual machine (VM) placement. These models impact how many servers the operators uses to host VMs and the frequency with which it has to live-migrate them because the servers run out of resources. SANJESH finds scenarios where these models cause $~4\\times$ worse performance than what simulation-based approaches detect.", "AI": {"tldr": "SANJESH\u662f\u4e00\u4e2a\u5e2e\u52a9\u4e91\u7cfb\u7edf\u64cd\u4f5c\u5458\u7406\u89e3\u591a\u4e2a\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5982\u4f55\u5f71\u54cd\u7aef\u5230\u7aef\u7cfb\u7edf\u6027\u80fd\u7684\u5de5\u5177\uff0c\u901a\u8fc7\u53cc\u5c42\u4f18\u5316\u652f\u6301\u591a\u79cd\u6027\u80fd\u67e5\u8be2\uff0c\u5e76\u80fd\u5feb\u901f\u89e3\u51b3\u5148\u524d\u5de5\u4f5c\u65e0\u6cd5\u572824\u5c0f\u65f6\u5185\u89e3\u51b3\u7684\u4f18\u5316\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u4e91\u7cfb\u7edf\u4f7f\u7528\u591a\u4e2a\u673a\u5668\u5b66\u4e60\u6a21\u578b\u6765\u63d0\u9ad8\u6548\u7387\u548c\u6027\u80fd\uff0c\u4f46\u64cd\u4f5c\u5458\u7f3a\u4e4f\u5de5\u5177\u6765\u7406\u89e3\u6bcf\u4e2a\u6a21\u578b\u4ee5\u53ca\u6a21\u578b\u4e4b\u95f4\u7684\u4ea4\u4e92\u5982\u4f55\u5f71\u54cd\u7aef\u5230\u7aef\u7cfb\u7edf\u6027\u80fd\u3002", "method": "SANJESH\u901a\u8fc7\u53cc\u5c42\u4f18\u5316\u6765\u56de\u7b54\u6027\u80fd\u76f8\u5173\u67e5\u8be2\uff0c\u5e76\u53d1\u660e\u4e86\u65b0\u9896\u7684\u673a\u5236\u6765\u52a0\u901f\u4f18\u5316\u6c42\u89e3\u8fc7\u7a0b\u3002", "result": "SANJESH\u80fd\u591f\u89e3\u51b3\u5148\u524d\u5de5\u4f5c\u65e0\u6cd5\u572824\u5c0f\u65f6\u5185\u89e3\u51b3\u7684\u4f18\u5316\u95ee\u9898\u3002\u5728\u865a\u62df\u673a\u653e\u7f6e\u7684\u6848\u4f8b\u4e2d\uff0cSANJESH\u53d1\u73b0\u4e86\u8fd9\u4e9b\u6a21\u578b\u5bfc\u81f4\u6bd4\u57fa\u4e8e\u6a21\u62df\u65b9\u6cd5\u68c0\u6d4b\u5230\u7684\u6027\u80fd\u5dee\u7ea64\u500d\u7684\u60c5\u51b5\u3002", "conclusion": "SANJESH\u662f\u4e00\u4e2a\u6709\u6548\u7684\u5de5\u5177\uff0c\u80fd\u591f\u5e2e\u52a9\u4e91\u7cfb\u7edf\u64cd\u4f5c\u5458\u7406\u89e3\u591a\u4e2a\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5bf9\u7cfb\u7edf\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5e76\u80fd\u8bc6\u522b\u51fa\u4f20\u7edf\u65b9\u6cd5\u65e0\u6cd5\u53d1\u73b0\u7684\u6027\u80fd\u95ee\u9898\u3002"}}
{"id": "2512.07312", "categories": ["cs.AR", "cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2512.07312", "abs": "https://arxiv.org/abs/2512.07312", "authors": ["Zhongchun Zhou", "Chengtao Lai", "Yuhang Gu", "Wei Zhang"], "title": "DCO: Dynamic Cache Orchestration for LLM Accelerators through Predictive Management", "comment": "\\c{opyright} 2025 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works", "summary": "The rapid adoption of large language models (LLMs) is pushing AI accelerators toward increasingly powerful and specialized designs. Instead of further complicating software development with deeply hierarchical scratchpad memories (SPMs) and their asynchronous management, we investigate the opposite point of the design spectrum: a multi-core AI accelerator equipped with a shared system-level cache and application-aware management policies, which keeps the programming effort modest. Our approach exploits dataflow information available in the software stack to guide cache replacement (including dead-block prediction), in concert with bypass decisions and mechanisms that alleviate cache thrashing.\n  We assess the proposal using a cycle-accurate simulator and observe substantial performance gains (up to 1.80x speedup) compared with conventional cache architectures. In addition, we build and validate an analytical model that takes into account the actual overlapping behaviors to extend the measurement results of our policies to real-world larger-scale workloads. Experiment results show that when functioning together, our bypassing and thrashing mitigation strategies can handle scenarios both with and without inter-core data sharing and achieve remarkable speedups.\n  Finally, we implement the design in RTL and the area of our design is $\\mathbf{0.064mm^2}$ with 15nm process, which can run at 2 GHz clock frequency. Our findings explore the potential of the shared cache design to assist the development of future AI accelerator systems.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9762\u5411\u5927\u8bed\u8a00\u6a21\u578b\u52a0\u901f\u5668\u7684\u5171\u4eab\u7cfb\u7edf\u7ea7\u7f13\u5b58\u67b6\u6784\uff0c\u901a\u8fc7\u5e94\u7528\u611f\u77e5\u7684\u7ba1\u7406\u7b56\u7565\uff08\u5305\u62ec\u7f13\u5b58\u66ff\u6362\u3001\u6b7b\u5757\u9884\u6d4b\u548c\u65c1\u8def\u51b3\u7b56\uff09\u6765\u7b80\u5316\u7f16\u7a0b\u5e76\u63d0\u5347\u6027\u80fd\uff0c\u76f8\u6bd4\u4f20\u7edf\u7f13\u5b58\u67b6\u6784\u5b9e\u73b0\u4e86\u6700\u9ad81.8\u500d\u7684\u52a0\u901f\u3002", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5feb\u901f\u91c7\u7528\uff0cAI\u52a0\u901f\u5668\u8bbe\u8ba1\u8d8b\u5411\u4e8e\u66f4\u5f3a\u5927\u548c\u4e13\u4e1a\u5316\uff0c\u4f46\u590d\u6742\u7684\u5c42\u6b21\u5316\u6682\u5b58\u5668\u5185\u5b58\u53ca\u5176\u5f02\u6b65\u7ba1\u7406\u589e\u52a0\u4e86\u8f6f\u4ef6\u5f00\u53d1\u96be\u5ea6\u3002\u672c\u6587\u63a2\u7d22\u76f8\u53cd\u7684\u8bbe\u8ba1\u65b9\u5411\uff1a\u91c7\u7528\u5171\u4eab\u7cfb\u7edf\u7ea7\u7f13\u5b58\u548c\u7b80\u5355\u7f16\u7a0b\u6a21\u578b\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u591a\u6838AI\u52a0\u901f\u5668\u67b6\u6784\uff0c\u914d\u5907\u5171\u4eab\u7cfb\u7edf\u7ea7\u7f13\u5b58\u548c\u5e94\u7528\u611f\u77e5\u7ba1\u7406\u7b56\u7565\u3002\u5229\u7528\u8f6f\u4ef6\u6808\u4e2d\u7684\u6570\u636e\u6d41\u4fe1\u606f\u6307\u5bfc\u7f13\u5b58\u66ff\u6362\uff08\u5305\u62ec\u6b7b\u5757\u9884\u6d4b\uff09\uff0c\u7ed3\u5408\u65c1\u8def\u51b3\u7b56\u548c\u7f13\u89e3\u7f13\u5b58\u98a0\u7c38\u7684\u673a\u5236\u3002\u901a\u8fc7\u5468\u671f\u7cbe\u786e\u6a21\u62df\u5668\u8bc4\u4f30\uff0c\u5efa\u7acb\u8003\u8651\u5b9e\u9645\u91cd\u53e0\u884c\u4e3a\u7684\u5206\u6790\u6a21\u578b\uff0c\u5e76\u5728RTL\u4e2d\u5b9e\u73b0\u8bbe\u8ba1\u3002", "result": "\u76f8\u6bd4\u4f20\u7edf\u7f13\u5b58\u67b6\u6784\uff0c\u5b9e\u73b0\u4e86\u6700\u9ad81.80\u500d\u7684\u6027\u80fd\u63d0\u5347\u3002\u65c1\u8def\u548c\u98a0\u7c38\u7f13\u89e3\u7b56\u7565\u80fd\u6709\u6548\u5904\u7406\u6709\u65e0\u6838\u95f4\u6570\u636e\u5171\u4eab\u7684\u573a\u666f\u3002RTL\u5b9e\u73b0\u9762\u79ef0.064mm\u00b2\uff0815nm\u5de5\u827a\uff09\uff0c\u53ef\u8fd0\u884c\u57282GHz\u65f6\u949f\u9891\u7387\u3002\u5206\u6790\u6a21\u578b\u6210\u529f\u5c06\u6d4b\u91cf\u7ed3\u679c\u6269\u5c55\u5230\u66f4\u5927\u89c4\u6a21\u5de5\u4f5c\u8d1f\u8f7d\u3002", "conclusion": "\u5171\u4eab\u7f13\u5b58\u8bbe\u8ba1\u5c55\u793a\u4e86\u7b80\u5316AI\u52a0\u901f\u5668\u7f16\u7a0b\u540c\u65f6\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u6f5c\u529b\uff0c\u4e3a\u672a\u6765AI\u52a0\u901f\u5668\u7cfb\u7edf\u5f00\u53d1\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u5411\uff0c\u5e73\u8861\u4e86\u7f16\u7a0b\u590d\u6742\u5ea6\u548c\u6027\u80fd\u9700\u6c42\u3002"}}
{"id": "2512.07792", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.07792", "abs": "https://arxiv.org/abs/2512.07792", "authors": ["Animesh Dangwal", "Yufeng Jiang", "Charlie Arnold", "Jun Fan", "Mohamed Bassem", "Aish Rajagopal"], "title": "Designing Co-operation in Systems of Hierarchical, Multi-objective Schedulers for Stream Processing", "comment": null, "summary": "Stream processing is a computing paradigm that supports real-time data processing for a wide variety of applications. At Meta, it's used across the company for various tasks such as deriving product insights, providing and improving user services, and enabling AI at scale for our ever-growing user base. Meta's current stream processing framework supports processing TerraBytes(TBs) of data in mere seconds. This is enabled by our efficient schedulers and multi-layered infrastructure, which allocate workloads across various compute resources, working together in hierarchies across various parts of the infrastructure. But with the ever growing complexity of applications, and user needs, areas of the infrastructure that previously required minimal load balancing, now must be made more robust and proactive to application load. In our work we explore how to build and design such a system that focuses on load balancing over key compute resources and properties of these applications. We also showcase how to integrate new schedulers into the hierarchy of the existing ones, allowing multiple schedulers to work together and perform load balancing, at their infrastructure level, effectively.", "AI": {"tldr": "Meta\u5f00\u53d1\u4e86\u4e00\u4e2a\u7528\u4e8e\u5927\u89c4\u6a21\u6d41\u5904\u7406\u7cfb\u7edf\u7684\u5206\u5c42\u8d1f\u8f7d\u5747\u8861\u6846\u67b6\uff0c\u901a\u8fc7\u96c6\u6210\u65b0\u8c03\u5ea6\u5668\u5230\u73b0\u6709\u5c42\u6b21\u7ed3\u6784\u4e2d\uff0c\u5b9e\u73b0\u5bf9\u8ba1\u7b97\u8d44\u6e90\u7684\u4e3b\u52a8\u8d1f\u8f7d\u5747\u8861", "motivation": "\u968f\u7740\u5e94\u7528\u590d\u6742\u6027\u548c\u7528\u6237\u9700\u6c42\u7684\u589e\u957f\uff0cMeta\u73b0\u6709\u7684\u6d41\u5904\u7406\u6846\u67b6\u9700\u8981\u66f4\u9c81\u68d2\u548c\u4e3b\u52a8\u7684\u8d1f\u8f7d\u5747\u8861\u673a\u5236\u6765\u5904\u7406\u65e5\u76ca\u589e\u957f\u7684\u6570\u636e\u91cf\u548c\u8ba1\u7b97\u9700\u6c42", "method": "\u8bbe\u8ba1\u548c\u6784\u5efa\u4e13\u6ce8\u4e8e\u5173\u952e\u8ba1\u7b97\u8d44\u6e90\u548c\u5e94\u7528\u5c5e\u6027\u7684\u8d1f\u8f7d\u5747\u8861\u7cfb\u7edf\uff0c\u5c06\u65b0\u8c03\u5ea6\u5668\u96c6\u6210\u5230\u73b0\u6709\u8c03\u5ea6\u5668\u5c42\u6b21\u7ed3\u6784\u4e2d\uff0c\u5b9e\u73b0\u591a\u8c03\u5ea6\u5668\u534f\u540c\u5de5\u4f5c", "result": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u80fd\u591f\u5904\u7406TB\u7ea7\u6570\u636e\u7684\u5206\u5c42\u8d1f\u8f7d\u5747\u8861\u7cfb\u7edf\uff0c\u652f\u6301\u591a\u8c03\u5ea6\u5668\u5728\u5404\u81ea\u57fa\u7840\u8bbe\u65bd\u5c42\u9762\u6709\u6548\u6267\u884c\u8d1f\u8f7d\u5747\u8861", "conclusion": "\u901a\u8fc7\u5206\u5c42\u8c03\u5ea6\u5668\u67b6\u6784\u548c\u4e3b\u52a8\u8d1f\u8f7d\u5747\u8861\u673a\u5236\uff0cMeta\u80fd\u591f\u5e94\u5bf9\u4e0d\u65ad\u589e\u957f\u7684\u5e94\u7528\u590d\u6742\u6027\uff0c\u786e\u4fdd\u5927\u89c4\u6a21\u6d41\u5904\u7406\u7cfb\u7edf\u7684\u9ad8\u6548\u8fd0\u884c"}}
{"id": "2512.07520", "categories": ["cs.AR", "cs.CR", "cs.SC"], "pdf": "https://arxiv.org/pdf/2512.07520", "abs": "https://arxiv.org/abs/2512.07520", "authors": ["No\u00e9 Amiot", "Quentin L. Meunier", "Karine Heydemann", "Emmanuelle Encrenaz"], "title": "aLEAKator: HDL Mixed-Domain Simulation for Masked Hardware \\& Software Formal Verification", "comment": null, "summary": "Verifying the security of masked hardware and software implementations, under advanced leakage models, remains a significant challenge, especially then accounting for glitches, transitions and CPU micro-architectural specifics. Existing verification approaches are either restricted to small hardware gadgets, small programs on CPUs such as Sboxes, limited leakage models, or require hardware-specific prior knowledge. In this work, we present aLEAKator, an open-source framework for the automated formal verification of masked cryptographic accelerators and software running on CPUs from their HDL descriptions. Our method introduces mixed-domain simulation, enabling precise modeling and verification under various (including robust and relaxed) 1-probing leakage models, and supports variable signal granularity without being restricted to 1-bit wires. aLEAKator also supports verification in the presence of lookup tables, and does not require prior knowledge of the target CPU architecture. Our approach is validated against existing tools and real-world measurements while providing innovative results such as the verification of a full, first-order masked AES on various CPUs", "AI": {"tldr": "aLEAKator\uff1a\u9996\u4e2a\u5f00\u6e90\u6846\u67b6\uff0c\u7528\u4e8e\u4eceHDL\u63cf\u8ff0\u81ea\u52a8\u5f62\u5f0f\u5316\u9a8c\u8bc1\u63a9\u7801\u52a0\u5bc6\u52a0\u901f\u5668\u548cCPU\u8f6f\u4ef6\uff0c\u652f\u6301\u591a\u79cd\u6cc4\u6f0f\u6a21\u578b\u548c\u53ef\u53d8\u4fe1\u53f7\u7c92\u5ea6", "motivation": "\u73b0\u6709\u9a8c\u8bc1\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff1a\u8981\u4e48\u4ec5\u9650\u4e8e\u5c0f\u578b\u786c\u4ef6\u6a21\u5757\u6216CPU\u4e0a\u7684\u5c0f\u7a0b\u5e8f\uff08\u5982S\u76d2\uff09\uff0c\u8981\u4e48\u53d7\u9650\u4e8e\u6cc4\u6f0f\u6a21\u578b\uff0c\u6216\u9700\u8981\u786c\u4ef6\u7279\u5b9a\u5148\u9a8c\u77e5\u8bc6\u3002\u5728\u8003\u8651\u6bdb\u523a\u3001\u8f6c\u6362\u548cCPU\u5fae\u67b6\u6784\u7279\u6027\u7684\u9ad8\u7ea7\u6cc4\u6f0f\u6a21\u578b\u4e0b\uff0c\u9a8c\u8bc1\u63a9\u7801\u786c\u4ef6\u548c\u8f6f\u4ef6\u5b9e\u73b0\u7684\u5b89\u5168\u6027\u4ecd\u7136\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218", "method": "\u63d0\u51faaLEAKator\u6846\u67b6\uff0c\u91c7\u7528\u6df7\u5408\u57df\u4eff\u771f\u65b9\u6cd5\uff0c\u80fd\u591f\u7cbe\u786e\u5efa\u6a21\u548c\u9a8c\u8bc1\u5404\u79cd\uff08\u5305\u62ec\u9c81\u68d2\u548c\u5bbd\u677e\u7684\uff091\u63a2\u6d4b\u6cc4\u6f0f\u6a21\u578b\uff0c\u652f\u6301\u53ef\u53d8\u4fe1\u53f7\u7c92\u5ea6\u800c\u4e0d\u9650\u4e8e1\u4f4d\u7ebf\u3002\u652f\u6301\u67e5\u627e\u8868\u5b58\u5728\u65f6\u7684\u9a8c\u8bc1\uff0c\u4e14\u4e0d\u9700\u8981\u76ee\u6807CPU\u67b6\u6784\u7684\u5148\u9a8c\u77e5\u8bc6", "result": "\u9a8c\u8bc1\u4e86\u4e0e\u73b0\u6709\u5de5\u5177\u548c\u5b9e\u9645\u6d4b\u91cf\u7684\u4e00\u81f4\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u521b\u65b0\u6027\u7ed3\u679c\uff0c\u5982\u5728\u5404\u79cdCPU\u4e0a\u9a8c\u8bc1\u5b8c\u6574\u7684\u4e00\u9636\u63a9\u7801AES\u5b9e\u73b0", "conclusion": "aLEAKator\u662f\u4e00\u4e2a\u5f3a\u5927\u7684\u5f00\u6e90\u9a8c\u8bc1\u6846\u67b6\uff0c\u80fd\u591f\u514b\u670d\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u63a9\u7801\u52a0\u5bc6\u786c\u4ef6\u548c\u8f6f\u4ef6\u5b9e\u73b0\u63d0\u4f9b\u5168\u9762\u3001\u81ea\u52a8\u5316\u7684\u5f62\u5f0f\u5316\u9a8c\u8bc1\u80fd\u529b"}}
{"id": "2512.07799", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.07799", "abs": "https://arxiv.org/abs/2512.07799", "authors": ["Roozbeh Bostandoost", "Adam Lechowicz", "Walid A. Hanafy", "Prashant Shenoy", "Mohammad Hajiesmaili"], "title": "Quantifying the Carbon Reduction of DAG Workloads: A Job Shop Scheduling Perspective", "comment": null, "summary": "Carbon-aware schedulers aim to reduce the operational carbon footprint of data centers by running flexible workloads during periods of low carbon intensity. Most schedulers treat workloads as single monolithic tasks, ignoring that many jobs, like video encoding or offline inference, consist of smaller tasks with specific dependencies and resource needs; however, knowledge of this structure enables opportunities for greater carbon efficiency.\n  We quantify the maximum benefit of a dependency-aware approach for batch workloads. We model the problem as a flexible job-shop scheduling variant and use an offline solver to compute upper bounds on carbon and energy savings. Results show up to $25\\%$ lower carbon emissions on average without increasing the optimal makespan (total job completion time) compared to a makespan-only baseline. Although in heterogeneous server setup, these schedules may use more energy than energy-optimal ones. Our results also show that allowing twice the optimal makespan nearly doubles the carbon savings, underscoring the tension between carbon, energy, and makespan. We also highlight key factors such as job structure and server count influence the achievable carbon reductions.", "AI": {"tldr": "\u4f9d\u8d56\u611f\u77e5\u7684\u6279\u5904\u7406\u8c03\u5ea6\u5668\u76f8\u6bd4\u4f20\u7edf\u5355\u4efb\u52a1\u8c03\u5ea6\u5668\uff0c\u5728\u4fdd\u6301\u6700\u4f18\u5b8c\u5de5\u65f6\u95f4\u7684\u540c\u65f6\uff0c\u5e73\u5747\u53ef\u964d\u4f4e25%\u78b3\u6392\u653e\uff0c\u5141\u8bb8\u4e24\u500d\u5b8c\u5de5\u65f6\u95f4\u65f6\u78b3\u51cf\u6392\u6548\u679c\u7ffb\u500d\u3002", "motivation": "\u73b0\u6709\u78b3\u611f\u77e5\u8c03\u5ea6\u5668\u5927\u591a\u5c06\u5de5\u4f5c\u8d1f\u8f7d\u89c6\u4e3a\u5355\u4e00\u6574\u4f53\u4efb\u52a1\uff0c\u5ffd\u7565\u4e86\u89c6\u9891\u7f16\u7801\u3001\u79bb\u7ebf\u63a8\u7406\u7b49\u4f5c\u4e1a\u7531\u5177\u6709\u7279\u5b9a\u4f9d\u8d56\u5173\u7cfb\u548c\u8d44\u6e90\u9700\u6c42\u7684\u5c0f\u4efb\u52a1\u7ec4\u6210\uff0c\u8fd9\u79cd\u7ed3\u6784\u77e5\u8bc6\u4e3a\u63d0\u5347\u78b3\u6548\u7387\u63d0\u4f9b\u4e86\u673a\u4f1a\u3002", "method": "\u5c06\u95ee\u9898\u5efa\u6a21\u4e3a\u7075\u6d3b\u7684\u4f5c\u4e1a\u8f66\u95f4\u8c03\u5ea6\u53d8\u4f53\uff0c\u4f7f\u7528\u79bb\u7ebf\u6c42\u89e3\u5668\u8ba1\u7b97\u78b3\u548c\u80fd\u6e90\u8282\u7ea6\u7684\u4e0a\u9650\uff0c\u91cf\u5316\u4f9d\u8d56\u611f\u77e5\u65b9\u6cd5\u5bf9\u6279\u5904\u7406\u5de5\u4f5c\u8d1f\u8f7d\u7684\u6700\u5927\u6548\u76ca\u3002", "result": "\u7ed3\u679c\u663e\u793a\uff1a\u5728\u4e0d\u589e\u52a0\u6700\u4f18\u5b8c\u5de5\u65f6\u95f4\u7684\u60c5\u51b5\u4e0b\uff0c\u5e73\u5747\u78b3\u6392\u653e\u964d\u4f4e25%\uff1b\u5728\u5f02\u6784\u670d\u52a1\u5668\u8bbe\u7f6e\u4e2d\uff0c\u8fd9\u4e9b\u8c03\u5ea6\u53ef\u80fd\u6bd4\u80fd\u6e90\u6700\u4f18\u8c03\u5ea6\u6d88\u8017\u66f4\u591a\u80fd\u6e90\uff1b\u5141\u8bb8\u4e24\u500d\u6700\u4f18\u5b8c\u5de5\u65f6\u95f4\u65f6\uff0c\u78b3\u8282\u7ea6\u51e0\u4e4e\u7ffb\u500d\u3002", "conclusion": "\u4f9d\u8d56\u611f\u77e5\u8c03\u5ea6\u80fd\u663e\u8457\u964d\u4f4e\u6570\u636e\u4e2d\u5fc3\u78b3\u6392\u653e\uff0c\u4f46\u5b58\u5728\u78b3\u3001\u80fd\u6e90\u548c\u5b8c\u5de5\u65f6\u95f4\u4e4b\u95f4\u7684\u6743\u8861\u5173\u7cfb\uff0c\u4f5c\u4e1a\u7ed3\u6784\u548c\u670d\u52a1\u5668\u6570\u91cf\u662f\u5f71\u54cd\u53ef\u8fbe\u6210\u78b3\u51cf\u6392\u7684\u5173\u952e\u56e0\u7d20\u3002"}}
{"id": "2512.07622", "categories": ["cs.AR", "cs.PF"], "pdf": "https://arxiv.org/pdf/2512.07622", "abs": "https://arxiv.org/abs/2512.07622", "authors": ["Martha Semken", "Mariano Vargas", "Ignacio Tula", "Giuliana Zorzoli", "Andr\u00e9s Rojas Paredes"], "title": "An\u00e1lisis de rendimiento y eficiencia energ\u00e9tica en el cluster Raspberry Pi Cronos", "comment": "in Spanish language", "summary": "This article presents an evaluation of the computational performance and energy efficiency of the Cronos cluster, composed of Raspberry Pi4 and 3b microcomputers designed for educational purposes. Experimental tests were performed using the High Performance Linpack (HPL) benchmark, under a resource management environment configured with Slurm and parallel communication via Open MPI. The study focuses on analyzing scalability, stability, and power consumption during the execution of computationally intensive workloads, considering different node configurations. The results show that the cluster achieves a performance of up to 6.91 GFLOPS in homogeneous configurations of 6 Raspberry Pi 4 nodes, and that the use of heterogeneous nodes (including Raspberry Pi 3b) can negatively impact stability and efficiency. Additionally, the total electrical consumption of the system was measured during the runs, allowing for the estimation of the performance-to-consumption ratio (GFLOPS/W) as a comparative metric. This study constitutes a concrete contribution to the design, evaluation, and utilization of low-cost ARM clusters in educational and research contexts.", "AI": {"tldr": "\u8bc4\u4f30\u57fa\u4e8eRaspberry Pi4\u548c3b\u7684Cronos\u96c6\u7fa4\u5728HPL\u57fa\u51c6\u6d4b\u8bd5\u4e0b\u7684\u8ba1\u7b97\u6027\u80fd\u4e0e\u80fd\u6548\uff0c\u5206\u6790\u4e0d\u540c\u8282\u70b9\u914d\u7f6e\u7684\u6269\u5c55\u6027\u3001\u7a33\u5b9a\u6027\u548c\u529f\u8017\u8868\u73b0\u3002", "motivation": "\u4e3a\u6559\u80b2\u76ee\u7684\u8bbe\u8ba1\u4f4e\u6210\u672cARM\u96c6\u7fa4\uff0c\u8bc4\u4f30\u5176\u5728\u8ba1\u7b97\u5bc6\u96c6\u578b\u5de5\u4f5c\u8d1f\u8f7d\u4e0b\u7684\u6027\u80fd\u3001\u7a33\u5b9a\u6027\u548c\u80fd\u6548\uff0c\u4e3a\u6559\u80b2\u7814\u7a76\u73af\u5883\u63d0\u4f9b\u5b9e\u7528\u53c2\u8003\u3002", "method": "\u4f7f\u7528High Performance Linpack (HPL)\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5728\u914d\u7f6eSlurm\u8d44\u6e90\u7ba1\u7406\u548cOpen MPI\u5e76\u884c\u901a\u4fe1\u7684\u73af\u5883\u4e2d\uff0c\u5bf9\u4e0d\u540c\u8282\u70b9\u914d\u7f6e\uff08\u540c\u6784\u548c\u5f02\u6784\uff09\u8fdb\u884c\u5b9e\u9a8c\u6d4b\u8bd5\u3002", "result": "6\u4e2aRaspberry Pi4\u8282\u70b9\u7684\u540c\u6784\u914d\u7f6e\u8fbe\u52306.91 GFLOPS\u6027\u80fd\uff1b\u5f02\u6784\u8282\u70b9\uff08\u5305\u542bPi3b\uff09\u4f1a\u964d\u4f4e\u7a33\u5b9a\u6027\u548c\u6548\u7387\uff1b\u6d4b\u91cf\u4e86\u7cfb\u7edf\u603b\u529f\u8017\u5e76\u8ba1\u7b97\u4e86\u6027\u80fd\u529f\u8017\u6bd4(GFLOPS/W)\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u4f4e\u6210\u672cARM\u96c6\u7fa4\u5728\u6559\u80b2\u7814\u7a76\u73af\u5883\u4e2d\u7684\u8bbe\u8ba1\u3001\u8bc4\u4f30\u548c\u4f7f\u7528\u63d0\u4f9b\u4e86\u5177\u4f53\u8d21\u732e\uff0c\u5c55\u793a\u4e86\u5176\u5728\u6559\u80b2\u573a\u666f\u4e0b\u7684\u53ef\u884c\u6027\u548c\u5c40\u9650\u6027\u3002"}}
