<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 6]
- [cs.AR](#cs.AR) [Total: 4]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [High Order Collaboration-Oriented Federated Graph Neural Network for Accurate QoS Prediction](https://arxiv.org/abs/2507.05308)
*Zehuan Chen,Xiangwei Lai*

Main category: cs.DC

TL;DR: HC-FGNN利用高阶协作和注意力机制提升QoS预测精度，同时保护用户隐私。


<details>
  <summary>Details</summary>
Motivation: 现有FGNN方法未能利用隐式用户交互，影响预测精度。

Method: 通过注意力机制扩展显式用户-服务图，捕捉高阶协作，并采用轻量级消息聚合提高效率。

Result: 在两个真实QoS数据集上，HC-FGNN表现出高预测精度和隐私保护优势。

Conclusion: HC-FGNN有效解决了隐式用户交互利用不足的问题，实现了高精度和隐私保护的平衡。

Abstract: Predicting Quality of Service (QoS) data crucial for cloud service selection,
where user privacy is a critical concern. Federated Graph Neural Networks
(FGNNs) can perform QoS data prediction as well as maintaining user privacy.
However, existing FGNN-based QoS predictors commonly implement on-device
training on scattered explicit user-service graphs, thereby failing to utilize
the implicit user-user interactions. To address this issue, this study proposes
a high order collaboration-oriented federated graph neural network (HC-FGNN) to
obtain accurate QoS prediction with privacy preservation. Concretely, it
magnifies the explicit user-service graphs following the principle of attention
mechanism to obtain the high order collaboration, which reflects the implicit
user-user interactions. Moreover, it utilizes a lightweight-based message
aggregation way to improve the computational efficiency. The extensive
experiments on two QoS datasets from real application indicate that the
proposed HC-FGNN possesses the advantages of high prediction accurate and
privacy protection.

</details>


### [2] [Archetype-Aware Predictive Autoscaling with Uncertainty Quantification for Serverless Workloads on Kubernetes](https://arxiv.org/abs/2507.05653)
*Guilin Zhang,Srinivas Vippagunta,Raghavendra Nandagopal,Suchitra Raman,Jeff Xu,Marcus Pfeiffer,Shree Chatterjee,Ziqi Tan,Wulan Guo,Hailong Jiang*

Main category: cs.DC

TL;DR: AAPA是一种基于原型感知的预测自动扩展系统，通过弱监督将工作负载分类为四种原型，显著减少SLO违规并提升响应时间，但资源成本有所增加。


<details>
  <summary>Details</summary>
Motivation: 高性能极端计算平台在动态工作负载管理和SLO维护方面面临挑战，需要更高效的解决方案。

Method: AAPA利用弱监督自动分类工作负载为四种原型（周期性、峰值、斜坡、平稳噪声），准确率达99.8%。

Result: 在Azure Functions测试中，AAPA减少SLO违规50%，响应时间提升40%，但峰值负载下资源成本增加2-8倍。

Conclusion: AAPA有效优化了动态工作负载管理，但需权衡资源成本与性能提升。

Abstract: High-performance extreme computing (HPEC) platforms increasingly adopt
serverless paradigms, yet face challenges in efficiently managing highly
dynamic workloads while maintaining service-level objectives (SLOs). We propose
**AAPA**, an archetype-aware predictive autoscaling system that leverages weak
supervision to automatically classify 300\,000\,+ workload windows into four
archetypes (PERIODIC, SPIKE, RAMP, STATIONARY\_NOISY) with 99.8\% accuracy.
Evaluation on publicly available Azure Functions traces shows that AAPA reduces
SLO violations by up to 50\%, improves response time by 40\%, albeit with a
2--8\,$\times$ increase in resource cost under spike-heavy loads.

</details>


### [3] [Air-FedGA: A Grouping Asynchronous Federated Learning Mechanism Exploiting Over-the-air Computation](https://arxiv.org/abs/2507.05704)
*Qianpiao Ma,Junlong Zhou,Xiangpeng Hou,Jianchun Liu,Hongli Xu,Jianeng Miao,Qingmin Jia*

Main category: cs.DC

TL;DR: 提出了一种基于空中计算的异步联邦学习机制（Air-FedGA），通过分组和异步通信优化资源利用和同步问题，显著加速模型训练。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习在通信资源受限、边缘设备异构和数据非独立同分布（Non-IID）情况下的挑战。

Method: 结合空中计算（AirComp）和异步联邦学习，将设备分组进行空中聚合，异步更新全局模型，并优化功率控制和分组策略。

Result: 实验表明，Air-FedGA比现有方案加速训练29.9%-71.6%，并理论证明了其收敛性。

Conclusion: Air-FedGA有效解决了同步和资源问题，显著提升了联邦学习的效率。

Abstract: Federated learning (FL) is a new paradigm to train AI models over distributed
edge devices (i.e., workers) using their local data, while confronting various
challenges including communication resource constraints, edge heterogeneity and
data Non-IID. Over-the-air computation (AirComp) is a promising technique to
achieve efficient utilization of communication resource for model aggregation
by leveraging the superposition property of a wireless multiple access channel
(MAC). However, AirComp requires strict synchronization among edge devices,
which is hard to achieve in heterogeneous scenarios. In this paper, we propose
an AirComp-based grouping asynchronous federated learning mechanism
(Air-FedGA), which combines the advantages of AirComp and asynchronous FL to
address the communication and heterogeneity challenges. Specifically, Air-FedGA
organizes workers into groups and performs over-the-air aggregation within each
group, while groups asynchronously communicate with the parameter server to
update the global model. In this way, Air-FedGA accelerates the FL model
training by over-the-air aggregation, while relaxing the synchronization
requirement of this aggregation technology. We theoretically prove the
convergence of Air-FedGA. We formulate a training time minimization problem for
Air-FedGA and propose the power control and worker grouping algorithm to solve
it, which jointly optimizes the power scaling factors at edge devices, the
denoising factors at the parameter server, as well as the worker grouping
strategy. We conduct experiments on classical models and datasets, and the
results demonstrate that our proposed mechanism and algorithm can speed up FL
model training by 29.9%-71.6% compared with the state-of-the-art solutions.

</details>


### [4] [ECORE: Energy-Conscious Optimized Routing for Deep Learning Models at the Edge](https://arxiv.org/abs/2507.06011)
*Daghash K. Alqahtani,Maria A. Rodriguez,Muhammad Aamir Cheema,Hamid Rezatofighi,Adel N. Toosi*

Main category: cs.DC

TL;DR: ECORE框架通过动态路由策略优化边缘设备上的对象检测任务，显著降低能耗和延迟，同时保持高检测精度。


<details>
  <summary>Details</summary>
Motivation: 边缘计算中实时视觉分析任务对资源受限设备的高需求，需要平衡能耗与检测精度。

Method: 提出ECORE框架，整合动态路由策略（估计技术和贪婪选择算法），根据对象特征动态分配任务。

Result: 实验表明，ECORE能耗降低45%，延迟减少49%，检测精度仅下降2%。

Conclusion: ECORE有效解决了边缘设备上能耗与检测精度的平衡问题。

Abstract: Edge computing enables data processing closer to the source, significantly
reducing latency an essential requirement for real-time vision-based analytics
such as object detection in surveillance and smart city environments. However,
these tasks place substantial demands on resource constrained edge devices,
making the joint optimization of energy consumption and detection accuracy
critical. To address this challenge, we propose ECORE, a framework that
integrates multiple dynamic routing strategies including estimation based
techniques and a greedy selection algorithm to direct image processing requests
to the most suitable edge device-model pair. ECORE dynamically balances energy
efficiency and detection performance based on object characteristics. We
evaluate our approach through extensive experiments on real-world datasets,
comparing the proposed routers against widely used baseline techniques. The
evaluation leverages established object detection models (YOLO, SSD,
EfficientDet) and diverse edge platforms, including Jetson Orin Nano, Raspberry
Pi 4 and 5, and TPU accelerators. Results demonstrate that our proposed
context-aware routing strategies can reduce energy consumption and latency by
45% and 49%, respectively, while incurring only a 2% loss in detection accuracy
compared to accuracy-centric methods.

</details>


### [5] [Efficient Federated Learning with Timely Update Dissemination](https://arxiv.org/abs/2507.06031)
*Juncheng Jia,Ji Liu,Chao Huo,Yihui Shen,Yang Zhou,Huaiyu Dai,Dejing Dou*

Main category: cs.DC

TL;DR: 本文提出了一种高效的联邦学习方法（FedASMU和FedSSMU），利用额外下行带宽资源，通过异步和同步框架提升模型更新的及时性、准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在分布式数据管理中具有潜力，但现有方法在更新传播的及时性和效率上存在不足。本文旨在通过优化服务器端和设备端的方法来提升性能。

Method: 提出异步框架FedASMU（动态模型聚合和自适应模型调整）和同步框架FedSSMU，结合理论分析和实验验证。

Result: 实验表明，FedASMU和FedSSMU在准确性和效率上显著优于基线方法（准确性提升高达145.87%，效率提升高达97.59%）。

Conclusion: 本文提出的方法在联邦学习中实现了显著的性能提升，为分布式数据管理提供了高效解决方案。

Abstract: Federated Learning (FL) has emerged as a compelling methodology for the
management of distributed data, marked by significant advancements in recent
years. In this paper, we propose an efficient FL approach that capitalizes on
additional downlink bandwidth resources to ensure timely update dissemination.
Initially, we implement this strategy within an asynchronous framework,
introducing the Asynchronous Staleness-aware Model Update (FedASMU), which
integrates both server-side and device-side methodologies. On the server side,
we present an asynchronous FL system model that employs a dynamic model
aggregation technique, which harmonizes local model updates with the global
model to enhance both accuracy and efficiency. Concurrently, on the device
side, we propose an adaptive model adjustment mechanism that integrates the
latest global model with local models during training to further elevate
accuracy. Subsequently, we extend this approach to a synchronous context,
referred to as FedSSMU. Theoretical analyses substantiate the convergence of
our proposed methodologies. Extensive experiments, encompassing six models and
five public datasets, demonstrate that FedASMU and FedSSMU significantly
surpass baseline methods in terms of both accuracy (up to 145.87%) and
efficiency (up to 97.59%).

</details>


### [6] [A Unified Ontology for Scalable Knowledge Graph-Driven Operational Data Analytics in High-Performance Computing Systems](https://arxiv.org/abs/2507.06107)
*Junaid Ahmed Khan,Andrea Bartolini*

Main category: cs.DC

TL;DR: 提出了一种统一的HPC系统ODA本体，支持异构数据中心间的语义互操作性，优化存储开销并验证了实用性。


<details>
  <summary>Details</summary>
Motivation: 解决HPC系统中异构遥测数据的语义集成和高效查询问题，克服现有本体存储开销大和适用性有限的挑战。

Method: 设计并验证了一个统一的ODA本体，基于两个大型公开数据集（M100和F-DATA），引入建模优化以减少存储开销。

Result: 存储开销减少38.84%（与之前方法相比），根据配置可额外减少26.82%，支持跨系统分析。

Conclusion: 该本体为可扩展的ODA知识图谱奠定了基础，支持异构HPC系统的跨系统分析。

Abstract: Modern high-performance computing (HPC) systems generate massive volumes of
heterogeneous telemetry data from millions of sensors monitoring compute,
memory, power, cooling, and storage subsystems. As HPC infrastructures scale to
support increasingly complex workloads-including generative AI-the need for
efficient, reliable, and interoperable telemetry analysis becomes critical.
Operational Data Analytics (ODA) has emerged to address these demands; however,
the reliance on schema-less storage solutions limits data accessibility and
semantic integration. Ontologies and knowledge graphs (KG) provide an effective
way to enable efficient and expressive data querying by capturing domain
semantics, but they face challenges such as significant storage overhead and
the limited applicability of existing ontologies, which are often tailored to
specific HPC systems only. In this paper, we present the first unified ontology
for ODA in HPC systems, designed to enable semantic interoperability across
heterogeneous data centers. Our ontology models telemetry data from the two
largest publicly available ODA datasets-M100 (Cineca, Italy) and F-DATA
(Fugaku, Japan)-within a single data model. The ontology is validated through
36 competency questions reflecting real-world stakeholder requirements, and we
introduce modeling optimizations that reduce knowledge graph (KG) storage
overhead by up to 38.84% compared to a previous approach, with an additional
26.82% reduction depending on the desired deployment configuration. This work
paves the way for scalable ODA KGs and supports not only analysis within
individual systems, but also cross-system analysis across heterogeneous HPC
systems.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [7] [Per-Row Activation Counting on Real Hardware: Demystifying Performance Overheads](https://arxiv.org/abs/2507.05556)
*Jumin Kim,Seungmin Baek,Minbok Wi,Hwayong Nam,Michael Jaemin Kim,Sukhan Lee,Kyomin Sohn,Jung Ho Ahn*

Main category: cs.AR

TL;DR: PRAC是一种DRAM读取干扰缓解方法，通过修改DRAM时序参数实现。研究发现其实际性能开销远低于模拟器预测，平均和最大开销仅为1.06%和3.28%。


<details>
  <summary>Details</summary>
Motivation: 模拟器与真实硬件存在差异，需通过真实机器实验准确评估PRAC的性能开销。

Method: 在最新CPU上验证时序修改，并使用SPEC CPU2017工作负载进行性能分析。

Result: PRAC的平均和最大性能开销分别为1.06%和3.28%，比模拟器预测低9.15倍。关闭页策略可进一步减少开销。

Conclusion: PRAC的实际性能开销显著低于模拟器预测，关闭页策略能有效隐藏其带来的时序延迟。

Abstract: Per-Row Activation Counting (PRAC), a DRAM read disturbance mitigation
method, modifies key DRAM timing parameters, reportedly causing significant
performance overheads in simulator-based studies. However, given known
discrepancies between simulators and real hardware, real-machine experiments
are vital for accurate PRAC performance estimation. We present the first
real-machine performance analysis of PRAC. After verifying timing modifications
on the latest CPUs using microbenchmarks, our analysis shows that PRAC's
average and maximum overheads are just 1.06% and 3.28% for the SPEC CPU2017
workloads -- up to 9.15x lower than simulator-based reports. Further, we show
that the close page policy minimizes this overhead by effectively hiding the
elongated DRAM row precharge operations due to PRAC from the critical path.

</details>


### [8] [GATMesh: Clock Mesh Timing Analysis using Graph Neural Networks](https://arxiv.org/abs/2507.05681)
*Muhammad Hadir Khan,Matthew Guthaus*

Main category: cs.AR

TL;DR: GATMesh是一种基于图神经网络（GNN）的框架，用于高效准确地分析时钟网格，显著提升速度并保持高精度。


<details>
  <summary>Details</summary>
Motivation: 时钟网格在高性能VLSI系统中至关重要，但传统分析方法（如SPICE仿真）速度慢，简化模型又忽略关键效应。

Method: 提出GATMesh，将时钟网格建模为带有增强结构和物理特征的图，利用GNN进行训练。

Result: 在未见过的基准测试中，平均延迟误差为5.27ps，速度比多线程SPICE仿真快47146倍。

Conclusion: GATMesh在时钟网格分析中实现了高精度和高效性，解决了传统方法的局限性。

Abstract: Clock meshes are essential in high-performance VLSI systems for minimizing
skew and handling PVT variations, but analyzing them is difficult due to
reconvergent paths, multi-source driving, and input mesh buffer skew. SPICE
simulations are accurate but slow; yet simplified models miss key effects like
slew and input skew. We propose GATMesh, a Graph Neural Network (GNN)-based
framework that models the clock mesh as a graph with augmented structural and
physical features. Trained on SPICE data, GATMesh achieves high accuracy with
average delay error of 5.27ps on unseen benchmarks, while achieving speed-ups
of 47146x over multi-threaded SPICE simulation.

</details>


### [9] [RTGPU: Real-Time Computing with Graphics Processing Units](https://arxiv.org/abs/2507.06069)
*Atiyeh Gheibi-Fetrat,Amirsaeed Ahmadi-Tonekaboni,Farzam Koohi-Ronaghi,Pariya Hajipour,Sana Babayan-Vanestan,Fatemeh Fotouhi,Elahe Mortazavian-Farsani,Pouria Khajehpour-Dezfouli,Sepideh Safari,Shaahin Hessabi,Hamid Sarbazi-Azad*

Main category: cs.AR

TL;DR: 本文综述了GPU在实时系统中的作用及其挑战，探讨了现有解决方案和未来研究方向。


<details>
  <summary>Details</summary>
Motivation: GPU因其高计算吞吐量被广泛应用于实时系统，但其集成带来非抢占式执行、执行时间可变性等挑战。

Method: 通过调度算法、资源管理技术和同步方法等现有解决方案进行分析。

Result: 总结了GPU在实时系统中的性能问题，并提出了改进方向。

Conclusion: 需进一步研究以提高GPU在实时环境中的可预测性和性能。

Abstract: In this work, we survey the role of GPUs in real-time systems. Originally
designed for parallel graphics workloads, GPUs are now widely used in
time-critical applications such as machine learning, autonomous vehicles, and
robotics due to their high computational throughput. Their parallel
architecture is well-suited for accelerating complex tasks under strict timing
constraints. However, their integration into real-time systems presents several
challenges, including non-preemptive execution, execution time variability, and
resource contention; factors that can lead to unpredictable delays and deadline
violations. We examine existing solutions that address these challenges,
including scheduling algorithms, resource management techniques, and
synchronization methods, and highlight open research directions to improve GPU
predictability and performance in real-time environments.

</details>


### [10] [PrefixAgent: An LLM-Powered Design Framework for Efficient Prefix Adder Optimization](https://arxiv.org/abs/2507.06127)
*Dongsheng Zuo,Jiadong Zhu,Yang Luo,Yuzhe Ma*

Main category: cs.AR

TL;DR: PrefixAgent是一种基于大型语言模型（LLM）的框架，用于高效优化前缀加法器设计，通过分解任务和利用E-graph数据，显著减少搜索空间并提升性能。


<details>
  <summary>Details</summary>
Motivation: 前缀加法器的设计空间随位宽指数增长，现有方法在性能、泛化和扩展性方面存在局限，需要一种更高效的优化方法。

Method: PrefixAgent将问题分解为子任务（如主干合成和结构优化），利用E-graph收集高质量数据和推理轨迹，并微调LLM。

Result: 实验表明，PrefixAgent生成的前缀加法器面积更小，同时在商业EDA流程中保持扩展性和泛化能力。

Conclusion: PrefixAgent通过LLM和任务分解，提供了一种高效、可扩展的前缀加法器优化方案。

Abstract: Prefix adders are fundamental arithmetic circuits, but their design space
grows exponentially with bit-width, posing significant optimization challenges.
Previous works face limitations in performance, generalization, and
scalability. To address these challenges, we propose PrefixAgent, a large
language model (LLM)-powered framework that enables efficient prefix adder
optimization. Specifically, PrefixAgent reformulates the problem into subtasks
including backbone synthesis and structure refinement, which effectively
reduces the search space. More importantly, this new design perspective enables
us to efficiently collect enormous high-quality data and reasoning traces with
E-graph, which further results in an effective fine-tuning of LLM. Experimental
results show that PrefixAgent synthesizes prefix adders with consistently
smaller areas compared to baseline methods, while maintaining scalability and
generalization in commercial EDA flows.

</details>
