<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 1]
- [cs.DC](#cs.DC) [Total: 6]
- [cs.AR](#cs.AR) [Total: 5]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Modular abstract syntax trees (MAST): substitution tensors with second-class sorts](https://arxiv.org/abs/2511.03946)
*Marcelo P. Fiore,Ohad Kammar,Georg Moser,Sam Staton*

Main category: cs.PL

TL;DR: 本文扩展了Fiore等人的抽象语法理论，以处理具有第二类排序的语言（如CBV和CBPV），通过将表征从幺半范畴中的幺半群改为actegories中的动作，并使用双范畴论证来证明CBV变体的替换引理。


<details>
  <summary>Details</summary>
Motivation: 现有的抽象语法理论无法很好地处理具有第二类排序的编程演算（如CBV和CBPV），需要扩展理论以支持这些语言的语法、绑定和替换操作。

Method: 将Fiore等人的抽象语法理论从幺半范畴中的幺半群表征扩展到actegories中的动作表征，使用双范畴论证方法，并禁止第二类排序出现在变量上下文中。

Result: 成功构建了适用于第二类排序语言的抽象语法理论框架，能够处理绑定、替换和空洞操作，并证明了CBV变体的替换引理。

Conclusion: 通过将抽象语法表征从幺半群改为动作，可以有效地处理具有第二类排序的语言，为这类编程演算的形式化分析提供了理论基础。

Abstract: We adapt Fiore, Plotkin, and Turi's treatment of abstract syntax with
binding, substitution, and holes to account for languages with second-class
sorts. These situations include programming calculi such as the Call-by-Value
lambda-calculus (CBV) and Levy's Call-by-Push-Value (CBPV). Prohibiting
second-class sorts from appearing in variable contexts changes the
characterisation of the abstract syntax from monoids in monoidal categories to
actions in actegories. We reproduce much of the development through
bicategorical arguments. We apply the resulting theory by proving substitution
lemmata for varieties of CBV.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [2] [OMPILOT: Harnessing Transformer Models for Auto Parallelization to Shared Memory Computing Paradigms](https://arxiv.org/abs/2511.03866)
*Arijit Bhattacharjee,Ali TehraniJamsaz,Le Chen,Niranjan Hasabnis,Mihai Capota,Nesreen Ahmed,Ali Jannesari*

Main category: cs.DC

TL;DR: OMPILOT是一个专门用于将C++代码翻译成OpenMP的领域特定编码器-解码器变换器，通过自定义预训练目标和混合学习策略提高代码翻译的鲁棒性，并在函数级别进行操作以捕获更广泛的语义上下文。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在代码翻译方面取得了显著进展，但在并行化代码翻译方面仍有改进空间。传统方法主要关注循环级转换，而OMPILOT旨在在函数级别进行更全面的并行化转换。

Method: 使用领域特定的编码器-解码器变换器架构，结合自定义预训练目标（包含并行构造语义），采用无监督和有监督学习策略的混合方法。

Result: 提出了OMPBLEU评估指标来专门评估OpenMP并行构造的正确性和质量，解决了传统翻译指标的局限性。

Conclusion: OMPILOT通过函数级别的操作和专门设计的评估指标，为C++到OpenMP的代码翻译提供了更准确和鲁棒的解决方案。

Abstract: Recent advances in large language models (LLMs) have significantly
accelerated progress in code translation, enabling more accurate and efficient
transformation across programming languages. While originally developed for
natural language processing, LLMs have shown strong capabilities in modeling
programming language syntax and semantics, outperforming traditional rule-based
systems in both accuracy and flexibility. These models have streamlined
cross-language conversion, reduced development overhead, and accelerated legacy
code migration. In this paper, we introduce OMPILOT, a novel domain-specific
encoder-decoder transformer tailored for translating C++ code into OpenMP,
enabling effective shared-memory parallelization. OMPILOT leverages custom
pre-training objectives that incorporate the semantics of parallel constructs
and combines both unsupervised and supervised learning strategies to improve
code translation robustness. Unlike previous work that focused primarily on
loop-level transformations, OMPILOT operates at the function level to capture a
wider semantic context. To evaluate our approach, we propose OMPBLEU, a novel
composite metric specifically crafted to assess the correctness and quality of
OpenMP parallel constructs, addressing limitations in conventional translation
metrics.

</details>


### [3] [Stochastic Modeling for Energy-Efficient Edge Infrastructure](https://arxiv.org/abs/2511.03941)
*Fabio Diniz Rossi*

Main category: cs.DC

TL;DR: 提出了一种基于马尔可夫链的随机建模方法来分析边缘计算中的电源状态转换，通过AI驱动的预测性功率缩放相比传统反应式方法能显著提高能源效率。


<details>
  <summary>Details</summary>
Motivation: 边缘计算虽然支持低延迟处理，但由于边缘设备的分布式特性和有限的能源资源，在电源管理方面面临挑战。

Method: 使用马尔可夫链进行随机建模，推导稳态概率并评估能耗，通过蒙特卡洛模拟验证模型，并进行敏感性分析。

Result: AI驱动的预测性功率缩放相比传统方法能最小化不必要的状态转换，提高系统响应性，并优化异构边缘节点间的负载分配。

Conclusion: 基于AI的电源管理策略通过预测工作负载需求并优化状态转换，能显著提高边缘计算系统的能源效率。

Abstract: Edge Computing enables low-latency processing for real-time applications but
introduces challenges in power management due to the distributed nature of edge
devices and their limited energy resources. This paper proposes a stochastic
modeling approach using Markov Chains to analyze power state transitions in
Edge Computing. By deriving steady-state probabilities and evaluating energy
consumption, we demonstrate the benefits of AI-driven predictive power scaling
over conventional reactive methods. Monte Carlo simulations validate the model,
showing strong alignment between theoretical and empirical results. Sensitivity
analysis highlights how varying transition probabilities affect power
efficiency, confirming that predictive scaling minimizes unnecessary
transitions and improves overall system responsiveness. Our findings suggest
that AI-based power management strategies significantly enhance energy
efficiency by anticipating workload demands and optimizing state transitions.
Experimental results indicate that AI-based power management optimizes workload
distribution across heterogeneous edge nodes, reducing energy consumption
disparities between devices, improving overall efficiency, and enhancing
adaptive power coordination in multi-node environments.

</details>


### [4] [Parallel Spawning Strategies for Dynamic-Aware MPI Applications](https://arxiv.org/abs/2511.04268)
*Iker Martín-Álvarez,José I. Aliaga,Maribel Castillo,Sergio Iserte*

Main category: cs.DC

TL;DR: 提出一种新颖的并行生成策略，通过进程协作在重新分配前进行生成，减少执行时间并消除收缩限制，显著降低重新配置成本。


<details>
  <summary>Details</summary>
Motivation: 现有MPI应用的可塑性方法存在局限性：要么重新生成整个应用成本高昂，要么在收缩时无法完全释放不需要的进程，导致节点无法返回系统。

Method: 采用并行生成策略，所有进程在重新分配前协作进行生成，同时移除收缩限制，使并行系统能更好地适应工作负载。

Result: 在保持最多1.25倍开销的竞争性扩展时间的同时，实现了至少20倍成本缩减的快速收缩操作，在异构和同构系统上均得到验证。

Conclusion: 该策略有效克服了现有方法的局限性，显著降低了可塑性带来的重新配置成本，提高了系统资源利用率。

Abstract: Dynamic resource management is an increasingly important capability of High
Performance Computing systems, as it enables jobs to adjust their resource
allocation at runtime. This capability has been shown to reduce workload
makespan, substantially decrease job waiting times and improve overall system
utilization. In this context, malleability refers to the ability of
applications to adapt to new resource allocations during execution. Although
beneficial, malleability incurs significant reconfiguration costs, making the
reduction of these costs an important research topic.
  Some existing methods for MPI applications respawn the entire application,
which is an expensive solution that avoids the reuse of original processes.
Other MPI methods reuse them, but fail to fully release unneeded processes when
shrinking, since some ranks within the same communicator remain active across
nodes, preventing the application from returning those nodes to the system.
This work overcomes both limitations by proposing a novel parallel spawning
strategy, in which all processes cooperate in spawning before redistribution,
thereby reducing execution time. Additionally, it removes shrinkage
limitations, allowing better adaptation of parallel systems to workload and
reducing their makespan. As a result, it preserves competitive expansion times
with at most a $1.25\times$ overhead, while enabling fast shrink operations
that reduce their cost by at least $20\times$. This strategy has been validated
on both homogeneous and heterogeneous systems and can also be applied in
shared-resource environments.

</details>


### [5] [Enabling Dynamic Sparsity in Quantized LLM Inference](https://arxiv.org/abs/2511.04477)
*Rongxiang Wang,Kangyuan Shu,Felix Xiaozhu Lin*

Main category: cs.DC

TL;DR: 提出了一种在低比特量化下实现动态稀疏推理的技术，包括zigzag量化布局、专用GEMV内核和紧凑运行时机制，在保持精度的同时实现最高1.55倍的解码吞吐量提升。


<details>
  <summary>Details</summary>
Motivation: 在终端设备上部署大语言模型面临内存和计算能力限制，而动态稀疏性和分组量化之间存在冲突，需要解决这一矛盾以实现高效推理。

Method: 采用zigzag模式量化布局以匹配激活稀疏性并改善GPU内存局部性；设计专用GEMV内核充分利用并行计算单元；开发紧凑运行时机制以最小开销收集稀疏索引。

Result: 在多种模型规模和硬件配置下，该方法实现了最高1.55倍解码吞吐量提升，同时保持与密集量化推理相当的精度。

Conclusion: 结构化稀疏性和量化可以在商用GPU上有效共存，为资源受限设备上的LLM部署提供了可行解决方案。

Abstract: Deploying large language models (LLMs) on end-user devices is gaining
importance due to benefits in responsiveness, privacy, and operational cost.
Yet the limited memory and compute capability of mobile and desktop GPUs make
efficient execution difficult. Recent observations suggest that the internal
activations of LLMs are often dynamically sparse, meaning that for each input,
only part of the network contributes significantly to the output. Such sparsity
could reduce computation, but it interacts poorly with group-wise quantization,
which remains the dominant approach for fitting LLMs onto resource-constrained
hardware. To reconcile these two properties, this study proposes a set of
techniques that realize dynamic sparse inference under low-bit quantization.
The method features: (1) a zigzag-patterned quantization layout that organizes
weights in a way consistent with activation sparsity and improves GPU memory
locality; (2) a specialized GEMV kernel designed for this layout to fully
utilize parallel compute units; and (3) a compact runtime mechanism that
gathers sparse indices with minimal overhead. Across several model scales and
hardware configurations, the approach achieves up to 1.55x faster decoding
throughput while maintaining accuracy comparable to dense quantized inference,
showing that structured sparsity and quantization can effectively coexist on
commodity GPUs.

</details>


### [6] [A New Probabilistic Mobile Byzantine Failure Model for Self-Protecting Systems](https://arxiv.org/abs/2511.04523)
*Silvia Bonomi,Giovanni Farina,Roy Friedman,Eviatar B. Procaccia,Sebastien Tixeuil*

Main category: cs.DC

TL;DR: 提出了一种基于MAPE-K架构的自保护分布式系统，引入概率性移动拜占庭故障模型来模拟动态攻击演化，并分析了拜占庭节点数量超过阈值或系统自恢复所需的时间。


<details>
  <summary>Details</summary>
Motivation: 现代分布式系统面临日益增长的安全威胁，攻击者技能不断提升，漏洞遍布整个系统栈。现有拜占庭故障模型虽能增强系统韧性，但在反映真实场景方面存在局限性。

Method: 基于MAPE-K架构构建自保护分布式系统，在分析组件中引入新的概率性移动拜占庭故障模型，通过数学分析和仿真来研究系统行为。

Result: 数学分析了拜占庭节点数量超过给定阈值或系统自恢复到安全状态所需的时间，取决于拜占庭感染传播速率与自恢复速率的对比关系。

Conclusion: 提出的概率性移动拜占庭故障模型能够更好地捕捉动态攻击演化，可用于驱动系统的自保护和重配置策略。

Abstract: Modern distributed systems face growing security threats, as attackers
continuously enhance their skills and vulnerabilities span across the entire
system stack, from hardware to the application layer. In the system design
phase, fault tolerance techniques can be employed to safeguard systems. From a
theoretical perspective, an attacker attempting to compromise a system can be
abstracted by considering the presence of Byzantine processes in the system.
Although this approach enhances the resilience of the distributed system, it
introduces certain limitations regarding the accuracy of the model in
reflecting real-world scenarios. In this paper, we consider a self-protecting
distributed system based on the \emph{Monitoring-Analyse-Plan-Execute over a
shared Knowledge} (MAPE-K) architecture, and we propose a new probabilistic
Mobile Byzantine Failure (MBF) that can be plugged into the Analysis component.
Our new model captures the dynamics of evolving attacks and can be used to
drive the self-protection and reconfiguration strategy. We analyze
mathematically the time that it takes until the number of Byzantine nodes
crosses given thresholds, or for the system to self-recover back into a safe
state, depending on the rates of Byzantine infection spreading \emph{vs.} the
rate of self-recovery. We also provide simulation results that illustrate the
behavior of the system under such assumptions.

</details>


### [7] [Resolving Conflicts with Grace: Dynamically Concurrent Universality](https://arxiv.org/abs/2511.04631)
*Petr Kuznetsov,Nathan Josia Schrodt*

Main category: cs.DC

TL;DR: 提出动态并发概念，仅在必要时使用强同步原语来仲裁并发操作


<details>
  <summary>Details</summary>
Motivation: 同步是分布式计算可扩展性的主要障碍，冲突通常只在罕见状态下发生，理想情况是根据当前系统状态动态检测冲突

Method: 定义动态并发概念，并提出了动态并发通用构造

Result: 未在摘要中明确说明

Conclusion: 动态并发方法可以根据系统状态动态调整同步需求，提高系统效率

Abstract: Synchronization is the major obstacle to scalability in distributed
computing. Concurrent operations on the shared data engage in synchronization
when they encounter a \emph{conflict}, i.e., their effects depend on the order
in which they are applied. Ideally, one would like to detect conflicts in a
\emph{dynamic} manner, i.e., adjusting to the current system state. Indeed, it
is very common that two concurrent operations conflict only in some rarely
occurring states. In this paper, we define the notion of \emph{dynamic
concurrency}: an operation employs strong synchronization primitives only if it
\emph{has} to arbitrate with concurrent operations, given the current system
state. We then present a dynamically concurrent universal construction.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [8] [From Minutes to Seconds: Redefining the Five-Minute Rule for AI-Era Memory Hierarchies](https://arxiv.org/abs/2511.03944)
*Tong Zhang,Vikram Sharma Mailthody,Fei Sun,Linsen Ma,Chris J. Newburn,Teresa Zhang,Yang Liu,Jiangpeng Li,Hao Zhong,Wen-Mei Hwu*

Main category: cs.AR

TL;DR: 本文重新审视了经典的五分钟规则，通过整合主机成本、DRAM带宽/容量限制以及基于物理的SSD性能模型，提出了一个约束和负载感知的分析框架，发现在现代AI平台中DRAM到闪存的缓存阈值从分钟级降至秒级。


<details>
  <summary>Details</summary>
Motivation: 传统的五分钟规则仅基于存储-内存经济学，忽略了主机成本、可行性限制和工作负载行为。随着现代AI平台（特别是GPU主机与高性能SSD配对）的发展，需要重新评估这一经典启发式方法。

Method: 从第一性原理出发，整合主机成本、DRAM带宽/容量限制、基于物理的SSD性能和成本模型，构建约束和负载感知的框架，并开发MQSim-Next SSD模拟器进行验证和敏感性分析。

Result: 对于现代AI平台，DRAM到闪存的缓存阈值从分钟级大幅降至秒级，这重新定义了NAND闪存作为活跃数据层的角色，并揭示了硬件-软件栈的广泛研究空间。

Conclusion: 将经典启发式方法转变为可行的、考虑可行性的分析和配置框架，为AI时代的内存层次结构研究奠定了基础，并通过具体案例展示了由此产生的软件系统设计空间。

Abstract: In 1987, Jim Gray and Gianfranco Putzolu introduced the five-minute rule, a
simple, storage-memory-economics-based heuristic for deciding when data should
live in DRAM rather than on storage. Subsequent revisits to the rule largely
retained that economics-only view, leaving host costs, feasibility limits, and
workload behavior out of scope. This paper revisits the rule from first
principles, integrating host costs, DRAM bandwidth/capacity, and
physics-grounded models of SSD performance and cost, and then embedding these
elements in a constraint- and workload-aware framework that yields actionable
provisioning guidance. We show that, for modern AI platforms, especially
GPU-centric hosts paired with ultra-high-IOPS SSDs engineered for fine-grained
random access, the DRAM-to-flash caching threshold collapses from minutes to a
few seconds. This shift reframes NAND flash memory as an active data tier and
exposes a broad research space across the hardware-software stack. We further
introduce MQSim-Next, a calibrated SSD simulator that supports validation and
sensitivity analysis and facilitates future architectural and system research.
Finally, we present two concrete case studies that showcase the software system
design space opened by such memory hierarchy paradigm shift. Overall, we turn a
classical heuristic into an actionable, feasibility-aware analysis and
provisioning framework and set the stage for further research on AI-era memory
hierarchy.

</details>


### [9] [PICNIC: Silicon Photonic Interconnected Chiplets with Computational Network and In-memory Computing for LLM Inference Acceleration](https://arxiv.org/abs/2511.04036)
*Yue Jiet Chong,Yimin Wang,Zhen Wu,Xuanyao Fong*

Main category: cs.AR

TL;DR: 提出基于3D堆叠芯粒的LLM推理加速器，采用非易失性内存计算处理单元和硅光子互连技术解决通信瓶颈，通过优化映射方案和芯片级功率门控实现显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型推理中的通信瓶颈问题，通过3D堆叠芯粒架构和硅光子互连技术提升计算效率和可扩展性。

Method: 使用3D堆叠芯粒架构，集成非易失性内存计算处理单元和硅光子互连的IPCN网络，开发LLM映射优化方案和芯片级功率门控技术。

Result: 相比Nvidia A100实现3.95倍加速和30倍效率提升；采用CCPG后相比H100在相同吞吐量下实现57倍效率提升。

Conclusion: 该3D堆叠芯粒架构能有效解决LLM推理的通信瓶颈，显著提升性能和能效，具有良好的可扩展性。

Abstract: This paper presents a 3D-stacked chiplets based large language model (LLM)
inference accelerator, consisting of non-volatile in-memory-computing
processing elements (PEs) and Inter-PE Computational Network (IPCN),
interconnected via silicon photonic to effectively address the communication
bottlenecks. A LLM mapping scheme was developed to optimize hardware scheduling
and workload mapping. Simulation results show it achieves $3.95\times$ speedup
and $30\times$ efficiency improvement over the Nvidia A100 before chiplet
clustering and power gating scheme (CCPG). Additionally, the system achieves
further scalability and efficiency improvement with the implementation of CCPG
to accommodate larger models, attaining $57\times$ efficiency improvement over
Nvidia H100 at similar throughput.

</details>


### [10] [Disaggregated Architectures and the Redesign of Data Center Ecosystems: Scheduling, Pooling, and Infrastructure Trade-offs](https://arxiv.org/abs/2511.04104)
*Chao Guo,Jiahe Xu,Moshe Zukerman*

Main category: cs.AR

TL;DR: 本文概述了硬件解耦的动机、进展、挑战和机遇，认为硬件解耦有潜力重塑整个数据中心生态系统。


<details>
  <summary>Details</summary>
Motivation: 硬件解耦旨在将传统服务器集群转变为统一资源池，以提升数据中心资源利用率和灵活性。

Method: 通过文献综述和数值研究，分析硬件解耦的动机、进展、挑战和机遇。

Result: 硬件解耦在工业界和学术界取得了显著进展，但仍面临诸多挑战。

Conclusion: 硬件解耦有潜力重塑数据中心生态系统，影响应用设计、资源调度、硬件配置、冷却和电力系统优化等方面。

Abstract: Hardware disaggregation seeks to transform Data Center (DC) resources from
traditional server fleets into unified resource pools. Despite existing
challenges that may hinder its full realization, significant progress has been
made in both industry and academia. In this article, we provide an overview of
the motivations and recent advancements in hardware disaggregation. We further
discuss the research challenges and opportunities associated with disaggregated
architectures, focusing on aspects that have received limited attention. We
argue that hardware disaggregation has the potential to reshape the entire DC
ecosystem, impacting application design, resource scheduling, hardware
configuration, cooling, and power system optimization. Additionally, we present
a numerical study to illustrate several key aspects of these challenges.

</details>


### [11] [AIM: Software and Hardware Co-design for Architecture-level IR-drop Mitigation in High-performance PIM](https://arxiv.org/abs/2511.04321)
*Yuanpeng Zhang,Xing Hu,Xi Chen,Zhihang Yuan,Cong Li,Jingchen Zhu,Zhao Wang,Chenguang Zhang,Xin Si,Wei Gao,Qiang Wu,Runsheng Wang,Guangyu Sun*

Main category: cs.AR

TL;DR: AIM是一个针对高性能SRAM存内计算(PIM)的软硬件协同设计方法，通过架构级IR-drop缓解技术，在7nm 256-TOPS PIM芯片上实现了69.2%的IR-drop缓解、2.29倍能效提升和1.152倍加速。


<details>
  <summary>Details</summary>
Motivation: SRAM存内计算虽然具有高性能优势，但追求更高性能会导致更复杂的电路设计和更高的工作频率，从而加剧IR-drop问题。传统的电路级IR-drop缓解方法资源密集且会牺牲PPA(功耗、性能、面积)。

Method: 提出AIM软硬件协同设计框架：1)利用PIM的位串行和原位数据流特性，建立工作负载与IR-drop的直接关联；2)通过软件优化实现架构级IR-drop缓解；3)开发IR-Booster动态调整机制，结合软件级HR信息和硬件IR-drop监控来调整PIM宏的电压-频率对；4)提出HR感知的任务映射方法。

Result: 在7nm 256-TOPS PIM芯片上的后布局仿真结果显示，AIM实现了最高69.2%的IR-drop缓解，带来2.29倍的能效提升和1.152倍的加速。

Conclusion: AIM通过软硬件协同设计有效解决了高性能PIM中的IR-drop问题，在保持计算精度的同时显著提升了能效和性能。

Abstract: SRAM Processing-in-Memory (PIM) has emerged as the most promising
implementation for high-performance PIM, delivering superior computing density,
energy efficiency, and computational precision. However, the pursuit of higher
performance necessitates more complex circuit designs and increased operating
frequencies, which exacerbate IR-drop issues. Severe IR-drop can significantly
degrade chip performance and even threaten reliability. Conventional
circuit-level IR-drop mitigation methods, such as back-end optimizations, are
resource-intensive and often compromise power, performance, and area (PPA). To
address these challenges, we propose AIM, comprehensive software and hardware
co-design for architecture-level IR-drop mitigation in high-performance PIM.
Initially, leveraging the bit-serial and in-situ dataflow processing properties
of PIM, we introduce Rtog and HR, which establish a direct correlation between
PIM workloads and IR-drop. Building on this foundation, we propose LHR and WDS,
enabling extensive exploration of architecture-level IR-drop mitigation while
maintaining computational accuracy through software optimization. Subsequently,
we develop IR-Booster, a dynamic adjustment mechanism that integrates
software-level HR information with hardware-based IR-drop monitoring to adapt
the V-f pairs of the PIM macro, achieving enhanced energy efficiency and
performance. Finally, we propose the HR-aware task mapping method, bridging
software and hardware designs to achieve optimal improvement. Post-layout
simulation results on a 7nm 256-TOPS PIM chip demonstrate that AIM achieves up
to 69.2% IR-drop mitigation, resulting in 2.29x energy efficiency improvement
and 1.152x speedup.

</details>


### [12] [Scalable and Efficient Intra- and Inter-node Interconnection Networks for Post-Exascale Supercomputers and Data centers](https://arxiv.org/abs/2511.04677)
*Joaquin Tarraga-Moreno,Daniel Barley,Francisco J. Andujar Munoz,Jesus Escudero-Sahuquillo,Holger Froning,Pedro Javier Garcia,Francisco J. Quiles,Jose Duato*

Main category: cs.AR

TL;DR: 现代超级计算机和数据中心正朝着异构集成架构发展，以应对数据密集型应用的增长，但面临通信瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 生成式AI、科学模拟和大规模分析等数据密集型应用的快速增长，推动系统采用异构集成架构来减少数据移动并提高计算效率。

Method: 结合强大的CPU和加速器，以及新兴的高带宽内存和存储技术。

Result: 随着每个节点加速器数量的增加，在节点内部和节点之间出现了通信瓶颈，特别是在网络资源被异构组件共享时。

Conclusion: 异构集成架构虽然能提高计算效率，但通信瓶颈成为需要解决的关键挑战。

Abstract: The rapid growth of data-intensive applications such as generative AI,
scientific simulations, and large-scale analytics is driving modern
supercomputers and data centers toward increasingly heterogeneous and tightly
integrated architectures. These systems combine powerful CPUs and accelerators
with emerging high-bandwidth memory and storage technologies to reduce data
movement and improve computational efficiency. However, as the number of
accelerators per node increases, communication bottlenecks emerge both within
and between nodes, particularly when network resources are shared among
heterogeneous components.

</details>
