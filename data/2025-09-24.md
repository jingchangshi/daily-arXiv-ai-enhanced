<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 1]
- [cs.DC](#cs.DC) [Total: 8]
- [cs.AR](#cs.AR) [Total: 2]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [A Verified Compiler for Quantum Simulation](https://arxiv.org/abs/2509.18583)
*Liyi Li,Fenfen An,Federico Zahariev,Zhi Xiang Chong,Amr Sabry,Mark Gordon*

Main category: cs.PL

TL;DR: QBlue是一个用于哈密顿量模拟的高层次、形式化验证的编译框架，基于二次量子化形式，提供类型系统确保正确性，并支持编译到数字和模拟量子电路。


<details>
  <summary>Details</summary>
Motivation: 现有的哈密顿量模拟编译器通常基于低层次的泡利算符表示，限制了可编程性且缺乏编译管道的形式化正确性保证。

Method: 基于二次量子化形式，使用产生和湮灭算符描述量子粒子系统；包含类型系统跟踪粒子类型并强制厄米结构；支持编译到数字和模拟量子电路；在Rocq证明框架中完全机械化验证。

Result: QBlue是第一个端到端验证的二次量子化哈密顿量模拟编译器，提供了从静态约束到动态演化的多层语义捕获。

Conclusion: QBlue框架通过形式化验证方法解决了哈密顿量模拟编译器的正确性问题，为量子计算在物理系统建模和优化问题求解中的应用提供了可靠基础。

Abstract: Hamiltonian simulation is a central application of quantum computing, with
significant potential in modeling physical systems and solving complex
optimization problems. Existing compilers for such simulations typically focus
on low-level representations based on Pauli operators, limiting programmability
and offering no formal guarantees of correctness across the compilation
pipeline. We introduce QBlue, a high-level, formally verified framework for
compiling Hamiltonian simulations. QBlue is based on the formalism of second
quantization, which provides a natural and expressive way to describe quantum
particle systems using creation and annihilation operators. To ensure safety
and correctness, QBlue includes a type system that tracks particle types and
enforces Hermitian structure. The framework supports compilation to both
digital and analog quantum circuits and captures multiple layers of semantics,
from static constraints to dynamic evolution. All components of QBlue,
including its language design, type system, and compilation correctness, are
fully mechanized in the Rocq proof framework, making it the first end-to-end
verified compiler for second-quantized Hamiltonian simulation.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [2] [Bridging Simulation and Silicon: A Study of RISC-V Hardware and FireSim Simulation](https://arxiv.org/abs/2509.18472)
*Atanu Barai,Kamalavasan Kamalakkannan,Patrick Diehl,Maxim Moraru,Jered Dominguez-Trujillo,Howard Pritchard,Nandakishore Santhi,Farzad Fatollahi-Fard,Galen Shipman*

Main category: cs.DC

TL;DR: 本文评估了FireSim框架在RISC-V处理器性能预测方面的准确性，通过对比模拟结果与真实硬件性能，发现虽然FireSim能提供有价值的架构性能趋势，但模拟与实测运行时间存在差异。


<details>
  <summary>Details</summary>
Motivation: RISC-V ISA处理器作为强大且节能的计算平台显示出在高性能计算环境中的应用潜力。FireSim作为开源FPGA加速框架虽然灵活可扩展，但缺乏对其在RISC-V平台上性能预测准确性的系统评估。

Method: 研究通过FireSim模拟商业单板计算机和桌面级RISC-V CPU，首先测量基准测试在单核和四核配置下的性能以比较运行时行为，然后使用代表性小型应用和LAMMPS分子动力学代码评估性能。

Result: 研究发现FireSim能提供有价值的架构性能趋势洞察，但模拟运行时间与实测结果存在偏差，这些差异源于模拟环境的固有限制以及CPU制造商提供的详细性能规格信息有限。

Conclusion: FireSim是RISC-V架构探索的有用工具，但需要更精确的配置匹配和更详细的硬件规格信息来提高模拟准确性。

Abstract: RISC-V ISA-based processors have recently emerged as both powerful and
energy-efficient computing platforms. The release of the MILK-V Pioneer marked
a significant milestone as the first desktop-grade RISC-V system. With
increasing engagement from both academia and industry, such platforms exhibit
strong potential for adoption in high-performance computing (HPC) environments.
  The open-source, FPGA-accelerated FireSim framework has emerged as a flexible
and scalable tool for architectural exploration, enabling simulation of various
system configurations using RISC-V cores. Despite its capabilities, there
remains a lack of systematic evaluation regarding the feasibility and
performance prediction accuracy of FireSim when compared to physical hardware.
  In this study, we address this gap by modeling a commercially available
single-board computer and a desktop-grade RISC-V CPU within FireSim. To ensure
fidelity between simulation and real hardware, we first measure the performance
of a series of benchmarks to compare runtime behavior under single-core and
four-core configurations. Based on the closest matching simulation parameters,
we subsequently evaluate performance using a representative mini-application
and the LAMMPS molecular dynamics code.
  Our findings indicate that while FireSim provides valuable insights into
architectural performance trends, discrepancies remain between simulated and
measured runtimes. These deviations stem from both inherent limitations of the
simulation environment and the restricted availability of detailed performance
specifications from CPU manufacturers, which hinder precise configuration
matching.

</details>


### [3] [6G Twin: Hybrid Gaussian Radio Fields for Channel Estimation and Non-Linear Precoder Design for Radio Access Networks](https://arxiv.org/abs/2509.18735)
*Muhammad Ahmed Mohsin,Muhammad Umer,Ahsan Bilal,Muhammad Ali Jamshed,Dean F. Hougen,John M. Cioffi*

Main category: cs.DC

TL;DR: 6G Twin是首个端到端AI原生无线接入网络设计，通过神经高斯无线电场压缩信道状态信息获取、持续信道预测和能量最优非线性预编码器，实现100倍导频开销降低、毫秒级闭环操作和4-10倍能耗降低。


<details>
  <summary>Details</summary>
Motivation: 解决传统RAN在高移动性和动态网络中信道状态信息获取开销大、预测不准确以及能量效率低的问题，为6G网络提供实时、鲁棒且高效的设计方案。

Method: 采用神经高斯无线电场（GRF）替代密集导频进行压缩CSI获取，使用重放驱动的持续学习器进行信道预测和切换持久性，设计最小功率多址接入预编码器（minPMAC）进行能量最优非线性预编码。

Result: GRF将导频开销降低约100倍，推理时间1.1毫秒，现场训练少于2分钟；持续学习器在移动性和小区切换下将信道NMSE改善超过10dB；minPMAC实现4-10倍能耗降低，在可比功率下数据速率提升高达5倍。

Conclusion: 6G Twin提供了一个实用的GPU就绪框架，在3GPP标准设置下实现了实时CSI、动态网络中的鲁棒跟踪和最优的吞吐量-能量权衡，为6G网络部署奠定了基础。

Abstract: This work introduces 6G Twin, the first end-to-end artificial intelligence
(AI)-native radio access network (RAN) design that unifies (i) neural Gaussian
Radio Fields (GRF) for compressed channel state information (CSI) acquisition,
(ii) continual channel prediction with handover persistence, and (iii) an
energy-optimal nonlinear precoder (minPMAC). GRF replaces dense pilots with a
sparse Gaussian field, cutting pilot overhead by about 100x while delivering
1.1 ms inference and less than 2 minutes on-site training, thus enabling
millisecond-scale closed-loop operation. A replay-driven continual learner
sustains accuracy under mobility and cell transitions, improving channel
normalized mean square error (NMSE) by more than 10 dB over frozen predictors
and an additional 2-5 dB over uniform replay, thereby stabilizing performance
across UMi/UMa handovers. Finally, minPMAC solves a convex, order-free MAC
precoder design that recovers the globally optimal order from Broadcast Channel
(BC) duals and minimizes transmit energy subject to minimum-rate guarantees,
achieving 4-10 times lower energy (scenario dependent) with monotonically
increasing bits per joule as SNR grows. This translates to up to 5 times higher
data rate at comparable power or the same rates at substantially lower power.
Together, these components form a practical, GPU-ready framework that attains
real-time CSI, robust tracking in dynamic networks with efficient handovers,
and state-of-the-art throughput-energy tradeoffs under 3GPP-style settings.

</details>


### [4] [On The Reproducibility Limitations of RAG Systems](https://arxiv.org/abs/2509.18869)
*Baiqiang Wang,Dongfang Zhao,Nathan R Tallent,Luanzheng Guo*

Main category: cs.DC

TL;DR: ReproRAG是一个用于系统测量和量化基于向量的检索系统可重现性的基准测试框架，旨在解决RAG系统中检索组件非确定性导致的可靠性问题。


<details>
  <summary>Details</summary>
Motivation: RAG在生成式AI驱动的科学工作流中应用日益广泛，但其可靠性常因检索组件的非确定性而受损，需要系统评估可重现性。

Method: ReproRAG框架通过分析嵌入模型、精度、检索算法、硬件配置和分布式执行环境等整个流程中的不确定性来源，使用精确匹配率、Jaccard相似度和Kendall's Tau等指标来量化可重现性。

Result: 大规模实证研究发现不同嵌入模型对RAG可重现性有显著影响，揭示了可重现性与性能之间的权衡关系。

Conclusion: 开源ReproRAG框架为研究人员和工程师提供了验证部署、基准测试可重现性和做出明智设计决策的工具，有助于构建更可信的科学AI系统。

Abstract: Retrieval-Augmented Generation (RAG) is increasingly employed in generative
AI-driven scientific workflows to integrate rapidly evolving scientific
knowledge bases, yet its reliability is frequently compromised by
non-determinism in their retrieval components. This paper introduces ReproRAG,
a comprehensive benchmarking framework designed to systematically measure and
quantify the reproducibility of vector-based retrieval systems. ReproRAG
investigates sources of uncertainty across the entire pipeline, including
different embedding models, precision, retrieval algorithms, hardware
configurations, and distributed execution environments. Utilizing a suite of
metrics, such as Exact Match Rate, Jaccard Similarity, and Kendall's Tau, the
proposed framework effectively characterizes the trade-offs between
reproducibility and performance. Our large-scale empirical study reveals
critical insights; for instance, we observe that different embedding models
have remarkable impact on RAG reproducibility. The open-sourced ReproRAG
framework provides researchers and engineers productive tools to validate
deployments, benchmark reproducibility, and make informed design decisions,
thereby fostering more trustworthy AI for science.

</details>


### [5] [TD3-Sched: Learning to Orchestrate Container-based Cloud-Edge Resources via Distributed Reinforcement Learning](https://arxiv.org/abs/2509.18957)
*Shengye Song,Minxian Xu,Kan Hu,Wenxia Guo,Kejiang Ye*

Main category: cs.DC

TL;DR: TD3-Sched是一个基于TD3强化学习的分布式调度器，用于云边系统中CPU和内存资源的连续控制分配，在动态工作负载下实现优化决策


<details>
  <summary>Details</summary>
Motivation: 云边系统中的资源调度面临挑战，边缘节点运行延迟敏感的工作负载且资源受限，而现有的集中式调度器存在性能瓶颈和用户体验下降问题

Method: 提出了基于Twin Delayed Deep Deterministic Policy Gradient (TD3)的分布式强化学习调度器TD3-Sched，用于CPU和内存分配的连续控制

Result: 在真实云边测试平台上，TD3-Sched相比其他强化学习和基于规则的基线方法，在相同负载下延迟降低17.9%-38.6%，高负载下降低16%-31.6%，SLO违规率仅为0.47%

Conclusion: TD3-Sched在容器化云边环境中表现出更快的收敛速度、更低的延迟和更稳定的性能，同时保持服务质量

Abstract: Resource scheduling in cloud-edge systems is challenging as edge nodes run
latency-sensitive workloads under tight resource constraints, while existing
centralized schedulers can suffer from performance bottlenecks and user
experience degradation. To address the issues of distributed decisions in
cloud-edge environments, we present TD3-Sched, a distributed reinforcement
learning (DRL) scheduler based on Twin Delayed Deep Deterministic Policy
Gradient (TD3) for continuous control of CPU and memory allocation, which can
achieve optimized decisions for resource provisioning under dynamic workloads.
On a realistic cloud-edge testbed with SockShop application and Alibaba traces,
TD3-Sched achieves reductions of 17.9% to 38.6% in latency under same loads
compared with other reinforcement-learning and rule-based baselines, and 16% to
31.6% under high loads. TD3-Sched also shows superior Service Level Objective
(SLO) compliance with only 0.47% violations. These results indicate faster
convergence, lower latency, and more stable performance while preserving
service quality in container-based cloud-edge environment compared with the
baselines.

</details>


### [6] [Scheduler-Driven Job Atomization](https://arxiv.org/abs/2509.19086)
*Michal Konopa,Jan Fesl,Ladislav Beránek*

Main category: cs.DC

TL;DR: 提出Scheduler-Driven Job Atomization (SJA)新范式，通过调度器与作业的双向交互，将作业分解为可适配执行间隙的子作业，提高GPU集群利用率


<details>
  <summary>Details</summary>
Motivation: 现代GPU集群（特别是基于NVIDIA MIG架构的）存在效率低下问题，作业被视为不可分割的刚性块，依赖静态峰值内存估计导致碎片化、利用率不足和作业拒绝

Method: SJA建立调度器与作业的双向交互：调度器发布可用执行间隙，作业响应表示能否生成适配该时间容量窗口的子作业，调度器基于分配策略选择作业并生成安全自包含的子作业

Result: 该方法主动在作业执行前进行工作负载整形，避免昂贵的状态转移和不可预测的中断，旨在提高GPU利用率、减少等待时间和最小化迁移开销

Conclusion: 本文作为概念论文介绍了SJA范式，定义了其构建模块并概述了未来研究方向，尚未提供完整的实验评估

Abstract: Modern GPU clusters, particularly those built on NVIDIA's Multi-Instance GPU
(MIG) architecture, often suffer from inefficiencies because jobs are treated
as rigid, indivisible blocks that occupy a fixed slice until completion. The
reliance on static peak memory estimates exacerbates fragmentation,
underutilization, and job rejections. We propose Scheduler-Driven Job
Atomization (SJA), a new paradigm that establishes a bidirectional interaction
between scheduler and jobs. In SJA, the scheduler advertises available
execution gaps, and jobs respond by signaling interest if they can potentially
generate a subjob that fits the offered time-capacity window. The scheduler may
collect multiple signals for the same slot and, based on its allocation policy
(e.g., fairness, efficiency, or SLA priorities), selects which job is granted
the slot. Only then does the chosen job materialize a safe, self-contained
subjob tailored to that opportunity. Unlike migration or preemption, SJA
proactively shapes workloads before execution, thereby avoiding costly state
transfers and unpredictable interruptions. It aims to increase GPU utilization,
reduce wait times, and minimize migration overhead by aligning jobs with
opportunities in real time, ensuring that each admitted subjob is correct by
construction. This paper is presented as a concept paper: it introduces the
paradigm, defines its building blocks, and outlines future research directions,
rather than offering a full experimental evaluation.

</details>


### [7] [In-Transit Data Transport Strategies for Coupled AI-Simulation Workflow Patterns](https://arxiv.org/abs/2509.19150)
*Harikrishna Tummalapalli,Riccardo Balin,Christine M. Simpson,Andrew Park,Aymen Alsaadi,Andrew E. Shao,Wesley Brewer,Shantenu Jha*

Main category: cs.DC

TL;DR: SimAI-Bench是一个用于原型设计和评估耦合AI-仿真工作流程的工具，在Aurora超级计算机上测试了两种常见模式的数据传输性能。


<details>
  <summary>Details</summary>
Motivation: 随着耦合AI-仿真工作流程在HPC设施中成为主要工作负载，其复杂性不断增加，需要新的性能分析和原型设计工具。

Method: 使用SimAI-Bench工具在Aurora超级计算机上对两种模式进行基准测试：一对一工作流程（仿真与AI训练实例共置）和多对一工作流程（从仿真集合训练单个AI模型）。

Result: 对于一对一模式，节点本地和DragonHPC数据暂存策略比Redis和Lustre文件系统性能更优；对于多对一模式，随着集合规模增大，数据传输成为主要瓶颈，文件系统是最佳解决方案。

Conclusion: SimAI-Bench是评估耦合AI-仿真工作流程的有效工具，不同模式需要不同的数据传输策略优化。

Abstract: Coupled AI-Simulation workflows are becoming the major workloads for HPC
facilities, and their increasing complexity necessitates new tools for
performance analysis and prototyping of new in-situ workflows. We present
SimAI-Bench, a tool designed to both prototype and evaluate these coupled
workflows. In this paper, we use SimAI-Bench to benchmark the data transport
performance of two common patterns on the Aurora supercomputer: a one-to-one
workflow with co-located simulation and AI training instances, and a
many-to-one workflow where a single AI model is trained from an ensemble of
simulations. For the one-to-one pattern, our analysis shows that node-local and
DragonHPC data staging strategies provide excellent performance compared Redis
and Lustre file system. For the many-to-one pattern, we find that data
transport becomes a dominant bottleneck as the ensemble size grows. Our
evaluation reveals that file system is the optimal solution among the tested
strategies for the many-to-one pattern.

</details>


### [8] [Non-Uniform Content-Oblivious Leader Election on Oriented Asynchronous Rings](https://arxiv.org/abs/2509.19187)
*Jérémie Chalopin,Yi-Jun Chang,Lyuting Chen,Giuseppe A. Di Luna,Haoran Zhou*

Main category: cs.DC

TL;DR: 本文研究了面向环网络中的领导者选举问题，在内容不可知异步消息传递系统中，对手可能任意破坏消息内容。论文分析了消息复杂度，证明了在均匀算法中每个进程只能发送恒定数量消息的限制下无法解决问题，但在非均匀设置下提出了改进算法。


<details>
  <summary>Details</summary>
Motivation: 研究在内容不可知模型下环网络中领导者选举的消息复杂度，探索在消息数量受限条件下的可行性，并比较均匀与非均匀算法的性能差异。

Method: 通过理论分析证明均匀算法的局限性，提出非均匀算法利用环大小上界U和最小标识符ID_min来优化消息复杂度，并设计随机化算法处理匿名设置。

Result: 证明了均匀算法在恒定消息限制下不可行；提出了消息复杂度为O(n·U·ID_min)的非均匀算法；展示了最优的O(log ID_min)因子算法；设计了匿名设置的随机化算法，每个进程仅发送O(log² U)消息。

Conclusion: 领导者选举在内容不可知环网络中的可行性高度依赖于均匀性假设，非均匀设置允许更高效的消息复杂度，而匿名设置可通过随机化实现低消息开销。

Abstract: We study the leader election problem in oriented ring networks under
content-oblivious asynchronous message-passing systems, where an adversary may
arbitrarily corrupt message contents.
  Frei et al. (DISC 2024) presented a uniform terminating leader election
algorithm for oriented rings in this setting, with message complexity $O(n
\cdot \mathsf{ID}_{\max})$ on a ring of size $n$, where $\mathsf{ID}_{\max}$ is
the largest identifier in the system, this result has been recently extended by
Chalopin et al. (DISC 2025) to unoriented rings.
  In this paper, we investigate the message complexity of leader election on
ring networks in the content-oblivious model, showing that no uniform algorithm
can solve the problem if each process is limited to sending a constant number
of messages in one direction.
  Interestingly, this limitation hinges on the uniformity assumption. In the
non-uniform setting, where processes know an upper bound $U \geq n$ on the ring
size, we present an algorithm with message complexity $O(n \cdot U \cdot
\mathsf{ID}_{\min})$, in which each process sends $O(U \cdot
\mathsf{ID}_{\min})$ messages clockwise and only three messages
counter-clockwise. Here, $\mathsf{ID}_{\min}$ is the smallest identifier in the
system. This dependence on the identifiers compares favorably with the
dependence on $\mathsf{ID}_{\max}$ of Frei et al.
  We also show a non-uniform algorithm where each process sends $O(U \cdot
\log\mathsf{ID}_{\min})$ messages in one direction and
$O(\log\mathsf{ID}_{\min})$ in the other. The factor $\log \mathsf{ID}_{\min}$
is optimal, matching the lower bound of Frei et al.
  Finally, in the anonymous setting, where processes do not have identifiers,
we propose a randomized algorithm where each process sends only $O(\log^2 U)$
messages, with a success probability of $1 - U^{-c}$.

</details>


### [9] [Accelerating Gravitational $N$-Body Simulations Using the RISC-V-Based Tenstorrent Wormhole](https://arxiv.org/abs/2509.19294)
*Jenny Lynn Almerol,Elisabetta Boella,Mario Spera,Daniele Gregori*

Main category: cs.DC

TL;DR: 本文展示了在RISC-V架构的Wormhole n300加速卡上加速天体物理N体代码的方法，相比优化CPU实现可获得2倍以上速度提升和约2倍能耗节省。


<details>
  <summary>Details</summary>
Motivation: RISC-V加速器虽然最初为AI工作负载开发，但正成为高性能科学计算的有吸引力的平台，特别是在天体物理模拟领域。

Method: 在Tenstorrent开发的RISC-V架构Wormhole n300加速卡上实现天体物理N体代码的加速方法。

Result: 该平台在天体物理模拟中表现出高度竞争力，相比高度优化的CPU实现，实现了超过2倍的速度提升和约2倍的能耗节省。

Conclusion: RISC-V加速器平台对于采用此类算法的天体物理模拟具有显著优势，在性能和能效方面都优于传统CPU实现。

Abstract: Although originally developed primarily for artificial intelligence
workloads, RISC-V-based accelerators are also emerging as attractive platforms
for high-performance scientific computing. In this work, we present our
approach to accelerating an astrophysical $N$-body code on the RISC-V-based
Wormhole n300 card developed by Tenstorrent. Our results show that this
platform can be highly competitive for astrophysical simulations employing this
class of algorithms, delivering more than a $2 \times$ speedup and
approximately $2 \times$ energy savings compared to a highly optimized CPU
implementation of the same code.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [10] [Lightweight Congruence Profiling for Early Design Exploration of Heterogeneous FPGAs](https://arxiv.org/abs/2509.18295)
*Allen Boston,Biruk Seyoum,Luca Carloni,Pierre-Emmanuel Gaillardon*

Main category: cs.AR

TL;DR: 提出一种基于Roofline模型的轻量级分析方法，通过三个一致性评分快速识别异构FPGA中的性能瓶颈


<details>
  <summary>Details</summary>
Motivation: FPGA从均匀逻辑阵列发展为集成DSP、存储器和专用加速器的异构架构，这虽然改善了PPA（功耗、性能、面积），但由于复杂的资源交互，增加了设计空间探索和应用优化的难度

Method: 受Roofline模型启发，提出轻量级分析方法，引入三个一致性评分来识别异构资源、架构和应用逻辑相关的瓶颈。在Stratix 10类似FPGA上使用Koios和VPR基准套件进行评估

Result: 该方法能够有效识别FPGA异构架构中的性能瓶颈

Conclusion: 该方法支持高效的FPGA架构协同设计，有助于提升异构FPGA的性能

Abstract: Field-Programmable Gate Arrays (FPGAs) have evolved from uniform logic arrays
into heterogeneous fabrics integrating digital signal processors (DSPs),
memories, and specialized accelerators to support emerging workloads such as
machine learning. While these enhancements improve power, performance, and area
(PPA), they complicate design space exploration and application optimization
due to complex resource interactions.
  To address these challenges, we propose a lightweight profiling methodology
inspired by the Roofline model. It introduces three congruence scores that
quickly identify bottlenecks related to heterogeneous resources, fabric, and
application logic. Evaluated on the Koios and VPR benchmark suites using a
Stratix 10 like FPGA, this approach enables efficient FPGA architecture
co-design to improve heterogeneous FPGA performance.

</details>


### [11] [Chiplet-Based RISC-V SoC with Modular AI Acceleration](https://arxiv.org/abs/2509.18355)
*P. Ramkumar,S. S. Bharadwaj*

Main category: cs.AR

TL;DR: 本文提出了一种基于小芯片的RISC-V SoC架构，通过模块化AI加速和智能系统级优化，解决了边缘AI设备在性能、能效和成本效益方面的挑战。


<details>
  <summary>Details</summary>
Motivation: 传统单片SoC设计在先进工艺节点下制造良率低（低于16%），难以平衡高性能、高能效和成本效益。需要一种新的架构来克服这些限制。

Method: 采用小芯片架构，在30mm x 30mm硅中介层上集成4项关键技术：自适应跨芯片动态电压频率调节、AI感知的UCIe协议扩展、分布式加密安全、智能传感器驱动的负载迁移。集成7nm RISC-V CPU芯片、双5nm AI加速器（各15 TOPS INT8）、16GB HBM3内存和专用电源管理控制器。

Result: 在MobileNetV2、ResNet-50等基准测试中，AI优化配置相比基础小芯片实现实现了14.7%延迟降低、17.3%吞吐量提升、16.2%功耗降低，总体效率提升40.1%，达到每MobileNetV2推理3.5mJ（860mW/244 images/s），所有工作负载均保持亚5ms实时能力。

Conclusion: 模块化小芯片设计能够实现接近单片的计算密度，同时具备成本效益、可扩展性和可升级性，为下一代边缘AI设备应用提供了关键解决方案。

Abstract: Achieving high performance, energy efficiency, and cost-effectiveness while
maintaining architectural flexibility is a critical challenge in the
development and deployment of edge AI devices. Monolithic SoC designs struggle
with this complex balance mainly due to low manufacturing yields (below 16%) at
advanced 360 mm^2 process nodes. This paper presents a novel chiplet-based
RISC-V SoC architecture that addresses these limitations through modular AI
acceleration and intelligent system level optimization. Our proposed design
integrates 4 different key innovations in a 30mm x 30mm silicon interposer:
adaptive cross-chiplet Dynamic Voltage and Frequency Scaling (DVFS); AI-aware
Universal Chiplet Interconnect Express (UCIe) protocol extensions featuring
streaming flow control units and compression-aware transfers; distributed
cryptographic security across heterogeneous chiplets; and intelligent
sensor-driven load migration. The proposed architecture integrates a 7nm RISC-V
CPU chiplet with dual 5nm AI accelerators (15 TOPS INT8 each), 16GB HBM3 memory
stacks, and dedicated power management controllers. Experimental results across
industry standard benchmarks like MobileNetV2, ResNet-50 and real-time video
processing demonstrate significant performance improvements. The AI-optimized
configuration achieves ~14.7% latency reduction, 17.3% throughput improvement,
and 16.2% power reduction compared to previous basic chiplet implementations.
These improvements collectively translate to a 40.1% efficiency gain
corresponding to ~3.5 mJ per MobileNetV2 inference (860 mW/244 images/s), while
maintaining sub-5ms real-time capability across all experimented workloads.
These performance upgrades demonstrate that modular chiplet designs can achieve
near-monolithic computational density while enabling cost efficiency,
scalability and upgradeability, crucial for next-generation edge AI device
applications.

</details>
