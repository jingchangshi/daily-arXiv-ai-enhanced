<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 6]
- [cs.AR](#cs.AR) [Total: 1]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [Space Filling Curves is All You Need: Communication-Avoiding Matrix Multiplication Made Simple](https://arxiv.org/abs/2601.16294)
*Evangelos Georganas,Alexander Heinecke,Pradeep Dubey*

Main category: cs.DC

TL;DR: 该论文提出使用广义希尔伯特曲线等空间填充曲线来划分矩阵乘法计算空间，实现平台无关和形状无关的高数据局部性GEMM方案，并通过通信避免算法进一步优化，在多个CPU平台上超越厂商库性能。


<details>
  <summary>Details</summary>
Motivation: 现代CPU平台上的矩阵乘法加速器具有高FLOP/Byte机器平衡，使得实现最优矩阵乘法具有挑战性。厂商库需要针对不同平台（核心数、内存层次、缓存大小）和矩阵形状进行繁琐调优，导致性能"玻璃下巴"问题，即某些情况下性能急剧下降。

Method: 采用广义希尔伯特曲线等空间填充曲线将多维计算空间（如2D）转换为1D顺序，保持高维空间中相邻点在1D顺序中的接近性。基于此划分矩阵乘法计算空间，实现平台无关和形状无关的数据局部性方案。进一步扩展为通信避免算法，复制输入张量并证明最小化关键路径上的通信/数据移动。

Result: 在多个CPU平台上实现了最先进的结果，代码紧凑（约30行），对于一系列GEMM形状，几何平均加速比超越厂商库高达2倍。

Conclusion: 空间填充曲线方法有效解决了矩阵乘法中繁琐的平台和形状特定调优问题，通过高数据局部性和通信避免实现了高性能、平台无关的GEMM实现，显著优于现有厂商库。

Abstract: General Matrix Multiplication (GEMM) is the cornerstone of Deep Learning and HPC workloads; accordingly, academia and industry have heavily optimized this kernel. Modern platforms with matrix multiplication accelerators exhibit high FLOP/Byte machine balance, which makes implementing optimal matrix multiplication challenging. On modern CPU platforms with matrix engines, state-of-the-art vendor libraries tune input tensor layouts, parallelization schemes, and cache blocking to minimize data movement across the memory hierarchy and maximize throughput. However, the best settings for these parameters depend strongly on the target platform (number of cores, memory hierarchy, cache sizes) and on the shapes of the matrices, making exhaustive tuning infeasible; in practice this leads to performance "glass jaws". In this work we revisit space filling curves (SFC) to alleviate the problem of this cumbersome tuning. SFC convert multi-dimensional coordinates (e.g. 2D) into a single dimension (1D), keeping nearby points in the high-dimensional space close in the 1D order. We partition the Matrix Multiplication computation space using recent advancements in generalized SFC (Generalized Hilbert Curves), and we obtain platform-oblivious and shape-oblivious matrix-multiplication schemes that exhibit inherently high degree of data locality. Furthermore, we extend the SFC-based work partitioning to implement Communication-Avoiding (CA) algorithms that replicate the input tensors and provably minimize communication/data-movement on the critical path. The integration of CA-algorithms is seamless and yields compact code (~30 LOC), yet it achieves state-of-the-art results on multiple CPU platforms, outperforming vendor libraries by up to 2x(geometric-mean speedup) for a range of GEMM shapes.

</details>


### [2] [Consensus In Asynchrony](https://arxiv.org/abs/2601.16460)
*Ivan Klianev*

Main category: cs.DC

TL;DR: 本文证明了基于事件的同步足以在异步环境中解决确定性容错共识问题，提出了一个能终止并达成有效向量协议的算法，同时识别了FLP不可能性定理中的三个隐含假设。


<details>
  <summary>Details</summary>
Motivation: 传统FLP不可能性定理表明在完全异步系统中无法实现确定性容错共识，但本文旨在探索在事件同步条件下是否可能绕过这一限制，实现安全、活性和容错性兼备的共识算法。

Method: 提出了一种基于事件同步的算法，能够终止并达成有效的向量协议。通过分析FLP定理，识别了三种隐含假设：1) 存在两种协议类型（数据无关和数据相关）；2) FLP定理的正确性依赖于三个隐含假设；3) 数据相关协议的共识不可能性依赖于其中两个假设。

Result: 算法实现了安全、活性和容忍一次崩溃的特性。实验结果表明，FLP定理所依赖的第三个隐含假设缺乏实证支持，从而为在事件同步条件下实现确定性容错共识提供了可能性。

Conclusion: 基于事件的同步足以解决异步环境中的确定性容错共识问题。FLP不可能性定理依赖于三个隐含假设，其中第三个假设缺乏实证证据，这为绕过FLP限制提供了理论依据。

Abstract: We demonstrate sufficiency of events-based synchronisation for solving deterministic fault-tolerant consensus in asynchrony. Main result is an algorithm that terminates with valid vector agreement, hence operates with safety, liveness, and tolerance to one crash. Reconciling with the FLP impossibility result, we identified: i) existence of two types of agreements: data-independent and data-dependent; and ii) dependence of FLP theorem correctness on three implicit assumptions. Consensus impossibility with data-dependent agreement is contingent on two of them. The theorem-stated impossibility with every agreement type hinges entirely on the third. We provide experimental results showing that the third assumption has no evidence in support.

</details>


### [3] [W4A16 Mixed-Precision Matrix Multiplication on Decoupled Architecture: Kernel Design and Memory Bottleneck Analysis for Ascend NPUs](https://arxiv.org/abs/2601.16536)
*Yuanhong He,Peiyu Niu,Jun Chen,Chenchen Zhang,Chao Yang*

Main category: cs.DC

TL;DR: 提出了首个面向华为昇腾910 NPU的W4A16矩阵乘法内核，通过向量核心实时反量化、立方核心GEMM和Split-K并行化，在LLM解码场景中实现1.01-1.74倍加速


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型规模扩大，W4A16量化对减少内存占用至关重要，但在华为昇腾910 NPU上高效部署面临挑战，因为该架构缺乏原生混合精度支持且采用解耦计算架构

Method: 设计针对昇腾910 NPU的W4A16矩阵乘法内核：1) 利用向量核心进行INT4到FP16的实时反量化；2) 使用立方核心进行高吞吐量GEMM计算；3) 采用Split-K并行化来缓解内存延迟

Result: 在不同矩阵形状和批量大小下，当K>>N（LLM解码典型场景）时优于数据并行方法，加速比1.01-1.74倍；分析显示主要瓶颈是权重额外全局内存传输而非反量化计算，W4A16相比原生FP16最大加速1.48倍

Conclusion: 该方法为在各类领域专用加速器上高效部署量化大语言模型奠定了坚实基础，并提供了有价值的见解

Abstract: As Large Language Models (LLMs) scale, weight-only quantization (W4A16: 4-bit weights, 16-bit activations) becomes critical for reducing memory footprint with minimal accuracy loss. However, its efficient deployment on Huawei's Ascend 910 Neural Processing Unit (NPU) is challenging due to limited native mixed-precision support and the accelerator's decoupled compute architecture. To enable quantization on such architecture, we present the first practical W4A16 matrix multiplication kernel tailored for the Ascend 910 NPU. Our design leverages vector cores for on-the-fly INT4-to-FP16 dequantization, cube cores for high-throughput GEMM, and Split-K parallelization to mitigate memory latency. Performance evaluations across diverse matrix shapes and batch sizes show our method outperforms data-parallel approaches when K >> N, a typical scenario in LLM decoding. Specially, our method can achieve a speedup ranging from 1.01x to 1.74x. In addition, our profile reveals the primary bottleneck is not dequantization compution itself, but extra global memory transfer for the weight, making W4A16 only reaching a maximum speedup of 1.48x over native FP16xFP16 matrix multiplication in PyTorch. In the long run, our method lays a solid foundation and provides insightful views for the efficient deployment of quantized large language models on various domain-specific accelerators.

</details>


### [4] [Artifact for Service-Level Energy Modeling and Experimentation for Cloud-Native Microservices](https://arxiv.org/abs/2601.16635)
*Julian Legler*

Main category: cs.DC

TL;DR: GOXN是一个Kubernetes微服务能源实验引擎，能够量化服务级别的计算、网络和存储能耗，解决了现有方法忽略分布式环境能耗的问题。


<details>
  <summary>Details</summary>
Motivation: 虽然云原生环境已经能够进行细粒度的能源测量（如容器或进程级别），但微服务应用的服务器能源测量仍然研究不足。现有方法通常只关注计算能耗，忽略了网络和存储能耗，这在分布式设置中会导致能耗低估。

Method: 提出了GOXN（Green Observability eXperiment eNginE），这是一个用于Kubernetes微服务的能源实验引擎。它使用Kepler和cAdvisor收集指标，通过加性能源模型从容器级数据推导出服务器能源消耗，能够量化计算、网络和存储三方面的能耗。

Result: 在OpenTelemetry Demo上的评估显示：1）忽略网络和存储能耗会导致辅助服务能耗低估高达63%；2）在高追踪负载下，网络和存储能耗成为主导因素，改变了能耗分布格局。

Conclusion: 微服务应用的服务器能源测量必须包含计算、网络和存储三个维度，否则会严重低估实际能耗。GOXN提供了一个有效的工具来全面评估微服务架构的能源效率。

Abstract: Recent advancements enable fine-grained energy measurements in cloud-native environments (e.g., at container or process level) beyond traditional coarse-grained scopes. However, service-level energy measurement for microservice-based applications remains underexplored. Such measurements must include compute, network, and storage energy to avoid underestimating consumption in distributed setups. We present GOXN (Green Observability eXperiment eNginE), an energy experimentation engine for Kubernetes-based microservices that quantifies compute, network, and storage energy at the service level. Using GOXN, we evaluated the OpenTelemetry Demo under varying configurations (monitoring, tracing, service mesh) and steady synthetic load, collecting metrics from Kepler and cAdvisor. Our additive energy model derives service-level energy from container-level data. Results show that excluding network and storage can underestimate auxiliary-service energy by up to 63%, and that high tracing loads shift energy dominance toward network and storage.

</details>


### [5] [GPU-Accelerated Selected Basis Diagonalization with Thrust for SQD-based Algorithms](https://arxiv.org/abs/2601.16637)
*Jun Doi,Tomonori Shirakawa,Yukio Kawashima,Seiji Yunoki,Hiroshi Horii*

Main category: cs.DC

TL;DR: GPU加速的Selected Basis Diagonalization (SBD)实现，使用Thrust库，在量子对角化计算中获得40倍加速


<details>
  <summary>Details</summary>
Motivation: SBD在基于样本的量子对角化(SQD)中起核心作用，其迭代对角化计算是主要经典计算负担，需要GPU加速来提高效率

Method: 使用Thrust库实现GPU加速的SBD，重构配置处理、激发生成和矩阵向量运算等关键组件，采用细粒度数据并行原语和扁平化的GPU友好数据布局

Result: Thrust-based SBD相比CPU执行获得高达40倍的加速，显著减少了SQD迭代的总运行时间

Conclusion: GPU原生并行原语为加速基于SQD的量子-经典工作流程提供了简单、可移植且高性能的基础

Abstract: Selected Basis Diagonalization (SBD) plays a central role in Sample-based Quantum Diagonalization (SQD), where iterative diagonalization of the Hamiltonian in selected configuration subspaces forms the dominant classical workload. We present a GPU-accelerated implementation of SBD using the Thrust library. By restructuring key components -- including configuration processing, excitation generation, and matrix-vector operations -- around fine-grained data-parallel primitives and flattened GPU-friendly data layouts, the proposed approach efficiently exploits modern GPU architectures. In our experiments, the Thrust-based SBD achieves up to $\sim$40$\times$ speedup over CPU execution and substantially reduces the total runtime of SQD iterations. These results demonstrate that GPU-native parallel primitives provide a simple, portable, and high-performance foundation for accelerating SQD-based quantum-classical workflows.

</details>


### [6] [DataStates-LLM: Scalable Checkpointing for Transformer Models Using Composable State Providers](https://arxiv.org/abs/2601.16956)
*Avinash Maurya,M. Mustafa Rafique,Franck Cappello,Bogdan Nicolae*

Main category: cs.DC

TL;DR: DataStates-LLM：一种针对大规模Transformer模型训练的新型检查点架构，通过状态提供者解耦状态抽象与数据移动，利用参数不变性实现异步懒快照，显著提升检查点吞吐量并减少训练时间。


<details>
  <summary>Details</summary>
Motivation: 现有检查点方案将模型状态视为不透明的二进制数据块，忽略了底层数据结构的"3D异质性"（内存位置、分片数量、数据类型、序列化需求差异），导致运行时开销大，包括阻塞的设备到主机传输、数据无关的序列化以及存储I/O争用。

Method: 引入DataStates-LLM架构，通过状态提供者（State Providers）解耦状态抽象与数据移动。利用前向和反向传播期间模型参数的不变性，执行"懒"非阻塞异步快照。有效合并碎片化、异构的分片，并将元数据序列化与批量张量I/O重叠。

Result: 在256个A100-40GB GPU上评估高达700亿参数的模型。DataStates-LLM相比最先进解决方案实现高达4倍的检查点吞吐量提升，并将端到端训练时间减少高达2.2倍，有效缓解了极端规模LLM训练中的序列化和异质性瓶颈。

Conclusion: DataStates-LLM通过创新的状态提供者架构和异步懒快照机制，成功解决了大规模Transformer模型训练中检查点操作的性能瓶颈，为极端规模LLM训练提供了高效的检查点解决方案。

Abstract: The rapid growth of Large Transformer-based models, specifically Large Language Models (LLMs), now scaling to trillions of parameters, has necessitated training across thousands of GPUs using complex hybrid parallelism strategies (e.g., data, tensor, and pipeline parallelism). Checkpointing this massive, distributed state is critical for a wide range of use cases, such as resilience, suspend-resume, investigating undesirable training trajectories, and explaining model evolution. However, existing checkpointing solutions typically treat model state as opaque binary blobs, ignoring the ``3D heterogeneity'' of the underlying data structures--varying by memory location (GPU vs. Host), number of ``logical'' objects sharded and split across multiple files, data types (tensors vs. Python objects), and their serialization requirements. This results in significant runtime overheads due to blocking device-to-host transfers, data-oblivious serialization, and storage I/O contention. In this paper, we introduce DataStates-LLM, a novel checkpointing architecture that leverages State Providers to decouple state abstraction from data movement. DataStates-LLM exploits the immutability of model parameters during the forward and backward passes to perform ``lazy'', non-blocking asynchronous snapshots. By introducing State Providers, we efficiently coalesce fragmented, heterogeneous shards and overlap the serialization of metadata with bulk tensor I/O. We evaluate DataStates-LLM on models up to 70B parameters on 256 A100-40GB GPUs. Our results demonstrate that DataStates-LLM achieves up to 4$\times$ higher checkpointing throughput and reduces end-to-end training time by up to 2.2$\times$ compared to state-of-the-art solutions, effectively mitigating the serialization and heterogeneity bottlenecks in extreme-scale LLM training.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [7] [AERO: Adaptive and Efficient Runtime-Aware OTA Updates for Energy-Harvesting IoT](https://arxiv.org/abs/2601.16935)
*Wei Wei,Jingye Xu,Sahidul Islam,Dakai Zhu,Chen Pan,Mimi Xie*

Main category: cs.AR

TL;DR: AERO是一种自适应、高效的运行时感知OTA更新机制，专为能量收集物联网设备设计，通过将更新任务集成到设备DAG中并与常规任务协同调度，解决间歇供电环境下的更新一致性问题。


<details>
  <summary>Details</summary>
Motivation: 能量收集物联网设备在间歇性能源供应下运行，传统OTA更新机制依赖重启且开销大，不适合间歇供电系统。现有实时OTA更新技术虽减少重启开销，但缺乏确保更新与运行时执行交互一致性的机制。

Method: 提出AERO机制：将更新任务集成到设备的有向无环图(DAG)中，在能量和时间约束下与常规任务协同调度；识别更新影响的执行区域，动态调整依赖关系，确保一致的更新集成并适应间歇性能源可用性。

Result: 在代表性工作负载上的实验表明，相比现有实时更新方法，AERO在更新可靠性和效率方面均有改进。

Conclusion: AERO通过运行时感知的更新集成和自适应调度，有效解决了间歇供电物联网设备OTA更新的挑战，实现了更可靠和高效的更新机制。

Abstract: Energy-harvesting (EH) Internet of Things (IoT) devices operate under intermittent energy availability, which disrupts task execution and makes energy-intensive over-the-air (OTA) updates particularly challenging. Conventional OTA update mechanisms rely on reboots and incur significant overhead, rendering them unsuitable for intermittently powered systems. Recent live OTA update techniques reduce reboot overhead but still lack mechanisms to ensure consistency when updates interact with runtime execution. This paper presents AERO, an Adaptive and Efficient Runtime-Aware OTA update mechanism that integrates update tasks into the device's Directed Acyclic Graph (DAG) and schedules them alongside routine tasks under energy and timing constraints. By identifying update-affected execution regions and dynamically adjusting dependencies, AERO ensures consistent up date integration while adapting to intermittent energy availability. Experiments on representative workloads demonstrate improved update reliability and efficiency compared to existing live update approaches.

</details>
