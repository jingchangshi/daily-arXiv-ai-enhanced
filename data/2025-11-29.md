<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 3]
- [cs.DC](#cs.DC) [Total: 10]
- [cs.AR](#cs.AR) [Total: 5]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [Optimism in Equality Saturation](https://arxiv.org/abs/2511.20782)
*Russel Arbore,Alvin Cheung,Max Willsey*

Main category: cs.PL

TL;DR: 提出一种用于等式饱和的抽象解释算法，能够精确分析循环程序，统一乐观分析和非破坏性重写。


<details>
  <summary>Details</summary>
Motivation: 当前的e类分析对循环程序（如SSA形式）的分析是悲观的且效果不佳，需要改进循环程序的分析能力。

Method: 设计抽象解释算法，在等式饱和过程中精确分析循环，并基于新的SSA语义构建原型抽象解释器。

Result: 原型系统在简单示例程序上的分析精度超过了clang和gcc。

Conclusion: 该方法实现了乐观分析和非破坏性重写的统一，提高了循环程序分析的精确性。

Abstract: Equality saturation is a technique for program optimization based on non-destructive rewriting and a form of program analysis called e-class analysis. The current form of e-class analysis is pessimistic and therefore ineffective at analyzing cyclic programs, such as those in SSA form. We propose an abstract interpretation algorithm that can precisely analyze cycles during equality saturation. This results in a unified algorithm for optimistic analysis and non-destructive rewriting. We instantiate this approach on a prototype abstract interpreter for SSA programs using a new semantics of SSA. Our prototype can analyze simple example programs more precisely than clang and gcc.

</details>


### [2] [Towards Computational UIP in Cubical Agda](https://arxiv.org/abs/2511.21209)
*Yee-Jian Tan,Andreas Nuyts,Dominique Devriese*

Main category: cs.PL

TL;DR: 该论文探讨了在Cubical Agda中实现h-集合立方类型理论的方法，分析了UIP（身份证明唯一性）的不同表述及其计算规则，并实现了一个无Glue类型的Cubical Agda变体。


<details>
  <summary>Details</summary>
Motivation: 立方类型理论具有商归纳类型和函数外延性等优势，但HoTT的无限等价层次在形式化中可能变得繁琐。通过截断到h-集合并移除Glue类型，可以保留这些优势同时避免复杂性。

Method: 分析UIP的不同表述及其计算规则，评估其在Cubical Agda中的实现适用性，并实现一个无Glue类型的Cubical Agda变体。

Result: 提出了在Cubical Agda中实现h-集合立方类型理论的方案，包括UIP的表述分析和无Glue类型的实现。

Conclusion: 为Cubical Agda中实现UIP奠定了基础，使得在保留函数外延性和QITs的同时，能够避免HoTT的复杂性。

Abstract: Some advantages of Cubical Type Theory, as implemented by Cubical Agda, over intensional Martin-Löf Type Theory include Quotient Inductive Types (QITs), which exist as instances of Higher Inductive Types, and functional extensionality, which is provable in Cubical Type Theory. However, HoTT features an infinite hierarchy of equalities that may become unwieldy in formalisations. Fortunately, QITs and functional extensionality are both preserved even if the equality levels of Cubical Type Theory are truncated to only homotopical Sets (h-Sets). In other words, removing the univalence axiom from Cubical Type Theory and instead postulating a conflicting axiom: the Uniqueness of Identity Proofs (UIP) postulate. Since univalence is proved in Cubical Type Theory from the so-called Glue Types, therefore, it is known that one can first remove the Glue Types (thus removing univalence) and then set-truncate all equalities (essentially assuming UIP), à la XTT. The result is a "h-Set Cubical Type Theory" that retains features such as functional extensionality and QITs.
  However, in Cubical Agda, there are currently only two unsatisfying ways to achieve h-Set Cubical Type Theory. The first is to give up on the canonicity of the theory and simply postulate the UIP axiom, while the second way is to use a standard result stating "type formers preserve h-levels" to manually prove UIP for every defined type. The latter is, however, laborious work best suited for an automatic implementation by the proof assistant. In this project, we analyse formulations of UIP and detail their computation rules for Cubical Agda, and evaluate their suitability for implementation. We also implement a variant of Cubical Agda without Glue, which is already compatible with postulated UIP, in anticipation of a future implementation of UIP in Cubical Agda.

</details>


### [3] [SV-LIB 1.0: A Standard Exchange Format for Software-Verification Tasks](https://arxiv.org/abs/2511.21509)
*Dirk Beyer,Gidon Ernst,Martin Jonáš,Marian Lingsch-Rosenfeld*

Main category: cs.PL

TL;DR: SV-LIB是一种用于软件验证任务的交换格式和中间语言，旨在解决不同编程语言验证工具之间的互操作性问题。


<details>
  <summary>Details</summary>
Motivation: 现有的验证工具多为特定语言开发，但许多验证方法实际上是语言无关的。为了促进技术转移和工具复用，需要一种通用的交换格式。

Method: 基于命令式编程语言概念，使用SMT-LIB表示表达式和类型，定义程序、规范和验证证据的格式，包括正确和不正确程序的证据格式。

Result: 提出了SV-LIB 1.0版本，包括设计目标、语法和非正式语义，支持独立证据验证器和验证工具复用为证据验证器。

Conclusion: SV-LIB为软件验证工具提供了统一的交换格式，促进了验证技术的互操作性和复用。并发扩展和形式语义将在未来版本中实现。

Abstract: In the past two decades, significant research and development effort went into the development of verification tools for individual languages, such asC, C++, and Java. Many of the used verification approaches are in fact language-agnostic and it would be beneficial for the technology transfer to allow for using the implementations also for other programming and modeling languages. To address the problem, we propose SV-LIB, an exchange format and intermediate language for software-verification tasks, including programs, specifications, and verification witnesses. SV-LIBis based on well-known concepts from imperative programming languages and uses SMT-LIB to represent expressions and sorts used in the program. This makes it easy to parse and to build into existing infrastructure, since many verification tools are based on SMT solvers already. Furthermore, SV-LIBdefines a witness format for both correct and incorrect SV-LIB programs, together with means for specifying witness-validation tasks. This makes it possible both to implement independent witness validators and to reuse some verifiers also as validators for witnesses. This paper presents version 1.0 of the SV-LIBformat, including its design goals, the syntax, and informal semantics. Formal semantics and further extensions to concurrency are planned for future versions.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [4] [Assessing Redundancy Strategies to Improve Availability in Virtualized System Architectures](https://arxiv.org/abs/2511.20780)
*Alison Silva,Gustavo Callou*

Main category: cs.DC

TL;DR: 提出了一种基于随机Petri网的方法来分析私有云环境中Nextcloud文件服务器的可用性，评估了不同冗余策略对系统可用性的影响。


<details>
  <summary>Details</summary>
Motivation: 随着云存储平台在学术和商业环境中的普及，特别是组织寻求公共云服务替代方案时，系统可靠性成为关键需求，需要评估这些系统的可靠性。

Method: 使用随机Petri网(SPNs)建模方法，在Apache CloudStack私有云环境中分析Nextcloud文件服务器的可用性，评估了四种架构配置：基线、主机级冗余、虚拟机冗余以及两者组合。

Result: 结果显示，在主机和虚拟机级别同时实施冗余策略能显著提高系统可用性并减少预期停机时间。

Conclusion: 该方法为评估私有云可用性和支持基础设施设计决策提供了一种有效途径。

Abstract: Cloud-based storage platforms are becoming more common in both academic and business settings due to their flexible access to data and support for collaborative functionalities. As reliability becomes a vital requirement, particularly for organizations looking for alternatives to public cloud services, assessing the dependability of these systems is crucial. This paper presents a methodology for analyzing the availability of a file server (Nextcloud) hosted in a private cloud environment using Apache CloudStack. The analysis is based on a modeling approach through Stochastic Petri Nets (SPNs) that allows the evaluation of different redundancy strategies to enhance the availability of such systems. Four architectural configurations were modeled, including the baseline, host-level redundancy, virtual machine (VM) redundancy, and a combination of both. The results show that redundancy at both the host and VM levels significantly improves availability and reduces expected downtime. The proposed approach provides a method to evaluate the availability of a private cloud and support infrastructure design decisions.

</details>


### [5] [Accelerating Sparse Convolutions in Voxel-Based Point Cloud Networks](https://arxiv.org/abs/2511.20834)
*Dionysios Adamopoulos,Anastasia Poulopoulou,Georgios Goumas,Christina Giannoula*

Main category: cs.DC

TL;DR: Spira是一个专门为GPU设计的稀疏卷积引擎，通过利用体素坐标的整数性、有界性和几何连续性等特性，显著提升了3D点云网络的处理性能。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏卷积引擎未能充分利用体素坐标的整数性、有界性和几何连续性等特性，导致内核映射构建时预处理和后处理开销过高。

Method: Spira提出：一次性搜索算法构建内核映射、打包原生处理方案访问体素坐标、双数据流执行机制适应层特性、网络级并行化策略同时构建所有层的内核映射。

Result: Spira在端到端推理中平均性能提升1.71倍（最高2.31倍），在逐层执行中平均提升2.13倍（最高3.32倍）。

Conclusion: Spira通过充分利用体素坐标特性，显著提升了稀疏卷积在3D点云处理中的性能表现。

Abstract: Sparse Convolution (SpC) powers 3D point cloud networks widely used in autonomous driving and AR/VR. SpC builds a kernel map that stores mappings between input voxel coordinates, output coordinates, and weight offsets, then uses this map to compute feature vectors for output coordinates. Our work identifies three key properties of voxel coordinates: they are integer-valued, bounded within a limited spatial range, and geometrically continuous-neighboring voxels on the same object surface are highly likely to exist at small spatial offsets from each other. Prior SpC engines do not fully exploit these properties and suffer from high pre-processing and post-processing overheads during kernel map construction. To address this, we design Spira, the first voxel-property-aware SpC engine for GPUs. Spira proposes: (i) a high-performance one-shot search algorithm that builds the kernel map with no preprocessing and high memory locality, (ii) an effective packed-native processing scheme that accesses packed voxel coordinates at low cost, (iii) a flexible dual-dataflow execution mechanism that efficiently computes output feature vectors by adapting to layer characteristics, and (iv) a network-wide parallelization strategy that builds kernel maps for all SpC layers concurrently at network start. Our evaluation shows that Spira significantly outperforms prior SpC engines by 1.71x on average and up to 2.31x for end-to-end inference, and by 2.13x on average and up to 3.32x for layer-wise execution across diverse layer configurations.

</details>


### [6] [Aragog: Just-in-Time Model Routing for Scalable Serving of Agentic Workflows](https://arxiv.org/abs/2511.20975)
*Yinwei Dai,Zhuofu Chen,Anand Iyer,Ravi Netravali*

Main category: cs.DC

TL;DR: Aragog系统通过动态调整工作流配置来优化多阶段AI任务的执行效率，在保持精度的同时显著提升吞吐量和降低延迟。


<details>
  <summary>Details</summary>
Motivation: 现有的工作流配置方法在请求执行前固定配置，无法适应系统负载的动态变化，导致在长时间执行过程中配置变得低效。

Method: 将问题分解为一次性路由步骤（识别所有保持精度的配置）和廉价每阶段调度器（使用最新系统观察选择配置），并引入新策略加速每个步骤。

Result: 在多样化工作流和模型家族中，Aragog将最大服务吞吐量提高50.0-217.0%，在峰值请求率下将中位延迟降低32.5-78.9%，同时保持与最昂贵配置相当的精度。

Conclusion: Aragog通过运行时动态配置调整，有效解决了工作流服务中的成本效率问题，为大规模AI工作流服务提供了实用解决方案。

Abstract: Agentic workflows have emerged as a powerful paradigm for solving complex, multi-stage tasks, but serving them at scale is computationally expensive given the many LLM inferences that each request must pass through. Configuration selection, or the cost-aware assignment of workflow agents to specific LLMs, can reduce these costs, but existing approaches bind configuration decisions before request execution, making them ill-suited for the heterogeneous and lengthy execution of workflows. Specifically, system loads can fluctuate rapidly and substantially during a request's lifetime, causing fixed configurations to quickly become suboptimal. We present Aragog, a system that progressively adapts a request's configuration throughout its execution to match runtime dynamics. To make this practical despite the massive space of workflow configurations, Aragog decouples the problem into two core elements -- a one-time routing step that identifies all accuracy-preserving configurations, and a cheap per-stage scheduler that selects among them using up-to-date system observations -- and introduces novel strategies to accelerate each. Across diverse workflows and model families, Aragog increases maximum serving throughput by 50.0--217.0\% and reduces median latency by 32.5--78.9\% at peak request rates, while maintaining accuracy comparable to the most expensive configurations.

</details>


### [7] [Handling of Memory Page Faults during Virtual-Address RDMA](https://arxiv.org/abs/2511.21018)
*Antonis Psistakis*

Main category: cs.DC

TL;DR: 该论文实现了一种与DMA引擎集成的页错误处理机制，通过硬件-软件协同方案解决RDMA通信中的页错误问题，避免了传统内存固定方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统RDMA技术无法容忍页错误，需要固定内存地址空间，这带来了编程复杂性、内存使用限制和效率低下等问题。现代操作系统如Linux的透明大页等优化机制使得固定方法无法完全避免页错误。

Method: 在ExaNeSt项目中，通过ARM SMMU检测页错误，开发硬件-软件解决方案，包括修改Linux SMMU驱动、开发新软件库、调整DMA引擎硬件和调度逻辑，在QFDB平台上实现页错误处理机制。

Result: 实现了能够处理页错误的RDMA机制，与传统的固定内存和预错误处理方法相比具有优势。

Conclusion: 提出的硬件-软件协同页错误处理机制解决了传统RDMA方法的局限性，提供了更高效的内存管理方案。

Abstract: Nowadays, avoiding system calls during cluster communication (e.g., in Data Centers and High Performance Computing) in modern high-speed interconnection networks has become a necessity, due to the high overhead of multiple data copies between kernel and user space. User-level zero-copy Remote Direct Memory Access (RDMA) technologies address this problem by improving performance and reducing system energy consumption. However, traditional RDMA engines cannot tolerate page faults and therefore use various techniques to avoid them.
  State-of-the-art RDMA approaches typically rely on pinning address spaces or multiple pages per application. This method introduces long-term disadvantages due to increased programming complexity (pinning and unpinning buffers), limits on how much memory can be pinned, and inefficient memory utilization. In addition, pinning does not fully prevent page faults because modern operating systems apply internal optimization mechanisms, such as Transparent Huge Pages (THP), which are enabled by default in Linux.
  This thesis implements a page-fault handling mechanism integrated with the DMA engine of the ExaNeSt project. Faults are detected by the ARM System Memory Management Unit (SMMU) and resolved through a hardware-software solution that can request retransmission when needed. This mechanism required modifications to the Linux SMMU driver, the development of a new software library, changes to the DMA engine hardware, and adjustments to the DMA scheduling logic. Experiments were conducted on the Quad-FPGA Daughter Board (QFDB) of ExaNeSt, which uses Xilinx Zynq UltraScale+ MPSoCs.
  Finally, we evaluate our mechanism and compare it against alternatives such as pinning and pre-faulting, and discuss the advantages of our approach.

</details>


### [8] [A Dynamic PD-Disaggregation Architecture for Maximizing Goodput in LLM Inference Serving](https://arxiv.org/abs/2511.20982)
*Junhan Liao,Minxian Xu,Wanyi Zheng,Yan Wang,Kejiang Ye,Rajkumar Buyya,Chengzhong Xu*

Main category: cs.DC

TL;DR: DOPD是一个动态LLM推理系统，通过实时监控负载动态调整预填充和解码实例的分配比例，解决异构工作负载下的生产者-消费者不平衡问题，显著提升系统吞吐量和响应性能。


<details>
  <summary>Details</summary>
Motivation: 当前LLM系统将预填充和解码阶段解耦到不同GPU上，但异构工作负载导致两个实例类型之间的生产者-消费者不平衡，造成资源分配不匹配。

Method: 提出DOPD系统，基于实时负载监控动态调整预填充/解码实例分配比例，结合适当的请求调度策略，解决预填充与解码实例之间的不平衡问题。

Result: 相比vLLM和DistServe，DOPD将系统吞吐量提升1.5倍，P90首token时间降低67.5%，P90每输出token时间降低22.8%，SLO达成率超过99%且使用更少额外资源。

Conclusion: DOPD通过动态调整预填充/解码比例有效解决了LLM推理中的资源不平衡问题，在保证SLO的同时显著提升了系统性能。

Abstract: To meet strict Service-Level Objectives (SLOs),contemporary Large Language Models (LLMs) decouple the prefill and decoding stages and place them on separate GPUs to mitigate the distinct bottlenecks inherent to each phase. However, the heterogeneity of LLM workloads causes producerconsumer imbalance between the two instance types in such disaggregated architecture. To address this problem, we propose DOPD (Dynamic Optimal Prefill/Decoding), a dynamic LLM inference system that adjusts instance allocations to achieve an optimal prefill-to-decoding (P/D) ratio based on real-time load monitoring. Combined with an appropriate request-scheduling policy, DOPD effectively resolves imbalances between prefill and decoding instances and mitigates resource allocation mismatches due to mixed-length requests under high concurrency. Experimental evaluations show that, compared with vLLM and DistServe (representative aggregation-based and disaggregationbased approaches), DOPD improves overall system goodput by up to 1.5X, decreases P90 time-to-first-token (TTFT) by up to 67.5%, and decreases P90 time-per-output-token (TPOT) by up to 22.8%. Furthermore, our dynamic P/D adjustment technique performs proactive reconfiguration based on historical load, achieving over 99% SLOs attainment while using less additional resources.

</details>


### [9] [Automated Dynamic AI Inference Scaling on HPC-Infrastructure: Integrating Kubernetes, Slurm and vLLM](https://arxiv.org/abs/2511.21413)
*Tim Trappen,Robert Keßler,Roland Pabel,Viktor Achter,Stefan Wesner*

Main category: cs.DC

TL;DR: 提出了一种在超级计算机RAMSES上集成vLLM、Slurm和Kubernetes来服务LLM的解决方案，能够高效处理100-1000个并发请求，仅产生约500ms的端到端延迟开销。


<details>
  <summary>Details</summary>
Motivation: 由于AI推理需求增长，特别是在高等教育领域，需要利用现有基础设施的新解决方案。传统HPC操作模型不适应同步、面向用户的动态AI应用工作负载需求。

Method: 在超级计算机RAMSES上集成vLLM、Slurm和Kubernetes来服务大型语言模型(LLM)。

Result: 初始基准测试表明，该架构能够高效扩展处理100、500和1000个并发请求，仅产生约500ms的端到端延迟开销。

Conclusion: 提出的架构成功解决了传统HPC不适应动态AI应用工作负载的问题，实现了高效的LLM服务扩展。

Abstract: Due to rising demands for Artificial Inteligence (AI) inference, especially in higher education, novel solutions utilising existing infrastructure are emerging. The utilisation of High-Performance Computing (HPC) has become a prevalent approach for the implementation of such solutions. However, the classical operating model of HPC does not adapt well to the requirements of synchronous, user-facing dynamic AI application workloads. In this paper, we propose our solution that serves LLMs by integrating vLLM, Slurm and Kubernetes on the supercomputer \textit{RAMSES}. The initial benchmark indicates that the proposed architecture scales efficiently for 100, 500 and 1000 concurrent requests, incurring only an overhead of approximately 500 ms in terms of end-to-end latency.

</details>


### [10] [MemFine: Memory-Aware Fine-Grained Scheduling for MoE Training](https://arxiv.org/abs/2511.21431)
*Lu Zhao,Rong Shi,Shaoqing Zhang,Yueqiang Chen,Baoguo He,Hongfeng Sun,Ziqing Yin,Shangchao Su,Zhiyan Cui,Liang Dong,Xiyuan Li,Lingbin Wang,Jianwei He,Jiesong Ma,Weikang Huang,Jianglei Tong,Dongdong Gao,Jian Zhang,Hong Tian*

Main category: cs.DC

TL;DR: MemFine是一个用于MoE训练的内存感知细粒度调度框架，通过分块重组策略减少48.03%的激活内存，在内存受限的GPU上实现稳定的大规模MoE训练。


<details>
  <summary>Details</summary>
Motivation: 大规模MoE模型训练面临严重的内存瓶颈，动态令牌路由导致的负载不平衡会造成GPU内存溢出，限制模型可扩展性。现有负载平衡方法会损害模型精度且在内存受限硬件上失效。

Method: MemFine将令牌分布和专家计算分解为可管理块，采用分块重组策略，通过理论内存模型动态优化以平衡内存效率和吞吐量。

Result: 实验表明MemFine相比完全重组基线减少48.03%的激活内存，提高4.42%的吞吐量。

Conclusion: MemFine能够在内存受限的GPU上实现稳定的大规模MoE训练，有效解决了内存瓶颈问题。

Abstract: The training of large-scale Mixture of Experts (MoE) models faces a critical memory bottleneck due to severe load imbalance caused by dynamic token routing. This imbalance leads to memory overflow on GPUs with limited capacity, constraining model scalability. Existing load balancing methods, which cap expert capacity, compromise model accuracy and fail on memory-constrained hardware. To address this, we propose MemFine, a memory-aware fine-grained scheduling framework for MoE training. MemFine decomposes the token distribution and expert computation into manageable chunks and employs a chunked recomputation strategy, dynamically optimized through a theoretical memory model to balance memory efficiency and throughput. Experiments demonstrate that MemFine reduces activation memory by 48.03% and improves throughput by 4.42% compared to full recomputation-based baselines, enabling stable large-scale MoE training on memory-limited GPUs.

</details>


### [11] [Modeling the Effect of Data Redundancy on Speedup in MLFMA Near-Field Computation](https://arxiv.org/abs/2511.21535)
*Morteza Sadeghi*

Main category: cs.DC

TL;DR: 通过引入数据冗余改善MLFMA中近场算子的内存局部性，在GPU上实现最高7倍的内核加速，但由于数据重组开销，端到端应用加速限制在1.04倍。


<details>
  <summary>Details</summary>
Motivation: MLFMA中的近场算子在GPU上因内存局部性差成为性能瓶颈，需要改善内存访问分散问题。

Method: 引入数据冗余减少内存访问分散，提出基于局部性度量的分析模型预测加速趋势，在DBIM-MLFMA和PhotoNs-2.0两个应用上验证。

Result: 内核速度提升最高达7倍，但数据重组开销限制了端到端应用加速至1.04倍，模型能可靠捕捉不同问题规模和密度下的性能趋势。

Conclusion: 数据冗余能提升GPU上P2P算子性能，前提是局部性收益超过数据移动成本，且该技术可最小代码修改注入现有实现。

Abstract: The near-field (P2P) operator in the Multilevel Fast Multipole Algorithm (MLFMA) is a performance bottleneck on GPUs due to poor memory locality. This work introduces data redundancy to improve spatial locality by reducing memory access dispersion. For validation of results, we propose an analytical model based on a Locality metric that combines data volume and access dispersion to predict speedup trends without hardware-specific profiling. The approach is validated on two MLFMA-based applications: an electromagnetic solver (DBIM-MLFMA) with regular structure, and a stellar dynamics code (PhotoNs-2.0) with irregular particle distribution. Results show up to 7X kernel speedup due to improved cache behavior. However, increased data volume raises overheads in data restructuring, limiting end-to-end application speedup to 1.04X. While the model cannot precisely predict absolute speedups, it reliably captures performance trends across different problem sizes and densities. The technique is injectable into existing implementations with minimal code changes. This work demonstrates that data redundancy can enhance GPU performance for P2P operator, provided locality gains outweigh data movement costs.

</details>


### [12] [Diagonal Scaling: A Multi-Dimensional Resource Model and Optimization Framework for Distributed Databases](https://arxiv.org/abs/2511.21612)
*Shahir Abdullah,Syed Rohit Zaman*

Main category: cs.DC

TL;DR: 提出二维扩展平面模型，结合水平扩展（节点数）和垂直扩展（单节点资源），通过DIAGONALSCALE算法计算最优对角线扩展路径，显著提升性能并降低成本。


<details>
  <summary>Details</summary>
Motivation: 现有云数据库将扩展视为一维决策（水平或垂直扩展），这种简化视图导致系统对负载波动的响应不理想，经常出现过度扩展、内存压力响应不足或在次优状态间振荡的问题。

Method: 引入扩展平面二维模型，将数据库配置表示为(H,V)点，定义延迟、吞吐量、协调开销和成本的平滑近似函数，提出DIAGONALSCALE离散局部搜索算法评估水平、垂直和对角线移动。

Result: 对角线扩展相比纯水平或垂直自动扩展，p95延迟降低达40%，每查询成本降低达37%，重新平衡次数减少2-5倍。

Conclusion: 研究结果强调需要多维扩展模型，为下一代云数据库自动扩展系统奠定基础。

Abstract: Modern cloud databases present scaling as a binary decision: scale-out by adding nodes or scale-up by increasing per-node resources. This one-dimensional view is limiting because database performance, cost, and coordination overhead emerge from the joint interaction of horizontal elasticity and per-node CPU, memory, network bandwidth, and storage IOPS. As a result, systems often overreact to load spikes, underreact to memory pressure, or oscillate between suboptimal states. We introduce the Scaling Plane, a two-dimensional model in which each distributed database configuration is represented as a point (H, V), with H denoting node count and V a vector of resources. Over this plane, we define smooth approximations of latency, throughput, coordination overhead, and monetary cost, providing a unified view of performance trade-offs. We show analytically and empirically that optimal scaling trajectories frequently lie along diagonal paths: sequences of joint horizontal and vertical adjustments that simultaneously exploit cluster parallelism and per-node improvements. To compute such actions, we propose DIAGONALSCALE, a discrete local-search algorithm that evaluates horizontal, vertical, and diagonal moves in the Scaling Plane and selects the configuration minimizing a multi-objective function subject to SLA constraints. Using synthetic surfaces, microbenchmarks, and experiments on distributed SQL and KV systems, we demonstrate that diagonal scaling reduces p95 latency by up to 40 percent, lowers cost-per-query by up to 37 percent, and reduces rebalancing by 2 to 5 times compared to horizontal-only and vertical-only autoscaling. Our results highlight the need for multi-dimensional scaling models and provide a foundation for next-generation autoscaling in cloud database systems.

</details>


### [13] [AI/ML Model Cards in Edge AI Cyberinfrastructure: towards Agentic AI](https://arxiv.org/abs/2511.21661)
*Beth Plale,Neelesh Karthikeyan,Isuru Gamage,Joe Stubbs,Sachith Withana*

Main category: cs.DC

TL;DR: 该研究评估了在Patra模型卡系统中采用模型上下文协议(MCP)作为接口的效益与权衡，包括MCP与REST接口的性能开销比较，以及MCP在支持动态模型卡活跃会话方面的适用性。


<details>
  <summary>Details</summary>
Motivation: 传统AI/ML模型卡在训练阶段的一次性评估无法反映模型在实际使用过程中的动态表现，需要研究模型卡作为动态对象在软件生态系统中的实现方式。

Method: 通过嵌入ICICLE AI研究所软件生态系统的Patra模型卡，研究模型卡作为动态对象，并评估采用MCP作为Patra模型卡服务器接口的效益与权衡。

Result: 定量评估显示MCP相比REST接口存在性能开销，但核心价值在于MCP支持的活跃会话能力，这是关于动态模型卡适用性的定性问题。

Conclusion: MCP作为接口在动态模型卡系统中具有独特价值，特别是在支持活跃会话方面，尽管存在一定的性能开销，但其在动态模型管理中的适用性值得关注。

Abstract: AI/ML model cards can contain a benchmarked evaluation of an AI/ML model against intended use but a one time assessment during model training does not get at how and where a model is actually used over its lifetime. Through Patra Model Cards embedded in the ICICLE AI Institute software ecosystem we study model cards as dynamic objects. The study reported here assesses the benefits and tradeoffs of adopting the Model Context Protocol (MCP) as an interface to the Patra Model Card server. Quantitative assessment shows the overhead of MCP as compared to a REST interface. The core question however is of active sessions enabled by MCP; this is a qualitative question of fit and use in the context of dynamic model cards that we address as well.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [14] [RISC-V Based TinyML Accelerator for Depthwise Separable Convolutions in Edge AI](https://arxiv.org/abs/2511.21232)
*Muhammed Yildirim,Ozcan Ozturk*

Main category: cs.AR

TL;DR: 提出一种融合像素级数据流硬件加速器，消除深度可分离卷积中间缓冲区，减少87%数据传输，在FPGA上实现59.3倍加速。


<details>
  <summary>Details</summary>
Motivation: 解决边缘AI和TinyML应用中深度可分离卷积逐层执行导致的高能耗和延迟问题，特别是中间特征图传输到片上缓冲区或片外DRAM的内存瓶颈。

Method: 设计融合像素级数据流的硬件加速器架构，作为RISC-V处理器的定制功能单元，通过紧密耦合流水线在所有DSC阶段（扩展、深度卷积、投影）中完成单个输出像素计算，无需中间缓冲区。

Result: 在Xilinx Artix-7 FPGA上相比RISC-V核心软件执行实现59.3倍加速；ASIC合成显示28nm工艺下0.284mm²面积、910mW功耗，40nm工艺下1.20mm²面积、233mW功耗。

Conclusion: 验证了在TinyML资源约束内实现零缓冲区数据流的可行性，为边缘AI加速器克服内存墙提供了新颖有效的策略。

Abstract: The increasing demand for on-device intelligence in Edge AI and TinyML applications requires the efficient execution of modern Convolutional Neural Networks (CNNs). While lightweight architectures like MobileNetV2 employ Depthwise Separable Convolutions (DSC) to reduce computational complexity, their multi-stage design introduces a critical performance bottleneck inherent to layer-by-layer execution: the high energy and latency cost of transferring intermediate feature maps to either large on-chip buffers or off-chip DRAM. To address this memory wall, this paper introduces a novel hardware accelerator architecture that utilizes a fused pixel-wise dataflow. Implemented as a Custom Function Unit (CFU) for a RISC-V processor, our architecture eliminates the need for intermediate buffers entirely, reducing the data movement up to 87\% compared to conventional layer-by-layer execution. It computes a single output pixel to completion across all DSC stages-expansion, depthwise convolution, and projection-by streaming data through a tightly-coupled pipeline without writing to memory. Evaluated on a Xilinx Artix-7 FPGA, our design achieves a speedup of up to 59.3x over the baseline software execution on the RISC-V core. Furthermore, ASIC synthesis projects a compact 0.284 mm$^2$ footprint with 910 mW power at 2 GHz in 28 nm, and a 1.20 mm$^2$ footprint with 233 mW power at 300 MHz in 40 nm. This work confirms the feasibility of a zero-buffer dataflow within a TinyML resource envelope, offering a novel and effective strategy for overcoming the memory wall in edge AI accelerators.

</details>


### [15] [Bombyx: OpenCilk Compilation for FPGA Hardware Acceleration](https://arxiv.org/abs/2511.21346)
*Mohamed Shahawy,Julien de Castelnau,Paolo Ienne*

Main category: cs.AR

TL;DR: Bombyx是一个编译器工具链，将OpenCilk程序转换为Cilk-1风格的中间表示，使CPU导向的任务级并行应用能高效映射到FPGA空间架构上。


<details>
  <summary>Details</summary>
Motivation: 现有FPGA上的任务级并行架构通常使用OpenCilk的隐式任务模型，这需要在硬件中进行昂贵的上下文切换。Cilk-1的显式延续传递模型更符合FPGA的流式特性。

Method: 开发Bombyx编译器工具链，支持多种编译目标：OpenCilk兼容运行时和使用Vitis HLS的可综合PE生成器。引入解耦访问-执行优化，自动生成高性能处理单元。

Result: 实现了高效的CPU任务级并行应用到FPGA的映射，改进了内存-计算重叠和整体吞吐量。

Conclusion: Bombyx通过Cilk-1风格的中间表示和优化技术，成功将CPU导向的任务级并行应用适配到FPGA空间架构，提供了比传统OpenCilk方法更高效的解决方案。

Abstract: Task-level parallelism (TLP) is a widely used approach in software where independent tasks are dynamically created and scheduled at runtime. Recent systems have explored architectural support for TLP on field-programmable gate arrays (FPGAs), often leveraging high-level synthesis (HLS) to create processing elements (PEs). In this paper, we present Bombyx, a compiler toolchain that lowers OpenCilk programs into a Cilk-1-inspired intermediate representation, enabling efficient mapping of CPU-oriented TLP applications to spatial architectures on FPGAs. Unlike OpenCilk's implicit task model, which requires costly context switching in hardware, Cilk-1 adopts explicit continuation-passing - a model that better aligns with the streaming nature of FPGAs. Bombyx supports multiple compilation targets: one is an OpenCilk-compatible runtime for executing Cilk-1-style code using the OpenCilk backend, and another is a synthesizable PE generator designed for HLS tools like Vitis HLS. Additionally, we introduce a decoupled access-execute optimization that enables automatic generation of high-performance PEs, improving memory-compute overlap and overall throughput.

</details>


### [16] [A Jammer-Resilient 2.87 mm$^2$ 1.28 MS/s 310 mW Multi-Antenna Synchronization ASIC in 65 nm](https://arxiv.org/abs/2511.21451)
*Flurin Arquint,Oscar Castañeda,Gian Marti,Christoph Studer*

Main category: cs.AR

TL;DR: 首个抗干扰多天线时间同步ASIC实现，支持单天线发射器与16天线接收器同步，可抵御最多2天线智能干扰器。


<details>
  <summary>Details</summary>
Motivation: 解决无线通信中同步信号易受干扰攻击的问题，提供硬件层面的抗干扰同步解决方案。

Method: 基于多天线处理算法，在65nm工艺下设计ASIC芯片，实现抗干扰时间同步功能。

Result: 芯片核心面积2.87mm²，功耗310mW，采样率1.28MS/s，成功实现抗干扰同步。

Conclusion: 该ASIC证明了多天线处理在硬件层面实现抗干扰时间同步的可行性，为安全通信系统提供了实用解决方案。

Abstract: We present the first ASIC implementation of jammer-resilient multi-antenna time synchronization. The ASIC implements a recent algorithm that mitigates jamming attacks on synchronization signals using multi-antenna processing. Our design supports synchronization between a single-antenna transmitter and a 16-antenna receiver while mitigating smart jammers with up to two transmit antennas. The fabricated 65 nm ASIC has a core area of 2.87 mm$^2$, consumes a power of 310 mW, and supports a sampling rate of 1.28 mega-samples per second (MS/s).

</details>


### [17] [A 0.32 mm$^2$ 100 Mb/s 223 mW ASIC in 22FDX for Joint Jammer Mitigation, Channel Estimation, and SIMO Data Detection](https://arxiv.org/abs/2511.21461)
*Jonas Elmiger,Fabian Stuber,Oscar Castañeda,Gian Marti,Christoph Studer*

Main category: cs.AR

TL;DR: 提出首个SIMO接收器ASIC，集成干扰抑制、信道估计和数据检测功能，采用MAED算法对抗智能干扰器，在22nm FD-SOI工艺下实现100Mb/s吞吐量。


<details>
  <summary>Details</summary>
Motivation: 传统接收器难以有效对抗智能干扰器，需要开发能同时处理干扰抑制、信道估计和数据检测的集成解决方案。

Method: 采用MAED算法，通过非线性优化问题统一干扰器估计与消除、信道估计和数据检测，支持8个接收天线。

Result: 在22nm FD-SOI工艺下实现0.32mm²核心面积，功耗223mW，吞吐量100Mb/s，相比现有技术用户吞吐量提升3倍，面积效率提升4.5倍。

Conclusion: 该ASIC证明了MAED算法在硬件实现中的可行性，为抗干扰通信系统提供了高效解决方案。

Abstract: We present the first single-input multiple-output (SIMO) receiver ASIC that jointly performs jammer mitigation, channel estimation, and data detection. The ASIC implements a recent algorithm called siMultaneous mitigAtion, Estimation, and Detection (MAED). MAED mitigates smart jammers via spatial filtering using a nonlinear optimization problem that unifies jammer estimation and nulling, channel estimation, and data detection to achieve state-of-the-art error-rate performance under jamming. The design supports eight receive antennas and enables mitigation of smart jammers as well as of barrage jammers. The ASIC is fabricated in 22 nm FD-SOI, has a core area of 0.32 mm$^2$, and achieves a throughput of 100 Mb/s at 223 mW, thus delivering 3$\times$ higher per-user throughput and 4.5$\times$ higher area efficiency than the state-of-the-art jammer-resilient detector.

</details>


### [18] [Modeling and Optimizing Performance Bottlenecks for Neuromorphic Accelerators](https://arxiv.org/abs/2511.21549)
*Jason Yik,Walter Gallego Gomez,Andrew Cheng,Benedetto Leto,Alessandro Pierro,Noah Pacik-Nelson,Korneel Van den Berghe,Vittorio Fra,Andreea Danielescu,Gianvito Urgese,Vijay Janapa Reddi*

Main category: cs.AR

TL;DR: 本文首次对神经形态加速器进行了全面的性能边界和瓶颈分析，揭示了传统指标的不足，提出了floorline性能模型和优化方法，在保持准确率的同时显著提升了性能和能效。


<details>
  <summary>Details</summary>
Motivation: 神经形态加速器具有独特的架构特性，其性能动态与传统加速器有根本差异。现有优化方法依赖网络级稀疏度和操作计数，但这些指标对实际部署性能的改善程度尚不明确。

Method: 通过理论分析建模和三种真实神经形态加速器（Brainchip AKD1000、Synsense Speck、Intel Loihi 2）的实证表征，建立了三种瓶颈状态：内存受限、计算受限和流量受限，并开发了floorline性能模型。

Result: 提出的优化方法结合稀疏感知训练和floorline指导的分区策略，相比之前手动调优配置，在保持相同准确率的情况下实现了最高3.86倍的运行时改进和3.38倍的能耗降低。

Conclusion: 研究揭示了神经形态加速器性能瓶颈的关键因素，提出的floorline模型和优化方法为工作负载优化提供了有效指导，显著提升了实际部署性能。

Abstract: Neuromorphic accelerators offer promising platforms for machine learning (ML) inference by leveraging event-driven, spatially-expanded architectures that naturally exploit unstructured sparsity through co-located memory and compute. However, their unique architectural characteristics create performance dynamics that differ fundamentally from conventional accelerators. Existing workload optimization approaches for neuromorphic accelerators rely on aggregate network-wide sparsity and operation counting, but the extent to which these metrics actually improve deployed performance remains unknown. This paper presents the first comprehensive performance bound and bottleneck analysis of neuromorphic accelerators, revealing the shortcomings of the conventional metrics and offering an understanding of what facets matter for workload performance. We present both theoretical analytical modeling and extensive empirical characterization of three real neuromorphic accelerators: Brainchip AKD1000, Synsense Speck, and Intel Loihi 2. From these, we establish three distinct accelerator bottleneck states, memory-bound, compute-bound, and traffic-bound, and identify which workload configuration features are likely to exhibit these bottleneck states. We synthesize all of our insights into the floorline performance model, a visual model that identifies performance bounds and informs how to optimize a given workload, based on its position on the model. Finally, we present an optimization methodology that combines sparsity-aware training with floorline-informed partitioning. Our methodology achieves substantial performance improvements at iso-accuracy: up to 3.86x runtime improvement and 3.38x energy reduction compared to prior manually-tuned configurations.

</details>
