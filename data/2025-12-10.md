<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 9]
- [cs.AR](#cs.AR) [Total: 1]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [Modeling the Potential of Message-Free Communication via CXL.mem](https://arxiv.org/abs/2512.08005)
*Stepan Vanecek,Matthew Turner,Manisha Gajbe,Matthew Wolf,Martin Schulz*

Main category: cs.DC

TL;DR: 开发了结合CXL.mem技术的MPI通信性能评估工具链和扩展性能模型，用于预测和优化高性能计算中的跨节点数据交换性能。


<details>
  <summary>Details</summary>
Motivation: CXL.mem技术允许多节点同时访问共享内存池，为高效跨节点通信提供了新可能。需要工具来评估和预测MPI应用如何从CXL.mem中获益，以优化数据交换性能。

Method: 扩展内存跟踪采样工具Mitos，分析MPI应用的数据访问模式（包括节点内MPI缓冲区访问和跨节点MPI流量），构建基于每个MPI调用的扩展性能模型，预测哪些数据传输可通过CXL.mem直接实现获得性能提升。

Result: 工具链成功提取数据访问行为并自动分析生成每个MPI调用的性能模型。在2D热传导迷你应用和HPCG基准测试上验证了模型的有效性，展示了支持针对性优化的能力。

Conclusion: 提出的工具链和性能模型能够有效评估和预测CXL.mem对MPI通信的性能影响，支持识别和优化具有最大加速潜力的MPI调用，为异构内存技术在高性能计算中的应用提供了实用工具。

Abstract: Heterogeneous memory technologies are increasingly important instruments in addressing the memory wall in HPC systems. While most are deployed in single node setups, CXL.mem is a technology that implements memories that can be attached to multiple nodes simultaneously, enabling shared memory pooling. This opens new possibilities, particularly for efficient inter-node communication.
  In this paper, we present a novel performance evaluation toolchain combined with an extended performance model for message-based communication, which can be used to predict potential performance benefits from using CXL.mem for data exchange. Our approach analyzes data access patterns of MPI applications: it analyzes on-node accesses to/from MPI buffers, as well as cross-node MPI traffic to gather a full understanding of the impact of memory performance. We combine this data in an extended performance model to predict which data transfers could benefit from direct CXL.mem implementations as compared to traditional MPI messages. Our model works on a per-MPI call granularity, allowing the identification and later optimizations of those MPI invocations in the code with the highest potential for speedup by using CXL.mem.
  For our toolchain, we extend the memory trace sampling tool Mitos and use it to extract data access behavior. In the post-processing step, the raw data is automatically analyzed to provide performance models for each individual MPI call. We validate the models on two sample applications -- a 2D heat transfer miniapp and the HPCG benchmark -- and use them to demonstrate their support for targeted optimizations by integrating CXL.mem.

</details>


### [2] [CapsuleFS A Multi-credential DataCapsule Filesystem](https://arxiv.org/abs/2512.08067)
*Qingyang Hu,Yucheng Huang,Manshi Yang*

Main category: cs.DC

TL;DR: CapsuleFS (CFS) 是首个在POSIX兼容框架内集成多凭证功能的文件系统，基于边缘计算中的全局数据平面构建，使用DataCapsule作为存储提供者。


<details>
  <summary>Details</summary>
Motivation: 在边缘计算环境中，需要一种能够支持多凭证访问控制的文件系统，以提供安全的数据共享和管理功能，同时保持POSIX兼容性。

Method: CFS采用三层架构：1) DataCapsule服务器负责边缘存储和复制；2) 在可信执行环境中运行的中间件管理写权限；3) POSIX兼容的客户端文件系统适配多种架构。

Result: 实验评估显示CFS读写性能相对一般，但功能正确性高，适合实际软件开发场景应用。

Conclusion: CFS是首个实现多凭证功能的POSIX兼容文件系统，虽然性能有待提升，但功能正确性使其具备实际应用价值，未来可进一步优化实用性。

Abstract: CapsuleFS (CFS) is the first filesystem to integrate multi-credential functionality within a POSIX-compliant framework, utilizing DataCapsule as the storage provider. This innovative system is established based on the Global Data Plane in the area of edge computing. Our comprehensive design and implementation of CFS successfully fulfill the objective of providing a multi-credential Common Access API. The architecture of CFS is methodically segmented into three integral components: Firstly, the DataCapsule server, tasked with the storage, dissemination, and replication of DataCapsules on the edge. Secondly, the middleware, a crucial element running in a Trusted Execution Environment responsible for the enforcement and management of write permissions and requests. Finally, the client component, which manifests as a POSIX-compliant filesystem, is adaptable and operational across many architectures. Experimental evaluations of CFS reveal that, while its read and write performances are comparatively modest, it upholds a high degree of functional correctness. This attribute distinctly positions CFS as a viable candidate for application in real-world software development scenarios. The paper also delineates potential future enhancements, aimed at augmenting the practicality of CFS in the landscape of software development.

</details>


### [3] [Chopper: A Multi-Level GPU Characterization Tool & Derived Insights Into LLM Training Inefficiency](https://arxiv.org/abs/2512.08242)
*Marco Kurzynski,Shaizeen Aga,Di Wu*

Main category: cs.DC

TL;DR: Chopper是一个用于分析和可视化多GPU LLM训练性能的框架，首次在AMD MI300X GPU上对Llama 3 8B训练进行了全面端到端分析，发现频率开销是性能差距的主要因素。


<details>
  <summary>Details</summary>
Motivation: 当前对LLM训练性能的理解主要局限于内核级性能或单GPU微基准测试，缺乏对多GPU训练中通信、计算、内存行为和电源管理之间复杂相互作用的深入理解。

Method: 开发了Chopper框架，收集、对齐和可视化GPU内核跟踪和硬件性能计数器，支持从内核到操作、层、阶段、迭代和GPU的多粒度分析。在8-GPU AMD MI300X节点上对Llama 3 8B的FSDP训练进行了全面分析。

Result: 发现内存确定性能够实现更高、更稳定的GPU和内存频率；频率开销（DVFS效应）是理论性能和实际性能差距的最大贡献者，超过了MFMA利用率损失、通信/计算重叠和内核启动开销的影响。

Conclusion: Chopper首次在AMD MI300X GPU上提供了多粒度的LLM训练全面分析，为优化训练框架、改进电源管理策略以及指导未来GPU架构和系统设计提供了可操作的见解。

Abstract: Training large language models (LLMs) efficiently requires a deep understanding of how modern GPU systems behave under real-world distributed training workloads. While prior work has focused primarily on kernel-level performance or single-GPU microbenchmarks, the complex interaction between communication, computation, memory behavior, and power management in multi-GPU LLM training remains poorly characterized. In this work, we introduce Chopper, a profiling and analysis framework that collects, aligns, and visualizes GPU kernel traces and hardware performance counters across multiple granularities (i.e., from individual kernels to operations, layers, phases, iterations, and GPUs). Using Chopper, we perform a comprehensive end-to-end characterization of Llama 3 8B training under fully sharded data parallelism (FSDP) on an eight-GPU AMD InstinctTM MI300X node. Our analysis reveals several previously underexplored bottlenecks and behaviors, such as memory determinism enabling higher, more stable GPU and memory frequencies. We identify several sources of inefficiencies, with frequency overhead (DVFS effects) being the single largest contributor to the gap between theoretical and observed performance, exceeding the impact of MFMA utilization loss, communication/computation overlap, and kernel launch overheads. Overall, Chopper provides the first holistic, multi-granularity characterization of LLM training on AMD InstinctTM MI300X GPUs, yielding actionable insights for optimizing training frameworks, improving power-management strategies, and guiding future GPU architecture and system design.

</details>


### [4] [Synergizing Monetization, Orchestration, and Semantics in Computing Continuum](https://arxiv.org/abs/2512.08288)
*Chinmaya Kumar Dehury,Lauri Lovén,Praveen Kumar Donta,Ilir Murturi,Schahram Dustdar*

Main category: cs.DC

TL;DR: HERMES是一个用于异构计算连续体的框架，通过资源货币化、编排和语义互操作性，解决云到边缘应用的扩展性、互操作性和信任问题。


<details>
  <summary>Details</summary>
Motivation: 工业对从云到边缘的超分布式应用需求增长，但现有解决方案在可扩展性、互操作性和信任方面存在固有局限性，无法满足这些需求。

Method: 引入HERMES框架，建立开放、无缝、安全的环境，实现从云服务器到边缘设备的智能资源编排，通过分布式市场进行数据和服务的货币化，并通过语义互操作性共享知识。

Result: HERMES通过整合资源编排、货币化和语义互操作性，为新一代分布式应用奠定了基础，使其更高效、可信和自主。

Conclusion: HERMES框架通过解决计算连续体中的关键挑战，为智能制造、交通和农业等领域的超分布式应用提供了创新解决方案。

Abstract: Industry demands are growing for hyper-distributed applications that span from the cloud to the edge in domains such as smart manufacturing, transportation, and agriculture. Yet today's solutions struggle to meet these demands due to inherent limitations in scalability, interoperability, and trust. In this article, we introduce HERMES (Heterogeneous Computing Continuum with Resource Monetization, Orchestration, and Semantic) - a novel framework designed to transform connectivity and data utilization across the computing continuum. HERMES establishes an open, seamless, and secure environment where resources, from cloud servers to tiny edge devices, can be orchestrated intelligently, data and services can be monetized in a distributed marketplace, and knowledge is shared through semantic interoperability. By bridging these key facets, HERMES lays a foundation for a new generation of distributed applications that are more efficient, trustworthy, and autonomous.

</details>


### [5] [Emulation of Complex Matrix Multiplication based on the Chinese Remainder Theorem](https://arxiv.org/abs/2512.08321)
*Yuki Uchino,Qianxiang Ma,Toshiyuki Imamura,Katsuhisa Ozaki,Patrick Lars Gutsche*

Main category: cs.DC

TL;DR: 提出基于Ozaki-II方案的高性能复数矩阵乘法仿真方法，在INT8矩阵引擎上实现单双精度复数矩阵运算，相比cuBLAS原生实现获得4-6.5倍加速。


<details>
  <summary>Details</summary>
Motivation: 现代计算架构中低精度矩阵乘法单元比高精度单元具有更高的吞吐量，因此利用低精度硬件仿真高精度矩阵乘法成为高性能计算领域的重要研究方向。

Method: 基于Ozaki-II方案框架，开发在INT8矩阵引擎上仿真单双精度复数矩阵乘法的高性能方法，扩展了先前针对实数矩阵乘法的研究。

Result: 在NVIDIA B200 GPU上，相比cuBLAS原生单双精度复数矩阵乘法，分别获得4.0-5.6倍和4.4-6.5倍加速。方法可根据需求在精度和速度之间灵活权衡。

Conclusion: 提出的方法具有作为默认算法在广泛应用中使用的潜力，能够在精度和计算效率之间提供灵活的权衡选择。

Abstract: Modern computing architectures feature low-precision matrix multiplication units that achieve substantially higher throughput than their high-precision counterparts. Motivated by this architectural trend, the emulation of high-precision matrix multiplication using low-precision hardware has attracted significant interest in the high-performance computing community. Ozaki, Uchino, and Imamura introduced the Ozaki-II scheme as a general framework for emulating matrix multiplication. Building on this framework, Uchino, Ozaki, and Imamura developed high-performance and power-efficient techniques for emulating single- and double-precision real matrix multiplication on INT8 matrix engines. Extending this line of research, the present study proposes high-performance emulation methods for single- and double-precision complex matrix multiplication on INT8 matrix engines, based on the Ozaki-II scheme. On an NVIDIA B200 GPU, the proposed methods achieve 4.0x--5.6x and 4.4x--6.5x speedups over the native single- and double-precision complex matrix multiplication routines from cuBLAS, respectively, for sufficiently large problem sizes. When lower accuracy than that of the standard routine is acceptable, the proposed methods can operate at even higher speed. Conversely, with only a modest increase in computation time, they can also deliver higher accuracy than the standard routines. These properties suggest that the proposed approach has the potential to serve as a default algorithm across a wide range of applications.

</details>


### [6] [Magneton: Optimizing Energy Efficiency of ML Systems via Differential Energy Debugging](https://arxiv.org/abs/2512.08365)
*Yi Pan,Wenbo Qian,Dedong Xie,Ruiyan Hu,Yigong Hu,Baris Kasikci*

Main category: cs.DC

TL;DR: 论文提出了一种名为Magneton的差分能量调试方法，通过比较相似ML系统的能量消耗来检测和诊断软件能量浪费问题。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型的训练和部署能耗巨大，现有优化主要关注硬件能效，但软件设计不良导致的能量浪费被忽视。ML框架和应用中普遍存在冗余或设计不良的操作，消耗更多能量却不提升性能，而开发者缺乏检测和诊断这些问题的工具和可见性。

Method: 提出差分能量调试方法，基于相似ML系统实现相同功能但能量消耗差异巨大的观察。设计并实现Magneton能量分析器，在算子级别比较相似ML系统的能量消耗，自动定位导致过度能量使用的代码区域和配置选择。

Result: 应用于9个流行的ML系统（涵盖LLM推理、通用ML框架和图像生成），Magneton检测并诊断了16个已知的软件能量低效案例，并进一步发现了8个先前未知的案例，其中7个已得到开发者确认。

Conclusion: 差分能量调试是有效的软件能量浪费检测方法，Magneton工具能够成功识别ML系统中的能量低效问题，包括已知和未知的案例，为开发者提供了检测和诊断软件能量浪费的能力。

Abstract: The training and deployment of machine learning (ML) models have become extremely energy-intensive. While existing optimization efforts focus primarily on hardware energy efficiency, a significant but overlooked source of inefficiency is software energy waste caused by poor software design. This often includes redundant or poorly designed operations that consume more energy without improving performance. These inefficiencies arise in widely used ML frameworks and applications, yet developers often lack the visibility and tools to detect and diagnose them.
  We propose differential energy debugging, a novel approach that leverages the observation that competing ML systems often implement similar functionality with vastly different energy consumption. Building on this insight, we design and implement Magneton, an energy profiler that compares energy consumption between similar ML systems at the operator level and automatically pinpoints code regions and configuration choices responsible for excessive energy use. Applied to 9 popular ML systems spanning LLM inference, general ML frameworks, and image generation, Magneton detects and diagnoses 16 known cases of software energy inefficiency and further discovers 8 previously unknown cases, 7 of which have been confirmed by developers.

</details>


### [7] [Basic Lock Algorithms in Lightweight Thread Environments](https://arxiv.org/abs/2512.08563)
*Taras Skazhenik,Nikolai Korobenikov,Andrei Churbanov,Anton Malakhov,Vitaly Aksenov*

Main category: cs.DC

TL;DR: 该论文研究了轻量级线程（协程）环境下的互斥锁实现，发现传统OS线程锁在轻量级线程中会导致死锁，提出了适用于不同轻量级线程库的TTAS和MCS锁改进方案，并推荐使用cohort锁作为通用解决方案。


<details>
  <summary>Details</summary>
Motivation: 传统多线程数据结构是为操作系统线程设计的，而轻量级线程（协程/异步调用）的锁实现研究不足。轻量级线程虽然启动和上下文切换开销低，但需要手动调用上下文切换才能实现并行，传统OS线程锁在这种新环境下无法有效工作且可能导致死锁。

Method: 针对轻量级线程环境，修改了TTAS和MCS锁的实现，特别关注轻量级线程的两种上下文切换机制：yielding（让出）和sleeping（休眠）。分析了不同轻量级线程库的差异，并提出了cohort锁作为通用解决方案，该锁结合了MCS队列和TTAS锁的优点。

Result: 研究表明TTAS和MCS锁在轻量级线程环境中的性能表现差异显著，取决于具体设置。轻量级线程的yielding和sleeping机制对锁性能至关重要。cohort锁通过使用多个MCS队列配合公共TTAS锁，在各种轻量级线程库中都能取得良好平衡。

Conclusion: 轻量级线程环境需要专门设计的互斥锁实现，传统OS线程锁不适用。cohort锁作为通用解决方案，结合了MCS和TTAS的优点，能够在不同轻量级线程库中提供良好性能，解决了轻量级线程环境下的锁实现问题。

Abstract: Traditionally, multithreaded data structures have been designed for access by the threads of Operating Systems (OS). However, implementations for access by programmable alternatives known as lightweight threads (also referred to as asynchronous calls or coroutines) have not been thoroughly studied. The main advantage of lightweight threads is their significantly lower overhead during launch and context switching. However, this comes at a cost: to achieve proper parallelism, context switches must be manually invoked in the code; without these switches, new lightweight threads will never be executed.
  In this paper, we focus on the simplest multithreaded data structure: a mutex (also known as a lock). We demonstrate that original implementations for OS threads cannot be used effectively in this new context due to the potential for deadlocks. Furthermore, correctness is not the only concern. In certain languages, such as C++, there are various lightweight thread libraries, each with different implementations and interfaces, which necessitate distinct lock implementations.
  In this work, we present a modification of TTAS and MCS locks for the use from lightweight threads and demonstrate that the two context switch mechanisms of lightweight threads, yielding and sleeping, are crucial. However, the performance of TTAS and MCS may differ significantly depending on the settings. If one wants to have a lock that works well for any library, we suggest using the cohort lock, which strikes a balance between MCS and TTAS by utilizing several MCS queues with a common TTAS.

</details>


### [8] [Model-based Testing of Practical Distributed Systems in Actor Model](https://arxiv.org/abs/2512.08698)
*Ilya Kokorin,Evgeny Chernatskiy,Vitaly Aksenov*

Main category: cs.DC

TL;DR: 提出一种基于有限状态自动机的模型测试方法，为分布式系统生成全覆盖测试套件，无需修改代码或干扰执行环境，以Viewstamped Replication复制算法为例验证


<details>
  <summary>Details</summary>
Motivation: 分布式系统设计与实现存在挑战，即使有形式化规范和模型验证，实现与规范之间仍有差距，无法保证实现无bug

Method: 使用基于模型的测试方法，将系统模型解释为有限状态自动机，生成覆盖所有可能状态和转换的穷举测试套件，特别针对actor模型编写的分布式系统，无需修改代码或干扰执行环境

Result: 成功验证了基于Viewstamped Replication的复制算法实现，该算法用于实际系统

Conclusion: 提出的方法能有效弥合分布式系统实现与形式化规范之间的差距，通过模型测试保证实现正确性，且对系统无侵入性

Abstract: Designing and implementing distributed systems correctly can be quite challenging. Although these systems are often accompanied by formal specifications that are verified using model-checking techniques, a gap still exists between the implementation and its formal specification: there is no guarantee that the implementation is free of bugs.
  To bridge this gap, we can use model-based testing. Specifically, if the model of the system can be interpreted as a finite-state automaton, we can generate an exhaustive test suite for the implementation that covers all possible states and transitions.
  In this paper, we discuss how to efficiently generate such a test suite for distributed systems written in the actor model. Importantly, our approach does not require any modifications to the code or interfering with the distributed system execution environment. As an example, we verified an implementation of a replication algorithm based on Viewstamped Replication, which is used in a real-world system.

</details>


### [9] [Spatio-Temporal Shifting to Reduce Carbon, Water, and Land-Use Footprints of Cloud Workloads](https://arxiv.org/abs/2512.08725)
*Giulio Attenni,Youssef Moawad,Novella Bartolini,Lauritz Thamsen*

Main category: cs.DC

TL;DR: 通过时空转移云工作负载可显著降低碳、水和土地使用足迹，空间转移效果最明显（20-85%），时间转移效果较小，两者结合效果最佳


<details>
  <summary>Details</summary>
Motivation: 研究如何通过云工作负载的时空转移来减少云计算的环境足迹（碳、水、土地使用），探索可持续云计算的可能性

Method: 使用真实世界数据进行模拟研究，包括AWS和Azure云提供商数据，以及大数据分析和FaaS应用的工作负载追踪，分析空间转移和时间转移策略

Result: 空间转移能显著降低环境足迹（20-85%），时间转移效果较小但仍有帮助，两者结合效果最佳；敏感性分析显示策略对电网数据预测误差和季节变化具有鲁棒性

Conclusion: 云工作负载的时空转移是减少环境足迹的有效策略，空间转移为主要驱动力，时间转移提供额外增量效益，该策略在不同条件下具有鲁棒性

Abstract: In this paper, we investigate the potential of spatial and temporal cloud workload shifting to reduce carbon, water, and land-use footprints. Specifically, we perform a simulation study using real-world data from multiple cloud providers (AWS and Azure) and workload traces for different applications (big data analytics and FaaS). Our simulation results indicate that spatial shifting can substantially lower carbon, water, and land use footprints, with observed reductions ranging from 20% to 85%, depending on the scenario and optimization criteria. Temporal shifting also decreases the footprint, though to a lesser extent. When applied together, the two strategies yield the greatest overall reduction, driven mainly by spatial shifting with temporal adjustments providing an additional, incremental benefit. Sensitivity analysis demonstrates that such shifting is robust to prediction errors in grid mix data and to variations across different seasons.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [10] [NysX: An Accurate and Energy-Efficient FPGA Accelerator for Hyperdimensional Graph Classification at the Edge](https://arxiv.org/abs/2512.08089)
*Jebacyril Arockiaraj,Dhruv Parikh,Viktor Prasanna*

Main category: cs.AR

TL;DR: NysX：首个面向边缘设备的端到端FPGA加速器，用于基于Nyström核近似的超维计算图分类，通过混合地标选择、流式架构、完美哈希查找和稀疏感知SpMV引擎等优化，实现实时高效推理。


<details>
  <summary>Details</summary>
Motivation: 边缘设备上的实时、高效能图分类应用需求日益增长。基于Nyström核近似的超维计算（HDC）方法虽然提升了图分类精度，但在边缘加速时面临多个挑战：均匀采样的地标样本冗余、片上内存有限导致Nyström投影矩阵存储困难、昂贵的码本查找操作，以及不规则稀疏性导致的负载不平衡。

Method: 提出NysX加速器，包含四项关键优化：1）混合地标选择策略，结合均匀采样和行列式点过程（DPPs）减少冗余；2）流式架构处理Nyström投影矩阵，最大化外部内存带宽利用率；3）最小完美哈希查找引擎，实现O(1)键值映射且片上内存开销低；4）稀疏感知SpMV引擎，采用静态负载平衡处理不规则稀疏性。

Result: 在AMD Zynq UltraScale+ (ZCU104) FPGA上实现，相比优化CPU基准获得6.85倍加速和169倍能效提升，相比GPU基准获得4.32倍加速和314倍能效提升。在TUDataset基准测试中，平均分类精度提升3.4%。

Conclusion: NysX通过四项创新优化成功解决了Nyström-based HDC图分类在边缘设备上的加速挑战，实现了实时、高效能的推理，为资源受限的边缘平台提供了有效的解决方案。

Abstract: Real-time, energy-efficient inference on edge devices is essential for graph classification across a range of applications. Hyperdimensional Computing (HDC) is a brain-inspired computing paradigm that encodes input features into low-precision, high-dimensional vectors with simple element-wise operations, making it well-suited for resource-constrained edge platforms. Recent work enhances HDC accuracy for graph classification via Nyström kernel approximations. Edge acceleration of such methods faces several challenges: (i) redundancy among (landmark) samples selected via uniform sampling, (ii) storing the Nyström projection matrix under limited on-chip memory, (iii) expensive, contention-prone codebook lookups, and (iv) load imbalance due to irregular sparsity in SpMV. To address these challenges, we propose NysX, the first end-to-end FPGA accelerator for Nyström-based HDC graph classification at the edge. NysX integrates four key optimizations: (i) a hybrid landmark selection strategy combining uniform sampling with determinantal point processes (DPPs) to reduce redundancy while improving accuracy; (ii) a streaming architecture for Nyström projection matrix maximizing external memory bandwidth utilization; (iii) a minimal-perfect-hash lookup engine enabling $O(1)$ key-to-index mapping with low on-chip memory overhead; and (iv) sparsity-aware SpMV engines with static load balancing. Together, these innovations enable real-time, energy-efficient inference on resource-constrained platforms. Implemented on an AMD Zynq UltraScale+ (ZCU104) FPGA, NysX achieves $6.85\times$ ($4.32\times$) speedup and $169\times$ ($314\times$) energy efficiency gains over optimized CPU (GPU) baselines, while improving classification accuracy by $3.4\%$ on average across TUDataset benchmarks, a widely used standard for graph classification.

</details>
