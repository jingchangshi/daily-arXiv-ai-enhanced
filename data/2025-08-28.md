<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 7]
- [cs.AR](#cs.AR) [Total: 5]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [HAP: Hybrid Adaptive Parallelism for Efficient Mixture-of-Experts Inference](https://arxiv.org/abs/2508.19373)
*Haoran Lin,Xianzhi Yu,Kang Zhao,Han Bao,Zongyuan Zhan,Ting Hu,Wulong Liu,Zekun Yin,Xin Li,Weiguo Liu*

Main category: cs.DC

TL;DR: HAP提出了一种动态混合并行策略，通过层次化分解MoE架构和整数线性规划优化，显著提升MoE模型推理效率


<details>
  <summary>Details</summary>
Motivation: 现有的MoE模型推理系统采用静态并行策略，无法适应不同推理场景的计算需求变化，缺乏灵活性

Method: 将MoE架构层次化分解为Attention模块和Expert模块，构建专用延迟模拟模型，使用整数线性规划(ILP)求解最优混合并行配置

Result: 在A100、A6000和V100 GPU上分别实现1.68x、1.77x和1.57x的加速比，在Mixtral和Qwen系列模型上展现出良好泛化能力

Conclusion: HAP方法能够动态选择最优并行策略，显著提升MoE模型推理效率，具有很好的通用性和适应性

Abstract: Current inference systems for Mixture-of-Experts (MoE) models primarily
employ static parallelization strategies. However, these static approaches
cannot consistently achieve optimal performance across different inference
scenarios, as they lack the flexibility to adapt to varying computational
requirements. In this work, we propose HAP (Hybrid Adaptive Parallelism), a
novel method that dynamically selects hybrid parallel strategies to enhance MoE
inference efficiency. The fundamental innovation of HAP lies in hierarchically
decomposing MoE architectures into two distinct computational modules: the
Attention module and the Expert module, each augmented with a specialized
inference latency simulation model. This decomposition promotes the
construction of a comprehensive search space for seeking model parallel
strategies. By leveraging Integer Linear Programming (ILP), HAP could solve the
optimal hybrid parallel configurations to maximize inference efficiency under
varying computational constraints. Our experiments demonstrate that HAP
consistently determines parallel configurations that achieve comparable or
superior performance to the TP strategy prevalent in mainstream inference
systems. Compared to the TP-based inference, HAP-based inference achieves
speedups of 1.68x, 1.77x, and 1.57x on A100, A6000, and V100 GPU platforms,
respectively. Furthermore, HAP showcases remarkable generalization capability,
maintaining performance effectiveness across diverse MoE model configurations,
including Mixtral and Qwen series models.

</details>


### [2] [Formal Modeling and Verification of the Algorand Consensus Protocol in CADP](https://arxiv.org/abs/2508.19452)
*Andrea Esposito,Francesco P. Rossi,Marco Bernardo,Francesco Fabris,Hubert Garavel*

Main category: cs.DC

TL;DR: 本文对Algorand共识协议进行了形式化建模和验证，使用概率进程演算分析其在无对抗和恶意节点攻击下的行为，通过CADP工具包实现等价性检查的非干扰框架。


<details>
  <summary>Details</summary>
Motivation: Algorand是一个可扩展且安全的无许可区块链，采用密码学自排序和二进制拜占庭协议实现权益证明共识。为了进行严格的形式化验证，需要建立其共识协议的进程代数模型。

Method: 使用概率进程演算对Algorand共识协议进行建模，捕获参与者行为和各共识步骤的结构化交替。在无对抗环境下验证协议正确性，然后扩展到恶意节点攻击场景，使用CADP验证工具包中的等价性检查非干扰框架进行分析。

Result: 模型验证了Algorand协议在无对抗情况下的正确性，并分析了在协调恶意节点攻击下的鲁棒性和局限性，恶意节点可能强制提交空块而非提议块。

Conclusion: 这项工作展示了形式化方法在分析区块链共识算法中的附加价值，既突出了Algorand协议在对抗假设下的鲁棒性，也揭示了其局限性。

Abstract: Algorand is a scalable and secure permissionless blockchain that achieves
proof-of-stake consensus via cryptographic self-sortition and binary Byzantine
agreement. In this paper, we present a process algebraic model of the Algorand
consensus protocol with the aim of enabling rigorous formal verification. Our
model captures the behavior of participants with respect to the structured
alternation of consensus steps toward a committee-based agreement by means of a
probabilistic process calculus. We validate the correctness of the protocol in
the absence of adversaries and then extend our model to capture the influence
of coordinated malicious nodes that can force the commit of an empty block
instead of the proposed one. The adversarial scenario is analyzed by using an
equivalence-checking-based noninterference framework that we have implemented
in the CADP verification toolkit. In addition to highlighting both the
robustness and the limitations of the Algorand protocol under adversarial
assumptions, this work illustrates the added value of using formal methods for
the analysis of blockchain consensus algorithms.

</details>


### [3] [Towards 6G Intelligence: The Role of Generative AI in Future Wireless Networks](https://arxiv.org/abs/2508.19495)
*Muhammad Ahmed Mohsin,Junaid Ahmad,Muhammad Hamza Nawaz,Muhammad Ali Jamshed*

Main category: cs.DC

TL;DR: 本文探讨了生成式人工智能（GenAI）作为6G网络实现环境智能（AmI）的核心技术，通过生成合成数据、语义消息转换、网络预测和数字孪生更新等能力，将6G从快速网络转变为环境智能生态系统。


<details>
  <summary>Details</summary>
Motivation: 实现全球规模的环境智能需要6G网络具备实时感知、推理和行动能力，而传统AI无法完全满足这些需求，因此需要GenAI来弥补关键差距。

Method: 回顾了基础GenAI模型（GANs、VAEs、扩散模型和生成式变换器），并将其与频谱共享、超可靠低延迟通信、智能安全和情境感知数字孪生等AmI用例相结合，同时探讨6G使能技术如何支持分布式GenAI。

Result: 研究表明GenAI能够有效生成合成传感器和信道数据、转换用户意图为语义消息、预测未来网络条件以及更新数字孪生而不损害隐私。

Conclusion: GenAI不是外围附加技术，而是将6G从更快网络转变为环境智能生态系统的基础要素，但仍面临能效设备训练、可信合成数据、联邦生成学习和AmI标准化等开放挑战。

Abstract: Ambient intelligence (AmI) is a computing paradigm in which physical
environments are embedded with sensing, computation, and communication so they
can perceive people and context, decide appropriate actions, and respond
autonomously. Realizing AmI at global scale requires sixth generation (6G)
wireless networks with capabilities for real time perception, reasoning, and
action aligned with human behavior and mobility patterns. We argue that
Generative Artificial Intelligence (GenAI) is the creative core of such
environments. Unlike traditional AI, GenAI learns data distributions and can
generate realistic samples, making it well suited to close key AmI gaps,
including generating synthetic sensor and channel data in under observed areas,
translating user intent into compact, semantic messages, predicting future
network conditions for proactive control, and updating digital twins without
compromising privacy.
  This chapter reviews foundational GenAI models, GANs, VAEs, diffusion models,
and generative transformers, and connects them to practical AmI use cases,
including spectrum sharing, ultra reliable low latency communication,
intelligent security, and context aware digital twins. We also examine how 6G
enablers, such as edge and fog computing, IoT device swarms, intelligent
reflecting surfaces (IRS), and non terrestrial networks, can host or accelerate
distributed GenAI. Finally, we outline open challenges in energy efficient on
device training, trustworthy synthetic data, federated generative learning, and
AmI specific standardization. We show that GenAI is not a peripheral addition,
but a foundational element for transforming 6G from a faster network into an
ambient intelligent ecosystem.

</details>


### [4] [Taming the Chaos: Coordinated Autoscaling for Heterogeneous and Disaggregated LLM Inference](https://arxiv.org/abs/2508.19559)
*Rongzhi Li,Ruogu Du,Zefang Chu,Sida Zhao,Chunlei Han,Zuocheng Shi,Yiwen Shao,Huanle Han,Long Huang,Zherui Liu,Shufan Liu*

Main category: cs.DC

TL;DR: HeteroScale是一个针对Prefill-Decode解耦架构的协调自动扩展框架，通过拓扑感知调度和基于生产环境实证研究的指标驱动策略，解决了异构硬件利用、网络瓶颈和阶段间负载平衡等核心挑战。


<details>
  <summary>Details</summary>
Motivation: 传统自动扩展器在处理大型语言模型服务时表现不足，特别是对于现代的Prefill-Decode解耦架构，这种架构虽然强大但带来了异构硬件利用效率低、网络瓶颈和预填充-解码阶段间严重不平衡等操作挑战。

Method: 结合拓扑感知调度器（适应异构硬件和网络约束）和基于大规模生产环境实证研究的新型指标驱动策略，使用单一稳健指标来联合扩展预填充和解码资源池。

Result: 在数万GPU的大规模生产环境中部署，平均GPU利用率显著提高26.6个百分点，每天节省数十万GPU小时，同时保持严格的服务水平目标。

Conclusion: HeteroScale有效解决了P/D解耦架构的服务扩展挑战，实现了高效的资源管理和架构平衡，在大规模生产环境中证明了其有效性。

Abstract: Serving Large Language Models (LLMs) is a GPU-intensive task where
traditional autoscalers fall short, particularly for modern Prefill-Decode
(P/D) disaggregated architectures. This architectural shift, while powerful,
introduces significant operational challenges, including inefficient use of
heterogeneous hardware, network bottlenecks, and critical imbalances between
prefill and decode stages. We introduce HeteroScale, a coordinated autoscaling
framework that addresses the core challenges of P/D disaggregated serving.
HeteroScale combines a topology-aware scheduler that adapts to heterogeneous
hardware and network constraints with a novel metric-driven policy derived from
the first large-scale empirical study of autoscaling signals in production. By
leveraging a single, robust metric to jointly scale prefill and decode pools,
HeteroScale maintains architectural balance while ensuring efficient, adaptive
resource management. Deployed in a massive production environment on tens of
thousands of GPUs, HeteroScale has proven its effectiveness, increasing average
GPU utilization by a significant 26.6 percentage points and saving hundreds of
thousands of GPU-hours daily, all while upholding stringent service level
objectives.

</details>


### [5] [Beyond the Bermuda Triangle of Contention: IOMMU Interference in Mixed Criticality Systems](https://arxiv.org/abs/2508.19670)
*Diogo Costa,Jose Martins,Sandro Pinto*

Main category: cs.DC

TL;DR: 这篇论文分析了混合关键性系统中IOMMU结构导致的性能干扰问题，发现小内存交易的翻译开销会导致DMA传输延迟达1.79倍


<details>
  <summary>Details</summary>
Motivation: 混合关键性系统整合异构计算平台时，加速器和DMA设备作为独立总线主控器直接访问内存，导致安全和时间预测性挑战，IOMMU在调节内存访问中发挥关键作用

Method: 使用Xilinx UltraScale+ ZCU104平台分析IOMMU结构内部争用效应，研究共享TLB、缓存效应和翻译开销导致的时间不可预测性

Result: 实验结果显示IOMMU干扰主要影响小内存交易，在Arm SMMUv2实现中对于小字节传输可导致DMA事务延迟达1.79倍

Conclusion: IOMMU结构的共享性质会引入时间不可预测性，尤其是小内存交易的翻译开销影响显著，这种争用效应在不同架构上可能呈现类似行为

Abstract: As Mixed Criticality Systems (MCSs) evolve, they increasingly integrate
heterogeneous computing platforms, combining general-purpose processors with
specialized accelerators such as AI engines, GPUs, and high-speed networking
interfaces. This heterogeneity introduces challenges, as these accelerators and
DMA-capable devices act as independent bus masters, directly accessing memory.
Consequently, ensuring both security and timing predictability in such
environments becomes critical. To address these concerns, the Input-Output
Memory Management Unit (IOMMU) plays a key role in mediating and regulating
memory access, preventing unauthorized transactions while enforcing isolation
and access control policies. While prior work has explored IOMMU-related
side-channel vulnerabilities from a security standpoint, its role in
performance interference remains largely unexplored. Moreover, many of the same
architectural properties that enable side-channel leakage, such as shared TLBs,
caching effects, and translation overheads, can also introduce timing
unpredictability. In this work, we analyze the contention effects within IOMMU
structures using the Xilinx UltraScale+ ZCU104 platform, demonstrating how
their shared nature introduce unpredictable delays. Our findings reveal that
IOMMU-induced interference primarily affects small memory transactions, where
translation overheads significantly impact execution time. Additionally, we
hypothesize that contention effects arising from IOTLBs exhibit similar
behavior across architectures due to shared caching principles, such as
prefetching and hierarchical TLB structures. Notably, our experiments show that
IOMMU interference can delay DMA transactions by up to 1.79x for lower-size
transfers on the Arm SMMUv2 implementation.

</details>


### [6] [Separation of Three or More Autonomous Mobile Models under Hierarchical Schedulers](https://arxiv.org/abs/2508.19805)
*Shota Naito,Tsukasa Ninomiya,Koichi Wada*

Main category: cs.DC

TL;DR: 本文通过引入新问题（ETE、HET、TAR(d)*等）分析了移动机器人模型中能力、灯光可观测性和调度器同步性之间的复杂交互关系，扩展了14个经典机器人模型的分离图谱。


<details>
  <summary>Details</summary>
Motivation: 理解移动机器人系统的计算能力是分布式计算中的基本挑战。先前工作主要关注模型间的两两分离，本文旨在探索机器人能力、灯光可观测性和调度器同步性之间更复杂的交互方式。

Method: 通过设计新的计算问题（ETE、HET、TAR(d)*等）并进行分类分析，研究不同模型下的可解性，包括完全同步机器人、弱同步设置和异步设置下的各种能力组合。

Result: 发现ETE问题仅在最强模型（完全同步机器人+完全互见灯光）中可解；在弱同步下内部内存不足，完全同步可替代灯光和内存；在异步设置下揭示了FSTA和FCOM机器人之间的细粒度分离。

Conclusion: 研究结果扩展了已知的机器人模型分离图谱，揭示了只有通过高阶比较才能看到的结构现象，提供了新的不可能性标准，深化了对可观测性、内存和同步性如何共同塑造移动机器人计算能力的理解。

Abstract: Understanding the computational power of mobile robot systems is a
fundamental challenge in distributed computing. While prior work has focused on
pairwise separations between models, we explore how robot capabilities, light
observability, and scheduler synchrony interact in more complex ways.
  We first show that the Exponential Times Expansion (ETE) problem is solvable
only in the strongest model -- fully-synchronous robots with full mutual lights
($\mathcal{LUMT}^F$). We then introduce the Hexagonal Edge Traversal (HET) and
TAR(d)* problems to demonstrate how internal memory and lights interact with
synchrony: under weak synchrony, internal memory alone is insufficient, while
full synchrony can substitute for both lights and memory.
  In the asynchronous setting, we classify problems such as LP-MLCv, VEC, and
ZCC to show fine-grained separations between $\mathcal{FSTA}$ and
$\mathcal{FCOM}$ robots. We also analyze Vertex Traversal Rendezvous (VTR) and
Leave Place Convergence (LP-Cv), illustrating the limitations of internal
memory in symmetric settings.
  These results extend the known separation map of 14 canonical robot models,
revealing structural phenomena only visible through higher-order comparisons.
Our work provides new impossibility criteria and deepens the understanding of
how observability, memory, and synchrony collectively shape the computational
power of mobile robots.

</details>


### [7] [HPC Digital Twins for Evaluating Scheduling Policies, Incentive Structures and their Impact on Power and Cooling](https://arxiv.org/abs/2508.20016)
*Matthias Maiterth,Wesley H. Brewer,Jaya S. Kuruvella,Arunavo Dey,Tanzima Z. Islam,Kevin Menear,Dmitry Duplyakin,Rashadul Kabir,Tapasya Patki,Terry Jones,Feiyi Wang*

Main category: cs.DC

TL;DR: 首个将调度器与数字孪生技术集成的高性能计算框架，支持部署前的参数配置和调度决策影响分析


<details>
  <summary>Details</summary>
Motivation: 传统调度器评估方法局限于部署后分析或模拟器，无法模拟相关基础设施，需要能够在部署前进行what-if研究的解决方案

Method: 开发首个具有调度能力的数字孪生框架，集成多种顶级HPC系统数据集，实现外部调度模拟器集成扩展

Result: 实现了激励结构评估和基于机器学习的调度评估，支持HPC系统的可持续性和系统影响评估的what-if场景分析

Conclusion: 该数字孪生元框架为HPC调度原型设计提供了创新解决方案，能够在部署前评估参数配置和调度决策对物理资产的影响

Abstract: Schedulers are critical for optimal resource utilization in high-performance
computing. Traditional methods to evaluate schedulers are limited to
post-deployment analysis, or simulators, which do not model associated
infrastructure. In this work, we present the first-of-its-kind integration of
scheduling and digital twins in HPC. This enables what-if studies to understand
the impact of parameter configurations and scheduling decisions on the physical
assets, even before deployment, or regarching changes not easily realizable in
production. We (1) provide the first digital twin framework extended with
scheduling capabilities, (2) integrate various top-tier HPC systems given their
publicly available datasets, (3) implement extensions to integrate external
scheduling simulators. Finally, we show how to (4) implement and evaluate
incentive structures, as-well-as (5) evaluate machine learning based
scheduling, in such novel digital-twin based meta-framework to prototype
scheduling. Our work enables what-if scenarios of HPC systems to evaluate
sustainability, and the impact on the simulated system.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [8] [OmniSim: Simulating Hardware with C Speed and RTL Accuracy for High-Level Synthesis Designs](https://arxiv.org/abs/2508.19299)
*Rishov Sarkar,Cong Hao*

Main category: cs.AR

TL;DR: OmniSim是一个HLS仿真框架，通过软件多线程和FIFO表查询机制，实现了对复杂数据流设计的高效准确仿真，相比传统方法速度提升35.9倍


<details>
  <summary>Details</summary>
Motivation: 现有HLS工具在仿真无限循环和数据流模块时存在局限性，功能验证需要缓慢的RTL仿真，性能指标也依赖RTL仿真，缺乏对高级数据流特性的支持

Method: 采用软件多线程技术，通过查询和更新记录精确硬件时序的FIFO表来协调线程执行，实现功能性和性能仿真的灵活耦合和重叠

Result: 成功仿真了11个先前不被支持的复杂设计，相比传统C/RTL协同仿真速度提升35.9倍，相比最先进的LightningSim在相同测试集上提升6.61倍

Conclusion: OmniSim框架显著扩展了HLS工具的仿真能力，实现了接近C语言仿真速度和接近RTL精度的功能与性能仿真，解决了复杂数据流设计的仿真挑战

Abstract: High-Level Synthesis (HLS) is increasingly popular for hardware design using
C/C++ instead of Register-Transfer Level (RTL). To express concurrent hardware
behavior in a sequential language like C/C++, HLS tools introduce constructs
such as infinite loops and dataflow modules connected by FIFOs. However,
efficiently and accurately simulating these constructs at C level remains
challenging. First, without hardware timing information, functional
verification typically requires slow RTL synthesis and simulation, as the
current approaches in commercial HLS tools. Second, cycle-accurate performance
metrics, such as end-to-end latency, also rely on RTL simulation. No existing
HLS tool fully overcomes the first limitation. For the second, prior work such
as LightningSim partially improves simulation speed but lacks support for
advanced dataflow features like cyclic dependencies and non-blocking FIFO
accesses.
  To overcome both limitations, we propose OmniSim, a framework that
significantly extends the simulation capabilities of both academic and
commercial HLS tools. First, OmniSim enables fast and accurate simulation of
complex dataflow designs, especially those explicitly declared unsupported by
commercial tools. It does so through sophisticated software multi-threading,
where threads are orchestrated by querying and updating a set of FIFO tables
that explicitly record exact hardware timing of each FIFO access. Second,
OmniSim achieves near-C simulation speed with near-RTL accuracy for both
functionality and performance, via flexibly coupled and overlapped
functionality and performance simulations.
  We demonstrate that OmniSim successfully simulates eleven designs previously
unsupported by any HLS tool, achieving up to 35.9x speedup over traditional
C/RTL co-simulation, and up to 6.61x speedup over the state-of-the-art yet less
capable simulator, LightningSim, on its own benchmark suite.

</details>


### [9] [GENIE-ASI: Generative Instruction and Executable Code for Analog Subcircuit Identification](https://arxiv.org/abs/2508.19393)
*Phuoc Pham,Arun Venkitaraman,Chia-Yu Hsieh,Andrea Bonetti,Stefan Uhlich,Markus Leibl,Simon Hofmann,Eisaku Ohbuchi,Lorenzo Servadei,Ulf Schlichtmann,Robert Wille*

Main category: cs.AR

TL;DR: GENIE-ASI是首个基于大语言模型的免训练模拟子电路识别方法，通过上下文学习和代码生成实现SPICE网表中的子电路识别，性能接近基于规则的方法。


<details>
  <summary>Details</summary>
Motivation: 传统模拟子电路识别方法需要大量人工专业知识、基于规则的编码或标注数据集，存在效率低和适应性差的问题。

Method: 采用两阶段方法：首先通过上下文学习从少量示例推导自然语言指令，然后将这些指令转换为可执行的Python代码来识别未见过的SPICE网表中的子电路。

Result: 在提出的新基准测试中，GENIE-ASI在简单结构上达到F1分数1.0（与基于规则方法相当），中等抽象度上为0.81，复杂子电路上为0.31，显示出良好潜力。

Conclusion: 大语言模型可以作为模拟设计自动化中适应性强的通用工具，为模拟设计自动化中基础模型应用开辟了新的研究方向。

Abstract: Analog subcircuit identification is a core task in analog design, essential
for simulation, sizing, and layout. Traditional methods often require extensive
human expertise, rule-based encoding, or large labeled datasets. To address
these challenges, we propose GENIE-ASI, the first training-free, large language
model (LLM)-based methodology for analog subcircuit identification. GENIE-ASI
operates in two phases: it first uses in-context learning to derive natural
language instructions from a few demonstration examples, then translates these
into executable Python code to identify subcircuits in unseen SPICE netlists.
In addition, to evaluate LLM-based approaches systematically, we introduce a
new benchmark composed of operational amplifier netlists (op-amps) that cover a
wide range of subcircuit variants. Experimental results on the proposed
benchmark show that GENIE-ASI matches rule-based performance on simple
structures (F1-score = 1.0), remains competitive on moderate abstractions
(F1-score = 0.81), and shows potential even on complex subcircuits (F1-score =
0.31). These findings demonstrate that LLMs can serve as adaptable,
general-purpose tools in analog design automation, opening new research
directions for foundation model applications in analog design automation.

</details>


### [10] [RARO: Reliability-aware Conversion with Enhanced Read Performance for QLC SSDs](https://arxiv.org/abs/2508.19530)
*Yanyun Wang,Dingcui Yu,Yina Lv,Yunpeng Song,Yumiao Zhao,Liang Shi*

Main category: cs.AR

TL;DR: RARO是一种可靠性感知的混合闪存管理方案，通过基于读取重试统计的智能数据迁移来优化QLC闪存的读取性能，同时最小化容量开销。


<details>
  <summary>Details</summary>
Motivation: QLC闪存虽然成本低容量大，但可靠性有限导致频繁读取重试，严重影响读取性能。现有混合存储方案主要关注写入性能且仅基于数据热度进行迁移，导致过多模式切换和容量开销。

Method: 提出RARO方案，仅在热数据位于高读取重试QLC块时才触发数据迁移，减少不必要转换。支持细粒度多模式转换(SLC-TLC-QLC)，利用实时读取重试统计和闪存特性来优化性能。

Result: 在FEMU平台上的实验表明，RARO在各种工作负载下显著提升读取性能，对可用容量的影响可忽略不计。

Conclusion: RARO通过可靠性感知的智能数据迁移策略，有效解决了QLC闪存读取性能问题，在保持高容量的同时实现了显著的性能提升。

Abstract: Quad-level cell (QLC) flash offers significant benefits in cost and capacity,
but its limited reliability leads to frequent read retries, which severely
degrade read performance. A common strategy in high-density flash storage is to
program selected blocks in a low-density mode (SLC), sacrificing some capacity
to achieve higher I/O performance. This hybrid storage architecture has been
widely adopted in consumer-grade storage systems. However, existing hybrid
storage schemes typically focus on write performance and rely solely on data
temperature for migration decisions. This often results in excessive mode
switching, causing substantial capacity overhead.
  In this paper, we present RARO (Reliability-Aware Read performance
Optimization), a hybrid flash management scheme designed to improve read
performance with minimal capacity cost. The key insight behind RARO is that
much of the read slowdown in QLC flash is caused by read retries. RARO triggers
data migration only when hot data resides in QLC blocks experiencing a high
number of read retries, significantly reducing unnecessary conversions and
capacity loss. Moreover, RARO supports fine-grained multi-mode conversions
(SLC-TLC-QLC) to further minimize capacity overhead. By leveraging real-time
read retry statistics and flash characteristics, RARO mitigates over-conversion
and optimizes I/O performance. Experiments on the FEMU platform demonstrate
that RARO significantly improves read performance across diverse workloads,
with negligible impact on usable capacity.

</details>


### [11] [Support Vector Machines Classification on Bendable RISC-V](https://arxiv.org/abs/2508.19656)
*Polykarpos Vergos,Theofanis Vergos,Florentia Afentaki,Konstantinos Balaskas,Georgios Zervakis*

Main category: cs.AR

TL;DR: 提出了一个用于柔性RISC-V核心的机器学习协处理器开源框架，包括支持SVM算法的定制加速器架构，实现了21倍的推理时间和能效提升


<details>
  <summary>Details</summary>
Motivation: 柔性电子技术具有低成本、轻量化和环保特性，但大特征尺寸和高功耗阻碍了柔性机器学习应用的发展，需要专门的低功耗ML加速解决方案

Method: 开发开源框架支持Bendable RISC-V核心的ML协处理器，设计支持OvO和OvR算法的SVM加速器架构，采用精度可扩展设计支持4/8/16位权重表示

Result: 实验结果显示平均实现了21倍的推理执行时间和能效提升

Conclusion: 该框架和加速器架构展示了在边缘设备上实现低功耗柔性智能的巨大潜力

Abstract: Flexible Electronics (FE) technology offers uniquecharacteristics in
electronic manufacturing, providing ultra-low-cost, lightweight, and
environmentally-friendly alternatives totraditional rigid electronics. These
characteristics enable a rangeof applications that were previously constrained
by the costand rigidity of conventional silicon technology. Machine learning
(ML) is essential for enabling autonomous, real-time intelligenceon devices
with smart sensing capabilities in everyday objects. However, the large feature
sizes and high power consumption ofthe devices oppose a challenge in the
realization of flexible ML applications. To address the above, we propose an
open-source framework for developing ML co-processors for the Bendable RISC-V
core. In addition, we present a custom ML accelerator architecture for Support
Vector Machine (SVM), supporting both one-vs-one (OvO) and one-vs-rest (OvR)
algorithms. Our ML accelerator adopts a generic, precision-scalable design,
supporting 4-, 8-, and 16-bit weight representations. Experimental results
demonstrate a 21x improvement in both inference execution time and energy
efficiency, on average, highlighting its potential for low-power, flexible
intelligence on the edge.

</details>


### [12] [New Tools, Programming Models, and System Support for Processing-in-Memory Architectures](https://arxiv.org/abs/2508.19868)
*Geraldo F. Oliveira*

Main category: cs.AR

TL;DR: 这篇论文提出了四个主要贡献：DAMOV（数据移动瓶颈分析方法和基准套件）、MIMDRAM（硬件/软件协同设计的DRAM处理架构）、Proteus（降低PUD操作延迟的硬件框架）和DaPPA（简化PNM架构编程的框架）。


<details>
  <summary>Details</summary>
Motivation: 为PIM架构（特别是基于DRAM的解决方案）提供工具、编程模型和系统支持，以促进PIM在当前和未来系统中的采用。

Method: 1. DAMOV：系统化分析内存相关数据移动瓶颈的方法论和基准套件
2. MIMDRAM：硬件/软件协同设计，解决DRAM处理架构的可编程性和灵活性限制
3. Proteus：数据感知运行时引擎，通过并行执行、动态精度调整和自适应算法选择降低延迟
4. DaPPA：编程框架，让程序员无需显式管理硬件资源即可编写高效的PIM友好代码

Result: 开发了一套完整的工具链和方法论，显著改善了PIM架构的可编程性、灵活性和执行效率。

Conclusion: 通过这四个创新贡献，为PIM架构的广泛采用提供了必要的工具和基础设施支持，解决了当前PIM系统在可编程性、灵活性和性能方面的关键挑战。

Abstract: Our goal in this dissertation is to provide tools, programming models, and
system support for PIM architectures (with a focus on DRAM-based solutions), to
ease the adoption of PIM in current and future systems. To this end, we make at
least four new major contributions.
  First, we introduce DAMOV, the first rigorous methodology to characterize
memory-related data movement bottlenecks in modern workloads, and the first
data movement benchmark suite. Second, we introduce MIMDRAM, a new
hardware/software co-designed substrate that addresses the major current
programmability and flexibility limitations of the bulk bitwise execution model
of processing-using-DRAM (PUD) architectures. MIMDRAM enables the allocation
and control of only the needed computing resources inside DRAM for PUD
computing. Third, we introduce Proteus, the first hardware framework that
addresses the high execution latency of bulk bitwise PUD operations in
state-of-the-art PUD architectures by implementing a data-aware runtime engine
for PUD. Proteus reduces the latency of PUD operations in three different ways:
(i) Proteus concurrently executes independent in-DRAM primitives belong to a
single PUD operation across DRAM arrays. (ii) Proteus dynamically reduces the
bit-precision (and consequentially the latency and energy consumption) of PUD
operations by exploiting narrow values (i.e., values with many leading zeros or
ones). (iii) Proteus chooses and uses the most appropriate data representation
and arithmetic algorithm implementation for a given PUD instruction
transparently to the programmer. Fourth, we introduce DaPPA (data-parallel
processing-in-memory architecture), a new programming framework that eases
programmability for general-purpose PNM architectures by allowing the
programmer to write efficient PIM-friendly code without the need to manage
hardware resources explicitly.

</details>
