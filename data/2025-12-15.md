<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 1]
- [cs.DC](#cs.DC) [Total: 14]
- [cs.AR](#cs.AR) [Total: 1]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [The Relative Monadic Metalanguage](https://arxiv.org/abs/2512.11762)
*Jack Liell-Cock,Zev Shirazi,Sam Staton*

Main category: cs.PL

TL;DR: 论文将相对单子理论扩展到编程语言设计，提出了两种新的程序演算：用于分级单子的LNL-RMM语言和用于箭头的ARMM语言，并证明了它们相对于现有语言的保守扩展性。


<details>
  <summary>Details</summary>
Motivation: 相对单子为计算提供了受控的视图，但现有的单子元语言需要扩展到相对设置。论文旨在通过相对单子的视角重新审视和扩展两种现有的程序演算：分级单子元语言和箭头演算。

Method: 1. 将单子元语言推广到相对设置，并提供强相对单子的完整语义
2. 提出LNL-RMM语言（线性-非线性语言用于分级单子），并证明它是分级单子元语言的保守扩展
3. 为箭头演算提供完整语义，展示它是受限的相对单子元语言
4. 引入ARMM语言（箭头计算lambda演算风格语言），证明它保守扩展了箭头演算

Result: 1. 成功将单子元语言推广到相对设置，建立了完整的语义理论
2. LNL-RMM语言被证明是分级单子元语言的保守扩展
3. 箭头演算被证明是受限的相对单子元语言
4. ARMM语言被提出并证明保守扩展了箭头演算

Conclusion: 相对单子理论为程序语言设计提供了统一的框架，能够自然地扩展现有的程序演算。论文提出的LNL-RMM和ARMM语言展示了相对单子视角在编程语言理论中的实用价值，为未来的语言设计和语义研究提供了新的工具。

Abstract: Relative monads provide a controlled view of computation. We generalise the monadic metalanguage to a relative setting and give a complete semantics with strong relative monads. Adopting this perspective, we generalise two existing program calculi from the literature. We provide a linear-non-linear language for graded monads, LNL-RMM, along with a semantic proof that it is a conservative extension of the graded monadic metalanguage. Additionally, we provide a complete semantics for the arrow calculus, showing it is a restricted relative monadic metalanguage. This motivates the introduction of ARMM, a computational lambda calculus-style language for arrows that conservatively extends the arrow calculus.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [2] [Agentic Operator Generation for ML ASICs](https://arxiv.org/abs/2512.10977)
*Alec M. Hammond,Aram Markosyan,Aman Dontula,Simon Mahns,Zacharias Fisches,Dmitrii Pedchenko,Keyur Muzumdar,Natacha Supper,Mark Saroufim,Joe Isaacson,Laura Wang,Warren Hunt,Kaustubh Gondkar,Roman Levenstein,Gabriel Synnaeve,Richard Li,Jacob Kahn,Ajit Mathews*

Main category: cs.DC

TL;DR: TritorX是一个AI系统，能够大规模生成功能正确的Triton PyTorch ATen内核，专注于新兴加速器平台，强调覆盖率和正确性而非性能。


<details>
  <summary>Details</summary>
Motivation: 为新兴加速器平台快速生成完整的PyTorch ATen后端，解决传统方法只关注少数高性能内核而缺乏全面覆盖的问题。

Method: 集成开源大语言模型、自定义代码检查器、JIT编译和基于PyTorch OpInfo的测试框架，支持真实MTIA芯片和硬件仿真环境。

Result: 成功为481个独特的ATen算子生成了通过所有PyTorch OpInfo测试的内核和包装器（总计超过20,000个测试）。

Conclusion: TritorX能够在一夜之间为新加速器平台生成完整的PyTorch ATen后端，实现了覆盖率和正确性的突破。

Abstract: We present TritorX, an agentic AI system designed to generate functionally correct Triton PyTorch ATen kernels at scale for emerging accelerator platforms. TritorX integrates open-source large language models with a custom linter, JIT compilation, and a PyTorch OpInfo-based test harness. This pipeline is compatible with both real Meta Training and Inference Accelerator (MTIA) silicon and in hardware simulation environments for next-generation devices. In contrast to previous kernel-generation approaches that prioritize performance for a limited set of high-usage kernels, TritorX prioritizes coverage. Our system emphasizes correctness and generality across the entire operator set, including diverse data types, shapes, and argument patterns. In our experiments, TritorX successfully generated kernels and wrappers for 481 unique ATen operators that pass all corresponding PyTorch OpInfo tests (over 20,000 in total). TritorX paves the way for overnight generation of complete PyTorch ATen backends for new accelerator platforms.

</details>


### [3] [Theoretical Foundations of GPU-Native Compilation for Rapid Code Iteration](https://arxiv.org/abs/2512.11200)
*Adilet Metinov,Gulida M. Kudakeeva,Gulnara D. Kabaeva*

Main category: cs.DC

TL;DR: 提出三种GPU原生编译方法消除CPU-GPU数据传输瓶颈，理论分析显示可实现10-100倍代码迭代加速


<details>
  <summary>Details</summary>
Motivation: 当前AI代码生成系统在编译、执行和测试阶段存在显著的CPU-GPU数据传输延迟瓶颈，限制了代码迭代效率

Method: 提出三种互补的GPU原生编译方法：1) 并行传统编译适配GPU执行；2) 神经编译使用学习到的序列到序列翻译与概率验证；3) 结合两者的混合架构

Result: 理论分析显示：传统GPU编译通过消除传输可获得2-5倍改进，神经编译通过大规模并行实现10-100倍加速，混合方法提供具有保证正确性的实用部署路径

Conclusion: GPU原生编译能显著加速代码迭代周期，概率验证框架允许在编译准确性和并行探索之间权衡，对自改进AI系统和未来模拟计算基板有重要影响

Abstract: Current AI code generation systems suffer from significant latency bottlenecks due to CPU-GPU data transfers during compilation, execution, and testing phases. We establish theoretical foundations for three complementary approaches to GPU-native compilation that eliminate these transfers: (1) parallel traditional compilation adapted for GPU execution, (2) neural compilation using learned sequence-to-sequence translation with probabilistic verification, and (3) hybrid architectures combining both strategies. We derive latency and energy bounds demonstrating potential speedups of 10-100x for code iteration cycles. Our analysis shows that traditional GPU compilation provides 2-5x improvements through transfer elimination, neural compilation achieves 10-100x speedups via massive parallelism, and hybrid approaches offer practical deployment paths with guaranteed correctness. We formalize the probabilistic verification framework that enables trading compilation accuracy for parallel exploration, and discuss implications for self-improving AI systems and future analog computing substrates.

</details>


### [4] [An Efficient Approach for Energy Conservation in Cloud Computing Environment](https://arxiv.org/abs/2512.10974)
*Sohan Kumar Pande,Sanjaya Kumar Panda,Preeti Ranjan Sahu*

Main category: cs.DC

TL;DR: 提出一种基于CPU、磁盘和I/O资源利用率的任务调度算法，通过提高资源利用率来降低云服务能耗


<details>
  <summary>Details</summary>
Motivation: 云服务能耗巨大，传统能源有限且对环境有温室效应，现有研究主要关注平均资源利用率或最小化完成时间，但未充分考虑物理机中不同类型资源（CPU、磁盘、I/O）的差异

Method: 提出基于适应度值的任务调度算法，适应度值是CPU、磁盘、I/O利用率和任务处理时间的函数，通过优化资源利用率来提高活跃资源利用率

Result: 通过合成数据集进行仿真实验，与现有MaxUtil算法比较，结果表明提出的算法更节能，能耗更低

Conclusion: 提出的算法通过显式优化多种资源利用率，实现了更好的能效表现，为云服务节能提供了有效解决方案

Abstract: Recent trends of technology have explored a numerous applications of cloud services, which require a significant amount of energy. In the present scenario, most of the energy sources are limited and have a greenhouse effect on the environment. Therefore, it is the need of the hour that the energy consumed by the cloud service providers must be reduced and it is a great challenge to the research community to develop energy-efficient algorithms. To design the same, some researchers tried to maximize the average resource utilization, whereas some researchers tried to minimize the makespan. However, they have not considered different types of resources that are present in the physical machines. In this paper, we propose a task scheduling algorithm, which tries to improve utilization of resources (like CPU, disk, I/O) explicitly, which in turn increases the utilization of active resources. For this, the proposed algorithm uses a fitness value, which is a function of CPU, disk and I/O utilization, and processing time of the task. To demonstrate the performance of the proposed algorithm, extensive simulations are performed on both proposed algorithm and existing algorithm MaxUtil using synthetic datasets. From the simulation results, it can be observed that the proposed algorithm is a better energy-efficient algorithm and consumes less energy than the MaxUtil algorithm.

</details>


### [5] [Seamless Transitions: A Comprehensive Review of Live Migration Technologies](https://arxiv.org/abs/2512.10979)
*Sima Attar-Khorasani,Lincoln Sherpa,Matthias Lieber,Siavash Ghiasvand*

Main category: cs.DC

TL;DR: 本文对实时迁移技术进行全面综述，重点分析容器和虚拟机迁移的技术现状、实际挑战及未来研究方向


<details>
  <summary>Details</summary>
Motivation: 现有综述往往忽视实时迁移在实际应用中的关键技术细节和实践挑战，本文旨在填补这一空白，为研究者和实践者提供全面的技术分析

Method: 整合现有综述内容，从迁移技术、迁移单元和基础设施特性等多个维度综合分析，重点关注容器和虚拟机两种迁移方式

Result: 发现实时迁移技术虽然广泛研究，但其对多种系统因素的依赖带来了实际挑战，在某些情况下复杂性和资源需求可能超过其收益

Conclusion: 本文通过概述当前技术挑战和提供未来研究方向指南，既为爱好者提供有价值的资源，又有助于推动实时迁移技术在不同计算环境中的实际应用

Abstract: Live migration, a technology enabling seamless transition of operational computational entities between various hosts while preserving continuous functionality and client connectivity, has been the subject of extensive research. However, existing reviews often overlook critical technical aspects and practical challenges integral to the usage of live migration techniques in real-world scenarios. This work bridges this gap by integrating the aspects explored in existing reviews together with a comprehensive analysis of live migration technologies across multiple dimensions, with focus on migration techniques, migration units, and infrastructure characteristics. Despite efforts to make live migration widely accessible, its reliance on multiple system factors can create challenges. In certain cases, the complexities and resource demands outweigh the benefits, making its implementation hard to justify. The focus of this work is mainly on container based and virtual machine-based migration technologies, examining the current state of the art and the disparity in adoption between these two approaches. Furthermore, this work explores the impact of migration objectives and operational constraints on the usability and efficacy of existing technologies. By outlining current technical challenges and providing guidelines for future research and development directions, this work serves a dual purpose: first, to equip enthusiasts with a valuable resource on live migration, and second, to contribute to the advancement of live migration technologies and their practical implementation across diverse computing environments.

</details>


### [6] [Reducing Fragmentation and Starvation in GPU Clusters through Dynamic Multi-Objective Scheduling](https://arxiv.org/abs/2512.10980)
*Akhmadillo Mamirov*

Main category: cs.DC

TL;DR: 论文提出三种动态调度器(HPS、PBS、SBS)解决GPU集群利用率低的问题，在64-GPU集群模拟测试中显著提升利用率至78.2%，吞吐量达25.8作业/小时，减少饥饿作业数量。


<details>
  <summary>Details</summary>
Motivation: GPU集群在AI训练和部署中至关重要，但实际部署的平均利用率仅约50%，主要原因是资源碎片化、异构工作负载和静态调度策略的限制。

Method: 提出三种专用动态调度器：混合优先级调度(HPS)、预测性回填调度(PBS)和智能批处理调度(SBS)，并在包含1000个AI作业的64-GPU、8节点集群上进行受控模拟评估。

Result: 静态基线调度(FIFO、SJF等)GPU利用率为45-67%，吞吐量12.5-18.3作业/小时，最多156个作业等待超过30分钟。动态调度器显著优于静态策略：HPS达到最高利用率78.2%和最高吞吐量25.8作业/小时，饥饿作业减少至12个；PBS达到76.1%利用率；SBS达到74.6%利用率。

Conclusion: 动态多目标调度器在吞吐量、作业等待时间、公平性方差和饥饿等方面均优于单目标启发式方法，表明有针对性和透明的调度策略能显著提高异构AI集群的GPU效率，为未来生产调度框架提供实用基础。

Abstract: GPU clusters have become essential for training and deploying modern AI systems, yet real deployments continue to report average utilization near 50%. This inefficiency is largely caused by fragmentation, heterogeneous workloads, and the limitations of static scheduling policies. This work presents a systematic evaluation of these issues and introduces three specialized dynamic schedulers: Hybrid Priority (HPS), Predictive Backfill (PBS), and Smart Batch (SBS). These schedulers are designed to improve utilization, fairness, and overall throughput in multi-tenant GPU clusters. We evaluate all schedulers using a controlled simulation of 1,000 AI jobs on a 64-GPU, 8-node cluster that includes a realistic mix of training, inference, and research workloads. Static baselines (FIFO, SJF, Shortest, Shortest-GPU) achieve 45 to 67% GPU utilization and 12.5 to 18.3 jobs per hour and experience severe starvation, with as many as 156 jobs waiting longer than 30 minutes. The dynamic schedulers significantly outperform these policies. HPS achieves the highest utilization (78.2%), highest throughput (25.8 jobs per hour), and the lowest fairness variance among dynamic methods (457), reducing starvation to 12 jobs. PBS improves fragmentation handling and reaches 76.1% utilization, while SBS increases efficiency for structurally similar jobs and reaches 74.6% utilization. Across all key metrics, including throughput, job wait times, fairness variance, and starvation, dynamic multi-objective schedulers consistently outperform single-objective heuristics. These results show that targeted and transparent scheduling strategies can meaningfully increase GPU efficiency in heterogeneous AI clusters and provide a practical foundation for future production scheduling frameworks.

</details>


### [7] [Evaluation Framework for Centralized and Decentralized Aggregation Algorithm in Federated Systems](https://arxiv.org/abs/2512.10987)
*Sumit Chongder*

Main category: cs.DC

TL;DR: 比较集中式分层联邦学习（HFL）与去中心化聚合联邦学习（AFL）和去中心化持续联邦学习（CFL）的性能，发现去中心化方法在多个指标上表现更优。


<details>
  <summary>Details</summary>
Motivation: 集中式分层联邦学习存在通信瓶颈和隐私问题，需要探索去中心化方法（AFL和CFL）作为替代方案，以改善联邦学习系统的性能和隐私保护。

Method: 使用Fashion MNIST和MNIST数据集，对HFL、AFL和CFL三种架构进行综合比较，评估它们在精度、召回率、F1分数和平衡准确率等指标上的表现。

Result: AFL和CFL在精度、召回率、F1分数和平衡准确率等多个指标上均优于HFL，证明了去中心化聚合机制在分布式设备协同模型训练中的有效性。

Conclusion: 去中心化联邦学习方法（AFL和CFL）相比集中式HFL具有性能优势，为联邦学习研究者和实践者提供了更好的选择，特别是在需要分布式协作模型训练的场景中。

Abstract: In recent years, the landscape of federated learning has witnessed significant advancements, particularly in decentralized methodologies. This research paper presents a comprehensive comparison of Centralized Hierarchical Federated Learning (HFL) with Decentralized Aggregated Federated Learning (AFL) and Decentralized Continual Federated Learning (CFL) architectures. While HFL, in its centralized approach, faces challenges such as communication bottlenecks and privacy concerns due to centralized data aggregation, AFL and CFL provide promising alternatives by distributing computation and aggregation processes across devices. Through evaluation of Fashion MNIST and MNIST datasets, this study demonstrates the advantages of decentralized methodologies, showcasing how AFL and CFL outperform HFL in precision, recall, F1 score, and balanced accuracy. The analysis highlights the importance of decentralized aggregation mechanisms in AFL and CFL, which effectively enables collaborative model training across distributed devices. This comparative study contributes valuable insights into the evolving landscape of federated learning, guiding researchers and practitioners towards decentralized methodologies for enhanced performance in collaborative model training scenarios.

</details>


### [8] [Dora: QoE-Aware Hybrid Parallelism for Distributed Edge AI](https://arxiv.org/abs/2512.10990)
*Jianli Jin,Ziyang Lin,Qianli Dong,Yi Chen,Jayanth Srinivasa,Myungjin Lee,Zhaowei Tan,Fan Lai*

Main category: cs.DC

TL;DR: Dora是一个面向边缘AI训练和推理的QoE感知混合并行框架，通过异构感知模型分区、竞争感知网络调度和运行时适配器，在满足QoE要求的同时提升执行效率并降低能耗。


<details>
  <summary>Details</summary>
Motivation: 随着边缘AI应用的普及，满足用户QoE要求（如模型推理延迟）成为首要目标。然而，现代AI模型通常超出单个设备的资源容量，需要在异构设备上进行分布式执行，而现有规划器主要优化吞吐量或设备利用率，忽略了QoE，导致资源效率低下或QoE违规。

Method: Dora通过三个关键机制实现QoE感知的混合并行：1）异构感知模型分区器，确定并分配模型分区，形成一组紧凑的QoE合规计划；2）竞争感知网络调度器，通过最大化计算通信重叠来优化候选计划；3）运行时适配器，自适应组合多个计划以最大化全局效率同时尊重整体QoE。

Result: 在代表性边缘部署场景（智能家居、交通分析、小型边缘集群）中，Dora实现了1.1-6.3倍的执行加速，同时将能耗降低21-82%，并在运行时动态变化下保持QoE。

Conclusion: Dora框架成功解决了边缘AI分布式执行中的QoE优化问题，通过联合优化异构计算、竞争网络和多维QoE目标，在保证用户体验的同时显著提升了资源效率。

Abstract: With the proliferation of edge AI applications, satisfying user quality of experience (QoE) requirements, such as model inference latency, has become a first class objective, as these models operate in resource constrained settings and directly interact with users. Yet, modern AI models routinely exceed the resource capacity of individual devices, necessitating distributed execution across heterogeneous devices over variable and contention prone networks. Existing planners for hybrid (e.g., data and pipeline) parallelism largely optimize for throughput or device utilization, overlooking QoE, leading to severe resource inefficiency (e.g., unnecessary energy drain) or QoE violations under runtime dynamics.
  We present Dora, a framework for QoE aware hybrid parallelism in distributed edge AI training and inference. Dora jointly optimizes heterogeneous computation, contention prone networks, and multi dimensional QoE objectives via three key mechanisms: (i) a heterogeneity aware model partitioner that determines and assigns model partitions across devices, forming a compact set of QoE compliant plans; (ii) a contention aware network scheduler that further refines these candidate plans by maximizing compute communication overlap; and (iii) a runtime adapter that adaptively composes multiple plans to maximize global efficiency while respecting overall QoEs. Across representative edge deployments, including smart homes, traffic analytics, and small edge clusters, Dora achieves 1.1--6.3 times faster execution and, alternatively, reduces energy consumption by 21--82 percent, all while maintaining QoE under runtime dynamics.

</details>


### [9] [RollMux: Phase-Level Multiplexing for Disaggregated RL Post-Training](https://arxiv.org/abs/2512.11306)
*Tianyuan Wu,Lunxi Cao,Yining Wei,Wei Gao,Yuheng Zhao,Dakai An,Shaopan Xiong,Zhiqiang Lv,Ju Huang,Siran Yang,Yinghao Yu,Jiamang Wang,Lin Qu,Wei Wang*

Main category: cs.DC

TL;DR: RollMux是一个用于强化学习后训练中rollout-training解耦架构的集群调度框架，通过跨集群编排消除同步依赖带来的空闲时间，将成本效率提升1.84倍。


<details>
  <summary>Details</summary>
Motivation: 在强化学习后训练中，rollout和training解耦到专用集群能最大化硬件效率，但on-policy算法的严格同步要求导致严重的依赖气泡，使得一个集群运行时另一个集群必须空闲等待。

Method: 提出RollMux框架，基于"一个作业的结构性空闲可以被另一个作业的活动阶段利用"的洞察。引入co-execution group抽象将集群划分为隔离的局部性域，采用两层调度架构：组间调度器使用保守随机规划优化作业放置，组内调度器编排可证明最优的轮询调度。

Result: 在包含328个H20和328个H800 GPU的生产级测试平台上，RollMux相比标准解耦架构将成本效率提升1.84倍，相比最先进的共置基线提升1.38倍，同时实现100%的服务水平目标达成率。

Conclusion: RollMux通过跨集群编排有效回收了rollout-training解耦架构中的依赖气泡，显著提升了硬件利用率和成本效率，同时保证了服务质量。

Abstract: Rollout-training disaggregation is emerging as the standard architecture for Reinforcement Learning (RL) post-training, where memory-bound rollout and compute-bound training are physically disaggregated onto purpose-built clusters to maximize hardware efficiency. However, the strict synchronization required by on-policy algorithms introduces severe dependency bubbles, forcing one cluster to idle while the dependent phase is running on the other. We present RollMux, a cluster scheduling framework that reclaims these bubbles through cross-cluster orchestration. RollMux is built on the insight that the structural idleness of one job can be effectively utilized by the active phase of another. To realize this, we introduce the co-execution group abstraction, which partitions the cluster into isolated locality domains. This abstraction enables a two-tier scheduling architecture: an inter-group scheduler that optimizes job placement using conservative stochastic planning, and an intra-group scheduler that orchestrates a provably optimal round-robin schedule. The group abstraction also imposes a residency constraint, ensuring that massive model states remain cached in host memory to enable "warm-star" context switching. We evaluate RollMux on a production-scale testbed with 328 H20 and 328 H800 GPUs. RollMux improves cost efficiency by 1.84x over standard disaggregation and 1.38x over state-of-the-art co-located baselines, all while achieving 100% SLO attainment.

</details>


### [10] [Enhanced Pruning for Distributed Closeness Centrality under Multi-Packet Messaging](https://arxiv.org/abs/2512.11512)
*Patrick D. Manya,Eugene M. Mbuyi,Gothy T. Ngoie,Jordan F. Masakuna*

Main category: cs.DC

TL;DR: 提出一种基于多包消息的分布式修剪方法，显著降低中心性计算中的通信开销


<details>
  <summary>Details</summary>
Motivation: 大规模复杂网络中基于接近中心性的中心节点识别至关重要，但现有分布式近似技术（如修剪）通信开销高，难以应对大型网络设置

Method: 提出增强的分布式修剪方法，利用多包消息技术，允许节点批量传输更大的数据块，减少消息交换数量并最小化数据丢失

Result: 多包方法在消息效率（更少总消息数）和计算时间上显著优于原始修剪技术，同时保持基线方法的近似特性

Conclusion: 尽管存在节点内存使用和本地开销的可管理权衡，但通信效率的提升优势明显，为大规模网络分析提供了更可扩展和高效的解决方案

Abstract: Identifying central nodes using closeness centrality is a critical task in analyzing large-scale complex networks, yet its decentralized computation remains challenging due to high communication overhead. Existing distributed approximation techniques, such as pruning, often fail to fully mitigate the cost of exchanging numerous data packets in large network settings. In this paper, we introduce a novel enhancement to the distributed pruning method specifically designed to overcome this communication bottleneck. Our core contribution is a technique that leverages multi-packet messaging, allowing nodes to batch and transmit larger, consolidated data blocks. This approach significantly reduces the number of exchanged messages and minimizes data loss without compromising the accuracy of the centrality estimates. We demonstrate that our multi-packet approach substantially outperforms the original pruning technique in both message efficiency (fewer overall messages) and computation time, preserving the core approximation properties of the baseline method. While we observe a manageable trade-off in increased per-node memory usage and local overhead, our findings show that this is outweighed by the gains in communication efficiency, particularly for very large networks and complex packet structures. Our work offers a more scalable and efficient solution for decentralized closeness centrality computation, promising a significant step forward for large-scale network analysis.

</details>


### [11] [Parallax: Runtime Parallelization for Operator Fallbacks in Heterogeneous Edge Systems](https://arxiv.org/abs/2512.11532)
*Chong Tang,Hao Dai,Jagmohan Chauhan*

Main category: cs.DC

TL;DR: Parallax是一个移动端DNN推理加速框架，通过计算图分区、分支感知内存管理和自适应调度，在不修改模型的情况下实现高达46%的延迟降低和30%的能耗节省。


<details>
  <summary>Details</summary>
Motivation: 移动设备上实时DNN应用需求增长，但现有框架在处理动态控制流操作符和不支持的核函数时，回退到CPU执行的方式效率低下，导致CPU核心闲置、高延迟和内存峰值问题。

Method: 1. 对计算DAG进行分区以暴露并行性；2. 采用分支感知内存管理，使用专用内存池和缓冲区重用减少运行时内存占用；3. 自适应调度器根据设备内存约束执行分支；4. 细粒度子图控制实现动态模型的异构推理。

Result: 在三种不同移动设备上评估五个代表性DNN模型，Parallax实现了高达46%的延迟降低，平均内存开销控制在26.5%，能耗节省高达30%，相比现有最先进框架有明显提升。

Conclusion: Parallax框架通过创新的并行化、内存管理和调度策略，在不需模型重构或自定义操作符实现的情况下，显著提升了移动端DNN推理性能，满足了实时移动推理的响应性需求。

Abstract: The growing demand for real-time DNN applications on edge devices necessitates faster inference of increasingly complex models. Although many devices include specialized accelerators (e.g., mobile GPUs), dynamic control-flow operators and unsupported kernels often fall back to CPU execution. Existing frameworks handle these fallbacks poorly, leaving CPU cores idle and causing high latency and memory spikes. We introduce Parallax, a framework that accelerates mobile DNN inference without model refactoring or custom operator implementations. Parallax first partitions the computation DAG to expose parallelism, then employs branch-aware memory management with dedicated arenas and buffer reuse to reduce runtime footprint. An adaptive scheduler executes branches according to device memory constraints, meanwhile, fine-grained subgraph control enables heterogeneous inference of dynamic models. By evaluating on five representative DNNs across three different mobile devices, Parallax achieves up to 46% latency reduction, maintains controlled memory overhead (26.5% on average), and delivers up to 30% energy savings compared with state-of-the-art frameworks, offering improvements aligned with the responsiveness demands of real-time mobile inference.

</details>


### [12] [FirecREST v2: lessons learned from redesigning an API for scalable HPC resource access](https://arxiv.org/abs/2512.11634)
*Elia Palme,Juan Pablo Dorsch,Ali Khosravi,Giovanni Pizzi,Francesco Pagnamenta,Andrea Ceriani,Eirini Koutsaniti,Rafael Sarmiento,Ivano Bonesana,Alejandro Dabin*

Main category: cs.DC

TL;DR: FirecREST v2 是下一代用于HPC资源程序化访问的RESTful API，相比前代性能提升100倍，重点改进了安全性和高吞吐量。


<details>
  <summary>Details</summary>
Motivation: 重新设计FirecEST以解决代理式API在密集I/O操作中的性能瓶颈，同时将增强安全性和高吞吐量作为核心需求。

Method: 采用系统化的性能测试方法，识别常见瓶颈，并进行全面的架构重新设计，包括关键的设计和架构变更。

Result: 实现了100倍的性能提升，并通过独立同行验证证明了改进效果，同时保持了开源特性。

Conclusion: FirecREST v2的成功重设计展示了解决HPC API性能瓶颈的有效方法，为未来进一步改进提供了机会。

Abstract: Introducing FirecREST v2, the next generation of our open-source RESTful API for programmatic access to HPC resources. FirecREST v2 delivers a 100x performance improvement over its predecessor. This paper explores the lessons learned from redesigning FirecREST from the ground up, with a focus on integrating enhanced security and high throughput as core requirements.
  We provide a detailed account of our systematic performance testing methodology, highlighting common bottlenecks in proxy-based APIs with intensive I/O operations. Key design and architectural changes that enabled these performance gains are presented. Finally, we demonstrate the impact of these improvements, supported by independent peer validation, and discuss opportunities for further improvements.

</details>


### [13] [Stateless Snowflake: A Cloud-Agnostic Distributed ID Generator Using Network-Derived Identity](https://arxiv.org/abs/2512.11643)
*Manideep Reddy Chinthareddy*

Main category: cs.DC

TL;DR: 提出一种无需显式worker ID的分布式ID生成协议，利用容器私有IPv4地址作为节点唯一性来源，消除中心化协调需求，实现无状态微服务的水平扩展。


<details>
  <summary>Details</summary>
Motivation: 传统Snowflake风格分布式ID生成器需要手动分配或中心化协调worker ID，这在容器编排环境（如Kubernetes）中带来显著摩擦。短暂、自动扩展的工作负载需要复杂的有状态集或外部协调服务，违背了无状态微服务的操作优势。

Method: 提出云无关、容器原生的ID生成协议，从短暂网络属性（容器私有IPv4地址）确定性推导节点唯一性。引入修改的位分配方案（1-41-16-6），容纳16位网络派生熵，同时保持严格单调性。

Result: 在AWS、GCP和Azure环境中验证该方法。理论单节点上限约64,000 TPS，实际微服务部署中网络I/O主导延迟，3节点集群端到端性能约31,000 TPS，与经典有状态生成器相当，同时提供有效无限水平扩展性。

Conclusion: 该方法消除了分布式ID生成对显式worker ID的依赖，通过利用容器网络属性实现无协调、无状态操作，在保持性能的同时提供更好的容器环境适应性。

Abstract: Snowflake-style distributed ID generators are the industry standard for producing k-ordered, unique identifiers at scale. However, the traditional requirement for manually assigned or centrally coordinated worker IDs introduces significant friction in modern container-orchestrated environments (e.g., Kubernetes), where workloads are ephemeral and autoscaled. In such systems, maintaining stable worker identities requires complex stateful sets or external coordination services (e.g., ZooKeeper), negating the operational benefits of stateless microservices.
  This paper presents a cloud-agnostic, container-native ID generation protocol that eliminates the dependency on explicit worker IDs. By deriving node uniqueness deterministically from ephemeral network properties - specifically the container's private IPv4 address - the proposed method removes the need for centralized coordination. We introduce a modified bit-allocation scheme (1-41-16-6) that accommodates 16 bits of network-derived entropy while preserving strict monotonicity. We validate the approach across AWS, GCP, and Azure environments. Evaluation results demonstrate that while the design has a theoretical single-node ceiling of approximately 64,000 TPS, in practical microservice deployments the network I/O dominates latency, resulting in end-to-end performance (approximately 31,000 TPS on a 3-node cluster) comparable to classic stateful generators while offering effectively unbounded horizontal scalability.

</details>


### [14] [ECCO: Leveraging Cross-Camera Correlations for Efficient Live Video Continuous Learning](https://arxiv.org/abs/2512.11727)
*Yuze He,Ferdi Kossmann,Srinivasan Seshan,Peter Steenkiste*

Main category: cs.DC

TL;DR: ECCO是一个视频分析框架，通过识别经历相似数据漂移的摄像头并为其训练共享模型，显著降低了连续学习的计算和通信成本。


<details>
  <summary>Details</summary>
Motivation: 当前为每个摄像头单独重新训练专用模型的方法计算和通信成本过高，无法扩展。数据漂移通常在相邻摄像头间存在时空相关性，这为共享模型训练提供了机会。

Method: ECCO包含三个核心组件：1) 轻量级分组算法动态形成和更新摄像头组；2) GPU分配器动态分配GPU资源以提高重训练准确性并确保公平性；3) 每个摄像头的传输控制器配置帧采样并基于分配的GPU资源协调带宽共享。

Result: 在三个不同数据集上的评估显示，ECCO在相同计算和通信资源下将重训练准确性提高了6.7%-18.1%，或在相同准确性下支持3.3倍更多的并发摄像头。

Conclusion: ECCO通过利用摄像头间数据漂移的时空相关性，实现了资源高效的连续学习，显著降低了视频分析系统的扩展成本。

Abstract: Recent advances in video analytics address real-time data drift by continuously retraining specialized, lightweight DNN models for individual cameras. However, the current practice of retraining a separate model for each camera suffers from high compute and communication costs, making it unscalable. We present ECCO, a new video analytics framework designed for resource-efficient continuous learning. The key insight is that the data drift, which necessitates model retraining, often shows temporal and spatial correlations across nearby cameras. By identifying cameras that experience similar drift and retraining a shared model for them, ECCO can substantially reduce the associated compute and communication costs. Specifically, ECCO introduces: (i) a lightweight grouping algorithm that dynamically forms and updates camera groups; (ii) a GPU allocator that dynamically assigns GPU resources across different groups to improve retraining accuracy and ensure fairness; and (iii) a transmission controller at each camera that configures frame sampling and coordinates bandwidth sharing with other cameras based on its assigned GPU resources. We conducted extensive evaluations on three distinctive datasets for two vision tasks. Compared to leading baselines, ECCO improves retraining accuracy by 6.7%-18.1% using the same compute and communication resources, or supports 3.3 times more concurrent cameras at the same accuracy.

</details>


### [15] [Hypergraph based Multi-Party Payment Channel](https://arxiv.org/abs/2512.11775)
*Ayush Nainwal,Atharva Kamble,Nitin Awathare*

Main category: cs.DC

TL;DR: H-MPCs是一种基于超图的多方支付通道，通过集体资金超边替代传统双边通道，实现无领导者的高并发链下支付，解决流动性碎片化和通道耗尽问题。


<details>
  <summary>Details</summary>
Motivation: 公共区块链吞吐量低、延迟高，现有支付通道网络存在流动性碎片化（资金锁定在单一通道无法复用）和通道耗尽问题，限制了路由效率和交易成功率。现有的多方通道方案依赖领导者或协调者，存在单点故障且跨通道支付灵活性有限。

Method: 提出基于超图的多方支付通道（H-MPCs），用集体资金超边替代传统双边通道。通过可验证的提议者有序DAG更新，实现完全并发的、无领导者的超边内和超边间支付。

Result: 在150个节点的网络上实现，交易成功率约94%，没有HTLC过期或路由失败，证明了H-MPCs的鲁棒性。

Conclusion: H-MPCs提供了比现有设计显著更高的灵活性和并发性，有效解决了支付通道网络的流动性碎片化和通道耗尽问题。

Abstract: Public blockchains inherently offer low throughput and high latency, motivating off-chain scalability solutions such as Payment Channel Networks (PCNs). However, existing PCNs suffer from liquidity fragmentation-funds locked in one channel cannot be reused elsewhere-and channel depletion, both of which limit routing efficiency and reduce transaction success rates. Multi-party channel (MPC) constructions mitigate these issues, but they typically rely on leaders or coordinators, creating single points of failure and providing only limited flexibility for inter-channel payments.
  We introduce Hypergraph-based Multi-Party Payment Channels (H-MPCs), a new off-chain construction that replaces bilateral channels with collectively funded hyperedges. These hyperedges enable fully concurrent, leaderless intra- and inter-hyperedge payments through verifiable, proposer-ordered DAG updates, offering significantly greater flexibility and concurrency than prior designs.
  Our implementation on a 150-node network demonstrates a transaction success rate of approximately 94% without HTLC expiry or routing failures, highlighting the robustness of H-MPCs.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [16] [PD-Swap: Prefill-Decode Logic Swapping for End-to-End LLM Inference on Edge FPGAs via Dynamic Partial Reconfiguration](https://arxiv.org/abs/2512.11550)
*Yifan Zhang,Zhiheng Chen,Ye Qiao,Sitao Huang*

Main category: cs.AR

TL;DR: PD-Swap是一种基于FPGA的LLM加速器，通过动态部分重配置技术将预填充和解码阶段分离，针对不同阶段优化硬件架构，显著提升长上下文推理性能。


<details>
  <summary>Details</summary>
Motivation: 当前边缘FPGA上的量化LLM在处理长上下文时面临性能瓶颈：预填充阶段计算密集，解码阶段内存带宽受限。静态加速器需要为两个阶段提供统一资源，导致资源浪费和性能下降。

Method: 提出PD-Swap架构，使用动态部分重配置技术，在FPGA上实现预填充-解码分离。核心的查表式三元矩阵乘法和权重缓冲引擎保持静态，注意力子系统作为可重配置分区，包含两个专门化架构：计算密集的预填充引擎和带宽优化的解码引擎。

Result: PD-Swap在边缘FPGA上实现高达27 tokens/s的解码吞吐量，比现有最优工作提升1.3-2.1倍（上下文越长提升越大），且不增加额外面积成本。

Conclusion: 通过动态部分重配置实现预填充-解码分离的架构能有效解决LLM推理中的阶段不对称问题，显著提升边缘设备上长上下文推理的性能，为低功耗AI部署提供新思路。

Abstract: Aggressively quantized large language models (LLMs), such as BitNet-style 1.58-bit Transformers with ternary weights, make it feasible to deploy generative AI on low-power edge FPGAs. However, as prompts grow to tens of thousands of tokens, edge hardware performance drops sharply with sequence length due to quadratic prefill cost and rapidly increasing KV-cache bandwidth demands, making inference latency of longer context length a first-order system concern. Recent studies on LLMs expose a fundamental prefill-decode asymmetry: prefill is compute-bound and dominated by dense matrix-matrix operations, whereas decoding is memory-bandwidth-bound and dominated by KV-cache traffic. A static accelerator must provision resources and a single dataflow for both regimes, leading to duplicated attention logic, underutilized fabric, and tight LUT/URAM limits that cap model size and usable context. We propose a prefill--decode disaggregated LLM accelerator, PD-Swap, that uses Dynamic Partial Reconfiguration (DPR) to time-multiplex the attention module on edge FPGAs. The core table-lookup ternary matrix multiplication and weight-buffering engines remain static, while the attention subsystem is a reconfigurable partition with two phase-specialized architectures: a compute-heavy, token-parallel prefill engine and a bandwidth-optimized, KV-cache-centric decoding engine. A roofline-inspired model and design space exploration jointly optimize reconfigurable-region size, parallelism under reconfiguration and routability constraints, and reconfiguration latency is hidden by computation latency. PD-Swap achieves up to 27~tokens/s decoding throughput, outperforming prior state-of-the-art works by 1.3x--2.1x (larger gains at longer context lengths), without extra area cost.

</details>
