{"id": "2507.07114", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.07114", "abs": "https://arxiv.org/abs/2507.07114", "authors": ["Erez Weintraub", "Ron Banner", "Ariel Orda"], "title": "Distributed Training under Packet Loss", "comment": null, "summary": "State-of-the-art language and vision models are routinely trained across\nthousands of GPUs, often spanning multiple data-centers, yet today's\ndistributed frameworks still assume reliable connections (e.g., InfiniBand or\nRoCE). The resulting acknowledgment traffic and retransmissions inflate tail\nlatencies and limit scalability. Leveraging unreliable connections will reduce\nlatency but may sacrifice model accuracy and convergence once packets are\ndropped. A principled, end-to-end solution that preserves accuracy and\nconvergence guarantees under genuine packet loss has previously been missing.\nWe address this critical gap by introducing a novel distributed training\nframework capable of operating over unreliable connections, offering unbiased\ngradient aggregation and bounded parameter drift without modifying model code\nor optimizers. The key insight is a two-stage defense against missing messages:\n(i) Unbiased gradient aggregation: each worker reconstructs a consistent\ngradient estimate from whatever packets arrive, guaranteeing expectation-level\ncorrectness; and (ii) Bounded-drift parameter broadcasts: we prove the\ninter-worker model discrepancy remains O(1) even after arbitrarily many\niterations, preventing the unbounded divergence typical of asynchronous setups.\nAnalytical bounds are matched by experiments on the LLAMA2 7B model with 64\nGPUs: tolerating 10% random packet loss yields at most 0.8% perplexity change.\nThis work bridges the gap between communication-efficient datacenter protocols\nand the accuracy and generalization guarantees demanded by modern large-model\ntraining, enabling robust, high-throughput learning on commodity or wide-area\nnetworks."}
{"id": "2507.07116", "categories": ["cs.DC", "cs.AI", "cs.ET"], "pdf": "https://arxiv.org/pdf/2507.07116", "abs": "https://arxiv.org/abs/2507.07116", "authors": ["Juan Cano-Benito", "Andrea Cimmino", "Sven Hertling", "Heiko Paulheim", "Raúl García-Castro"], "title": "Analysing semantic data storage in Distributed Ledger Technologies for Data Spaces", "comment": null, "summary": "Data spaces are emerging as decentralised infrastructures that enable\nsovereign, secure, and trustworthy data exchange among multiple participants.\nTo achieve semantic interoperability within these environments, the use of\nsemantic web technologies and knowledge graphs has been proposed. Although\ndistributed ledger technologies (DLT) fit as the underlying infrastructure for\ndata spaces, there remains a significant gap in terms of the efficient storage\nof semantic data on these platforms. This paper presents a systematic\nevaluation of semantic data storage across different types of DLT (public,\nprivate, and hybrid), using a real-world knowledge graph as an experimental\nbasis. The study compares performance, storage efficiency, resource\nconsumption, and the capabilities to update and query semantic data. The\nresults show that private DLTs are the most efficient for storing and managing\nsemantic content, while hybrid DLTs offer a balanced trade-off between public\nauditability and operational efficiency. This research leads to a discussion on\nthe selection of the most appropriate DLT infrastructure based on the data\nsovereignty requirements of decentralised data ecosystems."}
{"id": "2507.07117", "categories": ["cs.DC", "cs.AI", "cs.NI"], "pdf": "https://arxiv.org/pdf/2507.07117", "abs": "https://arxiv.org/abs/2507.07117", "authors": ["Jit Gupta", "Andrew Li", "Tarun Banka", "Ariel Cohen", "T. Sridhar", "Raj Yavatkar"], "title": "Collective Communication Profiling of Modern-day Machine Learning Workloads", "comment": "Poser, USENIX NSDI 2025, April 2025, Philadelphia, PA, USA", "summary": "Machine Learning jobs, carried out on large number of distributed high\nperformance systems, involve periodic communication using operations like\nAllReduce, AllGather, and Broadcast. These operations may create high bandwidth\nand bursty traffic patterns, leading to network congestion and packet loss,\nthus impacting the performance of these jobs. Hence it is imperative to analyze\nthese patterns, which can be helpful in provisioning network resources\ndepending on the type of machine learning workloads. In this poster we carry\nout extensive analysis of the collective communication behavior seen in a wide\nvariety of models (ex. DeepSeek, GPT, Llama, etc.) To achieve this we\ninstrument Nvidia Collective Communication Library logging functionality for\nricher context about the collectives and workloads. We adjust configuration\nparameters that influence collective communication behavior, such as\nparallelism, number of nodes, and model type. This overview presents and\ndiscusses some of the results on the collective communication behavior for the\nopen source DeepSeek V3 inferencing model, which includes operation type and\ncount, transfer sizes per operation, and request size distribution. Our\nanalysis shows that it makes sense to rethink current collective communication\nframeworks and network topologies so as to accommodate the effect of network\nanomalies on the mentioned workloads."}
{"id": "2507.07120", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.07120", "abs": "https://arxiv.org/abs/2507.07120", "authors": ["Nidhi Bhatia", "Ankit More", "Ritika Borkar", "Tiyasa Mitra", "Ramon Matas", "Ritchie Zhao", "Maximilian Golub", "Dheevatsa Mudigere", "Brian Pharris", "Bita Darvish Rouhani"], "title": "Helix Parallelism: Rethinking Sharding Strategies for Interactive Multi-Million-Token LLM Decoding", "comment": null, "summary": "As LLMs scale to multi-million-token KV histories, real-time autoregressive\ndecoding under tight Token-to-Token Latency (TTL) constraints faces growing\npressure. Two core bottlenecks dominate: accessing Feed-Forward Network (FFN)\nweights and reading long KV caches. While Tensor Parallelism (TP) helps\nmitigate the cost of FFN weight reads, it does not scale well for attention.\nWhen TP width exceeds the number of KV heads, it leads to inefficient KV\nduplication, limits parallelism, and constrains batch size. Simultaneously,\nDRAM reads for long KV histories scale linearly with batch size, further\ncapping efficiency.\n  We introduce Helix Parallelism, a hybrid execution strategy that applies KV\nparallelism during attention to shard KV caches across GPUs, then reuses the\nsame GPUs for TP in dense LLMs or TPxExpert Parallel (EP) in MoEs during FFN\ncomputation. To preserve exact attention behavior, Helix includes a lightweight\ncommunication step. To minimize the exposed communication cost, we introduce\nHelix HOP-B. Helix HOP-B effectively minimizes communication overhead through\nbatchwise overlap, preserving low TTL while improving GPU efficiency. Compared\nto conventional parallelism approaches, Helix reduces TTL by up to 1.5x at\nfixed batch sizes and supports up to 32x larger batches under the same latency\nbudget for DeepSeek-R1, pushing forward the throughput-latency Pareto on\nBlackwell and making real-time inference with ultra-long-sequence practical."}
{"id": "2507.07683", "categories": ["cs.AR", "cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.07683", "abs": "https://arxiv.org/abs/2507.07683", "authors": ["Jude Haris", "José Cano"], "title": "Accelerating Transposed Convolutions on FPGA-based Edge Devices", "comment": "Accepted to 35th International Conference on Field-Programmable Logic\n  and Applications (FPL) 2025", "summary": "Transposed Convolutions (TCONV) enable the up-scaling mechanism within\ngenerative Artificial Intelligence (AI) models. However, the predominant\nInput-Oriented Mapping (IOM) method for implementing TCONV has complex output\nmapping, overlapping sums, and ineffectual computations. These inefficiencies\nfurther exacerbate the performance bottleneck of TCONV and generative models on\nresource-constrained edge devices. To address this problem, in this paper we\npropose MM2IM, a hardware-software co-designed accelerator that combines Matrix\nMultiplication (MatMul) with col2IM to process TCONV layers on\nresource-constrained edge devices efficiently. Using the SECDA-TFLite design\ntoolkit, we implement MM2IM and evaluate its performance across 261 TCONV\nproblem configurations, achieving an average speedup of 1.9x against a\ndual-thread ARM Neon optimized CPU baseline. We then evaluate the performance\nof MM2IM on a range of TCONV layers from well-known generative models achieving\nup to 4.2x speedup, and compare it against similar resource-constrained TCONV\naccelerators, outperforming them by at least 2x GOPs/DSP. Finally, we evaluate\nMM2IM on the DCGAN and pix2pix GAN models, achieving up to 3x speedup and 2.4x\nenergy reduction against the CPU baseline."}
{"id": "2507.07480", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2507.07480", "abs": "https://arxiv.org/abs/2507.07480", "authors": ["Tobias Kappé"], "title": "On Propositional Program Equivalence (extended abstract)", "comment": null, "summary": "General program equivalence is undecidable. However, if we abstract away the\nsemantics of statements, then this problem becomes not just decidable, but\npractically feasible. For instance, a program of the form \"if $b$ then $e$ else\n$f$\" should be equivalent to \"if not $b$ then $f$ else $e$\" - no matter what\n$b$, $e$ and $f$ are. This kind of equivalence is known as propositional\nequivalence. In this extended abstract, we discuss recent developments in\npropositional program equivalence from the perspective of (Guarded) Kleene\nAlgebra with Tests, or (G)KAT."}
{"id": "2507.07130", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.07130", "abs": "https://arxiv.org/abs/2507.07130", "authors": ["Zihan Zhang", "Leon Wong", "Blesson Varghese"], "title": "Ampere: Communication-Efficient and High-Accuracy Split Federated Learning", "comment": null, "summary": "A Federated Learning (FL) system collaboratively trains neural networks\nacross devices and a server but is limited by significant on-device computation\ncosts. Split Federated Learning (SFL) systems mitigate this by offloading a\nblock of layers of the network from the device to a server. However, in doing\nso, it introduces large communication overheads due to frequent exchanges of\nintermediate activations and gradients between devices and the server and\nreduces model accuracy for non-IID data. We propose Ampere, a novel\ncollaborative training system that simultaneously minimizes on-device\ncomputation and device-server communication while improving model accuracy.\nUnlike SFL, which uses a global loss by iterative end-to-end training, Ampere\ndevelops unidirectional inter-block training to sequentially train the device\nand server block with a local loss, eliminating the transfer of gradients. A\nlightweight auxiliary network generation method decouples training between the\ndevice and server, reducing frequent intermediate exchanges to a single\ntransfer, which significantly reduces the communication overhead. Ampere\nmitigates the impact of data heterogeneity by consolidating activations\ngenerated by the trained device block to train the server block, in contrast to\nSFL, which trains on device-specific, non-IID activations. Extensive\nexperiments on multiple CNNs and transformers show that, compared to\nstate-of-the-art SFL baseline systems, Ampere (i) improves model accuracy by up\nto 13.26% while reducing training time by up to 94.6%, (ii) reduces\ndevice-server communication overhead by up to 99.1% and on-device computation\nby up to 93.13%, and (iii) reduces standard deviation of accuracy by 53.39% for\nvarious non-IID degrees highlighting superior performance when faced with\nheterogeneous data."}
{"id": "2507.07223", "categories": ["cs.DC", "cs.AR", "B.4.3; C.0; C.2.1; C.2.2"], "pdf": "https://arxiv.org/pdf/2507.07223", "abs": "https://arxiv.org/abs/2507.07223", "authors": ["Myoungsoo Jung"], "title": "Compute Can't Handle the Truth: Why Communication Tax Prioritizes Memory and Interconnects in Modern AI Infrastructure", "comment": null, "summary": "Modern AI workloads such as large language models (LLMs) and\nretrieval-augmented generation (RAG) impose severe demands on memory,\ncommunication bandwidth, and resource flexibility. Traditional GPU-centric\narchitectures struggle to scale due to growing inter-GPU communication\noverheads. This report introduces key AI concepts and explains how Transformers\nrevolutionized data representation in LLMs. We analyze large-scale AI hardware\nand data center designs, identifying scalability bottlenecks in hierarchical\nsystems. To address these, we propose a modular data center architecture based\non Compute Express Link (CXL) that enables disaggregated scaling of memory,\ncompute, and accelerators. We further explore accelerator-optimized\ninterconnects-collectively termed XLink (e.g., UALink, NVLink, NVLink\nFusion)-and introduce a hybrid CXL-over-XLink design to reduce long-distance\ndata transfers while preserving memory coherence. We also propose a\nhierarchical memory model that combines local and pooled memory, and evaluate\nlightweight CXL implementations, HBM, and silicon photonics for efficient\nscaling. Our evaluations demonstrate improved scalability, throughput, and\nflexibility in AI infrastructure."}
{"id": "2507.07144", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2507.07144", "abs": "https://arxiv.org/abs/2507.07144", "authors": ["Hongyi Xie", "Min Zhou", "Qiao Yu", "Jialiang Yu", "Zhenli Sheng", "Hong Xie", "Defu Lian"], "title": "M$^2$-MFP: A Multi-Scale and Multi-Level Memory Failure Prediction Framework for Reliable Cloud Infrastructure", "comment": null, "summary": "As cloud services become increasingly integral to modern IT infrastructure,\nensuring hardware reliability is essential to sustain high-quality service.\nMemory failures pose a significant threat to overall system stability, making\naccurate failure prediction through the analysis of memory error logs (i.e.,\nCorrectable Errors) imperative. Existing memory failure prediction approaches\nhave notable limitations: rule-based expert models suffer from limited\ngeneralizability and low recall rates, while automated feature extraction\nmethods exhibit suboptimal performance. To address these limitations, we\npropose M$^2$-MFP: a Multi-scale and hierarchical memory failure prediction\nframework designed to enhance the reliability and availability of cloud\ninfrastructure. M$^2$-MFP converts Correctable Errors (CEs) into multi-level\nbinary matrix representations and introduces a Binary Spatial Feature Extractor\n(BSFE) to automatically extract high-order features at both DIMM-level and\nbit-level. Building upon the BSFE outputs, we develop a dual-path temporal\nmodeling architecture: 1) a time-patch module that aggregates multi-level\nfeatures within observation windows, and 2) a time-point module that employs\ninterpretable rule-generation trees trained on bit-level patterns. Experiments\non both benchmark datasets and real-world deployment show the superiority of\nM$^2$-MFP as it outperforms existing state-of-the-art methods by significant\nmargins. Code and data are available at this repository:\nhttps://github.com/hwcloud-RAS/M2-MFP."}
{"id": "2507.07223", "categories": ["cs.DC", "cs.AR", "B.4.3; C.0; C.2.1; C.2.2"], "pdf": "https://arxiv.org/pdf/2507.07223", "abs": "https://arxiv.org/abs/2507.07223", "authors": ["Myoungsoo Jung"], "title": "Compute Can't Handle the Truth: Why Communication Tax Prioritizes Memory and Interconnects in Modern AI Infrastructure", "comment": null, "summary": "Modern AI workloads such as large language models (LLMs) and\nretrieval-augmented generation (RAG) impose severe demands on memory,\ncommunication bandwidth, and resource flexibility. Traditional GPU-centric\narchitectures struggle to scale due to growing inter-GPU communication\noverheads. This report introduces key AI concepts and explains how Transformers\nrevolutionized data representation in LLMs. We analyze large-scale AI hardware\nand data center designs, identifying scalability bottlenecks in hierarchical\nsystems. To address these, we propose a modular data center architecture based\non Compute Express Link (CXL) that enables disaggregated scaling of memory,\ncompute, and accelerators. We further explore accelerator-optimized\ninterconnects-collectively termed XLink (e.g., UALink, NVLink, NVLink\nFusion)-and introduce a hybrid CXL-over-XLink design to reduce long-distance\ndata transfers while preserving memory coherence. We also propose a\nhierarchical memory model that combines local and pooled memory, and evaluate\nlightweight CXL implementations, HBM, and silicon photonics for efficient\nscaling. Our evaluations demonstrate improved scalability, throughput, and\nflexibility in AI infrastructure."}
{"id": "2507.07352", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.07352", "abs": "https://arxiv.org/abs/2507.07352", "authors": ["Loïc Pottier", "Konstantia Georgouli", "Timothy S. Carpenter", "Fikret Aydin", "Jeremy O. B. Tempkin", "Dwight V. Nissley", "Frederick H. Streitz", "Thomas R. W. Scogland", "Peer-Timo Bremer", "Felice C. Lightstone", "Helgi I. Ingólfsson"], "title": "Machine Learning-driven Multiscale MD Workflows: The Mini-MuMMI Experience", "comment": null, "summary": "Computational models have become one of the prevalent methods to model\ncomplex phenomena. To accurately model complex interactions, such as detailed\nbiomolecular interactions, scientists often rely on multiscale models comprised\nof several internal models operating at difference scales, ranging from\nmicroscopic to macroscopic length and time scales. Bridging the gap between\ndifferent time and length scales has historically been challenging but the\nadvent of newer machine learning (ML) approaches has shown promise for tackling\nthat task. Multiscale models require massive amounts of computational power and\na powerful workflow management system. Orchestrating ML-driven multiscale\nstudies on parallel systems with thousands of nodes is challenging, the\nworkflow must schedule, allocate and control thousands of simulations operating\nat different scales. Here, we discuss the massively parallel Multiscale\nMachine-Learned Modeling Infrastructure (MuMMI), a multiscale workflow\nmanagement infrastructure, that can orchestrate thousands of molecular dynamics\n(MD) simulations operating at different timescales, spanning from millisecond\nto nanosecond. More specifically, we introduce a novel version of MuMMI called\n\"mini-MuMMI\". Mini-MuMMI is a curated version of MuMMI designed to run on\nmodest HPC systems or even laptops whereas MuMMI requires larger HPC systems.\nWe demonstrate mini-MuMMI utility by exploring RAS-RAF membrane interactions\nand discuss the different challenges behind the generalization of multiscale\nworkflows and how mini-MuMMI can be leveraged to target a broader range of\napplications outside of MD and RAS-RAF interactions."}
{"id": "2507.07400", "categories": ["cs.DC", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.07400", "abs": "https://arxiv.org/abs/2507.07400", "authors": ["Zaifeng Pan", "Ajjkumar Patel", "Zhengding Hu", "Yipeng Shen", "Yue Guan", "Wan-Lu Li", "Lianhui Qin", "Yida Wang", "Yufei Ding"], "title": "KVFlow: Efficient Prefix Caching for Accelerating LLM-Based Multi-Agent Workflows", "comment": null, "summary": "Large language model (LLM) based agentic workflows have become a popular\nparadigm for coordinating multiple specialized agents to solve complex tasks.\nTo improve serving efficiency, existing LLM systems employ prefix caching to\nreuse key-value (KV) tensors corresponding to agents' fixed prompts, thereby\navoiding redundant computation across repeated invocations. However, current\nsystems typically evict KV caches using a Least Recently Used (LRU) policy,\nwhich fails to anticipate future agent usage and often discards KV caches\nshortly before their reuse. This leads to frequent cache misses and substantial\nrecomputation or swapping overhead. We present KVFlow, a workflow-aware KV\ncache management framework tailored for agentic workloads. KVFlow abstracts the\nagent execution schedule as an Agent Step Graph and assigns each agent a\nsteps-to-execution value that estimates its temporal proximity to future\nactivation. These values guide a fine-grained eviction policy at the KV node\nlevel, allowing KVFlow to preserve entries likely to be reused and efficiently\nmanage shared prefixes in tree-structured caches. Moreover, KVFlow introduces a\nfully overlapped KV prefetching mechanism, which proactively loads required\ntensors from CPU to GPU in background threads for agents scheduled in the next\nstep, thereby avoiding cache miss stalls during generation. Compared to SGLang\nwith hierarchical radix cache, KVFlow achieves up to 1.83$\\times$ speedup for\nsingle workflows with large prompts, and up to 2.19$\\times$ speedup for\nscenarios with many concurrent workflows."}
{"id": "2507.07671", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2507.07671", "abs": "https://arxiv.org/abs/2507.07671", "authors": ["Jovan Prodanov", "Blaž Bertalanič", "Carolina Fortuna", "Shih-Kai Chou", "Matjaž Branko Jurič", "Ramon Sanchez-Iborra", "Jernej Hribar"], "title": "Multi-agent Reinforcement Learning-based In-place Scaling Engine for Edge-cloud Systems", "comment": "Accepted at IEEE Cloud 2025", "summary": "Modern edge-cloud systems face challenges in efficiently scaling resources to\nhandle dynamic and unpredictable workloads. Traditional scaling approaches\ntypically rely on static thresholds and predefined rules, which are often\ninadequate for optimizing resource utilization and maintaining performance in\ndistributed and dynamic environments. This inefficiency hinders the\nadaptability and performance required in edge-cloud infrastructures, which can\nonly be achieved through the newly proposed in-place scaling. To address this\nproblem, we propose the Multi-Agent Reinforcement Learning-based In-place\nScaling Engine (MARLISE) that enables seamless, dynamic, reactive control with\nin-place resource scaling. We develop our solution using two Deep Reinforcement\nLearning algorithms: Deep Q-Network (DQN), and Proximal Policy Optimization\n(PPO). We analyze each version of the proposed MARLISE solution using dynamic\nworkloads, demonstrating their ability to ensure low response times of\nmicroservices and scalability. Our results show that MARLISE-based approaches\noutperform heuristic method in managing resource elasticity while maintaining\nmicroservice response times and achieving higher resource efficiency."}
{"id": "2507.07932", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2507.07932", "abs": "https://arxiv.org/abs/2507.07932", "authors": ["Guilin Zhang", "Wulan Guo", "Ziqi Tan", "Qiang Guan", "Hailong Jiang"], "title": "KIS-S: A GPU-Aware Kubernetes Inference Simulator with RL-Based Auto-Scaling", "comment": "8 pages, 6 figures", "summary": "Autoscaling GPU inference workloads in Kubernetes remains challenging due to\nthe reactive and threshold-based nature of default mechanisms such as the\nHorizontal Pod Autoscaler (HPA), which struggle under dynamic and bursty\ntraffic patterns and lack integration with GPU-level metrics. We present KIS-S,\na unified framework that combines KISim, a GPU-aware Kubernetes Inference\nSimulator, with KIScaler, a Proximal Policy Optimization (PPO)-based\nautoscaler. KIScaler learns latency-aware and resource-efficient scaling\npolicies entirely in simulation, and is directly deployed without retraining.\nExperiments across four traffic patterns show that KIScaler improves average\nreward by 75.2%, reduces P95 latency up to 6.7x over CPU baselines, and\ngeneralizes without retraining. Our work bridges the gap between reactive\nautoscaling and intelligent orchestration for scalable GPU-accelerated\nenvironments."}
{"id": "2507.07683", "categories": ["cs.AR", "cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.07683", "abs": "https://arxiv.org/abs/2507.07683", "authors": ["Jude Haris", "José Cano"], "title": "Accelerating Transposed Convolutions on FPGA-based Edge Devices", "comment": "Accepted to 35th International Conference on Field-Programmable Logic\n  and Applications (FPL) 2025", "summary": "Transposed Convolutions (TCONV) enable the up-scaling mechanism within\ngenerative Artificial Intelligence (AI) models. However, the predominant\nInput-Oriented Mapping (IOM) method for implementing TCONV has complex output\nmapping, overlapping sums, and ineffectual computations. These inefficiencies\nfurther exacerbate the performance bottleneck of TCONV and generative models on\nresource-constrained edge devices. To address this problem, in this paper we\npropose MM2IM, a hardware-software co-designed accelerator that combines Matrix\nMultiplication (MatMul) with col2IM to process TCONV layers on\nresource-constrained edge devices efficiently. Using the SECDA-TFLite design\ntoolkit, we implement MM2IM and evaluate its performance across 261 TCONV\nproblem configurations, achieving an average speedup of 1.9x against a\ndual-thread ARM Neon optimized CPU baseline. We then evaluate the performance\nof MM2IM on a range of TCONV layers from well-known generative models achieving\nup to 4.2x speedup, and compare it against similar resource-constrained TCONV\naccelerators, outperforming them by at least 2x GOPs/DSP. Finally, we evaluate\nMM2IM on the DCGAN and pix2pix GAN models, achieving up to 3x speedup and 2.4x\nenergy reduction against the CPU baseline."}
