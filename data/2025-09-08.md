<div id=toc></div>

# Table of Contents

- [cs.PL](#cs.PL) [Total: 3]
- [cs.DC](#cs.DC) [Total: 5]


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [1] [A Large-Scale Study of Floating-Point Usage in Statically Typed Languages](https://arxiv.org/abs/2509.04936)
*Andrea Gilot,Tobias Wrigstad,Eva Darulova*

Main category: cs.PL

TL;DR: 首个大规模浮点运算使用实证研究，分析GitHub公开库中静态类型语言的浮点数代码特征，为自动化推理技术提供实际代码基准


<details>
  <summary>Details</summary>
Motivation: 理解实际世界中浮点数代码的真实面貌，以支持更有效的浮点运算自动化分析和程序修复技术的发展

Method: 采用随机采样和内在属性过滤的现代挖掘方法，通过关键词搜索和代码解析来识别浮点运算使用和程序构造

Result: 证实浮点运算被广泛使用，发现文献中的性能测试基准在某些方面能代表实际代码，但非全部方面

Conclusion: 研究成果和数据集将为未来浮点运算技术的设计和评估提供价值，使其更符合实际用户需求

Abstract: Reasoning about floating-point arithmetic is notoriously hard. While static
and dynamic analysis techniques or program repair have made significant
progress, more work is still needed to make them relevant to real-world code.
On the critical path to that goal is understanding what real-world
floating-point code looks like. To close that knowledge gap, this paper
presents the first large-scale empirical study of floating-point arithmetic
usage in statically typed languages across public GitHub repositories. We
follow state-of the art mining practices including random sampling and
filtering based on only intrinsic properties to avoid bias, and identify
floating-point usage by searching for keywords in the source code, and
programming language constructs (e.g., loops) by parsing the code. Our
evaluation supports the claim often made in papers that floating-point
arithmetic is widely used. Comparing statistics such as size and usage of
certain constructs and functions, we find that benchmarks used in literature to
evaluate automated reasoning techniques for floating-point arithmetic are in
certain aspects representative of 'real-world' code, but not in all. We aim for
our study and dataset to help future techniques for floating-point arithmetic
to be designed and evaluated to match actual users' expectations.

</details>


### [2] [AI-Assisted Modeling: DSL-Driven AI Interactions](https://arxiv.org/abs/2509.05160)
*Steven Smyth,Daniel Busch,Moez Ben Haj Hmida,Edward A. Lee,Bernhard Steffen*

Main category: cs.PL

TL;DR: 通过领域特定建模技术和即时图形可视化增强AI辅助编程的透明度，支持可视化检查和形式化验证，提高代码生成和验证效率


<details>
  <summary>Details</summary>
Motivation: AI辅助编程虽然提升了软件开发性能，但缺乏透明度。通过提供准确的语义可视化和形式化验证支持，可以增强对AI生成代码的理解和信任

Method: 集成领域特定建模技术，提供即时图形可视化表示AI生成代码的语义。支持编程、自然语言提示、语音命令和分阶段精化等多种方式开发形式模型，每个转换步骤后提供即时反馈

Result: 开发了作为Visual Studio Code扩展的原型（针对Lingua Franca语言），展示了新颖的领域特定建模实践潜力

Conclusion: 该方法在模型创建、可视化和验证方面提供了创新进展，能够针对特定领域或用途定制支持，显著改善代码生成和后续验证过程

Abstract: AI-assisted programming greatly increases software development performance.
We enhance this potential by integrating transparency through domain-specific
modeling techniques and providing instantaneous, graphical visualizations that
accurately represent the semantics of AI-generated code. This approach
facilitates visual inspection and formal verification, such as model checking.
  Formal models can be developed using programming, natural language prompts,
voice commands, and stage-wise refinement, with immediate feedback after each
transformation step. This support can be tailored to specific domains or
intended purposes, improving both code generation and subsequent validation
processes.
  To demonstrate the effectiveness of this approach, we have developed a
prototype as a Visual Studio Code extension for the Lingua Franca language.
This prototype showcases the potential for novel domain-specific modeling
practices, offering an advancement in how models are created, visualized, and
verified.

</details>


### [3] [Non-Termination Proving: 100 Million LoC and Beyond](https://arxiv.org/abs/2509.05293)
*Julien Vanegue,Jules Villard,Peter O'Hearn,Azalea Raad*

Main category: cs.PL

TL;DR: Pulse Infinite是一个使用证明技术检测大型程序中非终止（发散）行为的工具，通过组合性和欠近似方法实现了在亿级代码库中的实际应用，发现了30多个未知问题


<details>
  <summary>Details</summary>
Motivation: 现有工作主要针对小型基准测试（几十到几百行代码），而实际企业代码库可能达到数千万甚至数亿行代码，规模限制阻碍了这些工具的实用性

Method: 采用组合性和欠近似方法：组合性支持大规模分析，欠近似确保证明发散的可靠性

Result: 在超过1亿行的C、C++和Hack开源及专有软件中应用，识别出30多个先前未知的问题

Conclusion: Pulse Infinite为检测现实世界代码库中的发散行为建立了新的技术标准，证明了在大规模代码库中实际应用的可行性

Abstract: We report on our tool, Pulse Infinite, that uses proof techniques to show
non-termination (divergence) in large programs. Pulse Infinite works
compositionally and under-approximately: the former supports scale, and the
latter ensures soundness for proving divergence. Prior work focused on small
benchmarks in the tens or hundreds of lines of code (LoC), and scale limits
their practicality: a single company may have tens of millions, or even
hundreds of millions of LoC or more. We report on applying Pulse Infinite to
over a hundred million lines of open-source and proprietary software written in
C, C++, and Hack, identifying over 30 previously unknown issues, establishing a
new state of the art for detecting divergence in real-world codebases.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [4] [STADI: Fine-Grained Step-Patch Diffusion Parallelism for Heterogeneous GPUs](https://arxiv.org/abs/2509.04719)
*Han Liang,Jiahui Zhou,Zicheng Zhou,Xiaoxi Zhang,Xu Chen*

Main category: cs.DC

TL;DR: STADI是一个针对异构多GPU环境的扩散模型推理加速框架，通过时空自适应调度机制，在时间维度减少慢速GPU的去噪步骤，在空间维度弹性分配图像块，实现负载均衡，相比现有方法降低45%延迟并提高资源利用率


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像生成等应用中计算成本高昂，现有并行推理方案在异构多GPU环境下存在资源利用不足和负载不均衡的问题，需要更高效的并行推理技术

Method: 提出时空自适应扩散推理(STADI)框架，包含：1)计算感知的步骤分配器，使用最小公倍数最小化量化技术减少慢速GPU的去噪步骤；2)弹性块并行机制，根据GPU计算能力分配不同大小的图像块；3)混合调度器协调时空维度的细粒度并行

Result: 在负载不均衡和异构多GPU集群上的实验验证了STADI的有效性，相比最先进的块并行框架，端到端推理延迟降低高达45%，显著提高了异构GPU的资源利用率

Conclusion: STADI通过时空自适应调度机制成功解决了异构多GPU环境下扩散模型推理的负载均衡问题，显著提升了推理效率和资源利用率，为扩散模型的实际部署提供了有效的并行推理解决方案

Abstract: The escalating adoption of diffusion models for applications such as image
generation demands efficient parallel inference techniques to manage their
substantial computational cost. However, existing diffusion parallelism
inference schemes often underutilize resources in heterogeneous multi-GPU
environments, where varying hardware capabilities or background tasks cause
workload imbalance. This paper introduces Spatio-Temporal Adaptive Diffusion
Inference (STADI), a novel framework to accelerate diffusion model inference in
such settings. At its core is a hybrid scheduler that orchestrates fine-grained
parallelism across both temporal and spatial dimensions. Temporally, STADI
introduces a novel computation-aware step allocator applied after warmup
phases, using a least-common-multiple-minimizing quantization technique to
reduce denoising steps on slower GPUs and execution synchronization. To further
minimize GPU idle periods, STADI executes an elastic patch parallelism
mechanism that allocates variably sized image patches to GPUs according to
their computational capability, ensuring balanced workload distribution through
a complementary spatial mechanism. Extensive experiments on both
load-imbalanced and heterogeneous multi-GPU clusters validate STADI's efficacy,
demonstrating improved load balancing and mitigation of performance
bottlenecks. Compared to patch parallelism, a state-of-the-art diffusion
inference framework, our method significantly reduces end-to-end inference
latency by up to 45% and significantly improves resource utilization on
heterogeneous GPUs.

</details>


### [5] [VoltanaLLM: Feedback-Driven Frequency Control and State-Space Routing for Energy-Efficient LLM Serving](https://arxiv.org/abs/2509.04827)
*Jiahuan Yu,Aryan Taneja,Junfeng Lin,Minjia Zhang*

Main category: cs.DC

TL;DR: VoltanaLLM是一个基于控制理论的SLO感知、高能效LLM服务系统，通过协同设计频率缩放和请求路由，在预填充/解码分离架构中实现细粒度控制，节省高达36.3%的能源消耗。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在实时聊天助手、代码生成等交互式应用中的广泛使用，其推理过程的高能耗成本成为可持续部署的主要挑战，需要开发能效优化的服务系统。

Method: 采用控制理论视角，在预填充/解码分离架构中协同设计频率缩放和请求路由：1）反馈驱动的频率控制器动态调整GPU频率；2）状态空间路由器探索跨频率缩放实例的路由决策，在延迟约束下最小化能耗。

Result: 在SGLang中实现并在多个先进LLM和真实数据集上评估，结果显示VoltanaLLM实现了高达36.3%的能源节省，同时保持近乎完美的SLO达成率。

Conclusion: VoltanaLLM为可持续和智能的LLM服务铺平了道路，通过精细化的阶段特定控制实现了能效和性能的良好平衡。

Abstract: Modern Large Language Model (LLM) serving systems increasingly support
interactive applications, like real-time chat assistants, code generation
tools, and agentic workflows. However, the soaring energy cost of LLM inference
presents a growing challenge for sustainable and cost-effective deployment.
This paper introduces VoltanaLLM, a system for SLO-aware, energy-efficient LLM
serving, built from a control theory perspective. VoltanaLLM co-designs
frequency scaling and request routing in emerging prefill/decode disaggregated
architectures, leveraging their decoupled execution to enable fine-grained
phase-specific control. It consists of a feedback-driven frequency controller
that dynamically adapts GPU frequency for prefill and decode phases, and a
state-space router that explores routing decisions across frequency-scaled
instances to minimize energy under latency constraints. We implement VoltanaLLM
in SGLang and evaluate its performance over multiple state-of-the-art LLMs and
real-world datasets. The results demonstrate that VoltanaLLM achieves up to
36.3% energy savings while maintaining near-perfect SLO attainment rate, paving
the way for sustainable and intelligent LLM serving.

</details>


### [6] [Toward Distributed 3D Gaussian Splatting for High-Resolution Isosurface Visualization](https://arxiv.org/abs/2509.05216)
*Mengjiao Han,Andres Sewell,Joseph Insley,Janet Knowles,Victor A. Mateevitsi,Michael E. Papka,Steve Petruzza,Silvio Rizzi*

Main category: cs.DC

TL;DR: 多GPU扩展3D高斯散点技术，提升科学可视化数据处理规模和训练速度


<details>
  <summary>Details</summary>
Motivation: 解决单GPU在处理大规模科学数据时的容量和性能限制，为HPC环境下的实时科学可视化提供基础

Method: 基于3D高斯散点技术，采用Grendel-GS的多GPU训练后端，通过分布式优化在多个GPU上并行处理大规模数据

Result: 在4个GPU上实现了5.6倍速度提升，成功训练了18M高斯原语的Miranda数据集，这在单个A100 GPU上是不可行的

Conclusion: 该多GPU扩展为3D-GS技术集成到HPC科学工作流中奠定了基础，支持实时的后处理和实时可视化复杂模拟结果

Abstract: We present a multi-GPU extension of the 3D Gaussian Splatting (3D-GS)
pipeline for scientific visualization. Building on previous work that
demonstrated high-fidelity isosurface reconstruction using Gaussian primitives,
we incorporate a multi-GPU training backend adapted from Grendel-GS to enable
scalable processing of large datasets. By distributing optimization across
GPUs, our method improves training throughput and supports high-resolution
reconstructions that exceed single-GPU capacity. In our experiments, the system
achieves a 5.6X speedup on the Kingsnake dataset (4M Gaussians) using four GPUs
compared to a single-GPU baseline, and successfully trains the Miranda dataset
(18M Gaussians) that is an infeasible task on a single A100 GPU. This work lays
the groundwork for integrating 3D-GS into HPC-based scientific workflows,
enabling real-time post hoc and in situ visualization of complex simulations.

</details>


### [7] [Dynamic reconfiguration for malleable applications using RMA](https://arxiv.org/abs/2509.05248)
*Iker Martín-Álvarez,José I. Aliaga,Maribel Castillo*

Main category: cs.DC

TL;DR: 基于MPI RMA的单边通信方法用于可扩展应用的动态调整，与传统集体通信方法性能相当但初始化成本较高


<details>
  <summary>Details</summary>
Motivation: 研究基于远程内存访问(RMA)的单边通信方法，用于可扩展应用的动态调整和数据重分布，减少对应用执行的影响

Method: 将单边通信方法集成到MaM库中，与传统集体通信方法进行比较，并扩展Wait Drains策略以支持高效后台重配置

Result: 结果显示性能相当，但高初始化成本目前限制了其优势

Conclusion: 基于RMA的单边通信方法在可扩展应用动态调整方面具有潜力，但需要进一步优化初始化成本

Abstract: This paper investigates the novel one-sided communication methods based on
remote memory access (RMA) operations in MPI for dynamic resizing of malleable
applications, enabling data redistribution with minimal impact on application
execution. After their integration into the MaM library, these methods are
compared with traditional collective-based approaches. In addition, the
existing strategy Wait Drains is extended to support efficient background
reconfiguration. Results show comparable performance, though high
initialization costs currently limit their advantage.

</details>


### [8] [Scaling Performance of Large Language Model Pretraining](https://arxiv.org/abs/2509.05258)
*Alexander Interrante-Grant,Carla Varela-Rosa,Suhaas Narayan,Chris Connelly,Albert Reuther*

Main category: cs.DC

TL;DR: 本文揭秘大型语言模型预训练流程，重点关注分布式训练、大规模数据集管理和数据并行扩展方案


<details>
  <summary>Details</summary>
Motivation: 大型语言模型训练计算成本极高，但相关扩展性能和训练考虑因素的公开信息缺乏，实践推荐不足

Method: 研究大规模训练流程的分布式训练技术、数据集管理方法以及数据并行扩展策略

Result: 提供了在百级节点规模上管理大型数据集、充分利用GPU计算能力的实用方法和最佳实践

Conclusion: 本研究填补了大型语言模型训练技术公开知识的空白，为越来越多的研空间提供了具体的技术指南和实践建议

Abstract: Large language models (LLMs) show best-in-class performance across a wide
range of natural language processing applications. Training these models is an
extremely computationally expensive task; frontier Artificial Intelligence (AI)
research companies are investing billions of dollars into supercomputing
infrastructure to train progressively larger models on increasingly massive
datasets. Unfortunately, information about the scaling performance and training
considerations of these large training pipelines is scarce in public
literature. Working with large-scale datasets and models can be complex and
practical recommendations are scarce in the public literature for tuning
training performance when scaling up large language models. In this paper, we
aim to demystify the large language model pretraining pipeline somewhat - in
particular with respect to distributed training, managing large datasets across
hundreds of nodes, and scaling up data parallelism with an emphasis on fully
leveraging available GPU compute capacity.

</details>
