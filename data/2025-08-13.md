<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 4]
- [cs.AR](#cs.AR) [Total: 4]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [Profiling Concurrent Vision Inference Workloads on NVIDIA Jetson -- Extended](https://arxiv.org/abs/2508.08430)
*Abhinaba Chakraborty,Wouter Tavernier,Akis Kourtis,Mario Pickavet,Andreas Oikonomakis,Didier Colle*

Main category: cs.DC

TL;DR: 论文分析了边缘计算中GPU资源利用不足的问题，通过多工具追踪揭示了低层次资源利用率低和CPU事件成为瓶颈的现象，并提出了优化建议。


<details>
  <summary>Details</summary>
Motivation: 随着IoT设备和网络技术的发展，边缘计算对实时数据处理的需求增加，但GPU在计算密集型任务中仍存在资源利用不足的问题，尤其是对资源共享的理解有限。

Method: 通过分析高、低层次指标（如GPU利用率、内存使用、SM和Tensor核心利用率），结合多工具追踪，全面评估NVIDIA Jetson设备在并发视觉推理任务中的资源行为。

Result: 研究发现，尽管GPU利用率可达100%，但SM和Tensor核心利用率仅为15%-30%，且CPU事件（如线程调度）常成为性能瓶颈。

Conclusion: 论文为NVIDIA边缘设备的视觉推理任务用户提供了关键观察结果，指导硬件感知优化。

Abstract: The proliferation of IoT devices and advancements in network technologies
have intensified the demand for real-time data processing at the network edge.
To address these demands, low-power AI accelerators, particularly GPUs, are
increasingly deployed for inference tasks, enabling efficient computation while
mitigating cloud-based systems' latency and bandwidth limitations. Despite
their growing deployment, GPUs remain underutilised even in computationally
intensive workloads. This underutilisation stems from the limited understanding
of GPU resource sharing, particularly in edge computing scenarios. In this
work, we conduct a detailed analysis of both high- and low-level metrics,
including GPU utilisation, memory usage, streaming multiprocessor (SM)
utilisation, and tensor core usage, to identify bottlenecks and guide
hardware-aware optimisations. By integrating traces from multiple profiling
tools, we provide a comprehensive view of resource behaviour on NVIDIA Jetson
edge devices under concurrent vision inference workloads. Our findings indicate
that while GPU utilisation can reach $100\%$ under specific optimisations,
critical low-level resources, such as SMs and tensor cores, often operate only
at $15\%$ to $30\%$ utilisation. Moreover, we observe that certain CPU-side
events, such as thread scheduling, context switching, etc., frequently emerge
as bottlenecks, further constraining overall GPU performance. We provide
several key observations for users of vision inference workloads on NVIDIA edge
devices.

</details>


### [2] [Benchmarking Federated Learning for Throughput Prediction in 5G Live Streaming Applications](https://arxiv.org/abs/2508.08479)
*Yuvraj Dutta,Soumyajit Chatterjee,Sandip Chakraborty,Basabdatta Palit*

Main category: cs.DC

TL;DR: 论文首次全面评估了联邦学习（FL）策略在5G边缘场景中的吞吐量预测性能，比较了三种聚合算法和四种时间序列架构，发现FedBN在非IID数据下表现稳健，LSTM和Transformer模型优于CNN基线。


<details>
  <summary>Details</summary>
Motivation: 解决5G和6G网络中异构移动环境下非IID数据分布对吞吐量预测的限制，探索联邦学习的适用性。

Method: 评估了FedAvg、FedProx和FedBN三种聚合算法，以及LSTM、CNN、CNN+LSTM和Transformer四种时间序列架构，使用五个真实数据集分析性能。

Result: FedBN在非IID条件下表现稳健，LSTM和Transformer模型在R2分数上比CNN基线高80%，Transformer收敛更快但需要更长历史窗口。

Conclusion: FedBN结合LSTM或Transformer模型在吞吐量预测中表现最佳，为下一代无线网络提供了可扩展且隐私保护的解决方案。

Abstract: Accurate and adaptive network throughput prediction is essential for
latency-sensitive and bandwidth-intensive applications in 5G and emerging 6G
networks. However, most existing methods rely on centralized training with
uniformly collected data, limiting their applicability in heterogeneous mobile
environments with non-IID data distributions. This paper presents the first
comprehensive benchmarking of federated learning (FL) strategies for throughput
prediction in realistic 5G edge scenarios. We evaluate three aggregation
algorithms - FedAvg, FedProx, and FedBN - across four time-series
architectures: LSTM, CNN, CNN+LSTM, and Transformer, using five diverse
real-world datasets. We systematically analyze the effects of client
heterogeneity, cohort size, and history window length on prediction
performance. Our results reveal key trade-offs among model complexities,
convergence rates, and generalization. It is found that FedBN consistently
delivers robust performance under non-IID conditions. On the other hand, LSTM
and Transformer models outperform CNN-based baselines by up to 80% in R2
scores. Moreover, although Transformers converge in half the rounds of LSTM,
they require longer history windows to achieve a high R2, indicating higher
context dependence. LSTM is, therefore, found to achieve a favorable balance
between accuracy, rounds, and temporal footprint. To validate the end-to-end
applicability of the framework, we have integrated our FL-based predictors into
a live adaptive streaming pipeline. It is seen that FedBN-based LSTM and
Transformer models improve mean QoE scores by 11.7% and 11.4%, respectively,
over FedAvg, while also reducing the variance. These findings offer actionable
insights for building scalable, privacy-preserving, and edge-aware throughput
prediction systems in next-generation wireless networks.

</details>


### [3] [A Reinforcement Learning-Driven Task Scheduling Algorithm for Multi-Tenant Distributed Systems](https://arxiv.org/abs/2508.08525)
*Xiaopei Zhang,Xingang Wang,Xin Wang*

Main category: cs.DC

TL;DR: 本文提出了一种基于强化学习的自适应任务调度方法，用于解决多租户分布式系统中的动态资源变化、异构租户需求和公平性保障问题。


<details>
  <summary>Details</summary>
Motivation: 多租户分布式系统中的任务调度面临动态资源变化、异构租户需求和公平性保障等挑战，需要一种高效且自适应的解决方案。

Method: 通过将调度过程建模为马尔可夫决策过程，定义了状态空间、动作空间和奖励函数，并采用近端策略优化（PPO）算法设计调度策略学习框架。

Result: 实验结果表明，该方法在任务延迟控制、资源效率、策略稳定性和公平性方面优于现有调度方法，表现出较强的稳定性和泛化能力。

Conclusion: 该调度框架在策略设计、动态资源建模和多租户服务保障方面具有实用和工程价值，能有效提升复杂条件下分布式系统的调度效率和资源管理。

Abstract: This paper addresses key challenges in task scheduling for multi-tenant
distributed systems, including dynamic resource variation, heterogeneous tenant
demands, and fairness assurance. An adaptive scheduling method based on
reinforcement learning is proposed. By modeling the scheduling process as a
Markov decision process, the study defines the state space, action space, and
reward function. A scheduling policy learning framework is designed using
Proximal Policy Optimization (PPO) as the core algorithm. This enables dynamic
perception of complex system states and real-time decision-making. Under a
multi-objective reward mechanism, the scheduler jointly optimizes task latency,
resource utilization, and tenant fairness. The coordination between the policy
network and the value network continuously refines the scheduling strategy.
This enhances overall system performance. To validate the effectiveness of the
proposed method, a series of experiments were conducted in multi-scenario
environments built using a real-world public dataset. The experiments evaluated
task latency control, resource efficiency, policy stability, and fairness. The
results show that the proposed method outperforms existing scheduling
approaches across multiple evaluation metrics. It demonstrates strong stability
and generalization ability. The proposed scheduling framework provides
practical and engineering value in policy design, dynamic resource modeling,
and multi-tenant service assurance. It effectively improves scheduling
efficiency and resource management in distributed systems under complex
conditions.

</details>


### [4] [P/D-Device: Disaggregated Large Language Model between Cloud and Devices](https://arxiv.org/abs/2508.09035)
*Yibo Jin,Yixu Xu,Yue Chen,Chengbin Wang,Tao Wang,Jiaqi Huang,Rongfei Zhang,Yiming Dong,Yuting Yan,Ke Cheng,Yingjie Zhu,Shulan Wang,Qianqian Tang,Shuaishuai Meng,Guanxin Cheng,Ze Wang,Shuyan Miao,Ketao Wang,Wen Liu,Yifan Yang,Tong Zhang,Anran Wang,Chengzhou Lu,Tiantian Dong,Yongsheng Zhang,Zhe Wang,Hefei Guo,Hongjie Liu,Wei Lu,Zhengyong Zhang*

Main category: cs.DC

TL;DR: 论文提出了一种分离大型语言模型在云端和设备端的方法（P/D-Device），以解决云端资源占用和设备端计算能力限制的问题，显著降低了首次响应时间（TTFT）并提高了云端吞吐量。


<details>
  <summary>Details</summary>
Motivation: 工业实践中，大型语言模型的解码阶段生成过多令牌会长期占用云端资源，降低吞吐量；同时，设备端资源有限，提示长度增加会导致首次响应时间（TTFT）显著上升。

Method: 通过将大型语言模型分离到云端和设备端，云端仅在预填充阶段辅助设备端。设备端在接收到云端首个令牌后立即响应用户，降低TTFT；后续令牌通过速度控制器平滑输出，设备端逐步追赶进度。云端预填充期间还可优化提示以加速设备端推理。

Result: 实验显示，TTFT降低至少60%，最大TPOT为几十毫秒，云端吞吐量提升高达15倍。

Conclusion: P/D-Device方案有效解决了资源瓶颈问题，显著提升了系统性能。

Abstract: Serving disaggregated large language models has been widely adopted in
industrial practice for enhanced performance. However, too many tokens
generated in decoding phase, i.e., occupying the resources for a long time,
essentially hamper the cloud from achieving a higher throughput. Meanwhile, due
to limited on-device resources, the time to first token (TTFT), i.e., the
latency of prefill phase, increases dramatically with the growth on prompt
length. In order to concur with such a bottleneck on resources, i.e., long
occupation in cloud and limited on-device computing capacity, we propose to
separate large language model between cloud and devices. That is, the cloud
helps a portion of the content for each device, only in its prefill phase.
Specifically, after receiving the first token from the cloud, decoupling with
its own prefill, the device responds to the user immediately for a lower TTFT.
Then, the following tokens from cloud are presented via a speed controller for
smoothed TPOT (the time per output token), until the device catches up with the
progress. On-device prefill is then amortized using received tokens while the
resource usage in cloud is controlled. Moreover, during cloud prefill, the
prompt can be refined, using those intermediate data already generated, to
further speed up on-device inference. We implement such a scheme P/D-Device,
and confirm its superiority over other alternatives. We further propose an
algorithm to decide the best settings. Real-trace experiments show that TTFT
decreases at least 60%, maximum TPOT is about tens of milliseconds, and cloud
throughput increases by up to 15x.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [5] [XDMA: A Distributed, Extensible DMA Architecture for Layout-Flexible Data Movements in Heterogeneous Multi-Accelerator SoCs](https://arxiv.org/abs/2508.08396)
*Fanchen Kong,Yunhao Deng,Xiaoling Yi,Ryan Antonio,Marian Verhelst*

Main category: cs.AR

TL;DR: XDMA是一种分布式可扩展DMA架构，通过硬件优化实现高带宽和灵活布局的数据传输，显著提升异构加速器性能。


<details>
  <summary>Details</summary>
Motivation: 现代AI工作负载依赖异构加速器，但现有DMA引擎仅适用于连续内存访问，导致控制开销高和链路利用率低。

Method: 提出XDMA架构，包含硬件地址生成器、分布式DMA设计及灵活插件，优化数据传输和布局转换。

Result: 在合成工作负载中链路利用率提升151.2倍，实际应用中速度提升2.3倍，面积开销仅2%，功耗为17%。

Conclusion: XDMA通过协同优化内存访问、布局转换和互联协议，显著提升异构多加速器SoC性能。

Abstract: As modern AI workloads increasingly rely on heterogeneous accelerators,
ensuring high-bandwidth and layout-flexible data movements between accelerator
memories has become a pressing challenge. Direct Memory Access (DMA) engines
promise high bandwidth utilization for data movements but are typically optimal
only for contiguous memory access, thus requiring additional software loops for
data layout transformations. This, in turn, leads to excessive control overhead
and underutilized on-chip interconnects. To overcome this inefficiency, we
present XDMA, a distributed and extensible DMA architecture that enables
layout-flexible data movements with high link utilization. We introduce three
key innovations: (1) a data streaming engine as XDMA Frontend, replacing
software address generators with hardware ones; (2) a distributed DMA
architecture that maximizes link utilization and separates configuration from
data transfer; (3) flexible plugins for XDMA enabling on-the-fly data
manipulation during data transfers. XDMA demonstrates up to 151.2x/8.2x higher
link utilization than software-based implementations in synthetic workloads and
achieves 2.3x average speedup over accelerators with SoTA DMA in real-world
applications. Our design incurs <2% area overhead over SoTA DMA solutions while
consuming 17% of system power. XDMA proves that co-optimizing memory access,
layout transformation, and interconnect protocols is key to unlocking
heterogeneous multi-accelerator SoC performance.

</details>


### [6] [Architecting Long-Context LLM Acceleration with Packing-Prefetch Scheduler and Ultra-Large Capacity On-Chip Memories](https://arxiv.org/abs/2508.08457)
*Ming-Yen Lee,Faaiq Waqar,Hanchen Yang,Muhammed Ahosan Ul Karim,Harsono Simka,Shimeng Yu*

Main category: cs.AR

TL;DR: 提出了一种结合打包预取调度和3D嵌入式存储器的架构，显著加速长上下文LLM推理。


<details>
  <summary>Details</summary>
Motivation: 解决长上下文LLM推理中因KV缓存传输导致的HBM带宽瓶颈问题。

Method: 采用打包预取调度和3D嵌入式存储器优化KV缓存管理。

Result: 在Llama3.1-8B上实现8.06倍解码加速和1.83倍延迟降低，多请求吞吐提升1.7-2.4倍。

Conclusion: 通过打包、预取和3D存储器的协同设计，有效缓解HBM限制，提升长上下文LLM推理效率。

Abstract: Long-context Large Language Model (LLM) inference faces increasing compute
bottlenecks as attention calculations scale with context length, primarily due
to the growing KV-cache transfer overhead that saturates High Bandwidth Memory
(HBM). While prefetching techniques mitigate cache misses by fetching KV data
in advance, their spatial and temporal benefits present new opportunities to
exploit. This work proposes a packing-prefetch scheduling architecture with
monolithic 3D (M3D) back-end-of-line (BEOL) compatible embedded memories with
ultra-large on-chip capacity to accelerate long-context LLM inference. Our
optimizations demonstrate 8.06x decode speedup and 1.83x overall latency
reduction on Llama3.1-8B using TPUv6e-like hardware with additional 512MB BEOL
memories over the serial execution. Evaluations of multi-request workloads on
TPU-like architectures show 1.7x-2.4x throughput improvement and 1.5x-2.4x HBM
bandwidth reduction compared to packing-only methods on Llama3.1-8B and
Llama3.1-70B models. With the co-design of packing, prefetching, and BEOL
memories, our approach alleviates HBM constraints and enables efficient
long-context LLM inference.

</details>


### [7] [JSPIM: A Skew-Aware PIM Accelerator for High-Performance Databases Join and Select Operations](https://arxiv.org/abs/2508.08503)
*Sabiha Tajdari,Anastasia Ailamaki,Sandhya Dwarkadas*

Main category: cs.AR

TL;DR: JSPIM是一种通过算法-硬件协同设计加速哈希连接的PIM模块，显著提升数据库查询性能。


<details>
  <summary>Details</summary>
Motivation: 数据库应用受限于内存带宽和延迟，特别是连接查询效率低下，现有PIM设计未能解决数据倾斜问题。

Method: JSPIM在子阵列内部署并行搜索引擎，重新设计哈希表以实现O(1)查找，并利用子阵列级并行性和rank级处理缓解数据倾斜。

Result: JSPIM在连接查询上比DuckDB快400x至1000x，SSB基准测试中整体吞吐量提升2.5倍。

Conclusion: JSPIM通过高效利用PIM的细粒度并行性，显著提升数据库查询性能，同时硬件开销极小。

Abstract: Database applications are increasingly bottlenecked by memory bandwidth and
latency due to the memory wall and the limited scalability of DRAM. Join
queries, central to analytical workloads, require intensive memory access and
are particularly vulnerable to inefficiencies in data movement. While
Processing-in-Memory (PIM) offers a promising solution, existing designs
typically reuse CPU-oriented join algorithms, limiting parallelism and
incurring costly inter-chip communication. Additionally, data skew, a main
challenge in CPU-based joins, remains unresolved in current PIM architectures.
  We introduce JSPIM, a PIM module that accelerates hash join and, by
extension, corresponding select queries through algorithm-hardware co-design.
JSPIM deploys parallel search engines within each subarray and redesigns hash
tables to achieve O(1) lookups, fully exploiting PIM's fine-grained
parallelism. To mitigate skew, our design integrates subarray-level parallelism
with rank-level processing, eliminating redundant off-chip transfers.
Evaluations show JSPIM delivers 400x to 1000x speedup on join queries versus
DuckDB. When paired with DuckDB for the full SSB benchmark, JSPIM achieves an
overall 2.5x throughput improvement (individual query gains of 1.1x to 28x), at
just a 7% data overhead and 2.1% per-rank PIM-enabled chip area increase.

</details>


### [8] [OISMA: On-the-fly In-memory Stochastic Multiplication Architecture for Matrix-Multiplication Workloads](https://arxiv.org/abs/2508.08822)
*Shady Agwa,Yihan Pan,Georgios Papandroulidakis,Themis Prodromakis*

Main category: cs.AR

TL;DR: OISMA是一种新型内存计算架构，利用准随机计算域（Bent-Pyramid系统）简化计算，同时保持数字存储的高效性和可扩展性，显著提升了矩阵乘法的准确性和能效。


<details>
  <summary>Details</summary>
Motivation: 当前AI模型的计算瓶颈在于大规模矩阵乘法，而现有内存计算架构存在性能限制。OISMA旨在通过准随机计算域解决这一问题。

Method: OISMA将普通内存读取操作转换为原位随机乘法操作，并通过累加外围电路实现矩阵乘法功能。

Result: 实验显示，OISMA在4x4到512x512矩阵上的平均相对Frobenius误差显著降低，能效和面积效率分别为0.891 TOPS/W和3.98 GOPS/mm²。

Conclusion: OISMA在能效和面积效率上优于现有内存计算架构，未来技术节点缩放将进一步优化其性能。

Abstract: Artificial Intelligence models are currently driven by a significant
up-scaling of their complexity, with massive matrix multiplication workloads
representing the major computational bottleneck. In-memory computing
architectures are proposed to avoid the Von Neumann bottleneck. However, both
digital/binary-based and analogue in-memory computing architectures suffer from
various limitations, which significantly degrade the performance and energy
efficiency gains. This work proposes OISMA, a novel in-memory computing
architecture that utilizes the computational simplicity of a quasi-stochastic
computing domain (Bent-Pyramid system), while keeping the same efficiency,
scalability, and productivity of digital memories. OISMA converts normal memory
read operations into in-situ stochastic multiplication operations with a
negligible cost. An accumulation periphery then accumulates the output
multiplication bitstreams, achieving the matrix multiplication functionality.
Extensive matrix multiplication benchmarking was conducted to analyze the
accuracy of the Bent-Pyramid system, using matrix dimensions ranging from 4x4
to 512x512. The accuracy results show a significant decrease in the average
relative Frobenius error, from 9.42% (for 4x4) to 1.81% (for 512x512), compared
to 64-bit double precision floating-point format. A 1T1R OISMA array of 4 KB
capacity was implemented using a commercial 180nm technology node and in-house
RRAM technology. At 50 MHz, OISMA achieves 0.891 TOPS/W and 3.98 GOPS/mm2 for
energy and area efficiency, respectively, occupying an effective computing area
of 0.804241 mm2. Scaling OISMA from 180nm to 22nm technology shows a
significant improvement of two orders of magnitude in energy efficiency and one
order of magnitude in area efficiency, compared to dense matrix multiplication
in-memory computing architectures.

</details>
