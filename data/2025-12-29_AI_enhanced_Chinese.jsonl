{"id": "2512.21777", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2512.21777", "abs": "https://arxiv.org/abs/2512.21777", "authors": ["Zhenya Zang", "Xingda Li", "David Day Uei Li"], "title": "Online Learning Extreme Learning Machine with Low-Complexity Predictive Plasticity Rule and FPGA Implementation", "comment": null, "summary": "We propose a simplified, biologically inspired predictive local learning rule that eliminates the need for global backpropagation in conventional neural networks and membrane integration in event-based training. Weight updates are triggered only on prediction errors and are performed using sparse, binary-driven vector additions. We integrate this rule into an extreme learning machine (ELM), replacing the conventional computationally intensive matrix inversion. Compared to standard ELM, our approach reduces the complexity of the training from O(M^3) to O(M), in terms of M nodes in the hidden layer, while maintaining comparable accuracy (within 3.6% and 2.0% degradation on training and test datasets, respectively). We demonstrate an FPGA implementation and compare it with existing studies, showing significant reductions in computational and memory requirements. This design demonstrates strong potential for energy-efficient online learning on low-cost edge devices.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7b80\u5316\u7684\u751f\u7269\u542f\u53d1\u5f0f\u9884\u6d4b\u5c40\u90e8\u5b66\u4e60\u89c4\u5219\uff0c\u65e0\u9700\u5168\u5c40\u53cd\u5411\u4f20\u64ad\u548c\u4e8b\u4ef6\u9a71\u52a8\u8bad\u7ec3\u4e2d\u7684\u819c\u79ef\u5206\uff0c\u4ec5\u57fa\u4e8e\u9884\u6d4b\u8bef\u5dee\u89e6\u53d1\u6743\u91cd\u66f4\u65b0\uff0c\u901a\u8fc7\u7a00\u758f\u4e8c\u8fdb\u5236\u9a71\u52a8\u5411\u91cf\u52a0\u6cd5\u5b9e\u73b0\u3002\u5c06\u8be5\u89c4\u5219\u96c6\u6210\u5230\u6781\u9650\u5b66\u4e60\u673a\u4e2d\uff0c\u66ff\u4ee3\u8ba1\u7b97\u5bc6\u96c6\u7684\u77e9\u9635\u6c42\u9006\uff0c\u5c06\u8bad\u7ec3\u590d\u6742\u5ea6\u4eceO(M\u00b3)\u964d\u81f3O(M)\uff0c\u7cbe\u5ea6\u635f\u5931\u5f88\u5c0f\uff08\u8bad\u7ec3\u548c\u6d4b\u8bd5\u96c6\u5206\u522b\u4e0b\u964d3.6%\u548c2.0%\uff09\uff0cFPGA\u5b9e\u73b0\u663e\u793a\u8ba1\u7b97\u548c\u5185\u5b58\u9700\u6c42\u663e\u8457\u964d\u4f4e\uff0c\u9002\u5408\u4f4e\u529f\u8017\u8fb9\u7f18\u8bbe\u5907\u5728\u7ebf\u5b66\u4e60\u3002", "motivation": "\u4f20\u7edf\u795e\u7ecf\u7f51\u7edc\u9700\u8981\u5168\u5c40\u53cd\u5411\u4f20\u64ad\uff0c\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\uff1b\u4e8b\u4ef6\u9a71\u52a8\u8bad\u7ec3\u9700\u8981\u819c\u79ef\u5206\uff0c\u5b9e\u73b0\u590d\u6742\u3002\u73b0\u6709\u65b9\u6cd5\u4e0d\u9002\u5408\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u8fdb\u884c\u5728\u7ebf\u5b66\u4e60\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u9ad8\u6548\u3001\u4f4e\u529f\u8017\u7684\u5b66\u4e60\u7b97\u6cd5\u3002", "method": "\u63d0\u51fa\u751f\u7269\u542f\u53d1\u7684\u9884\u6d4b\u5c40\u90e8\u5b66\u4e60\u89c4\u5219\uff1a1\uff09\u4ec5\u5f53\u51fa\u73b0\u9884\u6d4b\u8bef\u5dee\u65f6\u89e6\u53d1\u6743\u91cd\u66f4\u65b0\uff1b2\uff09\u4f7f\u7528\u7a00\u758f\u4e8c\u8fdb\u5236\u9a71\u52a8\u5411\u91cf\u52a0\u6cd5\u8fdb\u884c\u66f4\u65b0\uff1b3\uff09\u5c06\u8be5\u89c4\u5219\u96c6\u6210\u5230\u6781\u9650\u5b66\u4e60\u673a\u4e2d\uff0c\u66ff\u4ee3\u4f20\u7edf\u7684\u77e9\u9635\u6c42\u9006\u64cd\u4f5c\uff1b4\uff09\u5b9e\u73b0FPGA\u786c\u4ef6\u52a0\u901f\u3002", "result": "1\uff09\u8bad\u7ec3\u590d\u6742\u5ea6\u4eceO(M\u00b3)\u964d\u81f3O(M)\uff0cM\u4e3a\u9690\u85cf\u5c42\u8282\u70b9\u6570\uff1b2\uff09\u7cbe\u5ea6\u635f\u5931\u5f88\u5c0f\uff1a\u8bad\u7ec3\u96c6\u4e0b\u964d3.6%\uff0c\u6d4b\u8bd5\u96c6\u4e0b\u964d2.0%\uff1b3\uff09FPGA\u5b9e\u73b0\u663e\u793a\u8ba1\u7b97\u548c\u5185\u5b58\u9700\u6c42\u663e\u8457\u964d\u4f4e\uff1b4\uff09\u4e0e\u73b0\u6709\u7814\u7a76\u76f8\u6bd4\uff0c\u5728\u8ba1\u7b97\u6548\u7387\u548c\u8d44\u6e90\u4f7f\u7528\u65b9\u9762\u6709\u660e\u663e\u4f18\u52bf\u3002", "conclusion": "\u63d0\u51fa\u7684\u7b80\u5316\u9884\u6d4b\u5c40\u90e8\u5b66\u4e60\u89c4\u5219\u7ed3\u5408\u6781\u9650\u5b66\u4e60\u673a\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4f4e\u529f\u8017\u7684\u5728\u7ebf\u5b66\u4e60\uff0c\u7279\u522b\u9002\u5408\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u5e94\u7528\uff0c\u5728\u4fdd\u6301\u53ef\u63a5\u53d7\u7cbe\u5ea6\u635f\u5931\u7684\u540c\u65f6\u5927\u5e45\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u3002"}}
{"id": "2512.22066", "categories": ["cs.AR", "cs.LG", "cs.PF"], "pdf": "https://arxiv.org/pdf/2512.22066", "abs": "https://arxiv.org/abs/2512.22066", "authors": ["Hannah Atmer", "Yuan Yao", "Thiemo Voigt", "Stefanos Kaxiras"], "title": "Prefill vs. Decode Bottlenecks: SRAM-Frequency Tradeoffs and the Memory-Bandwidth Ceiling", "comment": null, "summary": "Energy consumption dictates the cost and environmental impact of deploying Large Language Models. This paper investigates the impact of on-chip SRAM size and operating frequency on the energy efficiency and performance of LLM inference, focusing on the distinct behaviors of the compute-bound prefill and memory-bound decode phases. Our simulation methodology combines OpenRAM for energy modeling, LLMCompass for latency simulation, and ScaleSIM for systolic array operational intensity. Our findings show that total energy use is predominantly determined by SRAM size in both phases, with larger buffers significantly increasing static energy due to leakage, which is not offset by corresponding latency benefits. We quantitatively explore the memory-bandwidth bottleneck, demonstrating that while high operating frequencies reduce prefill latency, their positive impact on memory-bound decode latency is capped by the external memory bandwidth. Counter-intuitively, high compute frequency can reduce total energy by reducing execution time and consequently decreasing static energy consumption more than the resulting dynamic power increase. We identify an optimal hardware configuration for the simulated workload: high operating frequencies (1200MHz-1400MHz) and a small local buffer size of 32KB to 64KB. This combination achieves the best energy-delay product, balancing low latency with high energy efficiency. Furthermore, we demonstrate how memory bandwidth acts as a performance ceiling, and that increasing compute frequency only yields performance gains up to the point where the workload becomes memory-bound. This analysis provides concrete architectural insights for designing energy-efficient LLM accelerators, especially for datacenters aiming to minimize their energy overhead.", "AI": {"tldr": "\u7814\u7a76LLM\u63a8\u7406\u4e2dSRAM\u5927\u5c0f\u548c\u8fd0\u884c\u9891\u7387\u5bf9\u80fd\u6548\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u5c0f\u7f13\u51b2\u533a\uff0832-64KB\uff09\u548c\u9ad8\u9891\u7387\uff081200-1400MHz\uff09\u7ec4\u5408\u80fd\u5b9e\u73b0\u6700\u4f73\u80fd\u8017\u5ef6\u8fdf\u79ef\u3002", "motivation": "LLM\u7684\u80fd\u8017\u76f4\u63a5\u5f71\u54cd\u90e8\u7f72\u6210\u672c\u548c\u73af\u5883\u5f71\u54cd\uff0c\u9700\u8981\u7814\u7a76\u786c\u4ef6\u914d\u7f6e\uff08SRAM\u5927\u5c0f\u3001\u8fd0\u884c\u9891\u7387\uff09\u5982\u4f55\u5f71\u54cd\u63a8\u7406\u80fd\u6548\uff0c\u7279\u522b\u9488\u5bf9\u8ba1\u7b97\u5bc6\u96c6\u578b\u9884\u586b\u5145\u548c\u5185\u5b58\u5bc6\u96c6\u578b\u89e3\u7801\u4e24\u4e2a\u9636\u6bb5\u7684\u4e0d\u540c\u7279\u6027\u3002", "method": "\u91c7\u7528OpenRAM\u8fdb\u884c\u80fd\u8017\u5efa\u6a21\u3001LLMCompass\u8fdb\u884c\u5ef6\u8fdf\u6a21\u62df\u3001ScaleSIM\u8fdb\u884c\u8109\u52a8\u9635\u5217\u64cd\u4f5c\u5f3a\u5ea6\u5206\u6790\u7684\u4eff\u771f\u65b9\u6cd5\uff0c\u7814\u7a76SRAM\u5927\u5c0f\u548c\u8fd0\u884c\u9891\u7387\u5bf9LLM\u63a8\u7406\u80fd\u6548\u7684\u5f71\u54cd\u3002", "result": "\u603b\u80fd\u8017\u4e3b\u8981\u7531SRAM\u5927\u5c0f\u51b3\u5b9a\uff0c\u5927\u7f13\u51b2\u533a\u56e0\u6f0f\u7535\u663e\u8457\u589e\u52a0\u9759\u6001\u80fd\u8017\uff1b\u9ad8\u9891\u7387\u867d\u589e\u52a0\u52a8\u6001\u529f\u8017\u4f46\u80fd\u51cf\u5c11\u6267\u884c\u65f6\u95f4\uff0c\u4ece\u800c\u964d\u4f4e\u603b\u80fd\u8017\uff1b\u5185\u5b58\u5e26\u5bbd\u6210\u4e3a\u6027\u80fd\u74f6\u9888\uff0c\u8ba1\u7b97\u9891\u7387\u63d0\u5347\u53ea\u5728\u5185\u5b58\u5e26\u5bbd\u5141\u8bb8\u8303\u56f4\u5185\u6709\u6548\u3002", "conclusion": "\u9488\u5bf9\u6a21\u62df\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u6700\u4f73\u786c\u4ef6\u914d\u7f6e\u4e3a\u5c0f\u672c\u5730\u7f13\u51b2\u533a\uff0832-64KB\uff09\u548c\u9ad8\u8fd0\u884c\u9891\u7387\uff081200-1400MHz\uff09\uff0c\u8be5\u7ec4\u5408\u80fd\u5b9e\u73b0\u6700\u4f73\u80fd\u8017\u5ef6\u8fdf\u79ef\uff0c\u4e3a\u8bbe\u8ba1\u80fd\u6548\u578bLLM\u52a0\u901f\u5668\u63d0\u4f9b\u5177\u4f53\u67b6\u6784\u6307\u5bfc\u3002"}}
{"id": "2512.21596", "categories": ["cs.PL", "cs.FL", "cs.LG", "cs.LO", "cs.SC"], "pdf": "https://arxiv.org/pdf/2512.21596", "abs": "https://arxiv.org/abs/2512.21596", "authors": ["Peixin Wang", "Jianhao Bai", "Min Zhang", "C. -H. Luke Ong"], "title": "Quantitative Verification of Omega-regular Properties in Probabilistic Programming", "comment": null, "summary": "Probabilistic programming provides a high-level framework for specifying statistical models as executable programs with built-in randomness and conditioning. Existing inference techniques, however, typically compute posterior distributions over program states at fixed time points, most often at termination, thereby failing to capture the temporal evolution of probabilistic behaviors. We introduce temporal posterior inference (TPI), a new framework that unifies probabilistic programming with temporal logic by computing posterior distributions over execution traces that satisfy omega-regular specifications, conditioned on possibly temporal observations. To obtain rigorous quantitative guarantees, we develop a new method for computing upper and lower bounds on the satisfaction probabilities of omega-regular properties. Our approach decomposes Rabin acceptance conditions into persistence and recurrence components and constructs stochastic barrier certificates that soundly bound each component. We implement our approach in a prototype tool, TPInfer, and evaluate it on a suite of benchmarks, demonstrating effective and efficient inference over rich temporal properties in probabilistic models.", "AI": {"tldr": "\u63d0\u51faTPI\u6846\u67b6\uff0c\u5c06\u6982\u7387\u7f16\u7a0b\u4e0e\u65f6\u6001\u903b\u8f91\u7edf\u4e00\uff0c\u8ba1\u7b97\u6ee1\u8db3\u03c9-\u6b63\u5219\u89c4\u8303\u7684\u6267\u884c\u8f68\u8ff9\u7684\u540e\u9a8c\u5206\u5e03\uff0c\u5e76\u63d0\u4f9b\u4e25\u683c\u7684\u6982\u7387\u4e0a\u4e0b\u754c\u4fdd\u8bc1", "motivation": "\u73b0\u6709\u6982\u7387\u7f16\u7a0b\u63a8\u7406\u6280\u672f\u901a\u5e38\u53ea\u8ba1\u7b97\u7a0b\u5e8f\u7ec8\u6b62\u65f6\u7684\u540e\u9a8c\u5206\u5e03\uff0c\u65e0\u6cd5\u6355\u6349\u6982\u7387\u884c\u4e3a\u7684\u65f6\u5e8f\u6f14\u5316\uff0c\u9700\u8981\u652f\u6301\u65f6\u6001\u89c4\u8303\u548c\u89c2\u6d4b\u7684\u63a8\u7406\u6846\u67b6", "method": "\u5f00\u53d1TPI\u6846\u67b6\uff0c\u5c06Rabin\u63a5\u53d7\u6761\u4ef6\u5206\u89e3\u4e3a\u6301\u4e45\u6027\u548c\u5faa\u73af\u6027\u7ec4\u4ef6\uff0c\u6784\u5efa\u968f\u673a\u5c4f\u969c\u8bc1\u4e66\u6765\u4e25\u683c\u754c\u5b9a\u6bcf\u4e2a\u7ec4\u4ef6\u7684\u6982\u7387\u4e0a\u4e0b\u754c", "result": "\u5b9e\u73b0\u539f\u578b\u5de5\u5177TPInfer\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5c55\u793a\u4e86\u5bf9\u6982\u7387\u6a21\u578b\u4e2d\u4e30\u5bcc\u65f6\u6001\u5c5e\u6027\u8fdb\u884c\u6709\u6548\u9ad8\u6548\u63a8\u7406\u7684\u80fd\u529b", "conclusion": "TPI\u6846\u67b6\u6210\u529f\u7edf\u4e00\u4e86\u6982\u7387\u7f16\u7a0b\u4e0e\u65f6\u6001\u903b\u8f91\uff0c\u4e3a\u5177\u6709\u65f6\u6001\u89c4\u8303\u548c\u89c2\u6d4b\u7684\u6982\u7387\u6a21\u578b\u63d0\u4f9b\u4e86\u4e25\u683c\u7684\u5b9a\u91cf\u63a8\u7406\u65b9\u6cd5"}}
{"id": "2512.21340", "categories": ["cs.DC", "cs.DB", "cs.ET", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.21340", "abs": "https://arxiv.org/abs/2512.21340", "authors": ["Dimitrios Amaxilatis", "Themistoklis Sarantakos", "Nikolaos Tsironis", "Souvik Sengupta", "Kostas Ramantas", "Jhofre Ojeda"], "title": "Harnessing Data Spaces to Build Intelligent Smart City Infrastructures Across the Cloud-Edge Continuum", "comment": null, "summary": "Smart cities are increasingly adopting data-centric architectures to enhance the efficiency, sustainability, and resilience of urban services.", "AI": {"tldr": "\u667a\u6167\u57ce\u5e02\u91c7\u7528\u6570\u636e\u9a71\u52a8\u67b6\u6784\u63d0\u5347\u57ce\u5e02\u670d\u52a1\u6548\u7387\u3001\u53ef\u6301\u7eed\u6027\u548c\u97e7\u6027", "motivation": "\u667a\u6167\u57ce\u5e02\u9700\u8981\u66f4\u9ad8\u6548\u3001\u53ef\u6301\u7eed\u548c\u97e7\u6027\u7684\u57ce\u5e02\u670d\u52a1\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u6ee1\u8db3\u73b0\u4ee3\u57ce\u5e02\u590d\u6742\u9700\u6c42", "method": "\u91c7\u7528\u6570\u636e\u4e2d\u5fc3\u7684\u67b6\u6784\u8bbe\u8ba1\uff0c\u6574\u5408\u57ce\u5e02\u5404\u7c7b\u6570\u636e\u8d44\u6e90", "result": "\u6570\u636e\u9a71\u52a8\u67b6\u6784\u80fd\u591f\u663e\u8457\u63d0\u5347\u57ce\u5e02\u670d\u52a1\u7684\u8fd0\u884c\u6548\u7387\u3001\u73af\u5883\u53ef\u6301\u7eed\u6027\u548c\u7cfb\u7edf\u97e7\u6027", "conclusion": "\u6570\u636e\u4e2d\u5fc3\u7684\u67b6\u6784\u662f\u667a\u6167\u57ce\u5e02\u53d1\u5c55\u7684\u5173\u952e\u65b9\u5411\uff0c\u80fd\u591f\u6709\u6548\u5e94\u5bf9\u57ce\u5e02\u590d\u6742\u6311\u6218"}}
{"id": "2512.21473", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.21473", "abs": "https://arxiv.org/abs/2512.21473", "authors": ["Chencheng Deng", "Weiling Yang", "Jianbin Fang", "Dezun Dong"], "title": "Demystifying ARM SME to Optimize General Matrix Multiplications", "comment": null, "summary": "General Matrix Multiplication (GEMM) is a critical kernel in high-performance computing and deep learning. While modern architectures like ARM's Scalable Matrix Extension (SME) introduce dedicated hardware for matrix operations, existing linear algebra libraries fail to fully exploit its potential, particularly for large matrices. This paper presents MpGEMM, an open-source library that leverages key architectural features of SME to optimize GEMM across multiple precisions. Through a systematic characterization of SME, we derive optimization guidelines that inform our design. MpGEMM employs cache-aware partitioning, efficient data packing with on-the-fly transposition, and specialized micro-kernels that utilize multi-vector loads and all available tile registers. Evaluated on an Apple M4 Pro with real-world workloads from DeepSeek and LLaMA, MpGEMM achieves an average speedup of 1.23x over the vendor-optimized Apple Accelerate library and significantly outperforms other open-source alternatives.", "AI": {"tldr": "MpGEMM\u662f\u4e00\u4e2a\u9488\u5bf9ARM SME\u67b6\u6784\u4f18\u5316\u7684\u5f00\u6e90GEMM\u5e93\uff0c\u5728Apple M4 Pro\u4e0a\u6bd4Apple Accelerate\u5e93\u5e73\u5747\u52a0\u901f1.23\u500d", "motivation": "\u73b0\u4ee3\u67b6\u6784\u5982ARM SME\u5f15\u5165\u4e86\u4e13\u95e8\u7684\u77e9\u9635\u8fd0\u7b97\u786c\u4ef6\uff0c\u4f46\u73b0\u6709\u7ebf\u6027\u4ee3\u6570\u5e93\u672a\u80fd\u5145\u5206\u5229\u7528\u5176\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u5927\u578b\u77e9\u9635\u3002\u9700\u8981\u5f00\u53d1\u80fd\u591f\u5145\u5206\u5229\u7528SME\u67b6\u6784\u7279\u6027\u7684GEMM\u5b9e\u73b0\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u6027\u7684SME\u67b6\u6784\u7279\u6027\u5206\u6790\uff0c\u5236\u5b9a\u4f18\u5316\u6307\u5bfc\u539f\u5219\u3002\u91c7\u7528\u7f13\u5b58\u611f\u77e5\u7684\u5206\u533a\u7b56\u7565\u3001\u9ad8\u6548\u7684\u6570\u636e\u6253\u5305\u4e0e\u5373\u65f6\u8f6c\u7f6e\u3001\u4ee5\u53ca\u5229\u7528\u591a\u5411\u91cf\u52a0\u8f7d\u548c\u6240\u6709\u53ef\u7528tile\u5bc4\u5b58\u5668\u7684\u4e13\u7528\u5fae\u5185\u6838\u3002", "result": "\u5728Apple M4 Pro\u4e0a\u4f7f\u7528DeepSeek\u548cLLaMA\u7684\u771f\u5b9e\u5de5\u4f5c\u8d1f\u8f7d\u8fdb\u884c\u8bc4\u4f30\uff0cMpGEMM\u76f8\u6bd4\u4f9b\u5e94\u5546\u4f18\u5316\u7684Apple Accelerate\u5e93\u5e73\u5747\u83b7\u5f971.23\u500d\u52a0\u901f\uff0c\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u5f00\u6e90\u66ff\u4ee3\u65b9\u6848\u3002", "conclusion": "MpGEMM\u6210\u529f\u5c55\u793a\u4e86\u5982\u4f55\u901a\u8fc7\u5145\u5206\u5229\u7528ARM SME\u67b6\u6784\u7279\u6027\u6765\u4f18\u5316GEMM\u6027\u80fd\uff0c\u4e3a\u9ad8\u6027\u80fd\u8ba1\u7b97\u548c\u6df1\u5ea6\u5b66\u4e60\u4e2d\u7684\u77e9\u9635\u4e58\u6cd5\u63d0\u4f9b\u4e86\u9ad8\u6548\u7684\u5b9e\u73b0\u65b9\u6848\u3002"}}
{"id": "2512.21487", "categories": ["cs.DC", "cs.AI"], "pdf": "https://arxiv.org/pdf/2512.21487", "abs": "https://arxiv.org/abs/2512.21487", "authors": ["Xinglin Pan", "Shaohuai Shi", "Wenxiang Lin", "Yuxin Wang", "Zhenheng Tang", "Wei Wang", "Xiaowen Chu"], "title": "Efficient MoE Inference with Fine-Grained Scheduling of Disaggregated Expert Parallelism", "comment": null, "summary": "The mixture-of-experts (MoE) architecture scales model size with sublinear computational increase but suffers from memory-intensive inference due to KV caches and sparse expert activation. Recent disaggregated expert parallelism (DEP) distributes attention and experts to dedicated GPU groups but lacks support for shared experts and efficient task scheduling, limiting performance.\n  We propose FinDEP, a fine-grained task scheduling algorithm for DEP that maximizes task overlap to improve MoE inference throughput. FinDEP introduces three innovations: 1) partitioning computation/communication into smaller tasks for fine-grained pipelining, 2) formulating a scheduling optimization supporting variable granularity and ordering, and 3) developing an efficient solver for this large search space.\n  Experiments on four GPU systems with DeepSeek-V2 and Qwen3-MoE show FinDEP improves throughput by up to 1.61x over prior methods, achieving up to 1.24x speedup on a 32-GPU system.", "AI": {"tldr": "FinDEP\uff1a\u9488\u5bf9\u6df7\u5408\u4e13\u5bb6\u6a21\u578b\u63a8\u7406\u7684\u7ec6\u7c92\u5ea6\u4efb\u52a1\u8c03\u5ea6\u7b97\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u8ba1\u7b97/\u901a\u4fe1\u4efb\u52a1\u5212\u5206\u548c\u8c03\u5ea6\uff0c\u63d0\u5347\u63a8\u7406\u541e\u5410\u91cf", "motivation": "\u6df7\u5408\u4e13\u5bb6\u67b6\u6784\u867d\u7136\u80fd\u4ee5\u4e9a\u7ebf\u6027\u8ba1\u7b97\u589e\u957f\u6269\u5c55\u6a21\u578b\u89c4\u6a21\uff0c\u4f46\u63a8\u7406\u65f6\u56e0KV\u7f13\u5b58\u548c\u7a00\u758f\u4e13\u5bb6\u6fc0\u6d3b\u5bfc\u81f4\u5185\u5b58\u5bc6\u96c6\u3002\u73b0\u6709\u7684\u89e3\u8026\u4e13\u5bb6\u5e76\u884c\u65b9\u6cd5\u7f3a\u4e4f\u5bf9\u5171\u4eab\u4e13\u5bb6\u7684\u652f\u6301\u548c\u9ad8\u6548\u7684\u4efb\u52a1\u8c03\u5ea6\uff0c\u9650\u5236\u4e86\u6027\u80fd\u3002", "method": "\u63d0\u51faFinDEP\u7b97\u6cd5\uff1a1\uff09\u5c06\u8ba1\u7b97/\u901a\u4fe1\u5212\u5206\u4e3a\u66f4\u5c0f\u4efb\u52a1\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u6d41\u6c34\u7ebf\uff1b2\uff09\u5236\u5b9a\u652f\u6301\u53ef\u53d8\u7c92\u5ea6\u548c\u987a\u5e8f\u7684\u8c03\u5ea6\u4f18\u5316\u95ee\u9898\uff1b3\uff09\u5f00\u53d1\u9488\u5bf9\u5927\u641c\u7d22\u7a7a\u95f4\u7684\u9ad8\u6548\u6c42\u89e3\u5668\u3002", "result": "\u5728\u56db\u4e2aGPU\u7cfb\u7edf\u4e0a\u4f7f\u7528DeepSeek-V2\u548cQwen3-MoE\u6d4b\u8bd5\uff0cFinDEP\u76f8\u6bd4\u5148\u524d\u65b9\u6cd5\u63d0\u5347\u541e\u5410\u91cf\u6700\u9ad8\u8fbe1.61\u500d\uff0c\u572832-GPU\u7cfb\u7edf\u4e0a\u5b9e\u73b0\u6700\u9ad81.24\u500d\u52a0\u901f\u3002", "conclusion": "FinDEP\u901a\u8fc7\u7ec6\u7c92\u5ea6\u4efb\u52a1\u8c03\u5ea6\u6709\u6548\u89e3\u51b3\u4e86\u6df7\u5408\u4e13\u5bb6\u6a21\u578b\u63a8\u7406\u4e2d\u7684\u5185\u5b58\u74f6\u9888\u548c\u8c03\u5ea6\u6548\u7387\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u6027\u80fd\u3002"}}
{"id": "2512.21571", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.21571", "abs": "https://arxiv.org/abs/2512.21571", "authors": ["Hui Guo", "Qihang Zheng", "Chenghai Huo", "Dongliang Guo", "Haoqi Yang", "Yang Zhang"], "title": "nncase: An End-to-End Compiler for Efficient LLM Deployment on Heterogeneous Storage Architectures", "comment": null, "summary": "The efficient deployment of large language models (LLMs) is hindered by memory architecture heterogeneity, where traditional compilers suffer from fragmented workflows and high adaptation costs. We present nncase, an open-source, end-to-end compilation framework designed to unify optimization across diverse targets. Central to nncase is an e-graph-based term rewriting engine that mitigates the phase ordering problem, enabling global exploration of computation and data movement strategies. The framework integrates three key modules: Auto Vectorize for adapting to heterogeneous computing units, Auto Distribution for searching parallel strategies with cost-aware communication optimization, and Auto Schedule for maximizing on-chip cache locality. Furthermore, a buffer-aware Codegen phase ensures efficient kernel instantiation. Evaluations show that nncase outperforms mainstream frameworks like MLC LLM and Intel IPEX on Qwen3 series models and achieves performance comparable to the hand-optimized llama.cpp on CPUs, demonstrating the viability of automated compilation for high-performance LLM deployment. The source code is available at https://github.com/kendryte/nncase.", "AI": {"tldr": "nncase\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u7aef\u5230\u7aef\u7f16\u8bd1\u6846\u67b6\uff0c\u901a\u8fc7\u57fa\u4e8ee-graph\u7684\u9879\u91cd\u5199\u5f15\u64ce\u89e3\u51b3\u5185\u5b58\u67b6\u6784\u5f02\u6784\u6027\u95ee\u9898\uff0c\u7edf\u4e00\u4f18\u5316\u8de8\u4e0d\u540c\u76ee\u6807\u5e73\u53f0\uff0c\u5728Qwen3\u7cfb\u5217\u6a21\u578b\u4e0a\u8d85\u8d8a\u4e3b\u6d41\u6846\u67b6\uff0cCPU\u6027\u80fd\u63a5\u8fd1\u624b\u5de5\u4f18\u5316\u7684llama.cpp\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u9ad8\u6548\u90e8\u7f72\u53d7\u5230\u5185\u5b58\u67b6\u6784\u5f02\u6784\u6027\u7684\u963b\u788d\uff0c\u4f20\u7edf\u7f16\u8bd1\u5668\u5b58\u5728\u5de5\u4f5c\u6d41\u7a0b\u788e\u7247\u5316\u548c\u9ad8\u9002\u914d\u6210\u672c\u7684\u95ee\u9898\uff0c\u9700\u8981\u7edf\u4e00\u7684\u7f16\u8bd1\u6846\u67b6\u6765\u4f18\u5316\u8de8\u5e73\u53f0\u90e8\u7f72\u3002", "method": "\u57fa\u4e8ee-graph\u7684\u9879\u91cd\u5199\u5f15\u64ce\u89e3\u51b3\u9636\u6bb5\u6392\u5e8f\u95ee\u9898\uff0c\u5b9e\u73b0\u8ba1\u7b97\u548c\u6570\u636e\u79fb\u52a8\u7b56\u7565\u7684\u5168\u5c40\u63a2\u7d22\uff1b\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u6a21\u5757\uff1aAuto Vectorize\u9002\u914d\u5f02\u6784\u8ba1\u7b97\u5355\u5143\uff0cAuto Distribution\u641c\u7d22\u5e76\u884c\u7b56\u7565\u5e76\u8fdb\u884c\u6210\u672c\u611f\u77e5\u901a\u4fe1\u4f18\u5316\uff0cAuto Schedule\u6700\u5927\u5316\u7247\u4e0a\u7f13\u5b58\u5c40\u90e8\u6027\uff1b\u6700\u540e\u901a\u8fc7\u7f13\u51b2\u533a\u611f\u77e5\u7684Codegen\u9636\u6bb5\u5b9e\u73b0\u9ad8\u6548\u5185\u6838\u5b9e\u4f8b\u5316\u3002", "result": "nncase\u5728Qwen3\u7cfb\u5217\u6a21\u578b\u4e0a\u8d85\u8d8a\u4e86MLC LLM\u548cIntel IPEX\u7b49\u4e3b\u6d41\u6846\u67b6\uff0c\u5728CPU\u4e0a\u8fbe\u5230\u4e86\u4e0e\u624b\u5de5\u4f18\u5316\u7684llama.cpp\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u81ea\u52a8\u5316\u7f16\u8bd1\u5728\u9ad8\u6027\u80fdLLM\u90e8\u7f72\u4e2d\u7684\u53ef\u884c\u6027\u3002", "conclusion": "nncase\u901a\u8fc7\u7edf\u4e00\u7684\u7aef\u5230\u7aef\u7f16\u8bd1\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86LLM\u90e8\u7f72\u4e2d\u7684\u5185\u5b58\u67b6\u6784\u5f02\u6784\u6027\u95ee\u9898\uff0c\u5c55\u793a\u4e86\u81ea\u52a8\u5316\u7f16\u8bd1\u5728\u5b9e\u73b0\u9ad8\u6027\u80fdLLM\u90e8\u7f72\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2512.21615", "categories": ["cs.DC", "cs.DB"], "pdf": "https://arxiv.org/pdf/2512.21615", "abs": "https://arxiv.org/abs/2512.21615", "authors": ["Guopeng Li", "Haisheng Tan", "Chi Zhang", "Hongqiu Ni", "Zilong Wang", "Xinyue Zhang", "Yang Xu", "Han Tian"], "title": "Embedding Samples Dispatching for Recommendation Model Training in Edge Environments", "comment": "This paper is an English version of Samples Dispatching Mechanism for Accelerating Recommendation Model Training in Edge Intelligent Computing System published in 2025 in the Journal of Computer Research and Development", "summary": "Training deep learning recommendation models (DLRMs) on edge workers brings several benefits, particularly in terms of data privacy protection, low latency and personalization. However, due to the huge size of embedding tables, typical DLRM training frameworks adopt one or more parameter servers to maintain global embedding tables, while leveraging the edge workers cache part of them. This incurs significant transmission cost for embedding transmissions between workers and parameter servers, which can dominate the training cycle. In this paper, we investigate how to dispatch input embedding samples to appropriate edge workers to minimize the total embedding transmission cost when facing edge-specific challenges such as heterogeneous networks and limited resources. We develop ESD, a novel mechanism that optimizes the dispatch of input embedding samples to edge workers based on expected embedding transmission cost. We propose HybridDis as the dispatch decision method within ESD, which combines a resource-intensive optimal algorithm and a heuristic algorithm to balance decision quality and resource consumption. We implement a prototype of ESD and compare it with state-of-the-art mechanisms on real-world workloads. Extensive experimental results show that ESD reduces the embedding transmission cost by up to 36.76% and achieves up to 1.74 times speedup in end-to-end DLRM training.", "AI": {"tldr": "ESD\u673a\u5236\u901a\u8fc7\u4f18\u5316\u5d4c\u5165\u6837\u672c\u5206\u914d\u5230\u8fb9\u7f18\u5de5\u4f5c\u8282\u70b9\uff0c\u663e\u8457\u51cf\u5c11\u5d4c\u5165\u4f20\u8f93\u6210\u672c\uff0c\u52a0\u901fDLRM\u8bad\u7ec3", "motivation": "\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u8bad\u7ec3DLRM\u6709\u9690\u79c1\u4fdd\u62a4\u3001\u4f4e\u5ef6\u8fdf\u548c\u4e2a\u6027\u5316\u7b49\u4f18\u52bf\uff0c\u4f46\u5d4c\u5165\u8868\u5de8\u5927\uff0c\u4f20\u7edf\u6846\u67b6\u4f7f\u7528\u53c2\u6570\u670d\u52a1\u5668\u5bfc\u81f4\u5d4c\u5165\u4f20\u8f93\u6210\u672c\u8fc7\u9ad8\uff0c\u6210\u4e3a\u8bad\u7ec3\u74f6\u9888", "method": "\u63d0\u51faESD\u673a\u5236\uff0c\u57fa\u4e8e\u9884\u671f\u5d4c\u5165\u4f20\u8f93\u6210\u672c\u4f18\u5316\u8f93\u5165\u5d4c\u5165\u6837\u672c\u5206\u914d\u5230\u8fb9\u7f18\u5de5\u4f5c\u8282\u70b9\uff1b\u5f00\u53d1HybridDis\u5206\u914d\u51b3\u7b56\u65b9\u6cd5\uff0c\u7ed3\u5408\u8d44\u6e90\u5bc6\u96c6\u578b\u6700\u4f18\u7b97\u6cd5\u548c\u542f\u53d1\u5f0f\u7b97\u6cd5\u5e73\u8861\u51b3\u7b56\u8d28\u91cf\u548c\u8d44\u6e90\u6d88\u8017", "result": "\u5b9e\u9a8c\u663e\u793aESD\u51cf\u5c11\u5d4c\u5165\u4f20\u8f93\u6210\u672c\u6700\u9ad8\u8fbe36.76%\uff0c\u7aef\u5230\u7aefDLRM\u8bad\u7ec3\u52a0\u901f\u6700\u9ad8\u8fbe1.74\u500d", "conclusion": "ESD\u80fd\u6709\u6548\u89e3\u51b3\u8fb9\u7f18DLRM\u8bad\u7ec3\u4e2d\u7684\u5d4c\u5165\u4f20\u8f93\u74f6\u9888\uff0c\u663e\u8457\u63d0\u5347\u8bad\u7ec3\u6548\u7387"}}
{"id": "2512.21730", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.21730", "abs": "https://arxiv.org/abs/2512.21730", "authors": ["Linyi Jiang", "Yifei Zhu", "Hao Yin", "Bo Li"], "title": "Hyperion: Low-Latency Ultra-HD Video Analytics via Collaborative Vision Transformer Inference", "comment": "Accepted for publication in IEEE INFOCOM 2026", "summary": "Recent advancements in array-camera videography enable real-time capturing of ultra-high-definition (Ultra-HD) videos, providing rich visual information in a large field of view. However, promptly processing such data using state-of-the-art transformer-based vision foundation models faces significant computational overhead in on-device computing or transmission overhead in cloud computing. In this paper, we present Hyperion, the first cloud-device collaborative framework that enables low-latency inference on Ultra-HD vision data using off-the-shelf vision transformers over dynamic networks. Hyperion addresses the computational and transmission bottleneck of Ultra-HD vision transformers by exploiting the intrinsic property in vision Transformer models. Specifically, Hyperion integrates a collaboration-aware importance scorer that identifies critical regions at the patch level, a dynamic scheduler that adaptively adjusts patch transmission quality to balance latency and accuracy under dynamic network conditions, and a weighted ensembler that fuses edge and cloud results to improve accuracy. Experimental results demonstrate that Hyperion enhances frame processing rate by up to 1.61 times and improves the accuracy by up to 20.2% when compared with state-of-the-art baselines under various network environments.", "AI": {"tldr": "Hyperion\u662f\u4e00\u4e2a\u4e91-\u7aef\u534f\u540c\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u52a8\u6001\u7f51\u7edc\u4e0a\u5bf9\u8d85\u9ad8\u6e05\u89c6\u9891\u8fdb\u884c\u4f4e\u5ef6\u8fdf\u7684\u89c6\u89c9Transformer\u63a8\u7406\uff0c\u901a\u8fc7\u8bc6\u522b\u5173\u952e\u533a\u57df\u3001\u52a8\u6001\u8c03\u5ea6\u548c\u52a0\u6743\u878d\u5408\u6765\u63d0\u5347\u5904\u7406\u901f\u5ea6\u548c\u51c6\u786e\u7387\u3002", "motivation": "\u9635\u5217\u76f8\u673a\u6280\u672f\u80fd\u591f\u5b9e\u65f6\u6355\u6349\u8d85\u9ad8\u6e05\u89c6\u9891\uff0c\u4f46\u4f7f\u7528\u73b0\u6709\u7684Transformer\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u5904\u7406\u8fd9\u4e9b\u6570\u636e\u9762\u4e34\u8ba1\u7b97\u5f00\u9500\u5927\uff08\u8bbe\u5907\u7aef\uff09\u6216\u4f20\u8f93\u5f00\u9500\u5927\uff08\u4e91\u7aef\uff09\u7684\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u4f4e\u5ef6\u8fdf\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "Hyperion\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1) \u534f\u4f5c\u611f\u77e5\u7684\u91cd\u8981\u6027\u8bc4\u5206\u5668\uff0c\u5728patch\u7ea7\u522b\u8bc6\u522b\u5173\u952e\u533a\u57df\uff1b2) \u52a8\u6001\u8c03\u5ea6\u5668\uff0c\u6839\u636e\u7f51\u7edc\u6761\u4ef6\u81ea\u9002\u5e94\u8c03\u6574patch\u4f20\u8f93\u8d28\u91cf\u4ee5\u5e73\u8861\u5ef6\u8fdf\u548c\u51c6\u786e\u7387\uff1b3) \u52a0\u6743\u96c6\u6210\u5668\uff0c\u878d\u5408\u8fb9\u7f18\u548c\u4e91\u7aef\u7ed3\u679c\u4ee5\u63d0\u9ad8\u51c6\u786e\u7387\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5728\u5404\u79cd\u7f51\u7edc\u73af\u5883\u4e0b\uff0cHyperion\u76f8\u6bd4\u73b0\u6709\u6700\u4f18\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e27\u5904\u7406\u7387\u6700\u9ad8\u63d0\u53471.61\u500d\uff0c\u51c6\u786e\u7387\u6700\u9ad8\u63d0\u534720.2%\u3002", "conclusion": "Hyperion\u662f\u9996\u4e2a\u652f\u6301\u5728\u52a8\u6001\u7f51\u7edc\u4e0a\u4f7f\u7528\u73b0\u6210\u89c6\u89c9Transformer\u5bf9\u8d85\u9ad8\u6e05\u89c6\u89c9\u6570\u636e\u8fdb\u884c\u4f4e\u5ef6\u8fdf\u63a8\u7406\u7684\u4e91-\u7aef\u534f\u540c\u6846\u67b6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u8ba1\u7b97\u548c\u4f20\u8f93\u74f6\u9888\u95ee\u9898\u3002"}}
{"id": "2512.21835", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.21835", "abs": "https://arxiv.org/abs/2512.21835", "authors": ["Mingyu Sun", "Xiao Zhang", "Shen Qu", "Yan Li", "Mengbai Xiao", "Yuan Yuan", "Dongxiao Yu"], "title": "LIME:Accelerating Collaborative Lossless LLM Inference on Memory-Constrained Edge Devices", "comment": "15 pages, 18 figures", "summary": "Large language models (LLMs) have emerged as a powerful foundation for intelligent reasoning and decision-making, demonstrating substantial impact across a wide range of domains and applications. However, their massive parameter scales and substantial resource demands pose critical challenges for efficient inference on edge devices. These devices are inherently constrained by limited computational power and memory capacity, while bandwidth bottlenecks at the network edge further restrict distributed deployment and real-time responsiveness. Although existing research has explored lightweight optimization techniques to mitigate memory limitations, such approaches often incur significant degradation in model accuracy and performance. To address these challenges, we propose LIME, a collaborative system that enables lossless inference for large models across multiple memory-constrained edge devices under limited network bandwidth. LIME employs an interleaved pipeline parallelism in conjunction with model offloading to dynamically balance computation and communication. Furthermore, a fine-grained offline allocation scheduler and online memory adaptation strategy are introduced to enhance the device's computing and storage resources while minimizing inference latency. Extensive experiments demonstrate that LIME, deployed on four heterogeneous Nvidia Jetson edge devices for LLaMA3.3-70B-Instruct model inference, achieves 1.7$\\times$ and 3.7$\\times$ speedups over state-of-the-art baselines under sporadic and bursty request patterns respectively, without compromising model accuracy.", "AI": {"tldr": "LIME\u662f\u4e00\u4e2a\u534f\u4f5c\u7cfb\u7edf\uff0c\u53ef\u5728\u6709\u9650\u7f51\u7edc\u5e26\u5bbd\u4e0b\uff0c\u5728\u591a\u4e2a\u5185\u5b58\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5b9e\u73b0\u5927\u6a21\u578b\u7684\u65e0\u635f\u63a8\u7406\uff0c\u901a\u8fc7\u4ea4\u9519\u6d41\u6c34\u7ebf\u5e76\u884c\u548c\u6a21\u578b\u5378\u8f7d\u6765\u5e73\u8861\u8ba1\u7b97\u4e0e\u901a\u4fe1\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u90e8\u7f72\u9762\u4e34\u5de8\u5927\u6311\u6218\uff1a\u53c2\u6570\u89c4\u6a21\u5927\u3001\u8d44\u6e90\u9700\u6c42\u9ad8\uff0c\u800c\u8fb9\u7f18\u8bbe\u5907\u8ba1\u7b97\u80fd\u529b\u548c\u5185\u5b58\u5bb9\u91cf\u6709\u9650\uff0c\u7f51\u7edc\u5e26\u5bbd\u74f6\u9888\u8fdb\u4e00\u6b65\u9650\u5236\u4e86\u5206\u5e03\u5f0f\u90e8\u7f72\u548c\u5b9e\u65f6\u54cd\u5e94\u3002\u73b0\u6709\u8f7b\u91cf\u5316\u4f18\u5316\u6280\u672f\u5f80\u5f80\u5bfc\u81f4\u6a21\u578b\u7cbe\u5ea6\u663e\u8457\u4e0b\u964d\u3002", "method": "LIME\u91c7\u7528\u4ea4\u9519\u6d41\u6c34\u7ebf\u5e76\u884c\u7ed3\u5408\u6a21\u578b\u5378\u8f7d\uff0c\u52a8\u6001\u5e73\u8861\u8ba1\u7b97\u4e0e\u901a\u4fe1\u3002\u5f15\u5165\u7ec6\u7c92\u5ea6\u79bb\u7ebf\u5206\u914d\u8c03\u5ea6\u5668\u548c\u5728\u7ebf\u5185\u5b58\u9002\u914d\u7b56\u7565\uff0c\u4f18\u5316\u8bbe\u5907\u8ba1\u7b97\u548c\u5b58\u50a8\u8d44\u6e90\uff0c\u6700\u5c0f\u5316\u63a8\u7406\u5ef6\u8fdf\u3002", "result": "\u5728\u56db\u4e2a\u5f02\u6784Nvidia Jetson\u8fb9\u7f18\u8bbe\u5907\u4e0a\u90e8\u7f72LLaMA3.3-70B-Instruct\u6a21\u578b\u63a8\u7406\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u57fa\u7ebf\uff0c\u5728\u96f6\u661f\u548c\u7a81\u53d1\u8bf7\u6c42\u6a21\u5f0f\u4e0b\u5206\u522b\u5b9e\u73b01.7\u500d\u548c3.7\u500d\u52a0\u901f\uff0c\u4e14\u4e0d\u635f\u5931\u6a21\u578b\u7cbe\u5ea6\u3002", "conclusion": "LIME\u7cfb\u7edf\u6709\u6548\u89e3\u51b3\u4e86\u5927\u6a21\u578b\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684\u9ad8\u6548\u65e0\u635f\u63a8\u7406\u95ee\u9898\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u534f\u4f5c\u67b6\u6784\u548c\u8d44\u6e90\u4f18\u5316\u7b56\u7565\uff0c\u5728\u4fdd\u6301\u6a21\u578b\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u63d0\u5347\u63a8\u7406\u6027\u80fd\u3002"}}
{"id": "2512.21884", "categories": ["cs.DC", "cs.AI", "cs.NI"], "pdf": "https://arxiv.org/pdf/2512.21884", "abs": "https://arxiv.org/abs/2512.21884", "authors": ["Tingyang Sun", "Ting He", "Bo Ji", "Parimal Parag"], "title": "Optimizing Resource Allocation for Geographically-Distributed Inference by Large Language Models", "comment": null, "summary": "Large language models have demonstrated extraordinary performance in many AI tasks but are expensive to use, even after training, due to their requirement of high-end GPUs. Recently, a distributed system called PETALS was developed to lower the barrier for deploying LLMs by splitting the model blocks across multiple servers with low-end GPUs distributed over the Internet, which was much faster than swapping the model parameters between the GPU memory and other cheaper but slower local storage media. However, the performance of such a distributed system critically depends on the resource allocation, and how to do so optimally remains unknown. In this work, we present the first systematic study of the resource allocation problem in distributed LLM inference, with focus on two important decisions: block placement and request routing. Our main results include: experimentally validated performance models that can predict the inference performance under given block placement and request routing decisions, a formulation of the offline optimization of block placement and request routing as a mixed integer linear programming problem together with the NP-hardness proof and a polynomial-complexity algorithm with guaranteed performance, and an adaptation of the offline algorithm for the online setting with the same performance guarantee under bounded load. Through both experiments and experimentally-validated simulations, we have verified that the proposed solution can substantially reduce the inference time compared to the state-of-the-art solution in diverse settings with geographically-distributed servers. As a byproduct, we have also developed a light-weighted CPU-only simulator capable of predicting the performance of distributed LLM inference on GPU servers, which can evaluate large deployments and facilitate future research for researchers with limited GPU access.", "AI": {"tldr": "\u672c\u6587\u7cfb\u7edf\u7814\u7a76\u4e86\u5206\u5e03\u5f0fLLM\u63a8\u7406\u4e2d\u7684\u8d44\u6e90\u5206\u914d\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u6027\u80fd\u9884\u6d4b\u6a21\u578b\u3001\u4f18\u5316\u7b97\u6cd5\u548c\u5728\u7ebf\u9002\u5e94\u65b9\u6848\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u6548\u7387\u3002", "motivation": "\u867d\u7136PETALS\u7b49\u5206\u5e03\u5f0f\u7cfb\u7edf\u964d\u4f4e\u4e86LLM\u90e8\u7f72\u95e8\u69db\uff0c\u4f46\u5176\u6027\u80fd\u4e25\u91cd\u4f9d\u8d56\u8d44\u6e90\u5206\u914d\u7b56\u7565\uff0c\u800c\u5982\u4f55\u6700\u4f18\u5206\u914d\u4ecd\u672a\u77e5\u3002\u73b0\u6709\u65b9\u6848\u65e0\u6cd5\u5145\u5206\u5229\u7528\u5206\u5e03\u5f0f\u8d44\u6e90\uff0c\u5bfc\u81f4\u63a8\u7406\u6548\u7387\u4f4e\u4e0b\u3002", "method": "1) \u5efa\u7acb\u5b9e\u9a8c\u9a8c\u8bc1\u7684\u6027\u80fd\u9884\u6d4b\u6a21\u578b\uff1b2) \u5c06\u5757\u653e\u7f6e\u548c\u8bf7\u6c42\u8def\u7531\u79bb\u7ebf\u4f18\u5316\u5efa\u6a21\u4e3a\u6df7\u5408\u6574\u6570\u7ebf\u6027\u89c4\u5212\u95ee\u9898\uff0c\u8bc1\u660e\u5176NP\u96be\u5e76\u8bbe\u8ba1\u591a\u9879\u5f0f\u590d\u6742\u5ea6\u7b97\u6cd5\uff1b3) \u4e3a\u5728\u7ebf\u573a\u666f\u8bbe\u8ba1\u5177\u6709\u76f8\u540c\u6027\u80fd\u4fdd\u8bc1\u7684\u9002\u5e94\u7b97\u6cd5\uff1b4) \u5f00\u53d1\u8f7b\u91cf\u7ea7CPU\u6a21\u62df\u5668\u3002", "result": "\u901a\u8fc7\u5b9e\u9a8c\u548c\u6a21\u62df\u9a8c\u8bc1\uff0c\u76f8\u6bd4\u73b0\u6709\u6700\u4f18\u65b9\u6848\uff0c\u6240\u63d0\u65b9\u6cd5\u5728\u4e0d\u540c\u5730\u7406\u5206\u5e03\u670d\u52a1\u5668\u8bbe\u7f6e\u4e0b\u80fd\u663e\u8457\u51cf\u5c11\u63a8\u7406\u65f6\u95f4\u3002CPU\u6a21\u62df\u5668\u80fd\u51c6\u786e\u9884\u6d4bGPU\u670d\u52a1\u5668\u6027\u80fd\uff0c\u4e3a\u8d44\u6e90\u6709\u9650\u7684\u7814\u7a76\u8005\u63d0\u4f9b\u7814\u7a76\u5de5\u5177\u3002", "conclusion": "\u672c\u6587\u9996\u6b21\u7cfb\u7edf\u7814\u7a76\u4e86\u5206\u5e03\u5f0fLLM\u63a8\u7406\u7684\u8d44\u6e90\u5206\u914d\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u7406\u8bba\u4fdd\u8bc1\u7684\u4f18\u5316\u65b9\u6848\u548c\u5b9e\u7528\u5de5\u5177\uff0c\u4e3a\u9ad8\u6548\u5206\u5e03\u5f0fLLM\u90e8\u7f72\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u7840\u3002"}}
{"id": "2512.21967", "categories": ["cs.DC", "cs.DS"], "pdf": "https://arxiv.org/pdf/2512.21967", "abs": "https://arxiv.org/abs/2512.21967", "authors": ["Deniz Elbek", "Kamer Kaya"], "title": "BLEST: Blazingly Efficient BFS using Tensor Cores", "comment": "13 pages, 3 figures, 4 tables, 3 algorithms, 46 references", "summary": "Breadth-First Search (BFS) is a fundamental graph kernel that underpins a wide range of applications. While modern GPUs provide specialised Matrix-Multiply-Accumulate (MMA) units, e.g., Tensor Cores (TC), with extremely high throughput, they target dense operations, making it non-trivial to exploit them for irregular, unstructured graph computations. In particular, fully utilising them for a BFS requires an efficient mapping of the edge operations onto TCs while avoiding redundancy, load imbalance, and synchronisation. We present BLEST, a TC-accelerated framework that reformulates the pull-based BFS pipeline around a bitmap-oriented structure and a carefully engineered execution layout. BLEST introduces Binarised Virtual Slice Sets (BVSS) to enforce warp-level load balancing and to eliminate frontier-oblivious work assignment. To improve both memory efficiency and update locality across diverse graphs, we apply two complementary graph reordering strategies: a compression-oriented ordering for social-like graphs and a bandwidth-reducing ordering for non-social graphs. At the compute level, we develop a batched SpMSpV multiplication pattern that uses the bitwise TC tiles to handle dot products without wasting output entries, thereby reducing the number of required MMA calls. Finally, BLEST combines kernel fusion with a lazy vertex update scheme to reduce host-side synchronisation, mitigate atomic overheads, and improve cache locality. Experiments show that BLEST delivers, on average, $3.58\\times$, $4.64\\times$ and $4.9\\times$ speedup over BerryBees, Gunrock, and GSWITCH, respectively, across a broad set of real-world graphs.", "AI": {"tldr": "BLEST\u662f\u4e00\u4e2a\u5229\u7528GPU Tensor Core\u52a0\u901fBFS\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u4f4d\u56fe\u5bfc\u5411\u7ed3\u6784\u3001\u8d1f\u8f7d\u5747\u8861\u7b56\u7565\u548c\u56fe\u91cd\u6392\u5e8f\u6280\u672f\uff0c\u5728\u591a\u79cd\u771f\u5b9e\u56fe\u4e0a\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u73b0\u4ee3GPU\u7684Tensor Core\u867d\u7136\u63d0\u4f9b\u6781\u9ad8\u541e\u5410\u91cf\uff0c\u4f46\u4e3b\u8981\u9488\u5bf9\u5bc6\u96c6\u8fd0\u7b97\uff0c\u96be\u4ee5\u6709\u6548\u5e94\u7528\u4e8e\u4e0d\u89c4\u5219\u3001\u975e\u7ed3\u6784\u5316\u7684\u56fe\u8ba1\u7b97\uff08\u5982BFS\uff09\u3002\u9700\u8981\u89e3\u51b3\u8d1f\u8f7d\u4e0d\u5747\u8861\u3001\u5197\u4f59\u8ba1\u7b97\u548c\u540c\u6b65\u5f00\u9500\u7b49\u95ee\u9898\u3002", "method": "1. \u91c7\u7528\u57fa\u4e8e\u4f4d\u56fe\u7684\u7ed3\u6784\u548c\u6267\u884c\u5e03\u5c40\u91cd\u6784pull-based BFS\u6d41\u6c34\u7ebf\uff1b2. \u5f15\u5165\u4e8c\u503c\u5316\u865a\u62df\u5207\u7247\u96c6(BVSS)\u5b9e\u73b0warp\u7ea7\u8d1f\u8f7d\u5747\u8861\uff1b3. \u5e94\u7528\u4e24\u79cd\u56fe\u91cd\u6392\u5e8f\u7b56\u7565\uff1a\u793e\u4ea4\u7c7b\u56fe\u7528\u538b\u7f29\u5bfc\u5411\u6392\u5e8f\uff0c\u975e\u793e\u4ea4\u7c7b\u56fe\u7528\u5e26\u5bbd\u51cf\u5c11\u6392\u5e8f\uff1b4. \u5f00\u53d1\u6279\u91cfSpMSpV\u4e58\u6cd5\u6a21\u5f0f\uff0c\u5229\u7528\u4f4d\u8fd0\u7b97TC\u74e6\u7247\u5904\u7406\u70b9\u79ef\uff1b5. \u7ed3\u5408\u5185\u6838\u878d\u5408\u548c\u60f0\u6027\u9876\u70b9\u66f4\u65b0\u65b9\u6848\u51cf\u5c11\u540c\u6b65\u5f00\u9500\u3002", "result": "\u5728\u591a\u79cd\u771f\u5b9e\u56fe\u4e0a\uff0cBLEST\u76f8\u6bd4BerryBees\u3001Gunrock\u548cGSWITCH\u5206\u522b\u5b9e\u73b0\u4e86\u5e73\u57473.58\u500d\u30014.64\u500d\u548c4.9\u500d\u7684\u52a0\u901f\u3002", "conclusion": "BLEST\u6210\u529f\u5c06GPU Tensor Core\u6709\u6548\u5e94\u7528\u4e8eBFS\u8ba1\u7b97\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u8d1f\u8f7d\u5747\u8861\u3001\u56fe\u91cd\u6392\u5e8f\u548c\u8ba1\u7b97\u6a21\u5f0f\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u4e0d\u89c4\u5219\u56fe\u8ba1\u7b97\u7684\u6027\u80fd\u3002"}}
{"id": "2512.22035", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.22035", "abs": "https://arxiv.org/abs/2512.22035", "authors": ["Yanmeng Wang", "Zhiwen Dai", "Shuai Wang", "Jian Zhou", "Fu Xiao", "Tony Q. S. Quek", "Tsung-Hui Chang"], "title": "Robust Federated Fine-Tuning in Heterogeneous Networks with Unreliable Connections: An Aggregation View", "comment": null, "summary": "Federated Fine-Tuning (FFT) has attracted growing interest as it leverages both server- and client-side data to enhance global model generalization while preserving privacy, and significantly reduces the computational burden on edge devices by avoiding training from scratch. Despite these advantages, FFT performance is often degraded by unreliable server-client connections and heterogeneous client data distributions. Most existing methods assume homogeneous network conditions or require prior knowledge of connection failures. However, these assumptions are impractical in real-world networks characterized by diverse communication standards (e.g., wired, Wi-Fi, 4G, and 5G) and heterogeneous failure patterns. To address these limitations, we propose FedAuto, a novel FFT framework that mitigates the combined effects of connection failures and data heterogeneity via adaptive aggregation. FedAuto operates without prior knowledge of network conditions or modifications to existing infrastructure, enabling seamless plug-and-play deployment. Moreover, we establish a rigorous convergence guarantee for FedAuto. By adopting a novel per-round aggregation perspective, our analysis removes the need for assumptions on connection failures probabilities or client selection strategies commonly imposed in prior work, and guarantees convergence of FedAuto for each individual realization, providing a stronger theoretical assurance. Extensive experiments demonstrate that FedAuto consistently outperforms state-of-the-art baselines under diverse connection failure scenarios for both full-parameter and partial-parameter fine-tuning (e.g., LoRA), and even surpasses strategies that rely on complex communication resource optimization.", "AI": {"tldr": "FedAuto\uff1a\u4e00\u79cd\u65e0\u9700\u5148\u9a8c\u7f51\u7edc\u77e5\u8bc6\u7684\u81ea\u9002\u5e94\u805a\u5408\u8054\u90a6\u5fae\u8c03\u6846\u67b6\uff0c\u6709\u6548\u5e94\u5bf9\u8fde\u63a5\u5931\u8d25\u548c\u6570\u636e\u5f02\u6784\u95ee\u9898", "motivation": "\u8054\u90a6\u5fae\u8c03(FFT)\u7ed3\u5408\u670d\u52a1\u5668\u548c\u5ba2\u6237\u7aef\u6570\u636e\u63d0\u5347\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u5e76\u4fdd\u62a4\u9690\u79c1\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5047\u8bbe\u7f51\u7edc\u6761\u4ef6\u540c\u8d28\u6216\u9700\u8981\u8fde\u63a5\u5931\u8d25\u5148\u9a8c\u77e5\u8bc6\uff0c\u4e0d\u9002\u7528\u4e8e\u73b0\u5b9e\u4e16\u754c\u4e2d\u591a\u79cd\u901a\u4fe1\u6807\u51c6\u548c\u5f02\u6784\u6545\u969c\u6a21\u5f0f\u7684\u7f51\u7edc\u73af\u5883\u3002", "method": "\u63d0\u51faFedAuto\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u805a\u5408\u7f13\u89e3\u8fde\u63a5\u5931\u8d25\u548c\u6570\u636e\u5f02\u6784\u7684\u8054\u5408\u5f71\u54cd\u3002\u8be5\u6846\u67b6\u65e0\u9700\u7f51\u7edc\u6761\u4ef6\u5148\u9a8c\u77e5\u8bc6\u6216\u57fa\u7840\u8bbe\u65bd\u4fee\u6539\uff0c\u652f\u6301\u5373\u63d2\u5373\u7528\u90e8\u7f72\u3002\u91c7\u7528\u65b0\u9896\u7684\u6bcf\u8f6e\u805a\u5408\u89c6\u89d2\u8fdb\u884c\u5206\u6790\uff0c\u65e0\u9700\u8fde\u63a5\u5931\u8d25\u6982\u7387\u6216\u5ba2\u6237\u7aef\u9009\u62e9\u7b56\u7565\u5047\u8bbe\u3002", "result": "FedAuto\u5728\u5404\u79cd\u8fde\u63a5\u5931\u8d25\u573a\u666f\u4e0b\uff08\u5305\u62ec\u5168\u53c2\u6570\u548c\u90e8\u5206\u53c2\u6570\u5fae\u8c03\u5982LoRA\uff09\u59cb\u7ec8\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u751a\u81f3\u8d85\u8fc7\u4f9d\u8d56\u590d\u6742\u901a\u4fe1\u8d44\u6e90\u4f18\u5316\u7684\u7b56\u7565\u3002\u5efa\u7acb\u4e86\u4e25\u683c\u7684\u6536\u655b\u4fdd\u8bc1\uff0c\u4e3a\u6bcf\u4e2a\u5355\u72ec\u5b9e\u73b0\u63d0\u4f9b\u6536\u655b\u6027\u3002", "conclusion": "FedAuto\u662f\u9996\u4e2a\u65e0\u9700\u5148\u9a8c\u7f51\u7edc\u77e5\u8bc6\u5373\u53ef\u5e94\u5bf9\u8fde\u63a5\u5931\u8d25\u548c\u6570\u636e\u5f02\u6784\u95ee\u9898\u7684\u8054\u90a6\u5fae\u8c03\u6846\u67b6\uff0c\u5177\u6709\u5f3a\u5927\u7684\u7406\u8bba\u4fdd\u8bc1\u548c\u5b9e\u9645\u6027\u80fd\u4f18\u52bf\uff0c\u9002\u7528\u4e8e\u73b0\u5b9e\u4e16\u754c\u7684\u5f02\u6784\u7f51\u7edc\u73af\u5883\u3002"}}
{"id": "2512.22036", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.22036", "abs": "https://arxiv.org/abs/2512.22036", "authors": ["Zhuoran Zhu", "Chunyang Zhu", "Hao Lin", "Xu Fu", "Yiming Zhou", "Quanlu Zhang", "Zhenhua Li", "Feng Qian", "Chao Yu", "Boxun Li", "Guohao Dai", "Yu Wang"], "title": "FUSCO: High-Performance Distributed Data Shuffling via Transformation-Communication Fusion", "comment": null, "summary": "Large-scale Mixture-of-Experts (MoE) models rely on \\emph{expert parallelism} for efficient training and inference, which splits experts across devices and necessitates distributed data shuffling to route each token to its assigned experts. However, existing communication libraries handle this shuffling poorly; its overhead can account for over half of end-to-end runtime. We present FUSCO, an MoE-friendly communication library that achieves efficient and lightweight data shuffling through fused data transformation and communication, based on the key observation that MoE's expert-major data layout conflicts with the device-major layout expected by communication operations. FUSCO captures the fine-grained data layout, which is then interpreted by a pipelined communication engine that performs the required shuffling efficiently along the communication path. Lightweight planning and load-balancing mechanisms complement the engine by eliminating redundant communication and dispersing traffic. Evaluations on representative benchmarks illustrate that FUSCO achieves up to 3.84$\\times$ and 2.01$\\times$ speedups over NCCL and DeepEP (the state-of-the-art MoE communication library), respectively. In end-to-end MoE tasks, compared to NCCL and DeepEP, FUSCO reduces the training latency by 1.17-1.39$\\times$ and 1.10-1.19$\\times$, and lowers the first-token generation latency in inference by 1.09-1.25$\\times$ and 1.06-1.16$\\times$.", "AI": {"tldr": "FUSCO\u662f\u4e00\u4e2a\u4e13\u4e3aMoE\u6a21\u578b\u8bbe\u8ba1\u7684\u901a\u4fe1\u5e93\uff0c\u901a\u8fc7\u878d\u5408\u6570\u636e\u8f6c\u6362\u548c\u901a\u4fe1\u4f18\u5316\u4e13\u5bb6\u5e76\u884c\u4e2d\u7684\u6570\u636e\u91cd\u6392\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6848\u663e\u8457\u63d0\u5347\u8bad\u7ec3\u548c\u63a8\u7406\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u901a\u4fe1\u5e93\u5728\u5904\u7406MoE\u6a21\u578b\u7684\u4e13\u5bb6\u5e76\u884c\u6570\u636e\u91cd\u6392\u65f6\u6548\u7387\u4f4e\u4e0b\uff0c\u5176\u5f00\u9500\u53ef\u80fd\u5360\u7aef\u5230\u7aef\u8fd0\u884c\u65f6\u95f4\u7684\u4e00\u534a\u4ee5\u4e0a\u3002MoE\u7684\u4e13\u5bb6\u4e3b\u6570\u636e\u5e03\u5c40\u4e0e\u901a\u4fe1\u64cd\u4f5c\u671f\u671b\u7684\u8bbe\u5907\u4e3b\u5e03\u5c40\u5b58\u5728\u51b2\u7a81\uff0c\u5bfc\u81f4\u6027\u80fd\u74f6\u9888\u3002", "method": "FUSCO\u901a\u8fc7\u6355\u83b7\u7ec6\u7c92\u5ea6\u6570\u636e\u5e03\u5c40\uff0c\u7531\u6d41\u6c34\u7ebf\u901a\u4fe1\u5f15\u64ce\u5728\u901a\u4fe1\u8def\u5f84\u4e0a\u9ad8\u6548\u6267\u884c\u6570\u636e\u91cd\u6392\u3002\u7ed3\u5408\u8f7b\u91cf\u7ea7\u89c4\u5212\u548c\u8d1f\u8f7d\u5747\u8861\u673a\u5236\uff0c\u6d88\u9664\u5197\u4f59\u901a\u4fe1\u5e76\u5206\u6563\u6d41\u91cf\u3002", "result": "\u5728\u4ee3\u8868\u6027\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cFUSCO\u76f8\u6bd4NCCL\u548cDeepEP\u5206\u522b\u5b9e\u73b0\u6700\u9ad83.84\u500d\u548c2.01\u500d\u52a0\u901f\u3002\u5728\u7aef\u5230\u7aefMoE\u4efb\u52a1\u4e2d\uff0c\u8bad\u7ec3\u5ef6\u8fdf\u964d\u4f4e1.17-1.39\u500d\u548c1.10-1.19\u500d\uff0c\u63a8\u7406\u9996\u4ee4\u724c\u751f\u6210\u5ef6\u8fdf\u964d\u4f4e1.09-1.25\u500d\u548c1.06-1.16\u500d\u3002", "conclusion": "FUSCO\u901a\u8fc7\u4e13\u95e8\u9488\u5bf9MoE\u6570\u636e\u5e03\u5c40\u7279\u70b9\u8bbe\u8ba1\u7684\u901a\u4fe1\u4f18\u5316\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4e13\u5bb6\u5e76\u884c\u4e2d\u7684\u6570\u636e\u91cd\u6392\u74f6\u9888\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5927\u89c4\u6a21MoE\u6a21\u578b\u7684\u8bad\u7ec3\u548c\u63a8\u7406\u6548\u7387\u3002"}}
{"id": "2512.22113", "categories": ["cs.DC", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2512.22113", "abs": "https://arxiv.org/abs/2512.22113", "authors": ["Shengkun Cui", "Rahul Krishna", "Saurabh Jha", "Ravishankar K. Iyer"], "title": "Agentic Structured Graph Traversal for Root Cause Analysis of Code-related Incidents in Cloud Applications", "comment": null, "summary": "Cloud incidents pose major operational challenges in production, with unresolved production cloud incidents cost on average over $2M per hour. Prior research identifies code- and configuration-related issues as the predominant category of root causes in cloud incidents. This paper introduces PRAXIS, an orchestrator that manages and deploys an agentic workflow for diagnosing code- and configuration-caused cloud incidents. PRAXIS employs an LLM-driven structured traversal over two types of graph: (1) a service dependency graph (SDG) that captures microservice-level dependencies; and (2) a hammock-block program dependence graph (PDG) that captures code-level dependencies for each microservice. Together, these graphs encode microservice- and code-level dependencies and the LLM acts as a traversal policy over these graphs, moving between services and code dependencies to localize and explain failures. Compared to state-of-the-art ReAct baselines, PRAXIS improves RCA accuracy by up to 3.1x while reducing token consumption by 3.8x. PRAXIS is demonstrated on a set of 30 comprehensive real-world incidents that is being compiled into an RCA benchmark.", "AI": {"tldr": "PRAXIS\u662f\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u4e91\u4e8b\u6545\u8bca\u65ad\u7f16\u6392\u5668\uff0c\u901a\u8fc7\u904d\u5386\u670d\u52a1\u4f9d\u8d56\u56fe\u548c\u7a0b\u5e8f\u4f9d\u8d56\u56fe\u6765\u5b9a\u4f4d\u4ee3\u7801\u548c\u914d\u7f6e\u95ee\u9898\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5c06RCA\u51c6\u786e\u7387\u63d0\u53473.1\u500d\uff0c\u540c\u65f6\u51cf\u5c113.8\u500d\u7684token\u6d88\u8017\u3002", "motivation": "\u4e91\u4e8b\u6545\u9020\u6210\u5de8\u5927\u7ecf\u6d4e\u635f\u5931\uff08\u5e73\u5747\u6bcf\u5c0f\u65f6\u8d85\u8fc7200\u4e07\u7f8e\u5143\uff09\uff0c\u800c\u4ee3\u7801\u548c\u914d\u7f6e\u95ee\u9898\u662f\u4e91\u4e8b\u6545\u7684\u4e3b\u8981\u6839\u672c\u539f\u56e0\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u8bca\u65ad\u65b9\u6cd5\u3002", "method": "PRAXIS\u91c7\u7528LLM\u9a71\u52a8\u7684\u7ed3\u6784\u5316\u904d\u5386\u7b56\u7565\uff0c\u5728\u4e24\u79cd\u56fe\u4e0a\u8fdb\u884c\u904d\u5386\uff1a1) \u670d\u52a1\u4f9d\u8d56\u56fe(SDG)\u6355\u83b7\u5fae\u670d\u52a1\u7ea7\u4f9d\u8d56\u5173\u7cfb\uff1b2) \u7a0b\u5e8f\u4f9d\u8d56\u56fe(PDG)\u6355\u83b7\u6bcf\u4e2a\u5fae\u670d\u52a1\u7684\u4ee3\u7801\u7ea7\u4f9d\u8d56\u5173\u7cfb\u3002LLM\u4f5c\u4e3a\u904d\u5386\u7b56\u7565\u5728\u8fd9\u4e9b\u56fe\u4e0a\u79fb\u52a8\uff0c\u5b9a\u4f4d\u548c\u89e3\u91ca\u6545\u969c\u3002", "result": "\u76f8\u6bd4\u6700\u5148\u8fdb\u7684ReAct\u57fa\u7ebf\u65b9\u6cd5\uff0cPRAXIS\u5c06\u6839\u672c\u539f\u56e0\u5206\u6790(RCA)\u51c6\u786e\u7387\u63d0\u9ad8\u4e863.1\u500d\uff0c\u540c\u65f6\u51cf\u5c11\u4e863.8\u500d\u7684token\u6d88\u8017\u3002\u572830\u4e2a\u771f\u5b9e\u4e16\u754c\u4e8b\u6545\u6848\u4f8b\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002", "conclusion": "PRAXIS\u901a\u8fc7\u7ed3\u5408\u670d\u52a1\u7ea7\u548c\u4ee3\u7801\u7ea7\u4f9d\u8d56\u56fe\u7684LLM\u9a71\u52a8\u904d\u5386\uff0c\u6709\u6548\u63d0\u9ad8\u4e86\u4e91\u4e8b\u6545\u8bca\u65ad\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\uff0c\u4e3a\u4e91\u4e8b\u6545\u6839\u672c\u539f\u56e0\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
