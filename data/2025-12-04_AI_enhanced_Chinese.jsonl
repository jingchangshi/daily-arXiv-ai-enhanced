{"id": "2512.03083", "categories": ["cs.PL", "cs.SE"], "pdf": "https://arxiv.org/pdf/2512.03083", "abs": "https://arxiv.org/abs/2512.03083", "authors": ["ZeHao Yu"], "title": "Evaluate the Stack Management in Effect Handlers using the libseff C Library", "comment": null, "summary": "Effect handlers are increasingly prominent in modern programming for managing complex computational effects, including concurrency, asynchronous operations, and exception handling, in a modular and flexible manner. Efficient stack management remains a significant challenge for effect handlers due to the dynamic control flow changes they introduce. This paper explores a novel stack management approach using user-level overcommitting within the libseff C library, which leverages virtual memory mechanisms and protection-based lazy allocation combined with signal-driven memory commitment. Our user-level overcommitting implementation dynamically resizes stacks on-demand, improving memory utilization and reducing waste compared to traditional methods. We rigorously benchmark and evaluate this novel strategy against conventional fixed- size stacks, segmented stacks, and kernel-based overcommitting, using metrics such as context-switch latency, stack expansion efficiency, multi-threaded performance, and robustness under rapid stack growth conditions. Experimental results demonstrate that kernel-based overcommitting achieves an effective balance between performance and flexibility, whereas our user-level implementation, while flexible, incurs additional overheads, highlighting areas for optimization. This study provides a detailed comparative analysis of various stack management strate- gies, offering practical recommendations tailored to specific application requirements and operational constraints. Future work will focus on refining user-level overcommit- ting mechanisms, mitigating non-deterministic behaviors, and expanding benchmark frameworks to include real-world scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7528\u6237\u7ea7\u8fc7\u91cf\u63d0\u4ea4\u7684\u6808\u7ba1\u7406\u65b9\u6cd5\uff0c\u901a\u8fc7libseff C\u5e93\u5b9e\u73b0\uff0c\u5229\u7528\u865a\u62df\u5185\u5b58\u673a\u5236\u548c\u57fa\u4e8e\u4fdd\u62a4\u5ef6\u8fdf\u5206\u914d\u7ed3\u5408\u4fe1\u53f7\u9a71\u52a8\u5185\u5b58\u63d0\u4ea4\uff0c\u52a8\u6001\u6309\u9700\u8c03\u6574\u6808\u5927\u5c0f\uff0c\u63d0\u9ad8\u5185\u5b58\u5229\u7528\u7387\u3002", "motivation": "\u6548\u5e94\u5904\u7406\u5668\u5728\u73b0\u4ee3\u7f16\u7a0b\u4e2d\u65e5\u76ca\u91cd\u8981\uff0c\u7528\u4e8e\u7ba1\u7406\u5e76\u53d1\u3001\u5f02\u6b65\u64cd\u4f5c\u548c\u5f02\u5e38\u5904\u7406\u7b49\u590d\u6742\u8ba1\u7b97\u6548\u5e94\u3002\u7136\u800c\uff0c\u7531\u4e8e\u6548\u5e94\u5904\u7406\u5668\u5f15\u5165\u7684\u52a8\u6001\u63a7\u5236\u6d41\u53d8\u5316\uff0c\u9ad8\u6548\u7684\u6808\u7ba1\u7406\u4ecd\u7136\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\u3002\u4f20\u7edf\u6808\u7ba1\u7406\u65b9\u6cd5\u5b58\u5728\u5185\u5b58\u6d6a\u8d39\u548c\u6027\u80fd\u95ee\u9898\u3002", "method": "\u91c7\u7528\u7528\u6237\u7ea7\u8fc7\u91cf\u63d0\u4ea4\u65b9\u6cd5\uff0c\u5728libseff C\u5e93\u4e2d\u5b9e\u73b0\u3002\u5229\u7528\u865a\u62df\u5185\u5b58\u673a\u5236\u548c\u57fa\u4e8e\u4fdd\u62a4\u5ef6\u8fdf\u5206\u914d\u7ed3\u5408\u4fe1\u53f7\u9a71\u52a8\u5185\u5b58\u63d0\u4ea4\uff0c\u52a8\u6001\u6309\u9700\u8c03\u6574\u6808\u5927\u5c0f\u3002\u901a\u8fc7\u4fe1\u53f7\u5904\u7406\u5b9e\u73b0\u6808\u6269\u5c55\uff0c\u51cf\u5c11\u5185\u5b58\u6d6a\u8d39\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff1a1) \u5185\u6838\u7ea7\u8fc7\u91cf\u63d0\u4ea4\u5728\u6027\u80fd\u548c\u7075\u6d3b\u6027\u4e4b\u95f4\u8fbe\u5230\u6700\u4f73\u5e73\u8861\uff1b2) \u7528\u6237\u7ea7\u5b9e\u73b0\u867d\u7136\u7075\u6d3b\uff0c\u4f46\u5f15\u5165\u4e86\u989d\u5916\u5f00\u9500\uff1b3) \u4e0e\u4f20\u7edf\u56fa\u5b9a\u5927\u5c0f\u6808\u3001\u5206\u6bb5\u6808\u7b49\u65b9\u6cd5\u76f8\u6bd4\uff0c\u7528\u6237\u7ea7\u8fc7\u91cf\u63d0\u4ea4\u5728\u5185\u5b58\u5229\u7528\u7387\u65b9\u9762\u6709\u4f18\u52bf\uff0c\u4f46\u6027\u80fd\u9700\u8981\u4f18\u5316\u3002", "conclusion": "\u672c\u7814\u7a76\u5bf9\u5404\u79cd\u6808\u7ba1\u7406\u7b56\u7565\u8fdb\u884c\u4e86\u8be6\u7ec6\u6bd4\u8f83\u5206\u6790\uff0c\u63d0\u4f9b\u4e86\u9488\u5bf9\u7279\u5b9a\u5e94\u7528\u9700\u6c42\u548c\u64cd\u4f5c\u7ea6\u675f\u7684\u5b9e\u7528\u5efa\u8bae\u3002\u672a\u6765\u5de5\u4f5c\u5c06\u91cd\u70b9\u6539\u8fdb\u7528\u6237\u7ea7\u8fc7\u91cf\u63d0\u4ea4\u673a\u5236\uff0c\u51cf\u8f7b\u975e\u786e\u5b9a\u6027\u884c\u4e3a\uff0c\u5e76\u6269\u5c55\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\u4ee5\u5305\u542b\u771f\u5b9e\u573a\u666f\u3002"}}
{"id": "2512.03086", "categories": ["cs.PL", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2512.03086", "abs": "https://arxiv.org/abs/2512.03086", "authors": ["Le Chen", "Nuo Xu", "Winson Chen", "Bin Lei", "Pei-Hung Lin", "Dunzhi Zhou", "Rajeev Thakur", "Caiwen Ding", "Ali Jannesari", "Chunhua Liao"], "title": "Beyond Code Pairs: Dialogue-Based Data Generation for LLM Code Translation", "comment": null, "summary": "Large language models (LLMs) have shown remarkable capabilities in code translation, yet their performance deteriorates in low-resource programming domains such as Fortran and emerging frameworks like CUDA, where high-quality parallel data are scarce. We present an automated dataset generation pipeline featuring a dual-LLM Questioner-Solver design that incorporates external knowledge from compilers and runtime feedback. Beyond traditional source-target code pair datasets, our approach additionally generates (1) verified translations with unit tests for assessing functional consistency, and (2) multi-turn dialogues that capture the reasoning process behind translation refinement. Applied to Fortran -> C++ and C++ -> CUDA, the pipeline yields 3.64k and 3.93k dialogues, respectively. Fine-tuning on this data yields dramatic improvements in functional correctness, boosting unit test success rates by over 56% on the challenging C++-to-CUDA task. We show this data enables a 7B open-weight model to significantly outperform larger proprietary systems on key metrics like compilation success.", "AI": {"tldr": "\u63d0\u51fa\u81ea\u52a8\u5316\u6570\u636e\u96c6\u751f\u6210\u6d41\u6c34\u7ebf\uff0c\u901a\u8fc7\u53ccLLM\u95ee\u7b54\u8bbe\u8ba1\u7ed3\u5408\u7f16\u8bd1\u5668\u53cd\u9988\uff0c\u4e3a\u4f4e\u8d44\u6e90\u7f16\u7a0b\u8bed\u8a00\u7ffb\u8bd1\u751f\u6210\u5e26\u5355\u5143\u6d4b\u8bd5\u7684\u9a8c\u8bc1\u7ffb\u8bd1\u548c\u591a\u8f6e\u5bf9\u8bdd\u6570\u636e\uff0c\u663e\u8457\u63d0\u5347\u7ffb\u8bd1\u529f\u80fd\u6b63\u786e\u6027", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4ee3\u7801\u7ffb\u8bd1\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728Fortran\u3001CUDA\u7b49\u4f4e\u8d44\u6e90\u7f16\u7a0b\u9886\u57df\u6027\u80fd\u4e0b\u964d\uff0c\u56e0\u4e3a\u8fd9\u4e9b\u9886\u57df\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u7684\u5e76\u884c\u6570\u636e", "method": "\u91c7\u7528\u81ea\u52a8\u5316\u6570\u636e\u96c6\u751f\u6210\u6d41\u6c34\u7ebf\uff0c\u8bbe\u8ba1\u53ccLLM\u95ee\u7b54\u7cfb\u7edf\uff08Questioner-Solver\uff09\uff0c\u7ed3\u5408\u7f16\u8bd1\u5668\u77e5\u8bc6\u548c\u8fd0\u884c\u65f6\u53cd\u9988\uff0c\u751f\u6210\u5e26\u5355\u5143\u6d4b\u8bd5\u7684\u9a8c\u8bc1\u7ffb\u8bd1\u548c\u591a\u8f6e\u5bf9\u8bdd\u6570\u636e", "result": "\u4e3aFortran->C++\u548cC++->CUDA\u5206\u522b\u751f\u62103.64k\u548c3.93k\u5bf9\u8bdd\u6570\u636e\uff0c\u5fae\u8c03\u540e\u529f\u80fd\u6b63\u786e\u6027\u663e\u8457\u63d0\u5347\uff0cC++-to-CUDA\u4efb\u52a1\u7684\u5355\u5143\u6d4b\u8bd5\u6210\u529f\u7387\u63d0\u9ad8\u8d85\u8fc756%\uff0c7B\u5f00\u6e90\u6a21\u578b\u5728\u7f16\u8bd1\u6210\u529f\u7387\u7b49\u5173\u952e\u6307\u6807\u4e0a\u8d85\u8d8a\u5927\u578b\u4e13\u6709\u7cfb\u7edf", "conclusion": "\u63d0\u51fa\u7684\u81ea\u52a8\u5316\u6570\u636e\u96c6\u751f\u6210\u65b9\u6cd5\u80fd\u6709\u6548\u89e3\u51b3\u4f4e\u8d44\u6e90\u7f16\u7a0b\u8bed\u8a00\u7ffb\u8bd1\u7684\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0c\u901a\u8fc7\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u8bad\u7ec3\u6570\u636e\u663e\u8457\u63d0\u5347\u7ffb\u8bd1\u6a21\u578b\u6027\u80fd\uff0c\u4f7f\u5c0f\u578b\u5f00\u6e90\u6a21\u578b\u5728\u7279\u5b9a\u4efb\u52a1\u4e0a\u8d85\u8d8a\u5927\u578b\u4e13\u6709\u7cfb\u7edf"}}
{"id": "2512.03972", "categories": ["cs.PL"], "pdf": "https://arxiv.org/pdf/2512.03972", "abs": "https://arxiv.org/abs/2512.03972", "authors": ["Hassan Arafat", "David Bremner", "Kenneth B. Kent", "Julian Wang"], "title": "OOPredictor: Predicting Object-Oriented Accesses using Static Analysis", "comment": null, "summary": "Object-oriented Programming has become one of the most dominant design paradigms as the separation of concerns and adaptability of design reduce development and maintenance costs. However, the convenience is not without cost. The added indirection inherent in such designs causes excessive pointer chasing, negatively affecting locality, which in turn degrades the performance of cache structures. Furthermore, modern hardware prefetchers are mostly stride prefetchers that are ill-equipped to handle the unpredictability of access patterns generated by pointer chasing. Most software approaches that seek to address this problem resort to profiling the program as it runs, which comes with a significant run-time overhead or requires data from previous runs. In this paper, we propose the use of compile-time static analysis to predict the most common access patterns displayed by a program during run time. Since Java is one of the most popular object-oriented languages, we implement our prototype within the OpenJ9 JVM, inside the OMR optimizer infrastructure. The outputs of our proposed predictor are Markov chains that model the expected behavior of the program. The effectiveness of the proposed predictor is evaluated by comparing the model with the actual run-time behavior of the program measured using an instrumented interpreter. Our experiments show that the proposed predictor exhibits good accuracy and can be used to inform minimally intrusive load stall mitigation strategies, e.g. informing copying GCs on more locality-friendly copying orders", "AI": {"tldr": "\u4f7f\u7528\u7f16\u8bd1\u65f6\u9759\u6001\u5206\u6790\u9884\u6d4bJava\u7a0b\u5e8f\u8fd0\u884c\u65f6\u8bbf\u95ee\u6a21\u5f0f\uff0c\u901a\u8fc7\u9a6c\u5c14\u53ef\u592b\u94fe\u5efa\u6a21\u7a0b\u5e8f\u884c\u4e3a\uff0c\u4e3a\u5783\u573e\u6536\u96c6\u5668\u63d0\u4f9b\u66f4\u53cb\u597d\u7684\u5185\u5b58\u5e03\u5c40\u5efa\u8bae", "motivation": "\u9762\u5411\u5bf9\u8c61\u7f16\u7a0b\u4e2d\u7684\u6307\u9488\u8ffd\u9010\u5bfc\u81f4\u7f13\u5b58\u6027\u80fd\u4e0b\u964d\uff0c\u73b0\u4ee3\u786c\u4ef6\u9884\u53d6\u5668\u96be\u4ee5\u5904\u7406\u8fd9\u79cd\u4e0d\u53ef\u9884\u6d4b\u7684\u8bbf\u95ee\u6a21\u5f0f\uff0c\u73b0\u6709\u8f6f\u4ef6\u65b9\u6cd5\u9700\u8981\u8fd0\u884c\u65f6\u5206\u6790\u5e26\u6765\u663e\u8457\u5f00\u9500", "method": "\u5728OpenJ9 JVM\u7684OMR\u4f18\u5316\u5668\u57fa\u7840\u8bbe\u65bd\u4e2d\u5b9e\u73b0\u7f16\u8bd1\u65f6\u9759\u6001\u5206\u6790\u5668\uff0c\u9884\u6d4b\u7a0b\u5e8f\u6700\u5e38\u89c1\u7684\u8bbf\u95ee\u6a21\u5f0f\uff0c\u8f93\u51fa\u9a6c\u5c14\u53ef\u592b\u94fe\u6a21\u578b", "result": "\u9884\u6d4b\u5668\u8868\u73b0\u51fa\u826f\u597d\u7684\u51c6\u786e\u6027\uff0c\u80fd\u591f\u4e3a\u6700\u5c0f\u4fb5\u5165\u6027\u7684\u8d1f\u8f7d\u505c\u987f\u7f13\u89e3\u7b56\u7565\u63d0\u4f9b\u4fe1\u606f\uff0c\u5982\u6307\u5bfc\u590d\u5236\u5f0f\u5783\u573e\u6536\u96c6\u5668\u91c7\u7528\u66f4\u53cb\u597d\u7684\u5185\u5b58\u590d\u5236\u987a\u5e8f", "conclusion": "\u7f16\u8bd1\u65f6\u9759\u6001\u5206\u6790\u53ef\u4ee5\u6709\u6548\u9884\u6d4bJava\u7a0b\u5e8f\u7684\u8fd0\u884c\u65f6\u8bbf\u95ee\u6a21\u5f0f\uff0c\u4e3a\u4f18\u5316\u5185\u5b58\u5e03\u5c40\u548c\u63d0\u5347\u7f13\u5b58\u6027\u80fd\u63d0\u4f9b\u53ef\u884c\u65b9\u6848"}}
{"id": "2512.03594", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2512.03594", "abs": "https://arxiv.org/abs/2512.03594", "authors": ["Afsara Khan", "Austin Rovinski"], "title": "Accelerating Detailed Routing Convergence through Offline Reinforcement Learning", "comment": "To be published in the Design, Automation and Test in Europe (DATE) 2026 Conference", "summary": "Detailed routing remains one of the most complex and time-consuming steps in modern physical design due to the challenges posed by shrinking feature sizes and stricter design rules. Prior detailed routers achieve state-of-the-art results by leveraging iterative pathfinding algorithms to route each net. However, runtimes are a major issue in detailed routers, as converging to a solution with zero design rule violations (DRVs) can be prohibitively expensive.\n  In this paper, we propose leveraging reinforcement learning (RL) to enable rapid convergence in detailed routing by learning from previous designs. We make the key observation that prior detailed routers statically schedule the cost weights used in their routing algorithms, meaning they do not change in response to the design or technology. By training a conservative Q-learning (CQL) model to dynamically select the routing cost weights which minimize the number of algorithm iterations, we find that our work completes the ISPD19 benchmarks with 1.56x average and up to 3.01x faster runtime than the baseline router while maintaining or improving the DRV count in all cases. We also find that this learning shows signs of generalization across technologies, meaning that learning designs in one technology can translate to improved outcomes in other technologies.", "AI": {"tldr": "\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u52a8\u6001\u8c03\u6574\u5e03\u7ebf\u6210\u672c\u6743\u91cd\uff0c\u663e\u8457\u63d0\u5347\u8be6\u7ec6\u5e03\u7ebf\u6548\u7387\uff0c\u5728\u4fdd\u6301\u6216\u6539\u8fdbDRV\u6570\u91cf\u7684\u540c\u65f6\uff0c\u5b9e\u73b01.56\u500d\u5e73\u5747\u52a0\u901f\u548c\u6700\u9ad83.01\u500d\u52a0\u901f\u3002", "motivation": "\u8be6\u7ec6\u5e03\u7ebf\u662f\u73b0\u4ee3\u7269\u7406\u8bbe\u8ba1\u4e2d\u6700\u590d\u6742\u8017\u65f6\u7684\u6b65\u9aa4\u4e4b\u4e00\uff0c\u4f20\u7edf\u5e03\u7ebf\u5668\u4f7f\u7528\u9759\u6001\u6210\u672c\u6743\u91cd\u8c03\u5ea6\uff0c\u65e0\u6cd5\u6839\u636e\u8bbe\u8ba1\u548c\u5de5\u827a\u52a8\u6001\u8c03\u6574\uff0c\u5bfc\u81f4\u6536\u655b\u5230\u96f6\u8bbe\u8ba1\u89c4\u5219\u8fdd\u89c4\u7684\u89e3\u51b3\u65b9\u6848\u8017\u65f6\u8fc7\u957f\u3002", "method": "\u91c7\u7528\u4fdd\u5b88Q\u5b66\u4e60\uff08CQL\uff09\u6a21\u578b\uff0c\u5b66\u4e60\u4ece\u5148\u524d\u8bbe\u8ba1\u4e2d\u52a8\u6001\u9009\u62e9\u5e03\u7ebf\u6210\u672c\u6743\u91cd\uff0c\u4ee5\u6700\u5c0f\u5316\u7b97\u6cd5\u8fed\u4ee3\u6b21\u6570\u3002\u8be5\u65b9\u6cd5\u80fd\u591f\u6839\u636e\u5177\u4f53\u8bbe\u8ba1\u548c\u5de5\u827a\u7279\u6027\u81ea\u9002\u5e94\u8c03\u6574\u5e03\u7ebf\u7b56\u7565\u3002", "result": "\u5728ISPD19\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u6216\u6539\u8fdb\u6240\u6709\u60c5\u51b5\u4e0b\u7684DRV\u6570\u91cf\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e861.56\u500d\u5e73\u5747\u52a0\u901f\u548c\u6700\u9ad83.01\u500d\u7684\u8fd0\u884c\u65f6\u95f4\u52a0\u901f\u3002\u5b66\u4e60\u8fd8\u663e\u793a\u51fa\u8de8\u5de5\u827a\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u5f3a\u5316\u5b66\u4e60\u80fd\u591f\u6709\u6548\u4f18\u5316\u8be6\u7ec6\u5e03\u7ebf\u8fc7\u7a0b\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u6210\u672c\u6743\u91cd\u663e\u8457\u52a0\u901f\u6536\u655b\uff0c\u540c\u65f6\u4fdd\u6301\u5e03\u7ebf\u8d28\u91cf\uff0c\u5e76\u4e14\u8fd9\u79cd\u5b66\u4e60\u5177\u6709\u8de8\u5de5\u827a\u7684\u6cdb\u5316\u6f5c\u529b\u3002"}}
{"id": "2512.03608", "categories": ["cs.AR", "cs.AI", "cs.ET"], "pdf": "https://arxiv.org/pdf/2512.03608", "abs": "https://arxiv.org/abs/2512.03608", "authors": ["Lishuo Deng", "Shaojie Xu", "Jinwu Chen", "Changwei Yan", "Jiajie Wang", "Zhe Jiang", "Weiwei Shan"], "title": "KVNAND: Efficient On-Device Large Language Model Inference Using DRAM-Free In-Flash Computing", "comment": null, "summary": "Deploying large language models (LLMs) on edge devices enables personalized agents with strong privacy and low cost. However, with tens to hundreds of billions of parameters, single-batch autoregressive inference suffers from extremely low arithmetic intensity, creating severe weight-loading and bandwidth pressures on resource-constrained platforms. Recent in-flash computing (IFC) solutions alleviate this bottleneck by co-locating weight-related linear computations in the decode phase with flash, yet still rely on DRAM for the key-value (KV) cache. As context length grows, the KV cache can exceed model weights in size, imposing prohibitive DRAM cost and capacity requirements. Attempts to offload KV cache to flash suffer from severe performance penalties.\n  We propose KVNAND, the first DRAM-free, IFC-based architecture that stores both model weights and KV cache entirely in compute-enabled 3D NAND flash. KVNAND addresses the fundamental performance challenges of flash under intensive KV cache access by leveraging IFC for all memory-bound operations to reduce data transfer overhead, introducing head-group parallelism to boost throughput, and employing page-level KV cache mapping to align token access patterns with flash organization. In addition, we propose a design space exploration framework that evaluates discrete and compact KVNAND variants to balance weight and KV placement, automatically identifying the optimal design trade-off. These techniques mitigate latency, energy, and reliability concerns, turning flash into a practical medium for long-context KV storage. Evaluations on MHA 7B and GQA 70B LLMs show that KVNAND achieves 1.98\\(\\times\\)/1.94\\(\\times\\)/2.05\\(\\times\\) geomean speedup at 128/1K/10K-token contexts compared to DRAM-equipped IFC designs and addresses out-of-memory failures at 100K context length.", "AI": {"tldr": "KVNAND\uff1a\u9996\u4e2a\u5b8c\u5168\u57fa\u4e8e\u8ba1\u7b97\u578b3D NAND\u95ea\u5b58\u7684DRAM-free\u67b6\u6784\uff0c\u5c06\u6a21\u578b\u6743\u91cd\u548cKV\u7f13\u5b58\u90fd\u5b58\u50a8\u5728\u95ea\u5b58\u4e2d\uff0c\u901a\u8fc7IFC\u6280\u672f\u3001\u5934\u7ec4\u5e76\u884c\u548c\u9875\u9762\u7ea7KV\u6620\u5c04\u89e3\u51b3\u957f\u4e0a\u4e0b\u6587\u63a8\u7406\u7684\u5185\u5b58\u74f6\u9888\u95ee\u9898\u3002", "motivation": "\u5728\u8fb9\u7f18\u8bbe\u5907\u90e8\u7f72\u5927\u8bed\u8a00\u6a21\u578b\u9762\u4e34\u5185\u5b58\u74f6\u9888\uff1a\u5355\u6279\u6b21\u81ea\u56de\u5f52\u63a8\u7406\u7684\u7b97\u672f\u5f3a\u5ea6\u6781\u4f4e\uff0c\u9020\u6210\u4e25\u91cd\u7684\u6743\u91cd\u52a0\u8f7d\u548c\u5e26\u5bbd\u538b\u529b\u3002\u73b0\u6709IFC\u65b9\u6848\u867d\u7136\u5c06\u6743\u91cd\u8ba1\u7b97\u4e0e\u95ea\u5b58\u5171\u7f6e\uff0c\u4f46KV\u7f13\u5b58\u4ecd\u9700DRAM\uff0c\u968f\u7740\u4e0a\u4e0b\u6587\u957f\u5ea6\u589e\u957f\uff0cKV\u7f13\u5b58\u53ef\u80fd\u8d85\u8fc7\u6a21\u578b\u6743\u91cd\u5927\u5c0f\uff0c\u5bfc\u81f4DRAM\u6210\u672c\u8fc7\u9ad8\u4e14\u5bb9\u91cf\u4e0d\u8db3\u3002", "method": "1) \u5b8c\u5168\u57fa\u4e8e\u8ba1\u7b97\u578b3D NAND\u95ea\u5b58\u7684DRAM-free\u67b6\u6784\uff1b2) \u5229\u7528IFC\u5904\u7406\u6240\u6709\u5185\u5b58\u53d7\u9650\u64cd\u4f5c\u4ee5\u51cf\u5c11\u6570\u636e\u4f20\u8f93\u5f00\u9500\uff1b3) \u5f15\u5165\u5934\u7ec4\u5e76\u884c\u63d0\u5347\u541e\u5410\u91cf\uff1b4) \u91c7\u7528\u9875\u9762\u7ea7KV\u7f13\u5b58\u6620\u5c04\uff0c\u4f7f\u4ee4\u724c\u8bbf\u95ee\u6a21\u5f0f\u4e0e\u95ea\u5b58\u7ec4\u7ec7\u5bf9\u9f50\uff1b5) \u8bbe\u8ba1\u7a7a\u95f4\u63a2\u7d22\u6846\u67b6\u8bc4\u4f30\u79bb\u6563\u548c\u7d27\u51d1\u53d8\u4f53\uff0c\u81ea\u52a8\u5bfb\u627e\u6700\u4f18\u8bbe\u8ba1\u6743\u8861\u3002", "result": "\u5728MHA 7B\u548cGQA 70B LLM\u4e0a\u8bc4\u4f30\uff0cKVNAND\u5728128/1K/10K\u4ee4\u724c\u4e0a\u4e0b\u6587\u957f\u5ea6\u4e0b\u5206\u522b\u5b9e\u73b01.98\u00d7/1.94\u00d7/2.05\u00d7\u51e0\u4f55\u5e73\u5747\u52a0\u901f\u6bd4\uff08\u76f8\u6bd4DRAM-equipped IFC\u8bbe\u8ba1\uff09\uff0c\u5e76\u5728100K\u4e0a\u4e0b\u6587\u957f\u5ea6\u4e0b\u89e3\u51b3\u4e86\u5185\u5b58\u6ea2\u51fa\u95ee\u9898\u3002", "conclusion": "KVNAND\u901a\u8fc7\u5c06\u6a21\u578b\u6743\u91cd\u548cKV\u7f13\u5b58\u5b8c\u5168\u5b58\u50a8\u5728\u8ba1\u7b97\u578b\u95ea\u5b58\u4e2d\uff0c\u89e3\u51b3\u4e86\u957f\u4e0a\u4e0b\u6587LLM\u63a8\u7406\u7684\u5185\u5b58\u74f6\u9888\uff0c\u4f7f\u95ea\u5b58\u6210\u4e3aKV\u5b58\u50a8\u7684\u5b9e\u7528\u4ecb\u8d28\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5927\u8bed\u8a00\u6a21\u578b\u7684\u90e8\u7f72\u6548\u7387\u3002"}}
{"id": "2512.03416", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.03416", "abs": "https://arxiv.org/abs/2512.03416", "authors": ["Ruiqi Lai", "Hongrui Liu", "Chengzhi Lu", "Zonghao Liu", "Siyu Cao", "Siyang Shao", "Yixin Zhang", "Luo Mai", "Dmitrii Ustiugov"], "title": "TokenScale: Timely and Accurate Autoscaling for Disaggregated LLM Serving with Token Velocity", "comment": null, "summary": "The architectural shift to prefill/decode (PD) disaggregation in LLM serving improves resource utilization but struggles with the bursty nature of modern workloads. Existing autoscaling policies, often retrofitted from monolithic systems like those in AIBrix and DistServe, rely on lagging indicators such as GPU utilization or coarse-grained request counts. This results in slow reactions to load spikes, leading to significant Time-to First-Token (TTFT) and Time-Per-Output-Token (TPOT) SLO violations and costly over-provisioning. We introduce TokenScale, an autoscaling framework that resolves this performance mismatch through two innovations. First, we propose Token Velocity, a novel metric that unifies the prefill, network, and decode stages by quantifying their rate of work. As a leading indicator of system backpressure, it enables proactive scaling. Second, Convertible Decoders allow decoder GPUs to dynamically execute prefill tasks during traffic spikes, creating a rapid-response buffer that absorbs bursts and eliminates the initialization latency of new prefillers. Our evaluation on a GPU cluster with production traces shows TokenScale improves SLO attainment from 50-88% to 80-96% and reduces costs by 4-14% over state-of-the-art systems, including DistServe, BlitzScale, and AIBrix. By uniting a predictive metric with a flexible system design, TokenScale significantly boosts the performance and efficiency of disaggregated LLM serving infrastructure.", "AI": {"tldr": "TokenScale\uff1a\u4e00\u4e2a\u9488\u5bf9LLM\u670d\u52a1\u4e2dprefill/decode\u89e3\u8026\u67b6\u6784\u7684\u81ea\u52a8\u6269\u7f29\u5bb9\u6846\u67b6\uff0c\u901a\u8fc7Token Velocity\u6307\u6807\u548cConvertible Decoders\u521b\u65b0\uff0c\u663e\u8457\u63d0\u5347SLO\u8fbe\u6210\u7387\u5e76\u964d\u4f4e\u6210\u672c\u3002", "motivation": "LLM\u670d\u52a1\u4e2d\u7684prefill/decode\u89e3\u8026\u67b6\u6784\u63d0\u9ad8\u4e86\u8d44\u6e90\u5229\u7528\u7387\uff0c\u4f46\u73b0\u6709\u81ea\u52a8\u6269\u7f29\u5bb9\u7b56\u7565\uff08\u5982AIBrix\u548cDistServe\u4e2d\u7684\u65b9\u6848\uff09\u57fa\u4e8eGPU\u5229\u7528\u7387\u6216\u8bf7\u6c42\u6570\u7b49\u6ede\u540e\u6307\u6807\uff0c\u5bf9\u7a81\u53d1\u8d1f\u8f7d\u53cd\u5e94\u6162\uff0c\u5bfc\u81f4TTFT\u548cTPOT SLO\u8fdd\u89c4\u4ee5\u53ca\u8fc7\u5ea6\u914d\u7f6e\u6210\u672c\u9ad8\u3002", "method": "1. \u63d0\u51faToken Velocity\u6307\u6807\uff1a\u7edf\u4e00prefill\u3001\u7f51\u7edc\u548cdecode\u9636\u6bb5\u7684\u5de5\u4f5c\u901f\u7387\uff0c\u4f5c\u4e3a\u7cfb\u7edf\u80cc\u538b\u7684\u524d\u5bfc\u6307\u6807\uff0c\u5b9e\u73b0\u4e3b\u52a8\u6269\u7f29\u5bb9\u30022. \u5f15\u5165Convertible Decoders\uff1a\u5141\u8bb8\u89e3\u7801GPU\u5728\u6d41\u91cf\u9ad8\u5cf0\u65f6\u52a8\u6001\u6267\u884cprefill\u4efb\u52a1\uff0c\u521b\u5efa\u5feb\u901f\u54cd\u5e94\u7f13\u51b2\u533a\u5438\u6536\u7a81\u53d1\u6d41\u91cf\uff0c\u6d88\u9664\u65b0prefiller\u7684\u521d\u59cb\u5316\u5ef6\u8fdf\u3002", "result": "\u5728GPU\u96c6\u7fa4\u548c\u751f\u4ea7\u73af\u5883trace\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cTokenScale\u5c06SLO\u8fbe\u6210\u7387\u4ece50-88%\u63d0\u5347\u523080-96%\uff0c\u76f8\u6bd4DistServe\u3001BlitzScale\u548cAIBrix\u7b49\u5148\u8fdb\u7cfb\u7edf\uff0c\u6210\u672c\u964d\u4f4e4-14%\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u9884\u6d4b\u6027\u6307\u6807\u548c\u7075\u6d3b\u7684\u7cfb\u7edf\u8bbe\u8ba1\uff0cTokenScale\u663e\u8457\u63d0\u5347\u4e86LLM\u89e3\u8026\u670d\u52a1\u67b6\u6784\u7684\u6027\u80fd\u548c\u6548\u7387\uff0c\u89e3\u51b3\u4e86\u7a81\u53d1\u8d1f\u8f7d\u4e0b\u7684\u6269\u7f29\u5bb9\u6311\u6218\u3002"}}
{"id": "2512.03616", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2512.03616", "abs": "https://arxiv.org/abs/2512.03616", "authors": ["Christian Ewert", "Amrit Sharma Poudel", "Mouadh Ayache", "Andrija Neskovic", "Rainer Buchty", "Mladen Berekovic", "Sebastian Berndt", "Saleh Mulhem"], "title": "Lightweight Unified Sha-3/Shake Architecture with a Fault-Resilient State", "comment": "-", "summary": "Hash functions have become a key part of standard Post-quantum cryptography (PQC) schemes, especially Sha-3 and Shake, calling arXiv:submit/7045552 [cs.AR] 3 Dec 2025 for lightweight implementation. A fault-resilient design is always desirable to make the whole PQC system reliable. We, therefore, propose a) a unified hash engine supporting Sha-3 and Shake that follows a byte-wise in-place partitioning mechanism of the so-called Keccak state, and b) an according fault detection for Keccak state protection exploiting its cube structure by deploying two-dimensional parity checks. It outperforms the state-of-the-art (SoA) regarding area requirements at competitive register-level fault detection by achieving 100% detection of three and still near 100% of higher numbers of Keccak state faults. Unlike SoA solutions, the proposed unified hash engine covers all standard hash configurations. Moreover, the introduced multidimensional cross-parity check mechanism achieves a 3.7x improvement in area overhead, with an overall 4.5x smaller fault-resilient engine design as demonstrated in ASIC and FPGA implementations. Integrated into a RISC-V environment, the unified hash engine with the integrated fault-resilient mechanism introduced less than 8% area overhead. Our approach thus provides a robust and lightweight fault-detection solution for protecting hash functions deployed in resource-constrained PQC applications.", "AI": {"tldr": "\u63d0\u51fa\u7edf\u4e00\u54c8\u5e0c\u5f15\u64ce\u652f\u6301Sha-3\u548cShake\uff0c\u91c7\u7528\u5b57\u8282\u7ea7\u539f\u5730\u5206\u533aKeccak\u72b6\u6001\u673a\u5236\uff0c\u5e76\u57fa\u4e8e\u7acb\u65b9\u4f53\u7ed3\u6784\u8bbe\u8ba1\u4e8c\u7ef4\u5947\u5076\u6821\u9a8c\u6545\u969c\u68c0\u6d4b\uff0c\u5728\u4fdd\u6301100%\u4e09\u7ea7\u6545\u969c\u68c0\u6d4b\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u9762\u79ef\u5f00\u9500\u3002", "motivation": "\u54c8\u5e0c\u51fd\u6570\u5df2\u6210\u4e3a\u540e\u91cf\u5b50\u5bc6\u7801\u5b66\u6807\u51c6\u65b9\u6848\u7684\u5173\u952e\u90e8\u5206\uff0c\u4f46\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u9700\u8981\u8f7b\u91cf\u7ea7\u5b9e\u73b0\u3002\u6545\u969c\u5f39\u6027\u8bbe\u8ba1\u5bf9\u63d0\u5347\u6574\u4e2aPQC\u7cfb\u7edf\u53ef\u9760\u6027\u81f3\u5173\u91cd\u8981\uff0c\u73b0\u6709\u65b9\u6848\u5728\u9762\u79ef\u5f00\u9500\u548c\u914d\u7f6e\u8986\u76d6\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "1) \u63d0\u51fa\u7edf\u4e00\u54c8\u5e0c\u5f15\u64ce\uff0c\u652f\u6301Sha-3\u548cShake\uff0c\u91c7\u7528\u5b57\u8282\u7ea7\u539f\u5730\u5206\u533a\u673a\u5236\u5904\u7406Keccak\u72b6\u6001\uff1b2) \u57fa\u4e8eKeccak\u7acb\u65b9\u4f53\u7ed3\u6784\u8bbe\u8ba1\u4e8c\u7ef4\u5947\u5076\u6821\u9a8c\u6545\u969c\u68c0\u6d4b\u673a\u5236\uff0c\u5b9e\u73b0\u72b6\u6001\u4fdd\u62a4\u3002", "result": "\u5b9e\u73b0100%\u4e09\u7ea7\u6545\u969c\u68c0\u6d4b\u548c\u63a5\u8fd1100%\u66f4\u9ad8\u7ea7\u522b\u6545\u969c\u68c0\u6d4b\uff0c\u9762\u79ef\u5f00\u9500\u964d\u4f4e3.7\u500d\uff0c\u6574\u4f53\u6545\u969c\u5f39\u6027\u5f15\u64ce\u8bbe\u8ba1\u7f29\u5c0f4.5\u500d\u3002\u5728RISC-V\u73af\u5883\u4e2d\u96c6\u6210\u65f6\uff0c\u9762\u79ef\u5f00\u9500\u5c0f\u4e8e8%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u8d44\u6e90\u53d7\u9650PQC\u5e94\u7528\u4e2d\u7684\u54c8\u5e0c\u51fd\u6570\u63d0\u4f9b\u4e86\u9c81\u68d2\u4e14\u8f7b\u91cf\u7ea7\u7684\u6545\u969c\u68c0\u6d4b\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u4fdd\u6301\u9ad8\u6545\u969c\u68c0\u6d4b\u7387\u7684\u540c\u65f6\u663e\u8457\u4f18\u5316\u4e86\u9762\u79ef\u6548\u7387\u3002"}}
{"id": "2512.03487", "categories": ["cs.DC", "cs.IT"], "pdf": "https://arxiv.org/pdf/2512.03487", "abs": "https://arxiv.org/abs/2512.03487", "authors": ["Zhen Wang", "Bin Lin", "Qiang", "Ye"], "title": "Double-Edge-Assisted Computation Offloading and Resource Allocation for Space-Air-Marine Integrated Networks", "comment": null, "summary": "In this paper, we propose a double-edge-assisted computation offloading and resource allocation scheme tailored for space-air-marine integrated networks (SAMINs). Specifically, we consider a scenario where both unmanned aerial vehicles (UAVs) and a low earth orbit (LEO) satellite are equipped with edge servers, providing computing services for maritime autonomous surface ships (MASSs). Partial computation workloads of MASSs can be offloaded to both UAVs and the LEO satellite, concurrently, for processing via a multi-access approach. To minimize the energy consumption of SAMINs under latency constraints, we formulate an optimization problem and propose energy efficient algorithms to jointly optimize offloading mode, offloading volume, and computing resource allocation of the LEO satellite and the UAVs, respectively. We further exploit an alternating optimization (AO) method and a layered approach to decompose the original problem to attain the optimal solutions. Finally, we conduct simulations to validate the effectiveness and efficiency of the proposed scheme in comparison with benchmark algorithms.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9762\u5411\u7a7a\u5929\u5730\u6d77\u4e00\u4f53\u5316\u7f51\u7edc\u7684\u53cc\u8fb9\u7f18\u8f85\u52a9\u8ba1\u7b97\u5378\u8f7d\u4e0e\u8d44\u6e90\u5206\u914d\u65b9\u6848\uff0c\u5229\u7528\u65e0\u4eba\u673a\u548c\u4f4e\u8f68\u536b\u661f\u4e3a\u6d77\u4e0a\u81ea\u4e3b\u8239\u8236\u63d0\u4f9b\u5e76\u884c\u8ba1\u7b97\u670d\u52a1\uff0c\u901a\u8fc7\u4f18\u5316\u7b97\u6cd5\u6700\u5c0f\u5316\u80fd\u8017", "motivation": "\u7a7a\u5929\u5730\u6d77\u4e00\u4f53\u5316\u7f51\u7edc\u4e2d\u6d77\u4e8b\u81ea\u4e3b\u8239\u8236\u7684\u8ba1\u7b97\u9700\u6c42\u65e5\u76ca\u589e\u957f\uff0c\u9700\u8981\u9ad8\u6548\u7684\u8ba1\u7b97\u5378\u8f7d\u548c\u8d44\u6e90\u5206\u914d\u65b9\u6848\u6765\u6ee1\u8db3\u4f4e\u5ef6\u8fdf\u548c\u9ad8\u80fd\u6548\u7684\u8981\u6c42", "method": "\u91c7\u7528\u53cc\u8fb9\u7f18\u8ba1\u7b97\u67b6\u6784\uff0c\u5141\u8bb8\u6d77\u4e8b\u81ea\u4e3b\u8239\u8236\u540c\u65f6\u5411\u65e0\u4eba\u673a\u548c\u4f4e\u8f68\u536b\u661f\u5378\u8f7d\u90e8\u5206\u8ba1\u7b97\u4efb\u52a1\uff1b\u63d0\u51fa\u57fa\u4e8e\u4ea4\u66ff\u4f18\u5316\u548c\u5206\u5c42\u65b9\u6cd5\u7684\u80fd\u91cf\u9ad8\u6548\u7b97\u6cd5\uff0c\u8054\u5408\u4f18\u5316\u5378\u8f7d\u6a21\u5f0f\u3001\u5378\u8f7d\u91cf\u548c\u8ba1\u7b97\u8d44\u6e90\u5206\u914d", "result": "\u901a\u8fc7\u4eff\u771f\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6848\u5728\u80fd\u8017\u548c\u5ef6\u8fdf\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u76f8\u6bd4\u57fa\u51c6\u7b97\u6cd5\u8868\u73b0\u51fa\u66f4\u597d\u7684\u6027\u80fd", "conclusion": "\u8be5\u53cc\u8fb9\u7f18\u8f85\u52a9\u8ba1\u7b97\u5378\u8f7d\u65b9\u6848\u80fd\u591f\u6709\u6548\u964d\u4f4e\u7a7a\u5929\u5730\u6d77\u4e00\u4f53\u5316\u7f51\u7edc\u7684\u80fd\u8017\uff0c\u540c\u65f6\u6ee1\u8db3\u6d77\u4e8b\u81ea\u4e3b\u8239\u8236\u7684\u8ba1\u7b97\u5ef6\u8fdf\u8981\u6c42\uff0c\u4e3a\u672a\u6765\u6d77\u6d0b\u7269\u8054\u7f51\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2512.03781", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2512.03781", "abs": "https://arxiv.org/abs/2512.03781", "authors": ["Joscha Ilmberger", "Johannes Schemmel"], "title": "The BrainScaleS-2 multi-chip system: Interconnecting continuous-time neuromorphic compute substrates", "comment": null, "summary": "The BrainScaleS-2 SoC integrates analog neuron and synapse circuits with digital periphery, including two CPUs with SIMD extensions. Each ASIC is connected to a Node-FPGA, providing experiment control and Ethernet connectivity. This work details the scaling of the compute substrate through FPGA-based interconnection via an additional Aggregator unit. The Aggregator provides up to 12 transceiver links to a backplane of Node-FPGAs, as well as 4 transceiver lanes for further extension. Two such interconnected backplanes are integrated into a standard 19in rack case with 4U height together with an Ethernet switch, system controller and power supplies. For all spike rates, chip-to-chip latencies -- consisting of four hops across three FPGAs -- below 1.3$\u03bc$s are achieved within each backplane.", "AI": {"tldr": "BrainScaleS-2\u7cfb\u7edf\u901a\u8fc7FPGA\u4e92\u8fde\u6269\u5c55\u8ba1\u7b97\u89c4\u6a21\uff0c\u5b9e\u73b0\u82af\u7247\u95f4\u4f4e\u5ef6\u8fdf\u901a\u4fe1", "motivation": "\u6269\u5c55BrainScaleS-2\u795e\u7ecf\u5f62\u6001\u8ba1\u7b97\u7cfb\u7edf\u7684\u8ba1\u7b97\u89c4\u6a21\uff0c\u901a\u8fc7FPGA\u4e92\u8fde\u5b9e\u73b0\u591a\u82af\u7247\u534f\u540c\u5de5\u4f5c\uff0c\u6784\u5efa\u66f4\u5927\u89c4\u6a21\u7684\u795e\u7ecf\u5f62\u6001\u8ba1\u7b97\u5e73\u53f0", "method": "\u4f7f\u7528\u57fa\u4e8eFPGA\u7684Aggregator\u5355\u5143\u8fdb\u884c\u4e92\u8fde\uff0c\u63d0\u4f9b12\u4e2a\u6536\u53d1\u5668\u94fe\u8def\u8fde\u63a5Node-FPGA\u80cc\u677f\uff0c\u901a\u8fc7\u6807\u51c619\u82f1\u5bf84U\u673a\u67b6\u96c6\u6210\u7cfb\u7edf\u63a7\u5236\u5668\u3001\u4ee5\u592a\u7f51\u4ea4\u6362\u673a\u548c\u7535\u6e90", "result": "\u5728\u6240\u6709\u8109\u51b2\u901f\u7387\u4e0b\uff0c\u80cc\u677f\u5185\u82af\u7247\u95f4\u5ef6\u8fdf\uff08\u7ecf\u8fc7\u4e09\u4e2aFPGA\u7684\u56db\u8df3\uff09\u4f4e\u4e8e1.3\u5fae\u79d2\uff0c\u5b9e\u73b0\u4e86\u4f4e\u5ef6\u8fdf\u7684\u82af\u7247\u95f4\u901a\u4fe1", "conclusion": "\u901a\u8fc7FPGA\u4e92\u8fde\u6210\u529f\u6269\u5c55\u4e86BrainScaleS-2\u7cfb\u7edf\u7684\u8ba1\u7b97\u89c4\u6a21\uff0c\u6784\u5efa\u4e86\u53ef\u6269\u5c55\u7684\u795e\u7ecf\u5f62\u6001\u8ba1\u7b97\u5e73\u53f0\uff0c\u4e3a\u5927\u89c4\u6a21\u795e\u7ecf\u5f62\u6001\u8ba1\u7b97\u5e94\u7528\u5960\u5b9a\u4e86\u57fa\u7840"}}
{"id": "2512.03565", "categories": ["cs.DC", "cs.CE", "cs.PF"], "pdf": "https://arxiv.org/pdf/2512.03565", "abs": "https://arxiv.org/abs/2512.03565", "authors": ["Luis Gall", "Samuel James Newcome", "Fabio Alexander Gratl", "Markus M\u00fchlh\u00e4u\u00dfer", "Manish Kumar Mishra", "Hans-Joachim Bungartz"], "title": "Tuning of Vectorization Parameters for Molecular Dynamics Simulations in AutoPas", "comment": "20 pages, 8 figures. Submitted to the 5th International Conference on Computational Engineering (ICCE 2024). No changes were made after the peer review process", "summary": "Molecular Dynamics simulations can help scientists to gather valuable insights for physical processes on an atomic scale. This work explores various techniques for SIMD vectorization to improve the pairwise force calculation between molecules in the scope of the particle simulation library AutoPas. The focus lies on the order in which particle values are loaded into vector registers to achieve the most optimal performance regarding execution time or energy consumption.\n  As previous work indicates that the optimal MD algorithm can change during runtime, this paper investigates simulation-specific parameters like particle density and the impact of the neighbor identification algorithms, which distinguishes this work from related projects. Furthermore, AutoPas' dynamic tuning mechanism is extended to choose the optimal vectorization order during runtime.\n  The benchmarks show that considering different particle interaction orders during runtime can lead to a considerable performance improvement for the force calculation compared to AutoPas' previous approach.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u7d22\u4e86\u5728AutoPas\u7c92\u5b50\u6a21\u62df\u5e93\u4e2d\uff0c\u901a\u8fc7\u4e0d\u540c\u7684SIMD\u5411\u91cf\u5316\u6280\u672f\u4f18\u5316\u5206\u5b50\u95f4\u6210\u5bf9\u529b\u8ba1\u7b97\uff0c\u91cd\u70b9\u5173\u6ce8\u7c92\u5b50\u503c\u52a0\u8f7d\u5230\u5411\u91cf\u5bc4\u5b58\u5668\u7684\u987a\u5e8f\u5bf9\u6267\u884c\u65f6\u95f4\u548c\u80fd\u8017\u7684\u5f71\u54cd\u3002", "motivation": "\u5206\u5b50\u52a8\u529b\u5b66\u6a21\u62df\u5728\u539f\u5b50\u5c3a\u5ea6\u4e0a\u4e3a\u79d1\u5b66\u5bb6\u63d0\u4f9b\u6709\u4ef7\u503c\u7684\u7269\u7406\u8fc7\u7a0b\u6d1e\u5bdf\u3002\u5148\u524d\u7814\u7a76\u8868\u660e\u6700\u4f18MD\u7b97\u6cd5\u53ef\u80fd\u5728\u8fd0\u884c\u65f6\u53d1\u751f\u53d8\u5316\uff0c\u56e0\u6b64\u9700\u8981\u7814\u7a76\u6a21\u62df\u7279\u5b9a\u53c2\u6570\uff08\u5982\u7c92\u5b50\u5bc6\u5ea6\u548c\u90bb\u5c45\u8bc6\u522b\u7b97\u6cd5\uff09\u7684\u5f71\u54cd\uff0c\u5e76\u6269\u5c55AutoPas\u7684\u52a8\u6001\u8c03\u4f18\u673a\u5236\u4ee5\u5728\u8fd0\u884c\u65f6\u9009\u62e9\u6700\u4f18\u5411\u91cf\u5316\u987a\u5e8f\u3002", "method": "\u63a2\u7d22\u591a\u79cdSIMD\u5411\u91cf\u5316\u6280\u672f\uff0c\u91cd\u70b9\u7814\u7a76\u7c92\u5b50\u503c\u52a0\u8f7d\u5230\u5411\u91cf\u5bc4\u5b58\u5668\u7684\u987a\u5e8f\u4f18\u5316\u3002\u6269\u5c55AutoPas\u7684\u52a8\u6001\u8c03\u4f18\u673a\u5236\uff0c\u4f7f\u5176\u80fd\u591f\u5728\u8fd0\u884c\u65f6\u9009\u62e9\u6700\u4f18\u7684\u5411\u91cf\u5316\u987a\u5e8f\u3002\u7814\u7a76\u6a21\u62df\u7279\u5b9a\u53c2\u6570\u5982\u7c92\u5b50\u5bc6\u5ea6\u548c\u90bb\u5c45\u8bc6\u522b\u7b97\u6cd5\u7684\u5f71\u54cd\u3002", "result": "\u57fa\u51c6\u6d4b\u8bd5\u8868\u660e\uff0c\u5728\u8fd0\u884c\u65f6\u8003\u8651\u4e0d\u540c\u7684\u7c92\u5b50\u76f8\u4e92\u4f5c\u7528\u987a\u5e8f\uff0c\u76f8\u6bd4AutoPas\u5148\u524d\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u663e\u8457\u63d0\u9ad8\u529b\u8ba1\u7b97\u7684\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u52a8\u6001\u9009\u62e9\u6700\u4f18\u7684\u7c92\u5b50\u76f8\u4e92\u4f5c\u7528\u987a\u5e8f\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u5206\u5b50\u52a8\u529b\u5b66\u6a21\u62df\u4e2d\u6210\u5bf9\u529b\u8ba1\u7b97\u7684\u6027\u80fd\uff0c\u8fd9\u4e3aAutoPas\u5e93\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u4f18\u5316\u6539\u8fdb\u3002"}}
{"id": "2512.03644", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.03644", "abs": "https://arxiv.org/abs/2512.03644", "authors": ["Bohan Zhao", "Yuanhong Wang", "Chenglin Liu", "Jiagi Pan", "Guang Yang", "Ruitao Liu", "Tingrui Zhang", "Kai Luo", "Wei Xu"], "title": "FFTrainer: Fast Failover in Large-Language Model Training with Almost-Free State Management", "comment": null, "summary": "Recent developments in large language models (LLMs) have introduced new requirements for efficient and robust training. As LLM clusters scale, node failures, lengthy recoveries, and bulky checkpoints erode efficiency. Infrequent asynchronous checkpoints trigger costly rollbacks, yet higher frequencies add prohibitive overhead. To address these challenges, we propose FFTrainer, a system designed for robust LLM training. FFTrainer leverages surplus network capacity to quickly save and load states, thereby preventing rollbacks and accelerating recovery. Compared with prior checkpointing approaches, FFTrainer reduces recovery time by up to 98% and mitigates GPU utilization loss by up to 68% without hindering normal training.", "AI": {"tldr": "FFTrainer\u5229\u7528\u7f51\u7edc\u5269\u4f59\u5e26\u5bbd\u5feb\u901f\u4fdd\u5b58\u548c\u52a0\u8f7d\u72b6\u6001\uff0c\u51cf\u5c11LLM\u8bad\u7ec3\u4e2d\u7684\u56de\u6eda\u548c\u6062\u590d\u65f6\u95f4", "motivation": "\u968f\u7740LLM\u96c6\u7fa4\u89c4\u6a21\u6269\u5927\uff0c\u8282\u70b9\u6545\u969c\u3001\u6062\u590d\u65f6\u95f4\u957f\u548c\u5927\u68c0\u67e5\u70b9\u964d\u4f4e\u4e86\u8bad\u7ec3\u6548\u7387\u3002\u5f02\u6b65\u68c0\u67e5\u70b9\u8981\u4e48\u89e6\u53d1\u6602\u8d35\u7684\u56de\u6eda\uff0c\u8981\u4e48\u589e\u52a0\u8fc7\u9ad8\u5f00\u9500", "method": "FFTrainer\u5229\u7528\u5269\u4f59\u7f51\u7edc\u5bb9\u91cf\u5feb\u901f\u4fdd\u5b58\u548c\u52a0\u8f7d\u72b6\u6001\uff0c\u9632\u6b62\u56de\u6eda\u5e76\u52a0\u901f\u6062\u590d", "result": "\u76f8\u6bd4\u73b0\u6709\u68c0\u67e5\u70b9\u65b9\u6cd5\uff0cFFTrainer\u51cf\u5c11\u6062\u590d\u65f6\u95f4\u8fbe98%\uff0c\u51cf\u5c11GPU\u5229\u7528\u7387\u635f\u5931\u8fbe68%\uff0c\u4e14\u4e0d\u5f71\u54cd\u6b63\u5e38\u8bad\u7ec3", "conclusion": "FFTrainer\u901a\u8fc7\u5229\u7528\u7f51\u7edc\u5269\u4f59\u5e26\u5bbd\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u9c81\u68d2\u7684LLM\u8bad\u7ec3\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u6062\u590d\u65f6\u95f4\u548c\u8d44\u6e90\u6d6a\u8d39"}}
{"id": "2512.03697", "categories": ["cs.DC", "cs.MS"], "pdf": "https://arxiv.org/pdf/2512.03697", "abs": "https://arxiv.org/abs/2512.03697", "authors": ["Rafael Ravedutti Lucio Machado", "Jan Eitzinger", "Georg Hager", "Gerhard Wellein"], "title": "On the Challenges of Energy-Efficiency Analysis in HPC Systems: Evaluating Synthetic Benchmarks and Gromacs", "comment": "8 pages, 4 figures, conference", "summary": "This paper discusses the challenges encountered when analyzing the energy efficiency of synthetic benchmarks and the Gromacs package on the Fritz and Alex HPC clusters. Experiments were conducted using MPI parallelism on full sockets of Intel Ice Lake and Sapphire Rapids CPUs, as well as Nvidia A40 and A100 GPUs. The metrics and measurements obtained with the Likwid and Nvidia profiling tools are presented, along with the results. The challenges and pitfalls encountered during experimentation and analysis are revealed and discussed. Best practices for future energy efficiency analysis studies are suggested.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u5728Fritz\u548cAlex HPC\u96c6\u7fa4\u4e0a\u5206\u6790\u5408\u6210\u57fa\u51c6\u6d4b\u8bd5\u548cGromacs\u8f6f\u4ef6\u5305\u80fd\u6548\u65f6\u9047\u5230\u7684\u6311\u6218\uff0c\u4f7f\u7528Intel Ice Lake/Sapphire Rapids CPU\u548cNvidia A40/A100 GPU\u8fdb\u884c\u5b9e\u9a8c\uff0c\u63ed\u793a\u4e86\u5b9e\u9a8c\u548c\u5206\u6790\u4e2d\u7684\u95ee\u9898\u5e76\u63d0\u51fa\u4e86\u6700\u4f73\u5b9e\u8df5\u5efa\u8bae\u3002", "motivation": "\u7814\u7a76\u9ad8\u6027\u80fd\u8ba1\u7b97\u96c6\u7fa4\u4e2d\u80fd\u6548\u5206\u6790\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u4f7f\u7528\u4e0d\u540c\u786c\u4ef6\u67b6\u6784\uff08CPU\u548cGPU\uff09\u548c\u5e76\u884c\u7f16\u7a0b\u6a21\u578b\uff08MPI\uff09\u65f6\uff0c\u4e3a\u672a\u6765\u7684\u80fd\u6548\u7814\u7a76\u63d0\u4f9b\u6307\u5bfc\u3002", "method": "\u5728Fritz\u548cAlex HPC\u96c6\u7fa4\u4e0a\u4f7f\u7528MPI\u5e76\u884c\u5316\uff0c\u5728\u5b8c\u6574\u7684Intel Ice Lake\u548cSapphire Rapids CPU\u63d2\u69fd\u4ee5\u53caNvidia A40\u548cA100 GPU\u4e0a\u8fd0\u884c\u5408\u6210\u57fa\u51c6\u6d4b\u8bd5\u548cGromacs\u8f6f\u4ef6\u5305\uff0c\u4f7f\u7528Likwid\u548cNvidia\u6027\u80fd\u5206\u6790\u5de5\u5177\u8fdb\u884c\u6d4b\u91cf\u3002", "result": "\u5c55\u793a\u4e86\u4f7f\u7528Likwid\u548cNvidia\u5206\u6790\u5de5\u5177\u83b7\u5f97\u7684\u6307\u6807\u548c\u6d4b\u91cf\u7ed3\u679c\uff0c\u63ed\u793a\u4e86\u5728\u5b9e\u9a8c\u548c\u5206\u6790\u8fc7\u7a0b\u4e2d\u9047\u5230\u7684\u5404\u79cd\u6311\u6218\u548c\u9677\u9631\uff0c\u5305\u62ec\u6d4b\u91cf\u51c6\u786e\u6027\u3001\u5de5\u5177\u4f7f\u7528\u590d\u6742\u6027\u7b49\u95ee\u9898\u3002", "conclusion": "\u63d0\u51fa\u4e86\u672a\u6765\u80fd\u6548\u5206\u6790\u7814\u7a76\u7684\u6700\u4f73\u5b9e\u8df5\u5efa\u8bae\uff0c\u5f3a\u8c03\u4e86\u5728\u9ad8\u6027\u80fd\u8ba1\u7b97\u73af\u5883\u4e2d\u8fdb\u884c\u80fd\u6548\u8bc4\u4f30\u65f6\u9700\u8981\u7279\u522b\u6ce8\u610f\u7684\u65b9\u6cd5\u8bba\u548c\u6280\u672f\u6311\u6218\u3002"}}
{"id": "2512.03825", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.03825", "abs": "https://arxiv.org/abs/2512.03825", "authors": ["Aingeru Ramos", "Jose A Pascual", "Javier Navaridas", "Ivan Coluzza"], "title": "Acceleration of Parallel Tempering for Markov Chain Monte Carlo methods", "comment": "14 pages, 7 figures (5 of them composed by 2 subfigures)", "summary": "Markov Chain Monte Carlo methods are algorithms used to sample probability distributions, commonly used to sample the Boltzmann distribution of physical/chemical models (e.g., protein folding, Ising model, etc.). This allows us to study their properties by sampling the most probable states of those systems. However, the sampling capabilities of these methods are not sufficiently accurate when handling complex configuration spaces. This has resulted in the development of new techniques that improve sampling accuracy, usually at the expense of increasing the computational cost. One of such techniques is Parallel Tempering which improves accuracy by running several replicas which periodically exchange their states. Computationally, this imposes a significant slow-down, which can be counteracted by means of parallelization. These schemes enable MCMC/PT techniques to be run more effectively and allow larger models to be studied. In this work, we present a parallel implementation of Metropolis-Hastings with Parallel Tempering, using OpenMP and CUDA for the parallelization in modern CPUs and GPUs, respectively. The results show a maximum speed-up of 52x using OpenMP with 48 cores, and of 986x speed-up with the CUDA version. Furthermore, the results serve as a basic benchmark to compare a future quantum implementation of the same algorithm.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5e76\u884c\u5316\u7684Metropolis-Hastings\u5e76\u884c\u56de\u706b\u7b97\u6cd5\u5b9e\u73b0\uff0c\u4f7f\u7528OpenMP\u548cCUDA\u5206\u522b\u5728CPU\u548cGPU\u4e0a\u5b9e\u73b0\u5e76\u884c\u5316\uff0c\u83b7\u5f97\u4e86\u6700\u9ad852\u500d\u548c986\u500d\u7684\u52a0\u901f\u6bd4\u3002", "motivation": "\u4f20\u7edfMCMC\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u6784\u578b\u7a7a\u95f4\u65f6\u91c7\u6837\u7cbe\u5ea6\u4e0d\u8db3\uff0c\u800c\u5e76\u884c\u56de\u706b\u6280\u672f\u867d\u7136\u63d0\u9ad8\u4e86\u7cbe\u5ea6\u4f46\u663e\u8457\u589e\u52a0\u4e86\u8ba1\u7b97\u6210\u672c\u3002\u901a\u8fc7\u5e76\u884c\u5316\u53ef\u4ee5\u62b5\u6d88\u8fd9\u79cd\u8ba1\u7b97\u5f00\u9500\uff0c\u4f7fMCMC/PT\u6280\u672f\u66f4\u6709\u6548\u5730\u8fd0\u884c\u5e76\u7814\u7a76\u66f4\u5927\u89c4\u6a21\u7684\u6a21\u578b\u3002", "method": "\u5f00\u53d1\u4e86Metropolis-Hastings\u5e76\u884c\u56de\u706b\u7b97\u6cd5\u7684\u5e76\u884c\u5b9e\u73b0\uff0c\u4f7f\u7528OpenMP\u5728\u591a\u6838CPU\u4e0a\u5b9e\u73b0\u5e76\u884c\u5316\uff0c\u4f7f\u7528CUDA\u5728GPU\u4e0a\u5b9e\u73b0\u5e76\u884c\u5316\u3002\u8be5\u5b9e\u73b0\u4f5c\u4e3a\u672a\u6765\u91cf\u5b50\u7b97\u6cd5\u5b9e\u73b0\u7684\u57fa\u7840\u57fa\u51c6\u3002", "result": "\u4f7f\u7528OpenMP\u572848\u6838CPU\u4e0a\u83b7\u5f97\u4e86\u6700\u9ad852\u500d\u7684\u52a0\u901f\u6bd4\uff0c\u4f7f\u7528CUDA\u5728GPU\u4e0a\u83b7\u5f97\u4e86\u6700\u9ad8986\u500d\u7684\u52a0\u901f\u6bd4\u3002\u8be5\u7ed3\u679c\u53ef\u4f5c\u4e3a\u672a\u6765\u91cf\u5b50\u5b9e\u73b0\u6bd4\u8f83\u7684\u57fa\u51c6\u3002", "conclusion": "\u63d0\u51fa\u7684\u5e76\u884c\u5316\u5b9e\u73b0\u663e\u8457\u63d0\u5347\u4e86MCMC/PT\u7b97\u6cd5\u7684\u8ba1\u7b97\u6548\u7387\uff0c\u4e3a\u7814\u7a76\u66f4\u5927\u89c4\u6a21\u7269\u7406/\u5316\u5b66\u6a21\u578b\u63d0\u4f9b\u4e86\u6709\u6548\u5de5\u5177\uff0c\u5e76\u4e3a\u672a\u6765\u91cf\u5b50\u5b9e\u73b0\u5efa\u7acb\u4e86\u6027\u80fd\u57fa\u51c6\u3002"}}
{"id": "2512.03927", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.03927", "abs": "https://arxiv.org/abs/2512.03927", "authors": ["Liujianfu Wang", "Yuyang Du", "Yuchen Pan", "Soung Chang Liew", "Jiacheng Liu", "Kexin Chen"], "title": "OD-MoE: On-Demand Expert Loading for Cacheless Edge-Distributed MoE Inference", "comment": null, "summary": "Mixture-of-Experts (MoE), while offering significant advantages as a Large Language Model (LLM) architecture, faces substantial challenges when deployed on low-cost edge devices with tight memory constraints. Expert offloading mitigates this issue by storing expert parameters in CPU memory and caching a subset of popular experts in GPU memory. Although this approach improves GPU memory utilization by caching only the likely-used experts, the GPU memory reserved for expert caching is underutilized compared with dense LLMs. This paper presents OD-MoE, a distributed MoE inference framework that obviates the need for expert caches via fully on-demand expert loading. OD-MoE is built upon two key mechanisms: 1) parallelizing expert loading and expert computation across distributed edge nodes, and 2) an ultra-accurate emulative predictor that forecasts expert activations multiple layers ahead while expert computation is ongoing. With these innovations, OD-MoE dynamically loads each target expert to one of the distributed nodes just-in-time before its activation and promptly evicts it afterward, freeing GPU memory for subsequent experts. We comprehensively benchmark OD-MoE against state-of-the-art MoE offloading systems on a ten-node testbed. Experimental results show that: 1) OD-MoE achieves 99.94% expert activation prediction accuracy, substantially surpassing all existing methods; and 2) OD-MoE delivers approximately 75% of the decoding speed of a fully GPU-cached MoE deployment while using only 1/3 of the GPU memory. More importantly, by eliminating the need for expert caches, OD-MoE enables MoE inference on edge nodes with less-than-1GB GPU memory, paving the way for practical MoE deployment of low-cost IoT devices at the edge in the LLM era.", "AI": {"tldr": "OD-MoE\uff1a\u4e00\u79cd\u5206\u5e03\u5f0fMoE\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u6309\u9700\u52a0\u8f7d\u4e13\u5bb6\u53c2\u6570\uff0c\u65e0\u9700\u4e13\u5bb6\u7f13\u5b58\uff0c\u4f7fMoE\u6a21\u578b\u80fd\u5728GPU\u5185\u5b58\u5c0f\u4e8e1GB\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\u8fd0\u884c", "motivation": "MoE\u6a21\u578b\u5728\u5185\u5b58\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\u90e8\u7f72\u56f0\u96be\uff0c\u73b0\u6709\u4e13\u5bb6\u5378\u8f7d\u65b9\u6cd5\u867d\u7136\u5c06\u4e13\u5bb6\u53c2\u6570\u5b58\u50a8\u5728CPU\u5185\u5b58\u4e2d\uff0c\u4f46GPU\u5185\u5b58\u4e2d\u7684\u4e13\u5bb6\u7f13\u5b58\u5229\u7528\u7387\u4ecd\u7136\u8f83\u4f4e\uff0c\u9700\u8981\u66f4\u597d\u7684\u89e3\u51b3\u65b9\u6848", "method": "\u63d0\u51faOD-MoE\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u5173\u952e\u6280\u672f\uff1a1\uff09\u5728\u5206\u5e03\u5f0f\u8fb9\u7f18\u8282\u70b9\u4e0a\u5e76\u884c\u5316\u4e13\u5bb6\u52a0\u8f7d\u548c\u4e13\u5bb6\u8ba1\u7b97\uff1b2\uff09\u8d85\u51c6\u786e\u7684\u6a21\u62df\u9884\u6d4b\u5668\uff0c\u5728\u4e13\u5bb6\u8ba1\u7b97\u8fdb\u884c\u65f6\u63d0\u524d\u591a\u5c42\u9884\u6d4b\u4e13\u5bb6\u6fc0\u6d3b", "result": "OD-MoE\u5b9e\u73b0\u4e8699.94%\u7684\u4e13\u5bb6\u6fc0\u6d3b\u9884\u6d4b\u51c6\u786e\u7387\uff0c\u5728\u4ec5\u4f7f\u75281/3 GPU\u5185\u5b58\u7684\u60c5\u51b5\u4e0b\uff0c\u8fbe\u5230\u5b8c\u5168GPU\u7f13\u5b58MoE\u90e8\u7f72\u7ea675%\u7684\u89e3\u7801\u901f\u5ea6\uff0c\u4f7fMoE\u80fd\u5728\u5c0f\u4e8e1GB GPU\u5185\u5b58\u7684\u8fb9\u7f18\u8282\u70b9\u4e0a\u63a8\u7406", "conclusion": "OD-MoE\u901a\u8fc7\u6d88\u9664\u4e13\u5bb6\u7f13\u5b58\u9700\u6c42\uff0c\u4e3a\u4f4e\u6210\u672c\u7269\u8054\u7f51\u8bbe\u5907\u5728\u8fb9\u7f18\u90e8\u7f72MoE\u6a21\u578b\u94fa\u5e73\u4e86\u9053\u8def\uff0c\u89e3\u51b3\u4e86\u8fb9\u7f18\u8bbe\u5907\u5185\u5b58\u9650\u5236\u7684\u5173\u952e\u6311\u6218"}}
{"id": "2512.04054", "categories": ["cs.DC", "cs.NI"], "pdf": "https://arxiv.org/pdf/2512.04054", "abs": "https://arxiv.org/abs/2512.04054", "authors": ["Olasupo Ajayi", "Ryan Grant"], "title": "A Chronological Analysis of the Evolution of SmartNICs", "comment": "8 pages, 13 figures, 2 tables, Southern Africa Telecommunication Networks and Applications Conference (SATNAC) 2025", "summary": "Network Interface Cards (NICs) are one of the key enablers of the modern Internet. They serve as gateways for connecting computing devices to networks for the exchange of data with other devices. Recently, the pervasive nature of Internet-enabled devices coupled with the growing demands for faster network access have necessitated the enhancement of NICs to Smart NICs (SNICs), capable of processing enormous volumes of data at near real-time speed. However, despite their popularity, the exact use and applicability of SNICs remains an ongoing debate. These debates are exacerbated by the incorporation of accelerators into SNIC, allowing them to relieve their host's CPUs of various tasks. In this work, we carry out a chronological analysis of SNICs, using 370 articles published in the past 15 years, from 2010 to 2024, to gain some insight into SNICs; and shed some light on their evolution, manufacturers, use cases, and application domains.", "AI": {"tldr": "\u672c\u6587\u5bf9\u667a\u80fd\u7f51\u5361\uff08SNICs\uff09\u8fdb\u884c\u4e86\u5386\u65f6\u6027\u5206\u6790\uff0c\u901a\u8fc72009-2024\u5e74\u95f4370\u7bc7\u6587\u732e\u7684\u7cfb\u7edf\u56de\u987e\uff0c\u63a2\u8ba8\u4e86SNICs\u7684\u6f14\u53d8\u3001\u5236\u9020\u5546\u3001\u4f7f\u7528\u573a\u666f\u548c\u5e94\u7528\u9886\u57df\uff0c\u65e8\u5728\u5398\u6e05\u5176\u5b9e\u9645\u7528\u9014\u548c\u9002\u7528\u6027\u3002", "motivation": "\u968f\u7740\u7269\u8054\u7f51\u8bbe\u5907\u7684\u666e\u53ca\u548c\u5bf9\u9ad8\u901f\u7f51\u7edc\u8bbf\u95ee\u9700\u6c42\u7684\u589e\u957f\uff0c\u4f20\u7edf\u7f51\u5361\u5df2\u5347\u7ea7\u4e3a\u667a\u80fd\u7f51\u5361\uff08SNICs\uff09\uff0c\u80fd\u591f\u5904\u7406\u6d77\u91cf\u6570\u636e\u5e76\u5b9e\u73b0\u8fd1\u5b9e\u65f6\u5904\u7406\u3002\u7136\u800c\uff0c\u5c3d\u7ba1SNICs\u65e5\u76ca\u6d41\u884c\uff0c\u5176\u786e\u5207\u7528\u9014\u548c\u9002\u7528\u6027\u4ecd\u5b58\u5728\u4e89\u8bae\uff0c\u7279\u522b\u662f\u968f\u7740\u52a0\u901f\u5668\u7684\u96c6\u6210\u4f7fSNICs\u80fd\u591f\u5206\u62c5\u4e3b\u673aCPU\u4efb\u52a1\uff0c\u8fd9\u4e9b\u4e89\u8bae\u66f4\u52a0\u590d\u6742\u5316\u3002", "method": "\u91c7\u7528\u5386\u65f6\u6027\u5206\u6790\u65b9\u6cd5\uff0c\u7cfb\u7edf\u56de\u987e\u4e862009\u5e74\u81f32024\u5e7415\u5e74\u95f4\u53d1\u8868\u7684370\u7bc7\u76f8\u5173\u6587\u732e\uff0c\u5bf9SNICs\u7684\u6f14\u53d8\u8fc7\u7a0b\u3001\u4e3b\u8981\u5236\u9020\u5546\u3001\u5b9e\u9645\u4f7f\u7528\u573a\u666f\u548c\u5e94\u7528\u9886\u57df\u8fdb\u884c\u4e86\u5168\u9762\u68b3\u7406\u548c\u5206\u6790\u3002", "result": "\u901a\u8fc7\u5bf9\u5927\u91cf\u6587\u732e\u7684\u5206\u6790\uff0c\u63ed\u793a\u4e86SNICs\u7684\u53d1\u5c55\u8f68\u8ff9\u3001\u4e3b\u8981\u5236\u9020\u5546\u683c\u5c40\u3001\u5177\u4f53\u4f7f\u7528\u573a\u666f\u4ee5\u53ca\u5728\u4e0d\u540c\u5e94\u7528\u9886\u57df\u7684\u9002\u7528\u6027\uff0c\u4e3a\u7406\u89e3SNICs\u7684\u5b9e\u9645\u4ef7\u503c\u548c\u9002\u7528\u8303\u56f4\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u4f9d\u636e\u3002", "conclusion": "\u672c\u7814\u7a76\u901a\u8fc7\u7cfb\u7edf\u6587\u732e\u56de\u987e\uff0c\u9610\u660e\u4e86\u667a\u80fd\u7f51\u5361\u7684\u53d1\u5c55\u5386\u7a0b\u3001\u6280\u672f\u7279\u70b9\u548c\u5e94\u7528\u73b0\u72b6\uff0c\u4e3a\u5b66\u672f\u754c\u548c\u5de5\u4e1a\u754c\u7406\u89e3SNICs\u7684\u5b9e\u9645\u7528\u9014\u548c\u672a\u6765\u53d1\u5c55\u65b9\u5411\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\uff0c\u6709\u52a9\u4e8e\u89e3\u51b3\u5f53\u524d\u5173\u4e8eSNICs\u9002\u7528\u6027\u7684\u4e89\u8bae\u3002"}}
