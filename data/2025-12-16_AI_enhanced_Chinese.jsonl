{"id": "2512.11826", "categories": ["cs.AR", "eess.IV"], "pdf": "https://arxiv.org/pdf/2512.11826", "abs": "https://arxiv.org/abs/2512.11826", "authors": ["Weihong Xu", "Chang Eun Song", "Haichao Yang", "Leo Liu", "Meng-Fan Chang", "Carlos H. Diaz", "Tajana Rosing", "Mingu Kang"], "title": "FSL-HDnn: A 40 nm Few-shot On-Device Learning Accelerator with Integrated Feature Extraction and Hyperdimensional Computing", "comment": null, "summary": "This paper introduces FSL-HDnn, an energy-efficient accelerator that implements the end-to-end pipeline of feature extraction and on-device few-shot learning (FSL). The accelerator addresses fundamental challenges of on-device learning (ODL) for resource-constrained edge applications through two synergistic modules: a parameter-efficient feature extractor employing weight clustering and an FSL classifier based on hyperdimensional computing (HDC). The feature extractor exploits the weight clustering mechanism to reduce computational complexity, while the HDC-based FSL classifier eliminates gradient-based back propagation operations, enabling single-pass training with substantially reduced latency. Additionally, FSL-HDnn enables low-latency ODL and inference via two proposed optimization strategies, including an early-exit mechanism with branch feature extraction and batched single-pass training that improves hardware utilization. Measurement results demonstrate that our chip fabricated in a 40 nm CMOS process delivers superior training energy efficiency of 6 mJ/image and end-to-end training throughput of 28 images/s on a 10-way 5-shot FSL task. The end-to-end training latency is also reduced by 2x to 20.9x compared to state-of-the-art ODL chips.", "AI": {"tldr": "FSL-HDnn\u662f\u4e00\u79cd\u80fd\u6548\u4f18\u5316\u7684\u52a0\u901f\u5668\uff0c\u901a\u8fc7\u6743\u91cd\u805a\u7c7b\u7279\u5f81\u63d0\u53d6\u5668\u548c\u8d85\u7ef4\u5ea6\u8ba1\u7b97\u5206\u7c7b\u5668\u5b9e\u73b0\u7aef\u5230\u7aef\u5c11\u6837\u672c\u5b66\u4e60\uff0c\u572840nm\u5de5\u827a\u4e0b\u8fbe\u52306mJ/\u56fe\u50cf\u7684\u8bad\u7ec3\u80fd\u6548\u3002", "motivation": "\u89e3\u51b3\u8d44\u6e90\u53d7\u9650\u8fb9\u7f18\u8bbe\u5907\u4e0a\u8bbe\u5907\u7aef\u5b66\u4e60\uff08ODL\uff09\u7684\u57fa\u672c\u6311\u6218\uff0c\u5305\u62ec\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\u3001\u5ef6\u8fdf\u5927\u548c\u80fd\u6548\u4f4e\u7684\u95ee\u9898\u3002", "method": "\u91c7\u7528\u4e24\u4e2a\u534f\u540c\u6a21\u5757\uff1a1\uff09\u57fa\u4e8e\u6743\u91cd\u805a\u7c7b\u7684\u53c2\u6570\u9ad8\u6548\u7279\u5f81\u63d0\u53d6\u5668\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\uff1b2\uff09\u57fa\u4e8e\u8d85\u7ef4\u5ea6\u8ba1\u7b97\uff08HDC\uff09\u7684\u5c11\u6837\u672c\u5b66\u4e60\u5206\u7c7b\u5668\u6d88\u9664\u68af\u5ea6\u53cd\u5411\u4f20\u64ad\uff0c\u5b9e\u73b0\u5355\u6b21\u8bad\u7ec3\u3002\u8fd8\u63d0\u51fa\u5206\u652f\u7279\u5f81\u63d0\u53d6\u7684\u65e9\u9000\u673a\u5236\u548c\u6279\u5904\u7406\u5355\u6b21\u8bad\u7ec3\u4f18\u5316\u7b56\u7565\u3002", "result": "\u572840nm CMOS\u5de5\u827a\u4e0b\uff0c\u82af\u7247\u572810-way 5-shot\u5c11\u6837\u672c\u5b66\u4e60\u4efb\u52a1\u4e0a\u5b9e\u73b06mJ/\u56fe\u50cf\u7684\u8bad\u7ec3\u80fd\u6548\u548c28\u56fe\u50cf/\u79d2\u7684\u7aef\u5230\u7aef\u8bad\u7ec3\u541e\u5410\u91cf\u3002\u7aef\u5230\u7aef\u8bad\u7ec3\u5ef6\u8fdf\u6bd4\u73b0\u6709ODL\u82af\u7247\u964d\u4f4e2-20.9\u500d\u3002", "conclusion": "FSL-HDnn\u901a\u8fc7\u521b\u65b0\u7684\u6743\u91cd\u805a\u7c7b\u7279\u5f81\u63d0\u53d6\u548cHDC\u5206\u7c7b\u5668\u67b6\u6784\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5c11\u6837\u672c\u5b66\u4e60\u7684\u80fd\u6548\u548c\u5ef6\u8fdf\u6027\u80fd\uff0c\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u8bbe\u5907\u7aef\u5b66\u4e60\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.12106", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2512.12106", "abs": "https://arxiv.org/abs/2512.12106", "authors": ["Victor Cai", "Jennifer Zhou", "Haebin Do", "David Brooks", "Gu-Yeon Wei"], "title": "DreamRAM: A Fine-Grained Configurable Design Space Modeling Tool for Custom 3D Die-Stacked DRAM", "comment": "Design, Automation and Test in Europe Conference (DATE 2026)", "summary": "3D die-stacked DRAM has emerged as a key technology for delivering high bandwidth and high density for applications such as high-performance computing, graphics, and machine learning. However, different applications place diverse and sometimes diverging demands on power, performance, and area that cannot be universally satisfied with fixed commodity DRAM designs. Die stacking creates the opportunity for a large DRAM design space through 3D integration and expanded total die area. To open and navigate this expansive design space of customized memory architectures that cater to application-specific needs, we introduce DreamRAM, a configurable bandwidth, capacity, energy, latency, and area modeling tool for custom 3D die-stacked DRAM designs. DreamRAM exposes fine-grained design customization parameters at the MAT, subarray, bank, and inter-bank levels, including extensions of partial page and subarray parallelism proposals found in the literature, to open a large previously-unexplored design space. DreamRAM analytically models wire pitch, width, length, capacitance, and scaling parameters to capture the performance tradeoffs of physical layout and routing design choices. Routing awareness enables DreamRAM to model a custom MAT-level routing scheme, Dataline-Over-MAT (DLOMAT), to facilitate better bandwidth tradeoffs. DreamRAM is calibrated and validated against published industry HBM3 and HBM2E designs. Within DreamRAM's rich design space, we identify designs that achieve each of 66% higher bandwidth, 100% higher capacity, and 45% lower power and energy per bit compared to the baseline design, each on an iso-bandwidth, iso-capacity, and iso-power basis.", "AI": {"tldr": "DreamRAM\u662f\u4e00\u4e2a\u53ef\u914d\u7f6e\u76843D\u5806\u53e0DRAM\u5efa\u6a21\u5de5\u5177\uff0c\u7528\u4e8e\u5b9a\u5236\u5316\u5185\u5b58\u67b6\u6784\u8bbe\u8ba1\uff0c\u652f\u6301\u5e26\u5bbd\u3001\u5bb9\u91cf\u3001\u80fd\u8017\u3001\u5ef6\u8fdf\u548c\u9762\u79ef\u7b49\u591a\u7ef4\u5ea6\u53c2\u6570\u914d\u7f6e\uff0c\u80fd\u591f\u63a2\u7d22\u4f20\u7edf\u56fa\u5b9a\u8bbe\u8ba1\u65e0\u6cd5\u6ee1\u8db3\u7684\u591a\u6837\u5316\u5e94\u7528\u9700\u6c42\u3002", "motivation": "3D\u5806\u53e0DRAM\u5df2\u6210\u4e3a\u9ad8\u6027\u80fd\u8ba1\u7b97\u3001\u56fe\u5f62\u548c\u673a\u5668\u5b66\u4e60\u7b49\u5e94\u7528\u7684\u5173\u952e\u6280\u672f\uff0c\u4f46\u4e0d\u540c\u5e94\u7528\u5bf9\u529f\u8017\u3001\u6027\u80fd\u548c\u9762\u79ef\u7684\u9700\u6c42\u5dee\u5f02\u5f88\u5927\uff0c\u56fa\u5b9a\u7684\u5546\u54c1\u5316DRAM\u8bbe\u8ba1\u65e0\u6cd5\u6ee1\u8db3\u8fd9\u4e9b\u591a\u6837\u5316\u9700\u6c42\u3002\u5806\u53e0\u6280\u672f\u901a\u8fc73D\u96c6\u6210\u548c\u6269\u5c55\u7684\u603b\u82af\u7247\u9762\u79ef\u4e3aDRAM\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u5de8\u5927\u7684\u8bbe\u8ba1\u7a7a\u95f4\u3002", "method": "DreamRAM\u662f\u4e00\u4e2a\u53ef\u914d\u7f6e\u7684\u5efa\u6a21\u5de5\u5177\uff0c\u5728MAT\u3001\u5b50\u9635\u5217\u3001bank\u548cbank\u95f4\u7ea7\u522b\u66b4\u9732\u7ec6\u7c92\u5ea6\u8bbe\u8ba1\u53c2\u6570\uff0c\u5305\u62ec\u6269\u5c55\u6587\u732e\u4e2d\u7684\u90e8\u5206\u9875\u9762\u548c\u5b50\u9635\u5217\u5e76\u884c\u6027\u65b9\u6848\u3002\u5de5\u5177\u901a\u8fc7\u5206\u6790\u5efa\u6a21\u7ebf\u95f4\u8ddd\u3001\u5bbd\u5ea6\u3001\u957f\u5ea6\u3001\u7535\u5bb9\u548c\u7f29\u653e\u53c2\u6570\u6765\u6355\u6349\u7269\u7406\u5e03\u5c40\u548c\u5e03\u7ebf\u8bbe\u8ba1\u9009\u62e9\u7684\u6027\u80fd\u6743\u8861\u3002\u8def\u7531\u611f\u77e5\u80fd\u529b\u4f7fDreamRAM\u80fd\u591f\u5efa\u6a21\u81ea\u5b9a\u4e49\u7684MAT\u7ea7\u8def\u7531\u65b9\u6848Dataline-Over-MAT\uff08DLOMAT\uff09\u3002", "result": "DreamRAM\u5df2\u9488\u5bf9\u884c\u4e1aHBM3\u548cHBM2E\u8bbe\u8ba1\u8fdb\u884c\u6821\u51c6\u548c\u9a8c\u8bc1\u3002\u5728\u8be5\u5de5\u5177\u4e30\u5bcc\u7684\u8bbe\u8ba1\u7a7a\u95f4\u4e2d\uff0c\u8bc6\u522b\u51fa\u76f8\u6bd4\u57fa\u7ebf\u8bbe\u8ba1\u5206\u522b\u5b9e\u73b066%\u66f4\u9ad8\u5e26\u5bbd\u3001100%\u66f4\u9ad8\u5bb9\u91cf\u4ee5\u53ca\u6bcf\u6bd4\u7279\u529f\u8017\u548c\u80fd\u8017\u964d\u4f4e45%\u7684\u8bbe\u8ba1\u65b9\u6848\uff0c\u5206\u522b\u5728\u7b49\u5e26\u5bbd\u3001\u7b49\u5bb9\u91cf\u548c\u7b49\u529f\u8017\u57fa\u7840\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002", "conclusion": "DreamRAM\u4e3a\u5b9a\u5236\u53163D\u5806\u53e0DRAM\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f3a\u5927\u7684\u5efa\u6a21\u5de5\u5177\uff0c\u80fd\u591f\u63a2\u7d22\u4f20\u7edf\u56fa\u5b9a\u8bbe\u8ba1\u65e0\u6cd5\u6ee1\u8db3\u7684\u591a\u6837\u5316\u5e94\u7528\u9700\u6c42\uff0c\u901a\u8fc7\u7ec6\u7c92\u5ea6\u53c2\u6570\u914d\u7f6e\u548c\u8def\u7531\u611f\u77e5\u5efa\u6a21\uff0c\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u548c\u529f\u8017\u4f18\u5316\u3002"}}
{"id": "2512.12847", "categories": ["cs.AR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12847", "abs": "https://arxiv.org/abs/2512.12847", "authors": ["Jonathan Herbst", "Michael Pellauer", "Sherief Reda"], "title": "HaShiFlex: A High-Throughput Hardened Shifter DNN Accelerator with Fine-Tuning Flexibility", "comment": "12 pages, 6 figures, 5 tables", "summary": "We introduce a high-throughput neural network accelerator that embeds most network layers directly in hardware, minimizing data transfer and memory usage while preserving a degree of flexibility via a small neural processing unit for the final classification layer. By leveraging power-of-two (Po2) quantization for weights, we replace multiplications with simple rewiring, effectively reducing each convolution to a series of additions. This streamlined approach offers high-throughput, energy-efficient processing, making it highly suitable for applications where model parameters remain stable, such as continuous sensing tasks at the edge or large-scale data center deployments. Furthermore, by including a strategically chosen reprogrammable final layer, our design achieves high throughput without sacrificing fine-tuning capabilities. We implement this accelerator in a 7nm ASIC flow using MobileNetV2 as a baseline and report throughput, area, accuracy, and sensitivity to quantization and pruning - demonstrating both the advantages and potential trade-offs of the proposed architecture. We find that for MobileNetV2, we can improve inference throughput by 20x over fully programmable GPUs, processing 1.21 million images per second through a full forward pass while retaining fine-tuning flexibility. If absolutely no post-deployment fine tuning is required, this advantage increases to 67x at 4 million images per second.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u541e\u5410\u91cf\u795e\u7ecf\u7f51\u7edc\u52a0\u901f\u5668\uff0c\u901a\u8fc7\u5c06\u5927\u90e8\u5206\u7f51\u7edc\u5c42\u76f4\u63a5\u5d4c\u5165\u786c\u4ef6\u3001\u91c7\u7528Po2\u91cf\u5316\u6743\u91cd\u3001\u4ee5\u53ca\u53ef\u7f16\u7a0b\u6700\u7ec8\u5c42\uff0c\u5b9e\u73b0\u4e8620-67\u500d\u4e8eGPU\u7684\u63a8\u7406\u541e\u5410\u91cf\u63d0\u5347\u3002", "motivation": "\u9488\u5bf9\u8fb9\u7f18\u8fde\u7eed\u611f\u77e5\u6216\u6570\u636e\u4e2d\u5fc3\u90e8\u7f72\u7b49\u6a21\u578b\u53c2\u6570\u7a33\u5b9a\u7684\u5e94\u7528\u573a\u666f\uff0c\u9700\u8981\u9ad8\u541e\u5410\u91cf\u3001\u9ad8\u80fd\u6548\u7684\u795e\u7ecf\u7f51\u7edc\u63a8\u7406\u52a0\u901f\u65b9\u6848\uff0c\u540c\u65f6\u4fdd\u6301\u4e00\u5b9a\u7684\u7075\u6d3b\u6027\u4ee5\u9002\u5e94\u53ef\u80fd\u7684\u5fae\u8c03\u9700\u6c42\u3002", "method": "1) \u5c06\u5927\u90e8\u5206\u7f51\u7edc\u5c42\u76f4\u63a5\u786c\u4ef6\u5316\uff0c\u51cf\u5c11\u6570\u636e\u4f20\u8f93\u548c\u5185\u5b58\u4f7f\u7528\uff1b2) \u91c7\u7528\u5e42\u6b21\u4e8c(Po2)\u91cf\u5316\u6743\u91cd\uff0c\u7528\u52a0\u6cd5\u66ff\u4ee3\u4e58\u6cd5\uff1b3) \u4fdd\u7559\u5c0f\u578b\u795e\u7ecf\u5904\u7406\u5355\u5143\u4f5c\u4e3a\u53ef\u7f16\u7a0b\u6700\u7ec8\u5206\u7c7b\u5c42\uff1b4) \u57fa\u4e8eMobileNetV2\u57287nm ASIC\u6d41\u7a0b\u4e2d\u5b9e\u73b0\u3002", "result": "\u5728MobileNetV2\u4e0a\uff0c\u76f8\u6bd4\u5b8c\u5168\u53ef\u7f16\u7a0bGPU\uff0c\u63a8\u7406\u541e\u5410\u91cf\u63d0\u534720\u500d\uff08121\u4e07\u56fe\u50cf/\u79d2\uff09\uff0c\u4fdd\u7559\u5fae\u8c03\u7075\u6d3b\u6027\uff1b\u82e5\u65e0\u9700\u90e8\u7f72\u540e\u5fae\u8c03\uff0c\u541e\u5410\u91cf\u63d0\u5347\u53ef\u8fbe67\u500d\uff08400\u4e07\u56fe\u50cf/\u79d2\uff09\u3002", "conclusion": "\u8be5\u786c\u4ef6\u5316\u52a0\u901f\u5668\u8bbe\u8ba1\u5728\u4fdd\u6301\u4e00\u5b9a\u7075\u6d3b\u6027\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u795e\u7ecf\u7f51\u7edc\u63a8\u7406\u7684\u541e\u5410\u91cf\u548c\u80fd\u6548\uff0c\u7279\u522b\u9002\u7528\u4e8e\u6a21\u578b\u53c2\u6570\u7a33\u5b9a\u7684\u5e94\u7528\u573a\u666f\uff0c\u5c55\u793a\u4e86\u786c\u4ef6\u5316\u4e0e\u53ef\u7f16\u7a0b\u6027\u7ed3\u5408\u7684\u4f18\u8d8a\u6027\u3002"}}
{"id": "2512.12850", "categories": ["cs.AR", "cs.LG", "eess.SY", "hep-ex"], "pdf": "https://arxiv.org/pdf/2512.12850", "abs": "https://arxiv.org/abs/2512.12850", "authors": ["Duc Hoang", "Aarush Gupta", "Philip Harris"], "title": "KANEL\u00c9: Kolmogorov-Arnold Networks for Efficient LUT-based Evaluation", "comment": "International Symposium on Field-Programmable Gate Arrays 2026 (ISFPGA'2026)", "summary": "Low-latency, resource-efficient neural network inference on FPGAs is essential for applications demanding real-time capability and low power. Lookup table (LUT)-based neural networks are a common solution, combining strong representational power with efficient FPGA implementation. In this work, we introduce KANEL\u00c9, a framework that exploits the unique properties of Kolmogorov-Arnold Networks (KANs) for FPGA deployment. Unlike traditional multilayer perceptrons (MLPs), KANs employ learnable one-dimensional splines with fixed domains as edge activations, a structure naturally suited to discretization and efficient LUT mapping. We present the first systematic design flow for implementing KANs on FPGAs, co-optimizing training with quantization and pruning to enable compact, high-throughput, and low-latency KAN architectures. Our results demonstrate up to a 2700x speedup and orders of magnitude resource savings compared to prior KAN-on-FPGA approaches. Moreover, KANEL\u00c9 matches or surpasses other LUT-based architectures on widely used benchmarks, particularly for tasks involving symbolic or physical formulas, while balancing resource usage across FPGA hardware. Finally, we showcase the versatility of the framework by extending it to real-time, power-efficient control systems.", "AI": {"tldr": "KANEL\u00c9\u6846\u67b6\u9996\u6b21\u7cfb\u7edf\u5316\u5730\u5c06KAN\u7f51\u7edc\u90e8\u7f72\u5230FPGA\u4e0a\uff0c\u901a\u8fc7\u91cf\u5316\u526a\u679d\u534f\u540c\u4f18\u5316\uff0c\u5b9e\u73b0\u9ad8\u8fbe2700\u500d\u52a0\u901f\u548c\u663e\u8457\u8d44\u6e90\u8282\u7701\uff0c\u5728\u7b26\u53f7/\u7269\u7406\u516c\u5f0f\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "FPGA\u4e0a\u9700\u8981\u4f4e\u5ef6\u8fdf\u3001\u8d44\u6e90\u9ad8\u6548\u7684\u795e\u7ecf\u7f51\u7edc\u63a8\u7406\u4ee5\u6ee1\u8db3\u5b9e\u65f6\u6027\u548c\u4f4e\u529f\u8017\u9700\u6c42\u3002\u4f20\u7edfLUT\u7f51\u7edc\u867d\u6709\u4f18\u52bf\uff0c\u4f46KAN\u7f51\u7edc\u56e0\u5176\u53ef\u5b66\u4e60\u4e00\u7ef4\u6837\u6761\u6fc0\u6d3b\u51fd\u6570\uff0c\u5929\u7136\u9002\u5408\u79bb\u6563\u5316\u548c\u9ad8\u6548LUT\u6620\u5c04\uff0c\u4e3aFPGA\u90e8\u7f72\u63d0\u4f9b\u4e86\u65b0\u673a\u4f1a\u3002", "method": "\u63d0\u51faKANEL\u00c9\u6846\u67b6\uff0c\u9996\u6b21\u7cfb\u7edf\u5316\u8bbe\u8ba1KAN\u5728FPGA\u4e0a\u7684\u5b9e\u73b0\u6d41\u7a0b\u3002\u901a\u8fc7\u534f\u540c\u4f18\u5316\u8bad\u7ec3\u3001\u91cf\u5316\u548c\u526a\u679d\u6280\u672f\uff0c\u5c06KAN\u7f51\u7edc\u8f6c\u5316\u4e3a\u7d27\u51d1\u3001\u9ad8\u541e\u5410\u3001\u4f4e\u5ef6\u8fdf\u7684FPGA\u67b6\u6784\uff0c\u5145\u5206\u5229\u7528KAN\u7684\u6837\u6761\u6fc0\u6d3b\u51fd\u6570\u7279\u6027\u8fdb\u884c\u9ad8\u6548LUT\u6620\u5c04\u3002", "result": "\u76f8\u6bd4\u73b0\u6709KAN-on-FPGA\u65b9\u6cd5\uff0c\u5b9e\u73b0\u9ad8\u8fbe2700\u500d\u52a0\u901f\u548c\u6570\u91cf\u7ea7\u8d44\u6e90\u8282\u7701\u3002\u5728\u5e7f\u6cdb\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cKANEL\u00c9\u5339\u914d\u6216\u8d85\u8d8a\u5176\u4ed6LUT\u67b6\u6784\uff0c\u5c24\u5176\u5728\u7b26\u53f7/\u7269\u7406\u516c\u5f0f\u4efb\u52a1\u4e0a\u8868\u73b0\u7a81\u51fa\uff0c\u5e76\u80fd\u5e73\u8861FPGA\u786c\u4ef6\u8d44\u6e90\u4f7f\u7528\u3002\u6846\u67b6\u8fd8\u6210\u529f\u6269\u5c55\u5230\u5b9e\u65f6\u3001\u9ad8\u80fd\u6548\u63a7\u5236\u7cfb\u7edf\u3002", "conclusion": "KANEL\u00c9\u6846\u67b6\u6210\u529f\u5c06KAN\u7f51\u7edc\u4f18\u52bf\u4e0eFPGA\u786c\u4ef6\u7279\u6027\u7ed3\u5408\uff0c\u4e3a\u4f4e\u5ef6\u8fdf\u3001\u8d44\u6e90\u9ad8\u6548\u7684\u795e\u7ecf\u7f51\u7edc\u63a8\u7406\u63d0\u4f9b\u4e86\u521b\u65b0\u89e3\u51b3\u65b9\u6848\uff0c\u7279\u522b\u9002\u5408\u5b9e\u65f6\u63a7\u5236\u548c\u7b26\u53f7/\u7269\u7406\u516c\u5f0f\u5904\u7406\u4efb\u52a1\uff0c\u5c55\u73b0\u4e86KAN\u5728\u8fb9\u7f18\u8ba1\u7b97\u4e2d\u7684\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2512.12036", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.12036", "abs": "https://arxiv.org/abs/2512.12036", "authors": ["Shiju Li", "Younghoon Min", "Hane Yie", "Hoshik Kim", "Soohong Ahn", "Joonseop Sim", "Chul-Ho Lee", "Jongryool Kim"], "title": "Accelerating Sparse Matrix-Matrix Multiplication on GPUs with Processing Near HBMs", "comment": "13 pages, 11 figures", "summary": "Sparse General Matrix-Matrix Multiplication (SpGEMM) is a fundamental operation in numerous scientific computing and data analytics applications, often bottlenecked by irregular memory access patterns. This paper presents Hash based Multi-phase SpGEMM on GPU and the Acceleration of Indirect Memory Access (AIA) technique, a novel custom near-memory processing approach to optimizing SpGEMM on GPU HBM. Our hardware-software co-designed framework for SpGEMM demonstrates significant performance improvements over state-of-the-art methods, particularly in handling complex, application-specific workloads. We evaluate our approach on various graph workloads, including graph contraction, Markov clustering, and Graph Neural Networks (GNNs), showcasing its practical applicability. For graph analytics applications, AIA demonstrates up to 17.3% time reduction from the software-only implementation, while achieving time reduction of 76.5% for Graph Contraction and 58.4% for Markov Clustering compared to cuSPARSE. For GNN training applications with structured global pruning, our hybrid approach delivers an average of 1.43x speedup over software-only implementation across six benchmark datasets and three architectures (GCN, GIN, GraphSAGE), and shows 1.95x speedup for GNN workloads when compared to cuSPARSE, with up to 4.18x gains on large-scale datasets.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u54c8\u5e0c\u7684\u591a\u9636\u6bb5SpGEMM GPU\u5b9e\u73b0\u548c\u95f4\u63a5\u5185\u5b58\u8bbf\u95ee\u52a0\u901f\u6280\u672f\uff0c\u901a\u8fc7\u786c\u4ef6-\u8f6f\u4ef6\u534f\u540c\u8bbe\u8ba1\u663e\u8457\u63d0\u5347\u4e86\u7a00\u758f\u77e9\u9635\u4e58\u6cd5\u7684\u6027\u80fd\uff0c\u5728\u591a\u79cd\u56fe\u5206\u6790\u4efb\u52a1\u548cGNN\u8bad\u7ec3\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u52a0\u901f\u6548\u679c\u3002", "motivation": "\u7a00\u758f\u901a\u7528\u77e9\u9635\u4e58\u6cd5(SpGEMM)\u662f\u79d1\u5b66\u8ba1\u7b97\u548c\u6570\u636e\u5206\u6790\u4e2d\u7684\u57fa\u7840\u64cd\u4f5c\uff0c\u4f46\u901a\u5e38\u53d7\u9650\u4e8e\u4e0d\u89c4\u5219\u7684\u5185\u5b58\u8bbf\u95ee\u6a21\u5f0f\u3002\u73b0\u6709GPU\u5b9e\u73b0\u96be\u4ee5\u9ad8\u6548\u5904\u7406\u590d\u6742\u7684\u5e94\u7528\u7279\u5b9a\u5de5\u4f5c\u8d1f\u8f7d\uff0c\u9700\u8981\u65b0\u7684\u4f18\u5316\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u57fa\u4e8e\u54c8\u5e0c\u7684\u591a\u9636\u6bb5SpGEMM GPU\u5b9e\u73b0\u548c\u95f4\u63a5\u5185\u5b58\u8bbf\u95ee\u52a0\u901f(AIA)\u6280\u672f\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u7684\u5b9a\u5236\u8fd1\u5185\u5b58\u5904\u7406\u65b9\u6cd5\u3002\u91c7\u7528\u786c\u4ef6-\u8f6f\u4ef6\u534f\u540c\u8bbe\u8ba1\u6846\u67b6\uff0c\u4e13\u95e8\u9488\u5bf9GPU HBM\u8fdb\u884c\u4f18\u5316\u3002", "result": "\u5728\u56fe\u5206\u6790\u5e94\u7528\u4e2d\uff0cAIA\u76f8\u6bd4\u7eaf\u8f6f\u4ef6\u5b9e\u73b0\u51cf\u5c1117.3%\u65f6\u95f4\uff1b\u76f8\u6bd4cuSPARSE\uff0c\u56fe\u6536\u7f29\u51cf\u5c1176.5%\u65f6\u95f4\uff0c\u9a6c\u5c14\u53ef\u592b\u805a\u7c7b\u51cf\u5c1158.4%\u65f6\u95f4\u3002\u5728GNN\u8bad\u7ec3\u4e2d\uff0c\u6df7\u5408\u65b9\u6cd5\u76f8\u6bd4\u7eaf\u8f6f\u4ef6\u5b9e\u73b0\u5e73\u5747\u52a0\u901f1.43\u500d\uff0c\u76f8\u6bd4cuSPARSE\u52a0\u901f1.95\u500d\uff0c\u5728\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u6700\u9ad8\u53ef\u8fbe4.18\u500d\u52a0\u901f\u3002", "conclusion": "\u63d0\u51fa\u7684\u786c\u4ef6-\u8f6f\u4ef6\u534f\u540c\u8bbe\u8ba1\u6846\u67b6\u663e\u8457\u63d0\u5347\u4e86SpGEMM\u5728GPU\u4e0a\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u590d\u6742\u5e94\u7528\u7279\u5b9a\u5de5\u4f5c\u8d1f\u8f7d\u65f6\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u56fe\u5206\u6790\u548cGNN\u8bad\u7ec3\u7b49\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u9ad8\u6027\u80fd\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.12990", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2512.12990", "abs": "https://arxiv.org/abs/2512.12990", "authors": ["Yuseon Choi", "Sangjin Kim", "Jungjun Oh", "Gwangtae Park", "Byeongcheol Kim", "Hoi-Jun Yoo"], "title": "SliceMoE: Bit-Sliced Expert Caching under Miss-Rate Constraints for Efficient MoE Inference", "comment": null, "summary": "MoE models offer efficient scaling through conditional computation, but their large parameter size and expensive expert offloading make on-device deployment challenging. Existing acceleration techniques such as prefetching or expert clustering often increase energy usage or reduce expert diversity. We present SliceMoE, an energy-efficient MoE inference framework for miss-rate-constrained deployment. SliceMoE introduces Dynamic Bit-Sliced Caching (DBSC), which caches experts at slice-level granularity and assigns precision on demand to expand effective expert capacity. To support mixed-precision experts without memory duplication, we propose Calibration-Free Asymmetric Matryoshka Quantization (AMAT), a truncation-based scheme that maintains compatibility between low-bit and high-bit slices. We further introduce Predictive Cache Warmup (PCW) to reduce early-decode cold misses by reshaping cache contents during prefill. Evaluated on DeepSeek-V2-Lite and Qwen1.5-MoE-A2.7B, SliceMoE reduces decode-stage energy consumption by up to 2.37x and 2.85x, respectively, and improves decode latency by up to 1.81x and 1.64x, while preserving near-high-bit accuracy. These results demonstrate that slice-level caching enables an efficient on-device MoE deployment.", "AI": {"tldr": "SliceMoE\uff1a\u4e00\u79cd\u9762\u5411\u8bbe\u5907\u7aef\u90e8\u7f72\u7684\u8282\u80fdMoE\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u4f4d\u5207\u7247\u7f13\u5b58\u548c\u9884\u6d4b\u6027\u7f13\u5b58\u9884\u70ed\u6280\u672f\uff0c\u5728\u4fdd\u6301\u7cbe\u5ea6\u7684\u540c\u65f6\u663e\u8457\u964d\u4f4e\u80fd\u8017\u548c\u5ef6\u8fdf\u3002", "motivation": "MoE\u6a21\u578b\u901a\u8fc7\u6761\u4ef6\u8ba1\u7b97\u5b9e\u73b0\u9ad8\u6548\u6269\u5c55\uff0c\u4f46\u5176\u5e9e\u5927\u7684\u53c2\u6570\u91cf\u548c\u6602\u8d35\u7684\u4e13\u5bb6\u5378\u8f7d\u4f7f\u5f97\u8bbe\u5907\u7aef\u90e8\u7f72\u9762\u4e34\u6311\u6218\u3002\u73b0\u6709\u7684\u52a0\u901f\u6280\u672f\u5982\u9884\u53d6\u6216\u4e13\u5bb6\u805a\u7c7b\u5f80\u5f80\u4f1a\u589e\u52a0\u80fd\u8017\u6216\u964d\u4f4e\u4e13\u5bb6\u591a\u6837\u6027\u3002", "method": "\u63d0\u51faSliceMoE\u6846\u67b6\uff1a1\uff09\u52a8\u6001\u4f4d\u5207\u7247\u7f13\u5b58\uff08DBSC\uff09\uff1a\u4ee5\u5207\u7247\u7c92\u5ea6\u7f13\u5b58\u4e13\u5bb6\uff0c\u6309\u9700\u5206\u914d\u7cbe\u5ea6\u4ee5\u6269\u5c55\u6709\u6548\u4e13\u5bb6\u5bb9\u91cf\uff1b2\uff09\u6821\u51c6\u81ea\u7531\u975e\u5bf9\u79f0\u5957\u5a03\u91cf\u5316\uff08AMAT\uff09\uff1a\u57fa\u4e8e\u622a\u65ad\u7684\u91cf\u5316\u65b9\u6848\uff0c\u652f\u6301\u6df7\u5408\u7cbe\u5ea6\u4e13\u5bb6\u800c\u65e0\u9700\u5185\u5b58\u590d\u5236\uff1b3\uff09\u9884\u6d4b\u6027\u7f13\u5b58\u9884\u70ed\uff08PCW\uff09\uff1a\u5728\u9884\u586b\u5145\u9636\u6bb5\u91cd\u5851\u7f13\u5b58\u5185\u5bb9\u4ee5\u51cf\u5c11\u65e9\u671f\u89e3\u7801\u51b7\u7f3a\u5931\u3002", "result": "\u5728DeepSeek-V2-Lite\u548cQwen1.5-MoE-A2.7B\u4e0a\u8bc4\u4f30\uff0cSliceMoE\u5c06\u89e3\u7801\u9636\u6bb5\u80fd\u8017\u5206\u522b\u964d\u4f4e2.37\u500d\u548c2.85\u500d\uff0c\u89e3\u7801\u5ef6\u8fdf\u5206\u522b\u63d0\u53471.81\u500d\u548c1.64\u500d\uff0c\u540c\u65f6\u4fdd\u6301\u63a5\u8fd1\u9ad8\u6bd4\u7279\u7cbe\u5ea6\u3002", "conclusion": "\u5207\u7247\u7ea7\u7f13\u5b58\u6280\u672f\u80fd\u591f\u5b9e\u73b0\u9ad8\u6548\u7684\u8bbe\u5907\u7aefMoE\u90e8\u7f72\uff0c\u4e3a\u5927\u89c4\u6a21MoE\u6a21\u578b\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2512.12295", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2512.12295", "abs": "https://arxiv.org/abs/2512.12295", "authors": ["Wenjun Yu", "Sitian Chen", "Cheng Chen", "Amelie Chi Zhou"], "title": "Near-Zero-Overhead Freshness for Recommendation Systems via Inference-Side Model Updates", "comment": "13 Pages, 19 figures", "summary": "Deep Learning Recommendation Models (DLRMs) underpin personalized services but face a critical freshness-accuracy tradeoff due to massive parameter synchronization overheads. Production DLRMs deploy decoupled training/inference clusters, where synchronizing petabyte-scale embedding tables (EMTs) causes multi-minute staleness, degrading recommendation quality and revenue. We observe that (1) inference nodes exhibit sustained CPU underutilization (peak <= 20%), and (2) EMT gradients possess intrinsic low-rank structure, enabling compact update representation. We present LiveUpdate, a system that eliminates inter-cluster synchronization by colocating Low-Rank Adaptation (LoRA) trainers within inference nodes. LiveUpdate addresses two core challenges: (1) dynamic rank adaptation via singular value monitoring to constrain memory overhead (<2% of EMTs), and (2) NUMA-aware resource scheduling with hardware-enforced QoS to eliminate update inference contention (P99 latency impact <20ms). Evaluations show LiveUpdate reduces update costs by 2x versus delta-update baselines while achieving higher accuracy within 1-hour windows. By transforming idle inference resources into freshness engines, LiveUpdate delivers online model updates while outperforming state-of-the-art delta-update methods by 0.04% to 0.24% in accuracy.", "AI": {"tldr": "LiveUpdate\u901a\u8fc7\u5728\u63a8\u7406\u8282\u70b9\u4e0a\u90e8\u7f72\u4f4e\u79e9\u9002\u914d\u8bad\u7ec3\u5668\uff0c\u6d88\u9664\u8de8\u96c6\u7fa4\u540c\u6b65\u5f00\u9500\uff0c\u5b9e\u73b0\u6df1\u5ea6\u5b66\u4e60\u63a8\u8350\u6a21\u578b\u7684\u5728\u7ebf\u66f4\u65b0\uff0c\u5728\u4fdd\u8bc1\u4f4e\u5ef6\u8fdf\u7684\u540c\u65f6\u63d0\u5347\u6a21\u578b\u65b0\u9c9c\u5ea6\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u751f\u4ea7\u73af\u5883\u4e2d\u7684DLRMs\u9762\u4e34\u65b0\u9c9c\u5ea6\u4e0e\u51c6\u786e\u6027\u7684\u6743\u8861\u95ee\u9898\uff0c\u56e0\u4e3a\u8de8\u8bad\u7ec3/\u63a8\u7406\u96c6\u7fa4\u540c\u6b65\u6d77\u91cf\u5d4c\u5165\u8868\u53c2\u6570\u4f1a\u5bfc\u81f4\u6570\u5206\u949f\u7684\u5ef6\u8fdf\uff0c\u964d\u4f4e\u63a8\u8350\u8d28\u91cf\u548c\u6536\u5165\u3002\u540c\u65f6\u89c2\u5bdf\u5230\u63a8\u7406\u8282\u70b9CPU\u5229\u7528\u7387\u4f4e\uff08\u5cf0\u503c\u226420%\uff09\u4e14\u5d4c\u5165\u8868\u68af\u5ea6\u5177\u6709\u5185\u5728\u4f4e\u79e9\u7ed3\u6784\u3002", "method": "\u63d0\u51faLiveUpdate\u7cfb\u7edf\uff0c\u5c06\u4f4e\u79e9\u9002\u914d\u8bad\u7ec3\u5668\u4e0e\u63a8\u7406\u8282\u70b9\u5171\u7f6e\uff0c\u6d88\u9664\u8de8\u96c6\u7fa4\u540c\u6b65\u3002\u91c7\u7528\u52a8\u6001\u79e9\u9002\u914d\u901a\u8fc7\u5947\u5f02\u503c\u76d1\u63a7\u63a7\u5236\u5185\u5b58\u5f00\u9500\uff08<2%\u5d4c\u5165\u8868\u5927\u5c0f\uff09\uff0c\u4ee5\u53caNUMA\u611f\u77e5\u8d44\u6e90\u8c03\u5ea6\u914d\u5408\u786c\u4ef6\u5f3a\u5236QoS\u6d88\u9664\u66f4\u65b0\u5bf9\u63a8\u7406\u7684\u5e72\u6270\u3002", "result": "\u76f8\u6bd4\u57fa\u7ebfdelta-update\u65b9\u6cd5\uff0cLiveUpdate\u5c06\u66f4\u65b0\u6210\u672c\u964d\u4f4e2\u500d\uff0c\u57281\u5c0f\u65f6\u7a97\u53e3\u5185\u5b9e\u73b0\u66f4\u9ad8\u51c6\u786e\u6027\uff0cP99\u5ef6\u8fdf\u5f71\u54cd\u5c0f\u4e8e20ms\uff0c\u5185\u5b58\u5f00\u9500\u63a7\u5236\u5728\u5d4c\u5165\u8868\u76842%\u4ee5\u5185\uff0c\u51c6\u786e\u7387\u6bd4\u6700\u5148\u8fdb\u7684delta-update\u65b9\u6cd5\u63d0\u53470.04%\u52300.24%\u3002", "conclusion": "LiveUpdate\u901a\u8fc7\u5c06\u7a7a\u95f2\u63a8\u7406\u8d44\u6e90\u8f6c\u5316\u4e3a\u65b0\u9c9c\u5ea6\u5f15\u64ce\uff0c\u5b9e\u73b0\u4e86DLRMs\u7684\u5728\u7ebf\u6a21\u578b\u66f4\u65b0\uff0c\u5728\u964d\u4f4e\u540c\u6b65\u6210\u672c\u7684\u540c\u65f6\u63d0\u9ad8\u4e86\u63a8\u8350\u51c6\u786e\u6027\uff0c\u89e3\u51b3\u4e86\u751f\u4ea7\u73af\u5883\u4e2d\u6a21\u578b\u65b0\u9c9c\u5ea6\u4e0e\u51c6\u786e\u6027\u7684\u6838\u5fc3\u77db\u76fe\u3002"}}
{"id": "2512.13133", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2512.13133", "abs": "https://arxiv.org/abs/2512.13133", "authors": ["Shuo Liu"], "title": "An Optimal Alignment-Driven Iterative Closed-Loop Convergence Framework for High-Performance Ultra-Large Scale Layout Pattern Clustering", "comment": "First Place Winner of the 2025 China Postgraduate EDA Elite Challenge (Problem 7)", "summary": "With the aggressive scaling of VLSI technology, the explosion of layout patterns creates a critical bottleneck for DFM applications like OPC. Pattern clustering is essential to reduce data complexity, yet existing methods struggle with computational prohibitiveness ($O(N^2)$ comparisons), sub-optimal discrete sampling for center alignment, and difficult speed-quality trade-offs. To address these, we propose an Optimal Alignment-Driven Iterative Closed-Loop Convergence Framework. First, to resolve alignment ambiguity, we introduce a hybrid suite of high-performance algorithms: an FFT-based Phase Correlation method for cosine similarity constraints, and a Robust Geometric Min-Max strategy for edge displacement constraints that analytically solves for the global optimum. Second, we model clustering as a Set Cover Problem (SCP) using a Surprisal-Based Lazy Greedy heuristic within a coarse-to-fine iterative refinement loop to ensure convergence. Additionally, a multi-stage pruning mechanism filters over 99% of redundant computations. Experimental results on the 2025 China Postgraduate EDA Elite Challenge benchmark demonstrate a 93.4% compression ratio relative to raw inputs and an over 100x speedup compared to the official baseline, effectively handling tens of thousands of patterns in seconds. Securing First Place among 77 teams, this approach proves its superiority in solving the NP-Hard layout clustering problem with an optimal balance of scalability and precision.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6700\u4f18\u5bf9\u9f50\u7684\u8fed\u4ee3\u95ed\u73af\u6536\u655b\u6846\u67b6\uff0c\u89e3\u51b3VLSI\u5e03\u5c40\u6a21\u5f0f\u805a\u7c7b\u4e2d\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u3001\u5bf9\u9f50\u7cbe\u5ea6\u548c\u901f\u5ea6-\u8d28\u91cf\u6743\u8861\u95ee\u9898\uff0c\u5728EDA\u7ade\u8d5b\u4e2d\u5b9e\u73b093.4%\u538b\u7f29\u6bd4\u548c100\u500d\u52a0\u901f\u3002", "motivation": "\u968f\u7740VLSI\u6280\u672f\u7f29\u653e\uff0c\u5e03\u5c40\u6a21\u5f0f\u7206\u70b8\u5f0f\u589e\u957f\u6210\u4e3aDFM\u5e94\u7528\uff08\u5982OPC\uff09\u7684\u5173\u952e\u74f6\u9888\u3002\u73b0\u6709\u805a\u7c7b\u65b9\u6cd5\u9762\u4e34\u8ba1\u7b97\u590d\u6742\u5ea6\u9ad8\uff08O(N\u00b2)\u6bd4\u8f83\uff09\u3001\u4e2d\u5fc3\u5bf9\u9f50\u79bb\u6563\u91c7\u6837\u6b21\u4f18\u3001\u901f\u5ea6-\u8d28\u91cf\u6743\u8861\u56f0\u96be\u7b49\u95ee\u9898\u3002", "method": "1) \u63d0\u51fa\u6df7\u5408\u9ad8\u6027\u80fd\u7b97\u6cd5\u89e3\u51b3\u5bf9\u9f50\u6a21\u7cca\u6027\uff1a\u57fa\u4e8eFFT\u7684\u76f8\u4f4d\u76f8\u5173\u65b9\u6cd5\u7528\u4e8e\u4f59\u5f26\u76f8\u4f3c\u5ea6\u7ea6\u675f\uff0c\u9c81\u68d2\u51e0\u4f55\u6700\u5c0f-\u6700\u5927\u7b56\u7565\u7528\u4e8e\u8fb9\u7f18\u4f4d\u79fb\u7ea6\u675f\uff1b2) \u5c06\u805a\u7c7b\u5efa\u6a21\u4e3a\u96c6\u5408\u8986\u76d6\u95ee\u9898\uff0c\u5728\u7c97\u5230\u7ec6\u8fed\u4ee3\u7ec6\u5316\u5faa\u73af\u4e2d\u4f7f\u7528\u57fa\u4e8e\u4fe1\u606f\u91cf\u7684\u61d2\u60f0\u8d2a\u5a6a\u542f\u53d1\u5f0f\u7b97\u6cd5\u786e\u4fdd\u6536\u655b\uff1b3) \u591a\u9636\u6bb5\u526a\u679d\u673a\u5236\u8fc7\u6ee499%\u4ee5\u4e0a\u5197\u4f59\u8ba1\u7b97\u3002", "result": "\u57282025\u5e74\u4e2d\u56fd\u7814\u7a76\u751fEDA\u7cbe\u82f1\u6311\u6218\u8d5b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5b9e\u73b0\u76f8\u5bf9\u4e8e\u539f\u59cb\u8f93\u516593.4%\u7684\u538b\u7f29\u6bd4\uff0c\u6bd4\u5b98\u65b9\u57fa\u7ebf\u52a0\u901f\u8d85\u8fc7100\u500d\uff0c\u80fd\u5728\u6570\u79d2\u5185\u5904\u7406\u6570\u4e07\u4e2a\u6a21\u5f0f\uff0c\u572877\u652f\u961f\u4f0d\u4e2d\u83b7\u5f97\u7b2c\u4e00\u540d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u6700\u4f18\u5bf9\u9f50\u9a71\u52a8\u548c\u8fed\u4ee3\u95ed\u73af\u6536\u655b\u6846\u67b6\uff0c\u6210\u529f\u89e3\u51b3\u4e86NP\u96be\u7684\u5e03\u5c40\u805a\u7c7b\u95ee\u9898\uff0c\u5728\u53ef\u6269\u5c55\u6027\u548c\u7cbe\u5ea6\u4e4b\u95f4\u5b9e\u73b0\u4e86\u6700\u4f73\u5e73\u8861\uff0c\u8bc1\u660e\u4e86\u5176\u5728VLSI DFM\u5e94\u7528\u4e2d\u7684\u4f18\u8d8a\u6027\u3002"}}
{"id": "2512.12299", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.12299", "abs": "https://arxiv.org/abs/2512.12299", "authors": ["Vlad Popescu-Vifor", "Ilir Murturi", "Praveen Kumar Donta", "Schahram Dustdar"], "title": "A Conflict-Aware Resource Management Framework for the Computing Continuum", "comment": null, "summary": "The increasing device heterogeneity and decentralization requirements in the computing continuum (i.e., spanning edge, fog, and cloud) introduce new challenges in resource orchestration. In such environments, agents are often responsible for optimizing resource usage across deployed services. However, agent decisions can lead to persistent conflict loops, inefficient resource utilization, and degraded service performance. To overcome such challenges, we propose a novel framework for adaptive conflict resolution in resource-oriented orchestration using a Deep Reinforcement Learning (DRL) approach. The framework enables handling resource conflicts across deployments and integrates a DRL model trained to mediate such conflicts based on real-time performance feedback and historical state information. The framework has been prototyped and validated on a Kubernetes-based testbed, illustrating its methodological feasibility and architectural resilience. Preliminary results show that the framework achieves efficient resource reallocation and adaptive learning in dynamic scenarios, thus providing a scalable and resilient solution for conflict-aware orchestration in the computing continuum.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u81ea\u9002\u5e94\u51b2\u7a81\u89e3\u51b3\u6846\u67b6\uff0c\u7528\u4e8e\u8ba1\u7b97\u8fde\u7eed\u4f53\u4e2d\u7684\u8d44\u6e90\u7f16\u6392\uff0c\u89e3\u51b3\u5f02\u6784\u8bbe\u5907\u73af\u5883\u4e0b\u7684\u8d44\u6e90\u51b2\u7a81\u95ee\u9898", "motivation": "\u8ba1\u7b97\u8fde\u7eed\u4f53\uff08\u8fb9\u7f18\u3001\u96fe\u3001\u4e91\uff09\u4e2d\u8bbe\u5907\u5f02\u6784\u6027\u548c\u53bb\u4e2d\u5fc3\u5316\u9700\u6c42\u589e\u52a0\uff0c\u5bfc\u81f4\u8d44\u6e90\u7f16\u6392\u9762\u4e34\u65b0\u6311\u6218\u3002\u4ee3\u7406\u51b3\u7b56\u53ef\u80fd\u5bfc\u81f4\u6301\u7eed\u51b2\u7a81\u5faa\u73af\u3001\u8d44\u6e90\u5229\u7528\u6548\u7387\u4f4e\u4e0b\u548c\u670d\u52a1\u6027\u80fd\u4e0b\u964d", "method": "\u63d0\u51fa\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u7684\u81ea\u9002\u5e94\u51b2\u7a81\u89e3\u51b3\u6846\u67b6\uff0c\u96c6\u6210DRL\u6a21\u578b\uff0c\u57fa\u4e8e\u5b9e\u65f6\u6027\u80fd\u53cd\u9988\u548c\u5386\u53f2\u72b6\u6001\u4fe1\u606f\u8c03\u89e3\u8d44\u6e90\u51b2\u7a81\uff0c\u5728Kubernetes\u6d4b\u8bd5\u5e73\u53f0\u4e0a\u8fdb\u884c\u539f\u578b\u5b9e\u73b0\u548c\u9a8c\u8bc1", "result": "\u6846\u67b6\u5728\u52a8\u6001\u573a\u666f\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u6548\u8d44\u6e90\u91cd\u65b0\u5206\u914d\u548c\u81ea\u9002\u5e94\u5b66\u4e60\uff0c\u5c55\u793a\u4e86\u65b9\u6cd5\u53ef\u884c\u6027\u548c\u67b6\u6784\u5f39\u6027\uff0c\u4e3a\u8ba1\u7b97\u8fde\u7eed\u4f53\u4e2d\u7684\u51b2\u7a81\u611f\u77e5\u7f16\u6392\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u5f39\u6027\u7684\u89e3\u51b3\u65b9\u6848", "conclusion": "\u8be5DRL\u6846\u67b6\u80fd\u591f\u6709\u6548\u89e3\u51b3\u8ba1\u7b97\u8fde\u7eed\u4f53\u4e2d\u7684\u8d44\u6e90\u51b2\u7a81\u95ee\u9898\uff0c\u63d0\u9ad8\u8d44\u6e90\u5229\u7528\u6548\u7387\u548c\u670d\u52a1\u6027\u80fd\uff0c\u4e3a\u5f02\u6784\u5206\u5e03\u5f0f\u73af\u5883\u4e2d\u7684\u8d44\u6e90\u7f16\u6392\u63d0\u4f9b\u4e86\u521b\u65b0\u89e3\u51b3\u65b9\u6848"}}
{"id": "2512.13282", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2512.13282", "abs": "https://arxiv.org/abs/2512.13282", "authors": ["Endri Taka", "Andre Roesti", "Joseph Melber", "Pranathi Vasireddy", "Kristof Denolf", "Diana Marculescu"], "title": "Striking the Balance: GEMM Performance Optimization Across Generations of Ryzen AI NPUs", "comment": null, "summary": "The high computational and memory demands of modern deep learning (DL) workloads have led to the development of specialized hardware devices from cloud to edge, such as AMD's Ryzen AI XDNA NPUs. Optimizing general matrix multiplication (GEMM) algorithms for these architectures is critical for improving DL workload performance. To this end, this paper presents a common systematic methodology to optimize GEMM workloads across the two current NPU generations, namely XDNA and XDNA2. Our implementations exploit the unique architectural features of AMD's NPUs and address key performance bottlenecks at the system level. End-to-end performance evaluation across various GEMM sizes demonstrates state-of-the-art throughput of up to 6.76 TOPS (XDNA) and 38.05 TOPS (XDNA2) for 8-bit integer (int8) precision. Similarly, for brain floating-point (bf16) precision, our GEMM implementations attain up to 3.14 TOPS (XDNA) and 14.71 TOPS (XDNA2). This work provides significant insights into key performance aspects of optimizing GEMM workloads on Ryzen AI NPUs.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7cfb\u7edf\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u4f18\u5316AMD Ryzen AI NPU\uff08XDNA\u548cXDNA2\uff09\u4e0a\u7684GEMM\u8ba1\u7b97\uff0c\u5b9e\u73b0\u4e86int8\u7cbe\u5ea6\u4e0b\u6700\u9ad86.76 TOPS\uff08XDNA\uff09\u548c38.05 TOPS\uff08XDNA2\uff09\u7684\u541e\u5410\u91cf\u3002", "motivation": "\u73b0\u4ee3\u6df1\u5ea6\u5b66\u4e60\u5de5\u4f5c\u8d1f\u8f7d\u5bf9\u8ba1\u7b97\u548c\u5185\u5b58\u9700\u6c42\u6781\u9ad8\uff0c\u9700\u8981\u9488\u5bf9AMD Ryzen AI XDNA NPU\u7b49\u4e13\u7528\u786c\u4ef6\u8fdb\u884c\u4f18\u5316\u3002\u4f18\u5316GEMM\u7b97\u6cd5\u5bf9\u4e8e\u63d0\u5347\u6df1\u5ea6\u5b66\u4e60\u5de5\u4f5c\u8d1f\u8f7d\u6027\u80fd\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u7684\u7cfb\u7edf\u5316\u65b9\u6cd5\u6765\u4f18\u5316\u4e24\u4e2aNPU\u4e16\u4ee3\uff08XDNA\u548cXDNA2\uff09\u4e0a\u7684GEMM\u5de5\u4f5c\u8d1f\u8f7d\u3002\u5b9e\u73b0\u5229\u7528\u4e86AMD NPU\u7684\u72ec\u7279\u67b6\u6784\u7279\u6027\uff0c\u5e76\u5728\u7cfb\u7edf\u5c42\u9762\u89e3\u51b3\u4e86\u5173\u952e\u6027\u80fd\u74f6\u9888\u3002", "result": "\u5728\u4e0d\u540cGEMM\u89c4\u6a21\u4e0a\u7684\u7aef\u5230\u7aef\u6027\u80fd\u8bc4\u4f30\u663e\u793a\uff1a\u5bf9\u4e8e8\u4f4d\u6574\u6570\uff08int8\uff09\u7cbe\u5ea6\uff0c\u5b9e\u73b0\u4e86\u6700\u9ad86.76 TOPS\uff08XDNA\uff09\u548c38.05 TOPS\uff08XDNA2\uff09\u7684\u541e\u5410\u91cf\uff1b\u5bf9\u4e8e\u8111\u6d6e\u70b9\u6570\uff08bf16\uff09\u7cbe\u5ea6\uff0c\u5b9e\u73b0\u4e86\u6700\u9ad83.14 TOPS\uff08XDNA\uff09\u548c14.71 TOPS\uff08XDNA2\uff09\u7684\u541e\u5410\u91cf\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u4f18\u5316Ryzen AI NPU\u4e0a\u7684GEMM\u5de5\u4f5c\u8d1f\u8f7d\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u6027\u80fd\u6d1e\u5bdf\uff0c\u5c55\u793a\u4e86\u7cfb\u7edf\u5316\u4f18\u5316\u65b9\u6cd5\u5728\u4e13\u7528\u786c\u4ef6\u4e0a\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2512.12409", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.12409", "abs": "https://arxiv.org/abs/2512.12409", "authors": ["Xuyang Liu", "Zijian Zhang", "Zhen Li", "Jiahang Sun", "Jiamou Liu", "Peng Jiang"], "title": "Reputation-Based Leader Election under Partial Synchrony: Towards a Protocol-Independent Abstraction with Enhanced Guarantees", "comment": null, "summary": "Leader election serves a well-defined role in leader-based Byzantine Fault Tolerant (BFT) protocols. Existing reputation-based leader election frameworks for partially synchronous BFTs suffer from either protocol-specific proofs, narrow applicability, or unbounded recovery after network stabilization, leaving an open problem. This paper presents a novel protocol-independent abstraction formalizing generic correctness properties and effectiveness guarantees for leader election under partial synchrony, enabling protocol-independent analysis and design. Building on this, we design the Sliding Window Leader Election (SWLE) mechanism. SWLE dynamically adjusts leader nominations via consensus-behavior-based reputation scores, enforcing Byzantine-cost amplification. We demonstrate SWLE introduces minimal extra overhead to the base protocol and prove it satisfies all abstraction properties and provides superior effectiveness. We show, with a 16-server deployment across 4 different regions in northern China, SWLE achieves up to 4.2x higher throughput, 75% lower latency and 27% Byzantine leader frequency compared to the state-of-the-art solution under common Byzantine faults, while maintaining efficiency in fault-free scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u534f\u8bae\u65e0\u5173\u7684\u9886\u5bfc\u8005\u9009\u4e3e\u62bd\u8c61\u6846\u67b6SWLE\uff0c\u7528\u4e8e\u90e8\u5206\u540c\u6b65\u7684\u62dc\u5360\u5ead\u5bb9\u9519\u7cfb\u7edf\uff0c\u901a\u8fc7\u57fa\u4e8e\u5171\u8bc6\u884c\u4e3a\u7684\u4fe1\u8a89\u8bc4\u5206\u52a8\u6001\u8c03\u6574\u9886\u5bfc\u8005\u63d0\u540d\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u5e76\u964d\u4f4e\u62dc\u5360\u5ead\u9886\u5bfc\u8005\u9891\u7387\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u4fe1\u8a89\u7684\u9886\u5bfc\u8005\u9009\u4e3e\u6846\u67b6\u5b58\u5728\u534f\u8bae\u7279\u5b9a\u8bc1\u660e\u3001\u9002\u7528\u8303\u56f4\u7a84\u6216\u7f51\u7edc\u7a33\u5b9a\u540e\u6062\u590d\u65e0\u754c\u7b49\u95ee\u9898\uff0c\u7f3a\u4e4f\u901a\u7528\u89e3\u51b3\u65b9\u6848\u3002\u9700\u8981\u4e00\u79cd\u534f\u8bae\u65e0\u5173\u7684\u62bd\u8c61\u6846\u67b6\u6765\u5f62\u5f0f\u5316\u9886\u5bfc\u8005\u9009\u4e3e\u7684\u6b63\u786e\u6027\u548c\u6709\u6548\u6027\u4fdd\u8bc1\u3002", "method": "\u9996\u5148\u63d0\u51fa\u534f\u8bae\u65e0\u5173\u7684\u62bd\u8c61\u6846\u67b6\uff0c\u5f62\u5f0f\u5316\u9886\u5bfc\u8005\u9009\u4e3e\u7684\u6b63\u786e\u6027\u548c\u6709\u6548\u6027\u5c5e\u6027\u3002\u5728\u6b64\u57fa\u7840\u4e0a\u8bbe\u8ba1\u6ed1\u52a8\u7a97\u53e3\u9886\u5bfc\u8005\u9009\u4e3e(SWLE)\u673a\u5236\uff0c\u901a\u8fc7\u57fa\u4e8e\u5171\u8bc6\u884c\u4e3a\u7684\u4fe1\u8a89\u8bc4\u5206\u52a8\u6001\u8c03\u6574\u9886\u5bfc\u8005\u63d0\u540d\uff0c\u5b9e\u65bd\u62dc\u5360\u5ead\u6210\u672c\u653e\u5927\u3002", "result": "SWLE\u5728\u57fa\u7840\u534f\u8bae\u4e0a\u5f15\u5165\u6700\u5c0f\u989d\u5916\u5f00\u9500\uff0c\u6ee1\u8db3\u6240\u6709\u62bd\u8c61\u5c5e\u6027\u5e76\u63d0\u4f9b\u4f18\u8d8a\u6709\u6548\u6027\u3002\u572816\u670d\u52a1\u5668\u8de84\u4e2a\u4e2d\u56fd\u5317\u65b9\u533a\u57df\u7684\u90e8\u7f72\u4e2d\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u65b9\u6848\uff0c\u541e\u5410\u91cf\u63d0\u53474.2\u500d\uff0c\u5ef6\u8fdf\u964d\u4f4e75%\uff0c\u62dc\u5360\u5ead\u9886\u5bfc\u8005\u9891\u7387\u964d\u4f4e27%\uff0c\u540c\u65f6\u5728\u65e0\u6545\u969c\u573a\u666f\u4e0b\u4fdd\u6301\u6548\u7387\u3002", "conclusion": "SWLE\u4e3a\u90e8\u5206\u540c\u6b65\u62dc\u5360\u5ead\u5bb9\u9519\u7cfb\u7edf\u63d0\u4f9b\u4e86\u901a\u7528\u3001\u6709\u6548\u7684\u9886\u5bfc\u8005\u9009\u4e3e\u89e3\u51b3\u65b9\u6848\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u663e\u8457\u63d0\u5347\u4e86\u7cfb\u7edf\u6027\u80fd\u548c\u62dc\u5360\u5ead\u5bb9\u9519\u80fd\u529b\u3002"}}
{"id": "2512.13479", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2512.13479", "abs": "https://arxiv.org/abs/2512.13479", "authors": ["Kunal Pai", "Harshil Patel", "Erin Le", "Noah Krim", "Mahyar Samani", "Bobby R. Bruce", "Jason Lowe-Power"], "title": "Reproducibility and Standardization in gem5 Resources v25.0", "comment": null, "summary": "Reproducibility in simulation-based computer architecture research requires coordinating artifacts like disk images, kernels, and benchmarks, but existing workflows are inconsistent. We improve gem5, an open-source simulator with over 1600 forks, and gem5 Resources, a centralized repository of over 2000 pre-packaged artifacts, to address these issues. While gem5 Resources enables artifact sharing, researchers still face challenges. Creating custom disk images is complex and time-consuming, with no standardized process across ISAs, making it difficult to extend and share images. gem5 provides limited guest-host communication features through a set of predefined exit events that restrict researchers' ability to dynamically control and monitor simulations. Lastly, running simulations with multiple workloads requires researchers to write custom external scripts to coordinate multiple gem5 simulations which creates error-prone and hard-to-reproduce workflows. To overcome this, we introduce several features in gem5 and gem5 Resources. We standardize disk-image creation across x86, ARM, and RISC-V using Packer, and provide validated base images with pre-annotated benchmark suites (NPB, GAPBS). We provide 12 new disk images, 6 new kernels, and over 200 workloads across three ISAs. We refactor the exit event system to a class-based model and introduce hypercalls for enhanced guest-host communication that allows researchers to define custom behavior for their exit events. We also provide a utility to remotely monitor simulations and the gem5-bridge driver for user-space m5 operations. Additionally, we implemented Suites and MultiSim to enable parallel full-system simulations from gem5 configuration scripts, eliminating the need for external scripting. These features reduce setup complexity and provide extensible, validated resources that improve reproducibility and standardization.", "AI": {"tldr": "\u6539\u8fdbgem5\u4eff\u771f\u5668\u548cgem5 Resources\u8d44\u6e90\u5e93\uff0c\u901a\u8fc7\u6807\u51c6\u5316\u78c1\u76d8\u955c\u50cf\u521b\u5efa\u3001\u91cd\u6784\u9000\u51fa\u4e8b\u4ef6\u7cfb\u7edf\u3001\u5f15\u5165\u5e76\u884c\u4eff\u771f\u529f\u80fd\uff0c\u89e3\u51b3\u8ba1\u7b97\u673a\u67b6\u6784\u7814\u7a76\u4e2d\u53ef\u91cd\u590d\u6027\u3001\u5b9a\u5236\u5316\u548c\u5de5\u4f5c\u6d41\u534f\u8c03\u95ee\u9898\u3002", "motivation": "\u8ba1\u7b97\u673a\u67b6\u6784\u7814\u7a76\u4e2d\u57fa\u4e8e\u4eff\u771f\u7684\u53ef\u91cd\u590d\u6027\u9762\u4e34\u4e09\u5927\u6311\u6218\uff1a1\uff09\u521b\u5efa\u81ea\u5b9a\u4e49\u78c1\u76d8\u955c\u50cf\u590d\u6742\u4e14\u7f3a\u4e4f\u8de8ISA\u6807\u51c6\u5316\uff1b2\uff09gem5\u6709\u9650\u7684\u8bbf\u5ba2-\u4e3b\u673a\u901a\u4fe1\u529f\u80fd\u9650\u5236\u4e86\u52a8\u6001\u63a7\u5236\u548c\u76d1\u63a7\uff1b3\uff09\u591a\u5de5\u4f5c\u8d1f\u8f7d\u4eff\u771f\u9700\u8981\u6613\u9519\u7684\u5916\u90e8\u811a\u672c\u534f\u8c03\u3002", "method": "1\uff09\u4f7f\u7528Packer\u6807\u51c6\u5316x86\u3001ARM\u548cRISC-V\u7684\u78c1\u76d8\u955c\u50cf\u521b\u5efa\uff0c\u63d0\u4f9b\u5e26\u9884\u6807\u6ce8\u57fa\u51c6\u5957\u4ef6\u7684\u9a8c\u8bc1\u57fa\u7840\u955c\u50cf\uff1b2\uff09\u91cd\u6784\u9000\u51fa\u4e8b\u4ef6\u7cfb\u7edf\u4e3a\u57fa\u4e8e\u7c7b\u7684\u6a21\u578b\uff0c\u5f15\u5165\u8d85\u8c03\u7528\u589e\u5f3a\u8bbf\u5ba2-\u4e3b\u673a\u901a\u4fe1\uff1b3\uff09\u5b9e\u73b0Suites\u548cMultiSim\u652f\u6301\u4ecegem5\u914d\u7f6e\u811a\u672c\u76f4\u63a5\u8fdb\u884c\u5e76\u884c\u5168\u7cfb\u7edf\u4eff\u771f\u3002", "result": "\u63d0\u4f9b\u4e8612\u4e2a\u65b0\u78c1\u76d8\u955c\u50cf\u30016\u4e2a\u65b0\u5185\u6838\u548c\u8d85\u8fc7200\u4e2a\u8de8\u4e09\u79cdISA\u7684\u5de5\u4f5c\u8d1f\u8f7d\u3002\u65b0\u529f\u80fd\u964d\u4f4e\u4e86\u8bbe\u7f6e\u590d\u6742\u6027\uff0c\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u9a8c\u8bc1\u8d44\u6e90\uff0c\u663e\u8457\u6539\u5584\u4e86\u53ef\u91cd\u590d\u6027\u548c\u6807\u51c6\u5316\u3002", "conclusion": "\u901a\u8fc7\u6807\u51c6\u5316\u78c1\u76d8\u955c\u50cf\u521b\u5efa\u3001\u589e\u5f3a\u8bbf\u5ba2-\u4e3b\u673a\u901a\u4fe1\u529f\u80fd\u3001\u4ee5\u53ca\u5185\u7f6e\u5e76\u884c\u4eff\u771f\u652f\u6301\uff0c\u663e\u8457\u63d0\u5347\u4e86gem5\u4eff\u771f\u5668\u7684\u53ef\u7528\u6027\u548c\u7814\u7a76\u53ef\u91cd\u590d\u6027\uff0c\u4e3a\u8ba1\u7b97\u673a\u67b6\u6784\u793e\u533a\u63d0\u4f9b\u4e86\u66f4\u5f3a\u5927\u3001\u66f4\u6613\u7528\u7684\u7814\u7a76\u5de5\u5177\u3002"}}
{"id": "2512.12476", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.12476", "abs": "https://arxiv.org/abs/2512.12476", "authors": ["Yongjun He", "Shuai Zhang", "Jiading Gai", "Xiyuan Zhang", "Boran Han", "Bernie Wang", "Huzefa Rangwala", "George Karypis"], "title": "HetRL: Efficient Reinforcement Learning for LLMs in Heterogeneous Environments", "comment": null, "summary": "As large language models (LLMs) continue to scale and new GPUs are released even more frequently, there is an increasing demand for LLM post-training in heterogeneous environments to fully leverage underutilized mid-range or previous-generation GPUs across regions and alleviate the shortage of homogeneous high-end GPUs within a single region. However, achieving high-performance reinforcement learning (RL) training for LLMs on such computing resources remains challenging because the workflow involves multiple models and tasks with complex computation and data dependencies. In this paper, we present HetRL, a distributed system for efficient RL training in infrastructures with heterogeneous GPUs and networks. HetRL formulates the scheduling of RL training in heterogeneous environments as a constrained joint optimization problem and introduces a novel scheduling algorithm that (1) decomposes the complex search space with a multi-level search framework; and (2) allocates the search budget via successive halving. Our extensive evaluation, consuming 20,000 GPU-hours, shows that HetRL delivers up to 9.17x the throughput of state-of-the-art systems, and 3.17x on average, under various workloads and settings.", "AI": {"tldr": "HetRL\u662f\u4e00\u4e2a\u7528\u4e8e\u5f02\u6784GPU\u73af\u5883\u4e2d\u9ad8\u6548RL\u8bad\u7ec3\u7684\u7cfb\u7edf\uff0c\u901a\u8fc7\u591a\u7ea7\u641c\u7d22\u6846\u67b6\u548c\u8fde\u7eed\u51cf\u534a\u7b97\u6cd5\u4f18\u5316\u8c03\u5ea6\uff0c\u76f8\u6bd4\u73b0\u6709\u7cfb\u7edf\u5e73\u5747\u63d0\u53473.17\u500d\u541e\u5410\u91cf\u3002", "motivation": "\u968f\u7740LLM\u89c4\u6a21\u6269\u5927\u548cGPU\u66f4\u65b0\u9891\u7e41\uff0c\u9700\u8981\u5229\u7528\u8de8\u533a\u57df\u672a\u5145\u5206\u5229\u7528\u7684\u4e2d\u7aef\u6216\u65e7\u4ee3GPU\u6765\u7f13\u89e3\u5355\u4e00\u533a\u57df\u5185\u9ad8\u7aefGPU\u77ed\u7f3a\u95ee\u9898\u3002\u7136\u800c\uff0c\u5728\u5f02\u6784\u8ba1\u7b97\u8d44\u6e90\u4e0a\u5b9e\u73b0\u9ad8\u6027\u80fd\u7684LLM\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u9762\u4e34\u6311\u6218\uff0c\u56e0\u4e3a\u5de5\u4f5c\u6d41\u6d89\u53ca\u591a\u4e2a\u6a21\u578b\u548c\u4efb\u52a1\uff0c\u5177\u6709\u590d\u6742\u7684\u8ba1\u7b97\u548c\u6570\u636e\u4f9d\u8d56\u5173\u7cfb\u3002", "method": "HetRL\u5c06\u5f02\u6784\u73af\u5883\u4e2d\u7684RL\u8bad\u7ec3\u8c03\u5ea6\u5efa\u6a21\u4e3a\u7ea6\u675f\u8054\u5408\u4f18\u5316\u95ee\u9898\uff0c\u63d0\u51fa\u65b0\u9896\u7684\u8c03\u5ea6\u7b97\u6cd5\uff1a(1) \u4f7f\u7528\u591a\u7ea7\u641c\u7d22\u6846\u67b6\u5206\u89e3\u590d\u6742\u641c\u7d22\u7a7a\u95f4\uff1b(2) \u901a\u8fc7\u8fde\u7eed\u51cf\u534a\u5206\u914d\u641c\u7d22\u9884\u7b97\u3002", "result": "\u5728\u6d88\u801720,000 GPU\u5c0f\u65f6\u7684\u5e7f\u6cdb\u8bc4\u4f30\u4e2d\uff0cHetRL\u5728\u5404\u79cd\u5de5\u4f5c\u8d1f\u8f7d\u548c\u8bbe\u7f6e\u4e0b\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u7cfb\u7edf\u5b9e\u73b0\u4e86\u6700\u9ad89.17\u500d\u3001\u5e73\u57473.17\u500d\u7684\u541e\u5410\u91cf\u63d0\u5347\u3002", "conclusion": "HetRL\u7cfb\u7edf\u80fd\u591f\u6709\u6548\u89e3\u51b3\u5f02\u6784GPU\u73af\u5883\u4e2dLLM\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7684\u8c03\u5ea6\u6311\u6218\uff0c\u663e\u8457\u63d0\u5347\u8bad\u7ec3\u6548\u7387\uff0c\u5145\u5206\u5229\u7528\u5f02\u6784\u8ba1\u7b97\u8d44\u6e90\u3002"}}
{"id": "2512.13686", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2512.13686", "abs": "https://arxiv.org/abs/2512.13686", "authors": ["Juncheng Huo", "Yunfan Gao", "Xinxin Liu", "Sa Wang", "Yungang Bao", "Xitong Gao", "Kan Shi"], "title": "Lyra: A Hardware-Accelerated RISC-V Verification Framework with Generative Model-Based Processor Fuzzing", "comment": null, "summary": "As processor designs grow more complex, verification remains bottlenecked by slow software simulation and low-quality random test stimuli. Recent research has applied software fuzzers to hardware verification, but these rely on semantically blind random mutations that may generate shallow, low-quality stimuli unable to explore complex behaviors. These limitations result in slow coverage convergence and prohibitively high verification costs. In this paper, we present Lyra, a heterogeneous RISC-V verification framework that addresses both challenges by pairing hardware-accelerated verification with an ISA-aware generative model. Lyra executes the DUT and reference model concurrently on an FPGA SoC, enabling high-throughput differential checking and hardware-level coverage collection. Instead of creating verification stimuli randomly or through simple mutations, we train a domain-specialized generative model, LyraGen, with inherent semantic awareness to generate high-quality, semantically rich instruction sequences. Empirical results show Lyra achieves up to $1.27\\times$ higher coverage and accelerates end-to-end verification by up to $107\\times$ to $3343\\times$ compared to state-of-the-art software fuzzers, while consistently demonstrating lower convergence difficulty.", "AI": {"tldr": "Lyra\uff1a\u4e00\u4e2a\u5f02\u6784RISC-V\u9a8c\u8bc1\u6846\u67b6\uff0c\u7ed3\u5408\u786c\u4ef6\u52a0\u901f\u548cISA\u611f\u77e5\u751f\u6210\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u9a8c\u8bc1\u8986\u76d6\u7387\u548c\u901f\u5ea6", "motivation": "\u5904\u7406\u5668\u8bbe\u8ba1\u65e5\u76ca\u590d\u6742\uff0c\u4f46\u9a8c\u8bc1\u4ecd\u53d7\u9650\u4e8e\u7f13\u6162\u7684\u8f6f\u4ef6\u4eff\u771f\u548c\u4f4e\u8d28\u91cf\u7684\u968f\u673a\u6d4b\u8bd5\u6fc0\u52b1\u3002\u73b0\u6709\u8f6f\u4ef6\u6a21\u7cca\u6d4b\u8bd5\u65b9\u6cd5\u4f9d\u8d56\u8bed\u4e49\u76f2\u968f\u673a\u7a81\u53d8\uff0c\u96be\u4ee5\u751f\u6210\u80fd\u63a2\u7d22\u590d\u6742\u884c\u4e3a\u7684\u9ad8\u8d28\u91cf\u6fc0\u52b1\uff0c\u5bfc\u81f4\u8986\u76d6\u7387\u6536\u655b\u7f13\u6162\u548c\u9a8c\u8bc1\u6210\u672c\u8fc7\u9ad8\u3002", "method": "Lyra\u91c7\u7528\u5f02\u6784\u9a8c\u8bc1\u6846\u67b6\uff1a1) \u5728FPGA SoC\u4e0a\u5e76\u884c\u6267\u884c\u88ab\u6d4b\u8bbe\u8ba1\u548c\u53c2\u8003\u6a21\u578b\uff0c\u5b9e\u73b0\u9ad8\u541e\u5410\u91cf\u5dee\u5206\u68c0\u67e5\u548c\u786c\u4ef6\u7ea7\u8986\u76d6\u7387\u6536\u96c6\uff1b2) \u8bad\u7ec3\u9886\u57df\u4e13\u7528\u751f\u6210\u6a21\u578bLyraGen\uff0c\u8be5\u6a21\u578b\u5177\u6709\u5185\u5728\u8bed\u4e49\u611f\u77e5\u80fd\u529b\uff0c\u80fd\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u8bed\u4e49\u4e30\u5bcc\u7684\u6307\u4ee4\u5e8f\u5217\u3002", "result": "Lyra\u76f8\u6bd4\u6700\u5148\u8fdb\u7684\u8f6f\u4ef6\u6a21\u7cca\u6d4b\u8bd5\u5668\uff1a1) \u5b9e\u73b0\u9ad8\u8fbe1.27\u500d\u7684\u8986\u76d6\u7387\u63d0\u5347\uff1b2) \u7aef\u5230\u7aef\u9a8c\u8bc1\u52a0\u901f107\u500d\u81f33343\u500d\uff1b3) \u59cb\u7ec8\u8868\u73b0\u51fa\u66f4\u4f4e\u7684\u6536\u655b\u96be\u5ea6\u3002", "conclusion": "Lyra\u901a\u8fc7\u7ed3\u5408\u786c\u4ef6\u52a0\u901f\u9a8c\u8bc1\u548cISA\u611f\u77e5\u751f\u6210\u6a21\u578b\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5904\u7406\u5668\u9a8c\u8bc1\u4e2d\u7684\u8986\u76d6\u7387\u548c\u901f\u5ea6\u74f6\u9888\u95ee\u9898\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u9a8c\u8bc1\u6210\u672c\u3002"}}
{"id": "2512.12532", "categories": ["cs.DC", "cs.NI"], "pdf": "https://arxiv.org/pdf/2512.12532", "abs": "https://arxiv.org/abs/2512.12532", "authors": ["Duc A. Tran", "Dung Truong", "Duy Le"], "title": "Strategic Server Deployment under Uncertainty in Mobile Edge Computing", "comment": null, "summary": "Server deployment is a fundamental task in mobile edge computing: where to place the edge servers and what user cells to assign to them. To make this decision is context-specific, but common goals are 1) computing efficiency: maximize the amount of workload processed by the edge, and 2) communication efficiency: minimize the communication cost between the cells and their assigned servers. We focus on practical scenarios where the user workload in each cell is unknown and time-varying, and so are the effective capacities of the servers. Our research problem is to choose a subset of candidate servers and assign them to the user cells such that the above goals are sustainably achieved under the above uncertainties. We formulate this problem as a stochastic bilevel optimization, which is strongly NP-hard and unseen in the literature. By approximating the objective function with submodular functions, we can utilize state-of-the-art greedy algorithms for submodular maximization to effectively solve our problem. We evaluate the proposed algorithm using real-world data, showing its superiority to alternative methods; the improvement can be as high as 55%", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u968f\u673a\u53cc\u5c42\u4f18\u5316\u7684\u79fb\u52a8\u8fb9\u7f18\u8ba1\u7b97\u670d\u52a1\u5668\u90e8\u7f72\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b50\u6a21\u51fd\u6570\u8fd1\u4f3c\u548c\u8d2a\u5fc3\u7b97\u6cd5\u89e3\u51b3\u670d\u52a1\u5668\u9009\u62e9\u548c\u7528\u6237\u5206\u914d\u95ee\u9898\uff0c\u5728\u4e0d\u786e\u5b9a\u7684\u5de5\u4f5c\u8d1f\u8f7d\u548c\u670d\u52a1\u5668\u5bb9\u91cf\u4e0b\u5b9e\u73b0\u8ba1\u7b97\u6548\u7387\u548c\u901a\u4fe1\u6548\u7387\u7684\u4f18\u5316\u3002", "motivation": "\u79fb\u52a8\u8fb9\u7f18\u8ba1\u7b97\u4e2d\u670d\u52a1\u5668\u90e8\u7f72\u662f\u4e00\u4e2a\u57fa\u7840\u4efb\u52a1\uff0c\u9700\u8981\u5728\u4e0d\u786e\u5b9a\u7684\u5de5\u4f5c\u8d1f\u8f7d\u548c\u670d\u52a1\u5668\u5bb9\u91cf\u73af\u5883\u4e0b\uff0c\u540c\u65f6\u4f18\u5316\u8ba1\u7b97\u6548\u7387\uff08\u6700\u5927\u5316\u8fb9\u7f18\u5904\u7406\u7684\u5de5\u4f5c\u8d1f\u8f7d\uff09\u548c\u901a\u4fe1\u6548\u7387\uff08\u6700\u5c0f\u5316\u7528\u6237\u5355\u5143\u4e0e\u670d\u52a1\u5668\u95f4\u7684\u901a\u4fe1\u6210\u672c\uff09\u3002", "method": "\u5c06\u95ee\u9898\u5efa\u6a21\u4e3a\u968f\u673a\u53cc\u5c42\u4f18\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u5b50\u6a21\u51fd\u6570\u8fd1\u4f3c\u76ee\u6807\u51fd\u6570\uff0c\u5229\u7528\u6700\u5148\u8fdb\u7684\u5b50\u6a21\u6700\u5927\u5316\u8d2a\u5fc3\u7b97\u6cd5\u6709\u6548\u6c42\u89e3\u670d\u52a1\u5668\u9009\u62e9\u548c\u7528\u6237\u5206\u914d\u95ee\u9898\u3002", "result": "\u4f7f\u7528\u771f\u5b9e\u4e16\u754c\u6570\u636e\u8bc4\u4f30\u8868\u660e\uff0c\u6240\u63d0\u7b97\u6cd5\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\uff0c\u6539\u8fdb\u5e45\u5ea6\u6700\u9ad8\u53ef\u8fbe55%\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u79fb\u52a8\u8fb9\u7f18\u8ba1\u7b97\u4e2d\u7684\u670d\u52a1\u5668\u90e8\u7f72\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u968f\u673a\u4f18\u5316\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u4e0d\u786e\u5b9a\u73af\u5883\u4e0b\u6301\u7eed\u5b9e\u73b0\u8ba1\u7b97\u548c\u901a\u4fe1\u6548\u7387\u76ee\u6807\u3002"}}
{"id": "2512.13638", "categories": ["cs.DC", "cs.AR"], "pdf": "https://arxiv.org/pdf/2512.13638", "abs": "https://arxiv.org/abs/2512.13638", "authors": ["Aofeng Shen", "Chi Zhang", "Yakup Budanaz", "Alexandru Calotoiu", "Torsten Hoefler", "Luca Benini"], "title": "Design in Tiles: Automating GEMM Deployment on Tile-Based Many-PE Accelerators", "comment": null, "summary": "Tile-based many-Processing Element (PE) accelerators can achieve competitive performance on General Matrix Multiplication (GEMM), but they are extremely hard to program, as their optimal software mapping is deeply coupled with hardware design which is unwieldy to manual deployment. We propose \"Design in Tiles (DiT)\", an automated framework connecting a deployment toolchain with a configurable executable model for these accelerators. For evaluation, we apply our framework to GEMM targeting a large acceleration configuration (e.g., 32x32 tiles, 1979 TFLOPS@FP8, 4 TB/s Bandwidth) comparable to an NVIDIA GH200. We achieve higher PE utilization than GH200 with its expert-tuned GEMM libraries, achieving 1.2-2.0x speedup across diverse matrix shapes.", "AI": {"tldr": "\u63d0\u51faDiT\u6846\u67b6\uff0c\u81ea\u52a8\u5316\u4f18\u5316\u57fa\u4e8etile\u7684PE\u52a0\u901f\u5668\u4e0a\u7684GEMM\u90e8\u7f72\uff0c\u76f8\u6bd4NVIDIA GH200\u4e13\u5bb6\u8c03\u4f18\u5e93\u83b7\u5f971.2-2.0\u500d\u52a0\u901f", "motivation": "\u57fa\u4e8etile\u7684\u591aPE\u52a0\u901f\u5668\u5728GEMM\u4e0a\u80fd\u83b7\u5f97\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\uff0c\u4f46\u6781\u96be\u7f16\u7a0b\uff0c\u56e0\u4e3a\u5176\u6700\u4f18\u8f6f\u4ef6\u6620\u5c04\u4e0e\u786c\u4ef6\u8bbe\u8ba1\u6df1\u5ea6\u8026\u5408\uff0c\u96be\u4ee5\u624b\u52a8\u90e8\u7f72", "method": "\u63d0\u51fa\"Design in Tiles (DiT)\"\u6846\u67b6\uff0c\u5c06\u90e8\u7f72\u5de5\u5177\u94fe\u4e0e\u53ef\u914d\u7f6e\u7684\u53ef\u6267\u884c\u6a21\u578b\u8fde\u63a5\u8d77\u6765\uff0c\u81ea\u52a8\u5316\u4f18\u5316\u52a0\u901f\u5668\u4e0a\u7684GEMM\u90e8\u7f72", "result": "\u5728\u5927\u578b\u52a0\u901f\u914d\u7f6e\uff0832x32 tiles\uff0c1979 TFLOPS@FP8\uff0c4 TB/s\u5e26\u5bbd\uff09\u4e0a\u8bc4\u4f30\uff0c\u76f8\u6bd4NVIDIA GH200\u4e13\u5bb6\u8c03\u4f18\u7684GEMM\u5e93\uff0c\u83b7\u5f97\u66f4\u9ad8\u7684PE\u5229\u7528\u7387\uff0c\u5728\u591a\u6837\u77e9\u9635\u5f62\u72b6\u4e0a\u5b9e\u73b01.2-2.0\u500d\u52a0\u901f", "conclusion": "DiT\u6846\u67b6\u80fd\u6709\u6548\u81ea\u52a8\u5316\u57fa\u4e8etile\u7684PE\u52a0\u901f\u5668\u7684GEMM\u90e8\u7f72\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u5e76\u7b80\u5316\u7f16\u7a0b\u96be\u5ea6"}}
{"id": "2512.12732", "categories": ["cs.DC", "cs.CY"], "pdf": "https://arxiv.org/pdf/2512.12732", "abs": "https://arxiv.org/abs/2512.12732", "authors": ["Georgy Ishmaev", "Emmanuelle Anceaume", "Davide Frey", "Fran\u00e7ois Ta\u00efani"], "title": "Ethical Risk Analysis of L2 Rollups", "comment": null, "summary": "Layer 2 rollups improve throughput and fees, but can reintroduce risk through operator discretion and information asymmetry. We ask which operator and governance designs produce ethically problematic user risk. We adapt Ethical Risk Analysis to rollup architectures, build a role-based taxonomy of decision authority and exposure, and pair the framework with two empirical signals, a cross sectional snapshot of 129 projects from L2BEAT and a hand curated incident set covering 2022 to 2025. We analyze mechanisms that affect risks to users funds, including upgrade timing and exit windows, proposer liveness and whitelisting, forced inclusion usability, and data availability choices. We find that ethical hazards rooted in L2 components control arrangements are widespread: instant upgrades without exit windows appear in about 86 percent of projects, and proposer controls that can freeze withdrawals in about 50 percent. Reported incidents concentrate in sequencer liveness and inclusion, consistent with these dependencies. We translate these findings into ethically grounded suggestions on mitigation strategies including technical components and governance mechanisms.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5c06\u4f26\u7406\u98ce\u9669\u5206\u6790\u5e94\u7528\u4e8eLayer 2 rollup\u67b6\u6784\uff0c\u901a\u8fc7\u6784\u5efa\u51b3\u7b56\u6743\u9650\u4e0e\u98ce\u9669\u66b4\u9732\u7684\u5206\u7c7b\u6846\u67b6\uff0c\u7ed3\u5408\u5bf9129\u4e2a\u9879\u76ee\u7684\u6a2a\u622a\u9762\u5206\u6790\u548c2022-2025\u5e74\u4e8b\u4ef6\u96c6\uff0c\u53d1\u73b0L2\u7ec4\u4ef6\u63a7\u5236\u5b89\u6392\u4e2d\u7684\u4f26\u7406\u98ce\u9669\u666e\u904d\u5b58\u5728\uff0c\u5e76\u63d0\u51fa\u4e86\u57fa\u4e8e\u4f26\u7406\u7684\u7f13\u89e3\u7b56\u7565\u5efa\u8bae\u3002", "motivation": "Layer 2 rollup\u867d\u7136\u63d0\u9ad8\u4e86\u541e\u5410\u91cf\u548c\u964d\u4f4e\u4e86\u8d39\u7528\uff0c\u4f46\u901a\u8fc7\u8fd0\u8425\u8005\u81ea\u7531\u88c1\u91cf\u6743\u548c\u4fe1\u606f\u4e0d\u5bf9\u79f0\u53ef\u80fd\u91cd\u65b0\u5f15\u5165\u98ce\u9669\u3002\u7814\u7a76\u65e8\u5728\u63a2\u7a76\u54ea\u4e9b\u8fd0\u8425\u8005\u548c\u6cbb\u7406\u8bbe\u8ba1\u4f1a\u4ea7\u751f\u4f26\u7406\u4e0a\u6709\u95ee\u9898\u7684\u7528\u6237\u98ce\u9669\u3002", "method": "1. \u5c06\u4f26\u7406\u98ce\u9669\u5206\u6790\u9002\u914d\u5230rollup\u67b6\u6784\uff1b2. \u6784\u5efa\u57fa\u4e8e\u89d2\u8272\u7684\u51b3\u7b56\u6743\u9650\u4e0e\u98ce\u9669\u66b4\u9732\u5206\u7c7b\u6cd5\uff1b3. \u7ed3\u5408\u4e24\u4e2a\u7ecf\u9a8c\u4fe1\u53f7\uff1a\u5bf9L2BEAT\u4e0a129\u4e2a\u9879\u76ee\u7684\u6a2a\u622a\u9762\u5feb\u7167\u5206\u6790\uff0c\u4ee5\u53ca2022-2025\u5e74\u624b\u5de5\u6574\u7406\u7684\u4e8b\u4ef6\u96c6\uff1b4. \u5206\u6790\u5f71\u54cd\u7528\u6237\u8d44\u91d1\u98ce\u9669\u7684\u673a\u5236\uff0c\u5305\u62ec\u5347\u7ea7\u65f6\u673a\u548c\u9000\u51fa\u7a97\u53e3\u3001\u63d0\u8bae\u8005\u6d3b\u8dc3\u5ea6\u548c\u767d\u540d\u5355\u3001\u5f3a\u5236\u5305\u542b\u53ef\u7528\u6027\u4ee5\u53ca\u6570\u636e\u53ef\u7528\u6027\u9009\u62e9\u3002", "result": "\u7814\u7a76\u53d1\u73b0L2\u7ec4\u4ef6\u63a7\u5236\u5b89\u6392\u4e2d\u7684\u4f26\u7406\u98ce\u9669\u666e\u904d\u5b58\u5728\uff1a\u7ea686%\u7684\u9879\u76ee\u5b58\u5728\u65e0\u9000\u51fa\u7a97\u53e3\u7684\u5373\u65f6\u5347\u7ea7\uff0c\u7ea650%\u7684\u9879\u76ee\u5b58\u5728\u53ef\u80fd\u51bb\u7ed3\u63d0\u6b3e\u7684\u63d0\u8bae\u8005\u63a7\u5236\u3002\u62a5\u544a\u7684\u4e8b\u4ef6\u4e3b\u8981\u96c6\u4e2d\u5728\u6392\u5e8f\u5668\u6d3b\u8dc3\u5ea6\u548c\u5305\u542b\u95ee\u9898\u4e0a\uff0c\u4e0e\u8fd9\u4e9b\u4f9d\u8d56\u5173\u7cfb\u4e00\u81f4\u3002", "conclusion": "\u7814\u7a76\u5c06\u8fd9\u4e9b\u53d1\u73b0\u8f6c\u5316\u4e3a\u57fa\u4e8e\u4f26\u7406\u7684\u7f13\u89e3\u7b56\u7565\u5efa\u8bae\uff0c\u5305\u62ec\u6280\u672f\u7ec4\u4ef6\u548c\u6cbb\u7406\u673a\u5236\uff0c\u4e3aLayer 2 rollup\u7684\u4f26\u7406\u98ce\u9669\u7ba1\u7406\u548c\u6cbb\u7406\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u5b9e\u8bc1\u57fa\u7840\u548c\u7406\u8bba\u6846\u67b6\u3002"}}
{"id": "2512.12801", "categories": ["cs.DC", "cs.PF"], "pdf": "https://arxiv.org/pdf/2512.12801", "abs": "https://arxiv.org/abs/2512.12801", "authors": ["Anurag Dutt", "Young Won Choi", "Avirup Sil", "Anshul Gandhi", "Aruna Balasubramanian", "Niranjan Balasubramanian"], "title": "Fine-Grained Energy Prediction For Parallellized LLM Inference With PIE-P", "comment": null, "summary": "With the widespread adoption of Large Language Models (LLMs), energy costs of running LLMs is quickly becoming a critical concern. However, precisely measuring the energy consumption of LLMs is often infeasible because hardware-based power monitors are not always accessible and software-based energy measurement tools are not accurate. While various prediction techniques have been developed to estimate LLM energy consumption, these approaches are limited to single-GPU environments and thus are not applicable to modern LLM inference which is typically parallelized across multiple GPUs. In this work, we remedy this gap and introduce PIE-P, a fine-grained energy prediction framework for multi-GPU inference, including tensor, pipeline, and data parallelism. Predicting the energy under parallelized inference is complicated by the non-determinism in inter-GPU communication, additional communication overheads, and difficulties in isolating energy during the communication/synchronization phase. We develop a scalable prediction framework that addresses these issues via precise sampling, fine-grained modeling of inter-GPU communication, and careful accounting of parallelization overhead. Our evaluation results show that PIE-P yields accurate and fine-grained energy predictions across parallelism strategies, significantly outperforming baselines.", "AI": {"tldr": "PIE-P\uff1a\u4e00\u4e2a\u7528\u4e8e\u591aGPU\u5e76\u884c\u63a8\u7406\u7684\u7ec6\u7c92\u5ea6\u80fd\u8017\u9884\u6d4b\u6846\u67b6\uff0c\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u4ec5\u9002\u7528\u4e8e\u5355GPU\u73af\u5883\u7684\u95ee\u9898", "motivation": "\u968f\u7740\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u80fd\u8017\u6210\u672c\u6210\u4e3a\u5173\u952e\u95ee\u9898\u3002\u73b0\u6709\u80fd\u8017\u6d4b\u91cf\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\uff1a\u786c\u4ef6\u76d1\u63a7\u4e0d\u53ef\u53ca\uff0c\u8f6f\u4ef6\u5de5\u5177\u4e0d\u51c6\u786e\uff0c\u73b0\u6709\u9884\u6d4b\u6280\u672f\u4ec5\u9650\u4e8e\u5355GPU\u73af\u5883\uff0c\u65e0\u6cd5\u9002\u7528\u4e8e\u73b0\u4ee3\u591aGPU\u5e76\u884c\u63a8\u7406\u573a\u666f", "method": "\u5f00\u53d1\u4e86\u53ef\u6269\u5c55\u7684\u9884\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u7cbe\u786e\u91c7\u6837\u3001\u7ec6\u7c92\u5ea6\u5efa\u6a21GPU\u95f4\u901a\u4fe1\u3001\u4ed4\u7ec6\u8ba1\u7b97\u5e76\u884c\u5316\u5f00\u9500\u6765\u89e3\u51b3\u591aGPU\u73af\u5883\u4e2d\u7684\u975e\u786e\u5b9a\u6027\u901a\u4fe1\u3001\u989d\u5916\u901a\u4fe1\u5f00\u9500\u548c\u901a\u4fe1/\u540c\u6b65\u9636\u6bb5\u80fd\u8017\u9694\u79bb\u56f0\u96be\u7b49\u95ee\u9898", "result": "PIE-P\u5728\u5404\u79cd\u5e76\u884c\u7b56\u7565\uff08\u5f20\u91cf\u3001\u6d41\u6c34\u7ebf\u3001\u6570\u636e\u5e76\u884c\uff09\u4e0b\u90fd\u80fd\u63d0\u4f9b\u51c6\u786e\u4e14\u7ec6\u7c92\u5ea6\u7684\u80fd\u8017\u9884\u6d4b\uff0c\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5", "conclusion": "PIE-P\u586b\u8865\u4e86\u591aGPU\u5e76\u884c\u63a8\u7406\u80fd\u8017\u9884\u6d4b\u7684\u7a7a\u767d\uff0c\u4e3a\u73b0\u4ee3LLM\u63a8\u7406\u7684\u80fd\u8017\u4f18\u5316\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u9884\u6d4b\u5de5\u5177"}}
{"id": "2512.12928", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.12928", "abs": "https://arxiv.org/abs/2512.12928", "authors": ["Weizhe Huang", "Tao Peng", "Tongxuan Liu", "Donghe Jin", "Xianzhe Dong", "Ke Zhang"], "title": "PROSERVE: Unified Multi-Priority Request Scheduling for LLM Serving", "comment": "15 pages", "summary": "The widespread deployment of large language models (LLMs) for interactive applications necessitates serving systems that can handle thousands of concurrent requests with diverse Service Level Objective (SLO) requirements. A critical yet often overlooked dimension in this context is the inherent priority difference among clients; for instance, business-critical functions demand higher performance guarantees, as fulfilling such requests yields significantly greater business value. However, existing LLM serving schedulers fail to jointly optimize for both SLO attainment and client-level priorities.\n  To bridge this gap, we first \\textit{formalize multi-priority request scheduling as a service gain maximization problem}, where satisfying latency requirements for requests of different priorities contributes varying levels of gain. We then propose PROSERVE, a unified two-tier scheduling framework designed to maximize overall service gain. At the engine level, SlideBatching dynamically adapts batch formation and request ordering under varying load conditions, employing a sliding boundary mechanism to balance deadline-first and density-first strategies. At the service level, GoRouting performs gain-oriented and capability-aware dispatching across distributed instances, proactively reserving capacity for future high-priority or long requests. Extensive evaluation across four open-source datasets and a real-world industrial trace demonstrates that \\systemname{} consistently outperforms state-of-the-art baselines, improving system gain by up to 35% and boosting SLO attainment by up to 52%.", "AI": {"tldr": "PROSERVE\u662f\u4e00\u4e2a\u9762\u5411\u591a\u4f18\u5148\u7ea7LLM\u670d\u52a1\u7684\u4e24\u5c42\u6b21\u8c03\u5ea6\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u6279\u5904\u7406\u548c\u670d\u52a1\u589e\u76ca\u5bfc\u5411\u7684\u8def\u7531\uff0c\u5728\u6ee1\u8db3\u4e0d\u540c\u4f18\u5148\u7ea7\u8bf7\u6c42SLO\u7684\u540c\u65f6\u6700\u5927\u5316\u7cfb\u7edf\u670d\u52a1\u589e\u76ca\u3002", "motivation": "\u73b0\u6709LLM\u670d\u52a1\u8c03\u5ea6\u5668\u65e0\u6cd5\u540c\u65f6\u4f18\u5316SLO\u8fbe\u6210\u7387\u548c\u5ba2\u6237\u7aef\u4f18\u5148\u7ea7\u3002\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u4e0d\u540c\u4f18\u5148\u7ea7\u7684\u8bf7\u6c42\uff08\u5982\u4e1a\u52a1\u5173\u952e\u529f\u80fd\uff09\u5177\u6709\u4e0d\u540c\u7684\u4e1a\u52a1\u4ef7\u503c\uff0c\u9700\u8981\u5dee\u5f02\u5316\u7684\u6027\u80fd\u4fdd\u8bc1\u3002", "method": "\u63d0\u51faPROSERVE\u4e24\u5c42\u6b21\u8c03\u5ea6\u6846\u67b6\uff1a1) \u5f15\u64ce\u5c42\u7684SlideBatching\u52a8\u6001\u8c03\u6574\u6279\u5904\u7406\u5f62\u6210\u548c\u8bf7\u6c42\u6392\u5e8f\uff0c\u5e73\u8861\u622a\u6b62\u65f6\u95f4\u4f18\u5148\u548c\u5bc6\u5ea6\u4f18\u5148\u7b56\u7565\uff1b2) \u670d\u52a1\u5c42\u7684GoRouting\u57fa\u4e8e\u589e\u76ca\u5bfc\u5411\u548c\u80fd\u529b\u611f\u77e5\u8fdb\u884c\u5206\u5e03\u5f0f\u5b9e\u4f8b\u8c03\u5ea6\uff0c\u4e3a\u672a\u6765\u9ad8\u4f18\u5148\u7ea7\u6216\u957f\u8bf7\u6c42\u9884\u7559\u5bb9\u91cf\u3002", "result": "\u5728\u56db\u4e2a\u5f00\u6e90\u6570\u636e\u96c6\u548c\u771f\u5b9e\u5de5\u4e1a\u8ffd\u8e2a\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cPROSERVE\u76f8\u6bd4\u6700\u5148\u8fdb\u57fa\u7ebf\u63d0\u5347\u7cfb\u7edf\u589e\u76ca\u8fbe35%\uff0cSLO\u8fbe\u6210\u7387\u63d0\u5347\u8fbe52%\u3002", "conclusion": "PROSERVE\u901a\u8fc7\u5f62\u5f0f\u5316\u591a\u4f18\u5148\u7ea7\u8bf7\u6c42\u8c03\u5ea6\u4e3a\u670d\u52a1\u589e\u76ca\u6700\u5927\u5316\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u66f4\u597d\u5730\u6ee1\u8db3\u5b9e\u9645LLM\u670d\u52a1\u90e8\u7f72\u4e2d\u4e0d\u540c\u4f18\u5148\u7ea7\u5ba2\u6237\u7684\u9700\u6c42\u3002"}}
{"id": "2512.12949", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.12949", "abs": "https://arxiv.org/abs/2512.12949", "authors": ["Ziyu Huang", "Yangjie Zhou", "Zihan Liu", "Xinhao Luo", "Yijia Diao", "Minyi Guo", "Jidong Zhai", "Yu Feng", "Chen Zhang", "Anbang Wu", "Jingwen Leng"], "title": "FlashFuser: Expanding the Scale of Kernel Fusion for Compute-Intensive Operators via Inter-Core Connection", "comment": null, "summary": "The scaling of computation throughput continues to outpace improvements in memory bandwidth, making many deep learning workloads memory-bound. Kernel fusion is a key technique to alleviate this problem, but the fusion strategies of existing compilers and frameworks are limited to using local scratchpad memory. When the intermediate results exceed the limited capacity (such as FFN), the fusion fails. Although modern GPUs (like the NVIDIA H100) now incorporate an inter-core connection mechanism known as Distributed Shared Memory(DSM)--providing a larger, high-bandwidth, and low-latency on-chip memory pool--this hardware potential has yet to be exploited by software frameworks. To bridge this gap, we present FlashFuser, the first compiler framework to utilize inter-core connection for kernel fusion on modern GPUs. FlashFuser extends established fusion techniques to the DSM domain through three core contributions. First, we propose a powerful DSM-based communication abstraction that formalizes complex cluster-based data exchange patterns, such as reduce, shuffle and multiply. Second, we introduce a dataflow analyzer that generalizes loop scheduling, resource mapping, and tile selection to the distributed memory hierarchy; it determines the optimal execution order and tile sizes by quantifying data movement across memory levels. Finally, FlashFuser integrates these components into a unified search engine that employs analytical cost modeling and DSM-aware pruning strategies to efficiently discover the optimal execution plan. Our evaluation on an NVIDIA H100 GPU shows that FlashFuser reduces memory access by 58% and delivers kernel speedups of 3.3x against highly-tuned libraries and 4.1x against state-of-the-art compilers, resulting in a 1.24x end-to-end speedup.", "AI": {"tldr": "FlashFuser\uff1a\u9996\u4e2a\u5229\u7528\u73b0\u4ee3GPU\u6838\u95f4\u8fde\u63a5\uff08DSM\uff09\u8fdb\u884c\u5185\u6838\u878d\u5408\u7684\u7f16\u8bd1\u5668\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u5e03\u5f0f\u5185\u5b58\u5c42\u6b21\u4f18\u5316\u51cf\u5c11\u5185\u5b58\u8bbf\u95ee\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd", "motivation": "\u968f\u7740\u8ba1\u7b97\u541e\u5410\u91cf\u589e\u957f\u8d85\u8fc7\u5185\u5b58\u5e26\u5bbd\u6539\u8fdb\uff0c\u6df1\u5ea6\u5b66\u4e60\u5de5\u4f5c\u8d1f\u8f7d\u53d8\u5f97\u5185\u5b58\u53d7\u9650\u3002\u73b0\u6709\u7f16\u8bd1\u5668\u6846\u67b6\u7684\u878d\u5408\u7b56\u7565\u4ec5\u9650\u4e8e\u672c\u5730\u6682\u5b58\u5185\u5b58\uff0c\u5f53\u4e2d\u95f4\u7ed3\u679c\u8d85\u8fc7\u5bb9\u91cf\uff08\u5982FFN\uff09\u65f6\u878d\u5408\u5931\u8d25\u3002\u73b0\u4ee3GPU\uff08\u5982NVIDIA H100\uff09\u5f15\u5165\u4e86\u5206\u5e03\u5f0f\u5171\u4eab\u5185\u5b58\uff08DSM\uff09\u673a\u5236\uff0c\u63d0\u4f9b\u4e86\u66f4\u5927\u3001\u9ad8\u5e26\u5bbd\u3001\u4f4e\u5ef6\u8fdf\u7684\u7247\u4e0a\u5185\u5b58\u6c60\uff0c\u4f46\u8fd9\u4e00\u786c\u4ef6\u6f5c\u529b\u5c1a\u672a\u88ab\u8f6f\u4ef6\u6846\u67b6\u5145\u5206\u5229\u7528\u3002", "method": "FlashFuser\u901a\u8fc7\u4e09\u4e2a\u6838\u5fc3\u8d21\u732e\u5c06\u878d\u5408\u6280\u672f\u6269\u5c55\u5230DSM\u9886\u57df\uff1a1\uff09\u63d0\u51fa\u57fa\u4e8eDSM\u7684\u901a\u4fe1\u62bd\u8c61\uff0c\u5f62\u5f0f\u5316\u96c6\u7fa4\u6570\u636e\u4ea4\u6362\u6a21\u5f0f\uff08\u5982reduce\u3001shuffle\u3001multiply\uff09\uff1b2\uff09\u5f15\u5165\u6570\u636e\u6d41\u5206\u6790\u5668\uff0c\u5c06\u5faa\u73af\u8c03\u5ea6\u3001\u8d44\u6e90\u6620\u5c04\u548c\u5206\u5757\u9009\u62e9\u63a8\u5e7f\u5230\u5206\u5e03\u5f0f\u5185\u5b58\u5c42\u6b21\uff1b3\uff09\u96c6\u6210\u7edf\u4e00\u641c\u7d22\u5f15\u64ce\uff0c\u4f7f\u7528\u5206\u6790\u6210\u672c\u5efa\u6a21\u548cDSM\u611f\u77e5\u526a\u679d\u7b56\u7565\u53d1\u73b0\u6700\u4f18\u6267\u884c\u8ba1\u5212\u3002", "result": "\u5728NVIDIA H100 GPU\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cFlashFuser\u51cf\u5c1158%\u7684\u5185\u5b58\u8bbf\u95ee\uff0c\u76f8\u6bd4\u9ad8\u5ea6\u4f18\u5316\u7684\u5e93\u5b9e\u73b03.3\u500d\u5185\u6838\u52a0\u901f\uff0c\u76f8\u6bd4\u6700\u5148\u8fdb\u7f16\u8bd1\u5668\u5b9e\u73b04.1\u500d\u52a0\u901f\uff0c\u5e26\u67651.24\u500d\u7684\u7aef\u5230\u7aef\u52a0\u901f\u3002", "conclusion": "FlashFuser\u6210\u529f\u5229\u7528\u73b0\u4ee3GPU\u7684\u6838\u95f4\u8fde\u63a5\u673a\u5236\u8fdb\u884c\u5185\u6838\u878d\u5408\uff0c\u586b\u8865\u4e86\u786c\u4ef6\u6f5c\u529b\u4e0e\u8f6f\u4ef6\u6846\u67b6\u4e4b\u95f4\u7684\u7a7a\u767d\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5185\u5b58\u53d7\u9650\u6df1\u5ea6\u5b66\u4e60\u5de5\u4f5c\u8d1f\u8f7d\u7684\u6027\u80fd\u3002"}}
{"id": "2512.13096", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.13096", "abs": "https://arxiv.org/abs/2512.13096", "authors": ["Mohammad Walid Charrwi", "Zaid Hussain"], "title": "Toward Self-Healing Networks-on-Chip: RL-Driven Routing in 2D Torus Architectures", "comment": null, "summary": "We investigate adaptive minimal routing in 2D torus networks on chip NoCs under node fault conditions comparing a reinforcement learning RL based strategy to an adaptive routing baseline A torus topology is used for its low diameter high connectivity properties The RL approach models each router as an agent that learns to forward packets based on network state while the adaptive scheme uses fixed minimal paths with simple rerouting around faults We implement both methods in simulation injecting up to 50 node faults uniformly at random Key metrics are measured 1 throughput vs offered load at fault density 02 2 packet delivery ratio PDR vs fault density and 3 a fault adaptive score FT vs fault density Experimental results show the RL method achieves significantly higher throughput at high load approximately 2030 gain and maintains higher reliability under increasing faults The RL router delivers more packets per cycle and adapts to faults by exploiting path diversity whereas the adaptive scheme degrades sharply as faults accumulate In particular the RL approach preserves end to end connectivity longer PDR remains above 90 until approximately 3040 faults while adaptive PDR drops to approximately 70 at the same point The fault adaptive score likewise favors RL routing Thus RL based adaptive routing demonstrates clear advantages in throughput and fault resilience for torus NoCs", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u57282D\u73af\u9762\u7247\u4e0a\u7f51\u7edc(NoC)\u4e2d\u8282\u70b9\u6545\u969c\u6761\u4ef6\u4e0b\u7684\u81ea\u9002\u5e94\u6700\u5c0f\u8def\u7531\uff0c\u6bd4\u8f83\u4e86\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60(RL)\u7684\u7b56\u7565\u4e0e\u81ea\u9002\u5e94\u8def\u7531\u57fa\u7ebf\u3002RL\u65b9\u6cd5\u5728\u541e\u5410\u91cf\u548c\u6545\u969c\u6062\u590d\u80fd\u529b\u65b9\u9762\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u662f\u63a2\u7d22\u5728\u8282\u70b9\u6545\u969c\u6761\u4ef6\u4e0b\uff0c\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u81ea\u9002\u5e94\u8def\u7531\u7b56\u7565\u80fd\u5426\u6bd4\u4f20\u7edf\u81ea\u9002\u5e94\u8def\u7531\u65b9\u6cd5\u57282D\u73af\u9762NoC\u4e2d\u63d0\u4f9b\u66f4\u597d\u7684\u6027\u80fd\u3002\u73af\u9762\u62d3\u6251\u56e0\u5176\u4f4e\u76f4\u5f84\u548c\u9ad8\u8fde\u63a5\u6027\u800c\u88ab\u91c7\u7528\uff0c\u4f46\u5728\u8282\u70b9\u6545\u969c\u60c5\u51b5\u4e0b\u9700\u8981\u6709\u6548\u7684\u8def\u7531\u7b56\u7565\u6765\u7ef4\u6301\u7f51\u7edc\u6027\u80fd\u3002", "method": "\u7814\u7a76\u65b9\u6cd5\u5305\u62ec\uff1a1) \u5c06\u6bcf\u4e2a\u8def\u7531\u5668\u5efa\u6a21\u4e3a\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\uff0c\u5b66\u4e60\u57fa\u4e8e\u7f51\u7edc\u72b6\u6001\u8f6c\u53d1\u6570\u636e\u5305\uff1b2) \u4f7f\u7528\u56fa\u5b9a\u6700\u5c0f\u8def\u5f84\u5e76\u5728\u6545\u969c\u5468\u56f4\u7b80\u5355\u91cd\u8def\u7531\u7684\u81ea\u9002\u5e94\u65b9\u6848\u4f5c\u4e3a\u57fa\u7ebf\uff1b3) \u5728\u6a21\u62df\u4e2d\u5b9e\u73b0\u4e24\u79cd\u65b9\u6cd5\uff0c\u6ce8\u5165\u6700\u591a50\u4e2a\u5747\u5300\u968f\u673a\u5206\u5e03\u7684\u8282\u70b9\u6545\u969c\uff1b4) \u6d4b\u91cf\u5173\u952e\u6307\u6807\uff1a\u541e\u5410\u91cfvs\u8d1f\u8f7d\u3001\u5305\u4ea4\u4ed8\u7387vs\u6545\u969c\u5bc6\u5ea6\u3001\u6545\u969c\u81ea\u9002\u5e94\u8bc4\u5206vs\u6545\u969c\u5bc6\u5ea6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff1a1) RL\u65b9\u6cd5\u5728\u9ad8\u8d1f\u8f7d\u4e0b\u5b9e\u73b0\u663e\u8457\u66f4\u9ad8\u7684\u541e\u5410\u91cf(\u7ea620-30%\u589e\u76ca)\uff1b2) RL\u5728\u6545\u969c\u589e\u52a0\u65f6\u4fdd\u6301\u66f4\u9ad8\u7684\u53ef\u9760\u6027\uff0c\u5305\u4ea4\u4ed8\u7387\u5728\u7ea630-40\u4e2a\u6545\u969c\u524d\u4fdd\u6301\u572890%\u4ee5\u4e0a\uff0c\u800c\u81ea\u9002\u5e94\u65b9\u6848\u5728\u540c\u4e00\u6545\u969c\u70b9\u4e0b\u964d\u5230\u7ea670%\uff1b3) RL\u8def\u7531\u5668\u6bcf\u5468\u671f\u4f20\u9012\u66f4\u591a\u6570\u636e\u5305\uff0c\u901a\u8fc7\u5229\u7528\u8def\u5f84\u591a\u6837\u6027\u9002\u5e94\u6545\u969c\uff0c\u800c\u81ea\u9002\u5e94\u65b9\u6848\u5728\u6545\u969c\u7d2f\u79ef\u65f6\u6027\u80fd\u6025\u5267\u4e0b\u964d\uff1b4) \u6545\u969c\u81ea\u9002\u5e94\u8bc4\u5206\u540c\u6837\u652f\u6301RL\u8def\u7531\u3002", "conclusion": "\u7ed3\u8bba\u662f\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u81ea\u9002\u5e94\u8def\u7531\u5728\u73af\u9762NoC\u4e2d\u5c55\u793a\u4e86\u541e\u5410\u91cf\u548c\u6545\u969c\u6062\u590d\u80fd\u529b\u7684\u660e\u663e\u4f18\u52bf\u3002RL\u65b9\u6cd5\u80fd\u591f\u66f4\u597d\u5730\u9002\u5e94\u7f51\u7edc\u6545\u969c\uff0c\u7ef4\u6301\u66f4\u9ad8\u7684\u7f51\u7edc\u6027\u80fd\u548c\u53ef\u9760\u6027\uff0c\u4e3a\u6545\u969c\u5bb9\u5fcd\u7684\u7247\u4e0a\u7f51\u7edc\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u5411\u3002"}}
{"id": "2512.13268", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.13268", "abs": "https://arxiv.org/abs/2512.13268", "authors": ["Muhammad Alfian Amrizal", "Raka Satya Prasasta", "Santana Yuda Pradata", "Kadek Gemilang Santiyuda", "Reza Pulungan", "Hiroyuki Takizawa"], "title": "SPARS: A Reinforcement Learning-Enabled Simulator for Power Management in HPC Job Scheduling", "comment": "12 pages, 4 figures, 5 tables", "summary": "High-performance computing (HPC) clusters consume enormous amounts of energy, with idle nodes as a major source of waste. Powering down unused nodes can mitigate this problem, but poorly timed transitions introduce long delays and reduce overall performance. To address this trade-off, we present SPARS, a reinforcement learning-enabled simulator for power management in HPC job scheduling. SPARS integrates job scheduling and node power state management within a discrete-event simulation framework. It supports traditional scheduling policies such as First Come First Served and EASY Backfilling, along with enhanced variants that employ reinforcement learning agents to dynamically decide when nodes should be powered on or off. Users can configure workloads and platforms in JSON format, specifying job arrivals, execution times, node power models, and transition delays. The simulator records comprehensive metrics-including energy usage, wasted power, job waiting times, and node utilization-and provides Gantt chart visualizations to analyze scheduling dynamics and power transitions. Unlike widely used Batsim-based frameworks that rely on heavy inter-process communication, SPARS provides lightweight event handling and consistent simulation results, making experiments easier to reproduce and extend. Its modular design allows new scheduling heuristics or learning algorithms to be integrated with minimal effort. By providing a flexible, reproducible, and extensible platform, SPARS enables researchers and practitioners to systematically evaluate power-aware scheduling strategies, explore the trade-offs between energy efficiency and performance, and accelerate the development of sustainable HPC operations.", "AI": {"tldr": "SPARS\u662f\u4e00\u4e2a\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684HPC\u96c6\u7fa4\u529f\u8017\u7ba1\u7406\u6a21\u62df\u5668\uff0c\u901a\u8fc7\u96c6\u6210\u4f5c\u4e1a\u8c03\u5ea6\u548c\u8282\u70b9\u7535\u6e90\u72b6\u6001\u7ba1\u7406\uff0c\u5728\u8282\u80fd\u548c\u6027\u80fd\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u3002", "motivation": "\u9ad8\u6027\u80fd\u8ba1\u7b97\u96c6\u7fa4\u80fd\u8017\u5de8\u5927\uff0c\u7a7a\u95f2\u8282\u70b9\u9020\u6210\u5927\u91cf\u80fd\u6e90\u6d6a\u8d39\u3002\u5173\u95ed\u672a\u4f7f\u7528\u8282\u70b9\u53ef\u4ee5\u7f13\u89e3\u95ee\u9898\uff0c\u4f46\u4e0d\u5f53\u7684\u5f00\u5173\u65f6\u673a\u4f1a\u5bfc\u81f4\u5ef6\u8fdf\u548c\u6027\u80fd\u4e0b\u964d\uff0c\u9700\u8981\u5728\u8282\u80fd\u548c\u6027\u80fd\u4e4b\u95f4\u627e\u5230\u5e73\u8861\u3002", "method": "SPARS\u91c7\u7528\u79bb\u6563\u4e8b\u4ef6\u6a21\u62df\u6846\u67b6\uff0c\u96c6\u6210\u4f5c\u4e1a\u8c03\u5ea6\u548c\u8282\u70b9\u7535\u6e90\u72b6\u6001\u7ba1\u7406\u3002\u652f\u6301\u4f20\u7edf\u8c03\u5ea6\u7b56\u7565\uff08\u5982FCFS\u3001EASY Backfilling\uff09\u4ee5\u53ca\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u4ee3\u7406\u52a8\u6001\u51b3\u5b9a\u8282\u70b9\u5f00\u5173\u65f6\u673a\u7684\u589e\u5f3a\u53d8\u4f53\u3002\u7cfb\u7edf\u4f7f\u7528JSON\u914d\u7f6e\u5de5\u4f5c\u8d1f\u8f7d\u548c\u5e73\u53f0\u53c2\u6570\uff0c\u8bb0\u5f55\u80fd\u8017\u3001\u7b49\u5f85\u65f6\u95f4\u7b49\u6307\u6807\uff0c\u5e76\u63d0\u4f9b\u7518\u7279\u56fe\u53ef\u89c6\u5316\u3002", "result": "\u76f8\u6bd4\u57fa\u4e8eBatsim\u7684\u6846\u67b6\uff0cSPARS\u63d0\u4f9b\u8f7b\u91cf\u7ea7\u4e8b\u4ef6\u5904\u7406\u548c\u4e00\u81f4\u7684\u6a21\u62df\u7ed3\u679c\uff0c\u6613\u4e8e\u5b9e\u9a8c\u590d\u73b0\u548c\u6269\u5c55\u3002\u5176\u6a21\u5757\u5316\u8bbe\u8ba1\u5141\u8bb8\u8f7b\u677e\u96c6\u6210\u65b0\u7684\u8c03\u5ea6\u542f\u53d1\u5f0f\u7b97\u6cd5\u6216\u5b66\u4e60\u7b97\u6cd5\u3002", "conclusion": "SPARS\u4e3a\u7814\u7a76\u4eba\u5458\u548c\u4ece\u4e1a\u8005\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7075\u6d3b\u3001\u53ef\u590d\u73b0\u3001\u53ef\u6269\u5c55\u7684\u5e73\u53f0\uff0c\u80fd\u591f\u7cfb\u7edf\u8bc4\u4f30\u529f\u8017\u611f\u77e5\u8c03\u5ea6\u7b56\u7565\uff0c\u63a2\u7d22\u80fd\u6548\u4e0e\u6027\u80fd\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u52a0\u901f\u53ef\u6301\u7eedHPC\u8fd0\u8425\u7684\u53d1\u5c55\u3002"}}
{"id": "2512.13319", "categories": ["cs.DC", "eess.SP", "eess.SY", "stat.CO"], "pdf": "https://arxiv.org/pdf/2512.13319", "abs": "https://arxiv.org/abs/2512.13319", "authors": ["Hassan Razavi", "\u00c1ngel F. Garc\u00eda-Fern\u00e1ndez", "Simo S\u00e4rkk\u00e4"], "title": "Temporal parallelisation of continuous-time maximum-a-posteriori trajectory estimation", "comment": null, "summary": "This paper proposes a parallel-in-time method for computing continuous-time maximum-a-posteriori (MAP) trajectory estimates of the states of partially observed stochastic differential equations (SDEs), with the goal of improving computational speed on parallel architectures. The MAP estimation problem is reformulated as a continuous-time optimal control problem based on the Onsager-Machlup functional. This reformulation enables the use of a previously proposed parallel-in-time solution for optimal control problems, which we adapt to the current problem. The structure of the resulting optimal control problem admits a parallel solution based on parallel associative scan algorithms. In the linear Gaussian special case, it yields a parallel Kalman-Bucy filter and a parallel continuous-time Rauch-Tung-Striebel smoother. These linear computational methods are further extended to nonlinear continuous-time state-space models through Taylor expansions. We also present the corresponding parallel two-filter smoother. The graphics processing unit (GPU) experiments on linear and nonlinear models demonstrate that the proposed framework achieves a significant speedup in computations while maintaining the accuracy of sequential algorithms.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5e76\u884c\u65f6\u95f4\u65b9\u6cd5\uff0c\u7528\u4e8e\u8ba1\u7b97\u90e8\u5206\u89c2\u6d4b\u968f\u673a\u5fae\u5206\u65b9\u7a0b\u7684\u8fde\u7eed\u65f6\u95f4MAP\u8f68\u8ff9\u4f30\u8ba1\uff0c\u901a\u8fc7\u5e76\u884c\u67b6\u6784\u63d0\u5347\u8ba1\u7b97\u901f\u5ea6\u3002", "motivation": "\u4f20\u7edf\u8fde\u7eed\u65f6\u95f4MAP\u8f68\u8ff9\u4f30\u8ba1\u7b97\u6cd5\u901a\u5e38\u662f\u987a\u5e8f\u6267\u884c\u7684\uff0c\u65e0\u6cd5\u5145\u5206\u5229\u7528\u73b0\u4ee3\u5e76\u884c\u8ba1\u7b97\u67b6\u6784\uff08\u5982GPU\uff09\u7684\u4f18\u52bf\uff0c\u9650\u5236\u4e86\u8ba1\u7b97\u901f\u5ea6\u7684\u63d0\u5347\u3002", "method": "\u5c06MAP\u4f30\u8ba1\u95ee\u9898\u91cd\u65b0\u8868\u8ff0\u4e3a\u57fa\u4e8eOnsager-Machlup\u6cdb\u51fd\u7684\u8fde\u7eed\u65f6\u95f4\u6700\u4f18\u63a7\u5236\u95ee\u9898\uff0c\u7136\u540e\u91c7\u7528\u5e76\u884c\u65f6\u95f4\u6c42\u89e3\u65b9\u6cd5\uff0c\u5229\u7528\u5e76\u884c\u5173\u8054\u626b\u63cf\u7b97\u6cd5\u5b9e\u73b0\u5e76\u884c\u5316\u3002\u5728\u7ebf\u6027\u9ad8\u65af\u60c5\u51b5\u4e0b\u5f97\u5230\u5e76\u884cKalman-Bucy\u6ee4\u6ce2\u5668\u548c\u5e76\u884c\u8fde\u7eed\u65f6\u95f4Rauch-Tung-Striebel\u5e73\u6ed1\u5668\uff0c\u5e76\u901a\u8fc7\u6cf0\u52d2\u5c55\u5f00\u6269\u5c55\u5230\u975e\u7ebf\u6027\u6a21\u578b\u3002", "result": "GPU\u5b9e\u9a8c\u8868\u660e\uff0c\u5728\u7ebf\u6027\u548c\u975e\u7ebf\u6027\u6a21\u578b\u4e0a\uff0c\u6240\u63d0\u6846\u67b6\u5728\u4fdd\u6301\u987a\u5e8f\u7b97\u6cd5\u7cbe\u5ea6\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u8ba1\u7b97\u52a0\u901f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u5730\u5c06\u8fde\u7eed\u65f6\u95f4MAP\u8f68\u8ff9\u4f30\u8ba1\u95ee\u9898\u5e76\u884c\u5316\uff0c\u4e3a\u5229\u7528\u73b0\u4ee3\u5e76\u884c\u8ba1\u7b97\u786c\u4ef6\u52a0\u901f\u72b6\u6001\u4f30\u8ba1\u63d0\u4f9b\u4e86\u6709\u6548\u6846\u67b6\u3002"}}
{"id": "2512.13488", "categories": ["cs.DC", "cs.CL"], "pdf": "https://arxiv.org/pdf/2512.13488", "abs": "https://arxiv.org/abs/2512.13488", "authors": ["Lei Qu", "Lianhai Ren", "Peng Cheng", "Rui Gao", "Ruizhe Wang", "Tianyu Chen", "Xiao Liu", "Xingjian Zhang", "Yeyun Gong", "Yifan Xiong", "Yucheng Ding", "Yuting Jiang", "Zhenghao Lin", "Zhongxin Guo", "Ziyue Yang"], "title": "SIGMA: An AI-Empowered Training Stack on Early-Life Hardware", "comment": "22 pages, 7 figures", "summary": "An increasing variety of AI accelerators is being considered for large-scale training. However, enabling large-scale training on early-life AI accelerators faces three core challenges: frequent system disruptions and undefined failure modes that undermine reliability; numerical errors and training instabilities that threaten correctness and convergence; and the complexity of parallelism optimization combined with unpredictable local noise that degrades efficiency. To address these challenges, SIGMA is an open-source training stack designed to improve the reliability, stability, and efficiency of large-scale distributed training on early-life AI hardware. The core of this initiative is the LUCIA TRAINING PLATFORM (LTP), the system optimized for clusters with early-life AI accelerators. Since its launch in March 2025, LTP has significantly enhanced training reliability and operational productivity. Over the past five months, it has achieved an impressive 94.45% effective cluster accelerator utilization, while also substantially reducing node recycling and job-recovery times. Building on the foundation of LTP, the LUCIA TRAINING FRAMEWORK (LTF) successfully trained SIGMA-MOE, a 200B MoE model, using 2,048 AI accelerators. This effort delivered remarkable stability and efficiency outcomes, achieving 21.08% MFU, state-of-the-art downstream accuracy, and encountering only one stability incident over a 75-day period. Together, these advances establish SIGMA, which not only tackles the critical challenges of large-scale training but also establishes a new benchmark for AI infrastructure and platform innovation, offering a robust, cost-effective alternative to prevailing established accelerator stacks and significantly advancing AI capabilities and scalability. The source code of SIGMA is available at https://github.com/microsoft/LuciaTrainingPlatform.", "AI": {"tldr": "SIGMA\u662f\u4e00\u4e2a\u5f00\u6e90\u8bad\u7ec3\u6808\uff0c\u65e8\u5728\u63d0\u9ad8\u65e9\u671fAI\u786c\u4ef6\u4e0a\u5927\u89c4\u6a21\u5206\u5e03\u5f0f\u8bad\u7ec3\u7684\u53ef\u9760\u6027\u3001\u7a33\u5b9a\u6027\u548c\u6548\u7387\uff0c\u901a\u8fc7LUCIA\u8bad\u7ec3\u5e73\u53f0(LTP)\u548cLUCIA\u8bad\u7ec3\u6846\u67b6(LTF)\u6210\u529f\u8bad\u7ec3\u4e86200B\u53c2\u6570\u7684MoE\u6a21\u578b\u3002", "motivation": "\u89e3\u51b3\u65e9\u671fAI\u52a0\u901f\u5668\u5728\u5927\u89c4\u6a21\u8bad\u7ec3\u4e2d\u9762\u4e34\u7684\u4e09\u5927\u6311\u6218\uff1a\u7cfb\u7edf\u9891\u7e41\u4e2d\u65ad\u548c\u672a\u5b9a\u4e49\u6545\u969c\u6a21\u5f0f\u5f71\u54cd\u53ef\u9760\u6027\uff1b\u6570\u503c\u9519\u8bef\u548c\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u6027\u5a01\u80c1\u6b63\u786e\u6027\u548c\u6536\u655b\u6027\uff1b\u5e76\u884c\u4f18\u5316\u590d\u6742\u6027\u4e0e\u4e0d\u53ef\u9884\u6d4b\u7684\u672c\u5730\u566a\u58f0\u964d\u4f4e\u6548\u7387\u3002", "method": "\u5f00\u53d1\u4e86SIGMA\u5f00\u6e90\u8bad\u7ec3\u6808\uff0c\u5305\u542bLUCIA\u8bad\u7ec3\u5e73\u53f0(LTP)\u9488\u5bf9\u65e9\u671fAI\u52a0\u901f\u5668\u96c6\u7fa4\u4f18\u5316\u7684\u7cfb\u7edf\uff0c\u4ee5\u53caLUCIA\u8bad\u7ec3\u6846\u67b6(LTF)\u7528\u4e8e\u5927\u89c4\u6a21\u6a21\u578b\u8bad\u7ec3\u3002LTP\u4e13\u6ce8\u4e8e\u63d0\u9ad8\u96c6\u7fa4\u53ef\u9760\u6027\u548c\u5229\u7528\u7387\uff0cLTF\u5728\u6b64\u57fa\u7840\u4e0a\u8bad\u7ec3\u5927\u89c4\u6a21\u6a21\u578b\u3002", "result": "LTP\u5b9e\u73b0\u4e8694.45%\u7684\u6709\u6548\u96c6\u7fa4\u52a0\u901f\u5668\u5229\u7528\u7387\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u8282\u70b9\u56de\u6536\u548c\u4f5c\u4e1a\u6062\u590d\u65f6\u95f4\u3002LTF\u4f7f\u75282,048\u4e2aAI\u52a0\u901f\u5668\u6210\u529f\u8bad\u7ec3\u4e86200B\u53c2\u6570\u7684SIGMA-MOE\u6a21\u578b\uff0c\u5b9e\u73b0\u4e8621.08%\u7684MFU\u3001\u6700\u5148\u8fdb\u7684\u4e0b\u6e38\u7cbe\u5ea6\uff0c\u572875\u5929\u8bad\u7ec3\u4e2d\u4ec5\u9047\u5230\u4e00\u6b21\u7a33\u5b9a\u6027\u4e8b\u4ef6\u3002", "conclusion": "SIGMA\u4e0d\u4ec5\u89e3\u51b3\u4e86\u5927\u89c4\u6a21\u8bad\u7ec3\u7684\u5173\u952e\u6311\u6218\uff0c\u8fd8\u4e3aAI\u57fa\u7840\u8bbe\u65bd\u548c\u5e73\u53f0\u521b\u65b0\u8bbe\u7acb\u4e86\u65b0\u57fa\u51c6\uff0c\u63d0\u4f9b\u4e86\u5f3a\u5927\u4e14\u7ecf\u6d4e\u9ad8\u6548\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u663e\u8457\u63a8\u8fdb\u4e86AI\u80fd\u529b\u548c\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2512.13525", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2512.13525", "abs": "https://arxiv.org/abs/2512.13525", "authors": ["Zhexiang Zhang", "Ye Wang", "Xiangyu Wang", "Yumiao Zhao", "Jingzhe Jiang", "Qizhen Weng", "Shaohuai Shi", "Yin Chen", "Minchen Yu"], "title": "Janus: Disaggregating Attention and Experts for Scalable MoE Inference", "comment": null, "summary": "Large Mixture-of-Experts (MoE) model inference is challenging due to high resource demands and dynamic workloads. Existing solutions often deploy the entire model as a single monolithic unit, which applies a unified resource configuration to both attention and expert modules despite their different requirements, leading to limited scalability and resource inefficiency. In this paper, we propose Janus, a scalable MoE inference system that disaggregates attention and experts on separate GPU sub-clusters, enabling each module to be managed and scaled independently. Janus incorporates three key designs for efficient, disaggregated MoE inference. First, it proposes an adaptive two-phase communication scheme that exploits intra- and inter-node bandwidth hierarchies for low-latency data exchange. Second, motivated by the memory-bound nature of MoE modules, Janus introduces a lightweight scheduler and implements it as a GPU kernel to balance the number of activated experts across GPUs at minimal overhead, thereby reducing inference latency. Third, Janus performs fine-grained resource management to dynamically adjust expert placement and independently scale attention and MoE resources to improve overall efficiency. Evaluation shows Janus achieves up to 3.9 higher perGPU throughput than state-of-the-art systems while meeting per-token latency requirements.", "AI": {"tldr": "Janus\u662f\u4e00\u4e2a\u53ef\u6269\u5c55\u7684MoE\u63a8\u7406\u7cfb\u7edf\uff0c\u901a\u8fc7\u5c06\u6ce8\u610f\u529b\u6a21\u5757\u548c\u4e13\u5bb6\u6a21\u5757\u5206\u79bb\u5230\u4e0d\u540c\u7684GPU\u5b50\u96c6\u7fa4\u4e0a\uff0c\u5b9e\u73b0\u72ec\u7acb\u7ba1\u7406\u548c\u6269\u5c55\uff0c\u663e\u8457\u63d0\u5347\u63a8\u7406\u6548\u7387\u548c\u541e\u5410\u91cf\u3002", "motivation": "\u73b0\u6709MoE\u63a8\u7406\u7cfb\u7edf\u901a\u5e38\u5c06\u6574\u4e2a\u6a21\u578b\u4f5c\u4e3a\u5355\u4e00\u6574\u4f53\u90e8\u7f72\uff0c\u5bf9\u6ce8\u610f\u529b\u6a21\u5757\u548c\u4e13\u5bb6\u6a21\u5757\u91c7\u7528\u7edf\u4e00\u7684\u8d44\u6e90\u914d\u7f6e\uff0c\u5ffd\u7565\u4e86\u5b83\u4eec\u4e0d\u540c\u7684\u9700\u6c42\uff0c\u5bfc\u81f4\u53ef\u6269\u5c55\u6027\u6709\u9650\u548c\u8d44\u6e90\u6548\u7387\u4f4e\u4e0b\u3002", "method": "1. \u5c06\u6ce8\u610f\u529b\u6a21\u5757\u548c\u4e13\u5bb6\u6a21\u5757\u5206\u79bb\u5230\u72ec\u7acb\u7684GPU\u5b50\u96c6\u7fa4\uff1b2. \u91c7\u7528\u81ea\u9002\u5e94\u4e24\u9636\u6bb5\u901a\u4fe1\u65b9\u6848\uff0c\u5229\u7528\u8282\u70b9\u5185\u548c\u8282\u70b9\u95f4\u5e26\u5bbd\u5c42\u6b21\u7ed3\u6784\uff1b3. \u5f15\u5165\u8f7b\u91cf\u7ea7\u8c03\u5ea6\u5668\u4f5c\u4e3aGPU\u5185\u6838\uff0c\u5e73\u8861GPU\u95f4\u7684\u6fc0\u6d3b\u4e13\u5bb6\u6570\u91cf\uff1b4. \u6267\u884c\u7ec6\u7c92\u5ea6\u8d44\u6e90\u7ba1\u7406\uff0c\u52a8\u6001\u8c03\u6574\u4e13\u5bb6\u653e\u7f6e\u5e76\u72ec\u7acb\u6269\u5c55\u8d44\u6e90\u548cMoE\u8d44\u6e90\u3002", "result": "Janus\u76f8\u6bd4\u73b0\u6709\u6700\u5148\u8fdb\u7cfb\u7edf\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8fbe3.9\u500d\u7684\u6bcfGPU\u541e\u5410\u91cf\u63d0\u5347\uff0c\u540c\u65f6\u6ee1\u8db3\u6bcf\u4e2atoken\u7684\u5ef6\u8fdf\u8981\u6c42\u3002", "conclusion": "Janus\u901a\u8fc7\u89e3\u8026\u6ce8\u610f\u529b\u6a21\u5757\u548c\u4e13\u5bb6\u6a21\u5757\uff0c\u7ed3\u5408\u81ea\u9002\u5e94\u901a\u4fe1\u3001\u8f7b\u91cf\u7ea7\u8c03\u5ea6\u548c\u7ec6\u7c92\u5ea6\u8d44\u6e90\u7ba1\u7406\uff0c\u4e3a\u5927\u89c4\u6a21MoE\u6a21\u578b\u63a8\u7406\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2512.13591", "categories": ["cs.DC", "astro-ph.IM", "cs.PF"], "pdf": "https://arxiv.org/pdf/2512.13591", "abs": "https://arxiv.org/abs/2512.13591", "authors": ["Denisa-Andreea Constantinescu", "Rub\u00e9n Rodr\u00edguez \u00c1lvarez", "Jacques Morin", "Etienne Orliac", "Micka\u00ebl Dardaillon", "Sunrise Wang", "Hugo Miomandre", "Miguel Pe\u00f3n-Quir\u00f3s", "Jean-Fran\u00e7ois Nezan", "David Atienza"], "title": "astroCAMP: A Community Benchmark and Co-Design Framework for Sustainable SKA-Scale Radio Imaging", "comment": "11 pages, 10 figures", "summary": "The Square Kilometre Array (SKA) project will operate one of the world's largest continuous scientific data systems, sustaining petascale imaging under strict power caps. Yet, current radio-interferometric pipelines utilize only a small fraction of hardware peak performance, typically 4-14%, due to memory and I/O bottlenecks, resulting in poor energy efficiency and high operational and carbon costs. Progress is further limited by the absence of standardised metrics and fidelity tolerances, preventing principled hardware-software co-design and rigorous exploration of quality-efficiency trade-offs. We introduce astroCAMP, a framework for guiding the co-design of next-generation imaging pipelines and sustainable HPC architectures that maximise scientific return within SKA's operational and environmental limits. astroCAMP provides: (1) a unified, extensible metric suite covering scientific fidelity, computational performance, sustainability, and lifecycle economics; (2) standardised SKA-representative datasets and reference outputs enabling reproducible benchmarking across CPUs, GPUs, and emerging accelerators; and (3) a multi-objective co-design formulation linking scientific-quality constraints to time-, energy-, carbon-to-solution, and total cost of ownership. We release datasets, benchmarking results, and a reproducibility kit, and evaluate co-design metrics for WSClean and IDG on an AMD EPYC 9334 processor and an NVIDIA H100 GPU. Further, we illustrate the use of astroCAMP for heterogeneous CPU-FPGA design-space exploration, and its potential to facilitate the identification of Pareto-optimal operating points for SKA-scale imaging deployments. Last, we make a call to the SKA community to define quantifiable fidelity metrics and thresholds to accelerate principled optimisation for SKA-scale imaging.", "AI": {"tldr": "astroCAMP\u6846\u67b6\u4e3aSKA\u5c04\u7535\u671b\u8fdc\u955c\u9879\u76ee\u63d0\u4f9b\u786c\u4ef6-\u8f6f\u4ef6\u534f\u540c\u8bbe\u8ba1\uff0c\u901a\u8fc7\u7edf\u4e00\u5ea6\u91cf\u6807\u51c6\u3001\u6807\u51c6\u5316\u6570\u636e\u96c6\u548c\u591a\u76ee\u6807\u4f18\u5316\uff0c\u89e3\u51b3\u5f53\u524d\u6210\u50cf\u7ba1\u7ebf\u6027\u80fd\u4f4e\u4e0b\uff08\u4ec54-14%\u786c\u4ef6\u5cf0\u503c\uff09\u548c\u80fd\u6548\u5dee\u7684\u95ee\u9898\uff0c\u4ee5\u5728\u529f\u7387\u9650\u5236\u4e0b\u6700\u5927\u5316\u79d1\u5b66\u4ea7\u51fa\u3002", "motivation": "SKA\u9879\u76ee\u9762\u4e34\u4e25\u91cd\u6027\u80fd\u74f6\u9888\uff1a\u5f53\u524d\u5c04\u7535\u5e72\u6d89\u6210\u50cf\u7ba1\u7ebf\u4ec5\u80fd\u5229\u75284-14%\u7684\u786c\u4ef6\u5cf0\u503c\u6027\u80fd\uff0c\u5b58\u5728\u5185\u5b58\u548cI/O\u74f6\u9888\uff0c\u5bfc\u81f4\u80fd\u6548\u4f4e\u4e0b\u3001\u8fd0\u8425\u6210\u672c\u548c\u78b3\u6392\u653e\u9ad8\u3002\u540c\u65f6\u7f3a\u4e4f\u6807\u51c6\u5316\u5ea6\u91cf\u548c\u4fdd\u771f\u5ea6\u5bb9\u5dee\uff0c\u963b\u788d\u4e86\u786c\u4ef6-\u8f6f\u4ef6\u534f\u540c\u8bbe\u8ba1\u548c\u8d28\u91cf-\u6548\u7387\u6743\u8861\u7684\u63a2\u7d22\u3002", "method": "\u63d0\u51faastroCAMP\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1) \u7edf\u4e00\u7684\u6269\u5c55\u5ea6\u91cf\u5957\u4ef6\uff0c\u6db5\u76d6\u79d1\u5b66\u4fdd\u771f\u5ea6\u3001\u8ba1\u7b97\u6027\u80fd\u3001\u53ef\u6301\u7eed\u6027\u548c\u751f\u547d\u5468\u671f\u7ecf\u6d4e\u5b66\uff1b2) \u6807\u51c6\u5316\u7684SKA\u4ee3\u8868\u6027\u6570\u636e\u96c6\u548c\u53c2\u8003\u8f93\u51fa\uff0c\u652f\u6301\u8de8CPU\u3001GPU\u548c\u65b0\u5174\u52a0\u901f\u5668\u7684\u53ef\u91cd\u590d\u57fa\u51c6\u6d4b\u8bd5\uff1b3) \u591a\u76ee\u6807\u534f\u540c\u8bbe\u8ba1\u516c\u5f0f\uff0c\u5c06\u79d1\u5b66\u8d28\u91cf\u7ea6\u675f\u4e0e\u65f6\u95f4\u3001\u80fd\u8017\u3001\u78b3\u6392\u653e\u548c\u603b\u62e5\u6709\u6210\u672c\u8054\u7cfb\u8d77\u6765\u3002", "result": "\u53d1\u5e03\u4e86\u6570\u636e\u96c6\u3001\u57fa\u51c6\u6d4b\u8bd5\u7ed3\u679c\u548c\u53ef\u91cd\u590d\u6027\u5de5\u5177\u5305\uff0c\u5728AMD EPYC 9334\u5904\u7406\u5668\u548cNVIDIA H100 GPU\u4e0a\u8bc4\u4f30\u4e86WSClean\u548cIDG\u7684\u534f\u540c\u8bbe\u8ba1\u5ea6\u91cf\u3002\u5c55\u793a\u4e86astroCAMP\u5728\u5f02\u6784CPU-FPGA\u8bbe\u8ba1\u7a7a\u95f4\u63a2\u7d22\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u80fd\u591f\u8bc6\u522bSKA\u89c4\u6a21\u6210\u50cf\u90e8\u7f72\u7684\u5e15\u7d2f\u6258\u6700\u4f18\u64cd\u4f5c\u70b9\u3002", "conclusion": "astroCAMP\u4e3a\u4e0b\u4e00\u4ee3\u6210\u50cf\u7ba1\u7ebf\u548c\u53ef\u6301\u7eedHPC\u67b6\u6784\u7684\u534f\u540c\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u6846\u67b6\uff0c\u80fd\u591f\u5728SKA\u7684\u8fd0\u8425\u548c\u73af\u5883\u9650\u5236\u4e0b\u6700\u5927\u5316\u79d1\u5b66\u56de\u62a5\u3002\u547c\u5401SKA\u793e\u533a\u5b9a\u4e49\u53ef\u91cf\u5316\u7684\u4fdd\u771f\u5ea6\u5ea6\u91cf\u548c\u9608\u503c\uff0c\u4ee5\u52a0\u901fSKA\u89c4\u6a21\u6210\u50cf\u7684\u539f\u5219\u6027\u4f18\u5316\u3002"}}
