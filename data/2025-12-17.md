<div id=toc></div>

# Table of Contents

- [cs.DC](#cs.DC) [Total: 4]
- [cs.AR](#cs.AR) [Total: 5]


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [1] [Real-Time Service Subscription and Adaptive Offloading Control in Vehicular Edge Computing](https://arxiv.org/abs/2512.14002)
*Chuanchao Gao,Arvind Easwaran*

Main category: cs.DC

TL;DR: 本文提出了一种用于车载边缘计算中带截止时间约束的任务卸载和资源分配问题的近似算法SARound，将最佳近似比从1/6提升到1/4，并设计了在线服务订阅和卸载控制框架。


<details>
  <summary>Details</summary>
Motivation: 车载边缘计算中，由于网络带宽和计算资源受限、任务截止时间严格以及网络条件快速变化，高效的任务卸载和资源分配对于时间关键型应用仍然具有挑战性。

Method: 1) 将问题形式化为带截止时间约束的任务卸载和资源分配问题(DOAP)；2) 提出基于线性规划舍入和局部比率技术的近似算法SARound；3) 设计在线服务订阅和卸载控制框架处理短期任务截止时间和快速变化的无线网络条件；4) 开发综合VEC模拟器VecSim进行验证。

Result: SARound算法将DOAP问题的最佳近似比从1/6提升到1/4。基于对象检测应用和真实出租车轨迹数据的实验结果表明，SARound在不同网络条件下始终优于现有基线方法，同时保持运行时效率。

Conclusion: 提出的SARound算法和在线框架有效解决了车载边缘计算中带截止时间约束的任务卸载和资源分配问题，显著提升了系统性能，为智能交通系统中的实时应用提供了可行的解决方案。

Abstract: Vehicular Edge Computing (VEC) has emerged as a promising paradigm for enhancing the computational efficiency and service quality in intelligent transportation systems by enabling vehicles to wirelessly offload computation-intensive tasks to nearby Roadside Units. However, efficient task offloading and resource allocation for time-critical applications in VEC remain challenging due to constrained network bandwidth and computational resources, stringent task deadlines, and rapidly changing network conditions. To address these challenges, we formulate a Deadline-Constrained Task Offloading and Resource Allocation Problem (DOAP), denoted as $\mathbf{P}$, in VEC with both bandwidth and computational resource constraints, aiming to maximize the total vehicle utility. To solve $\mathbf{P}$, we propose $\mathtt{SARound}$, an approximation algorithm based on Linear Program rounding and local-ratio techniques, that improves the best-known approximation ratio for DOAP from $\frac{1}{6}$ to $\frac{1}{4}$. Additionally, we design an online service subscription and offloading control framework to address the challenges of short task deadlines and rapidly changing wireless network conditions. To validate our approach, we develop a comprehensive VEC simulator, VecSim, using the open-source simulation libraries OMNeT++ and Simu5G. VecSim integrates our designed framework to manage the full life-cycle of real-time vehicular tasks. Experimental results, based on profiled object detection applications and real-world taxi trace data, show that $\mathtt{SARound}$ consistently outperforms state-of-the-art baselines under varying network conditions while maintaining runtime efficiency.

</details>


### [2] [A Hybrid Reactive-Proactive Auto-scaling Algorithm for SLA-Constrained Edge Computing](https://arxiv.org/abs/2512.14290)
*Suhrid Gupta,Muhammed Tawfiqul Islam,Rajkumar Buyya*

Main category: cs.DC

TL;DR: 提出一种结合机器学习的混合自动扩缩算法，用于边缘计算环境，相比现有方案将SLA违规率从23%降低到6%。


<details>
  <summary>Details</summary>
Motivation: 边缘计算通过微服务架构实现低延迟应用，但需要满足严格的SLA（性能、可靠性、可用性）。现有自动扩缩算法存在性能问题和配置复杂性，无法保证SLA合规性。

Method: 提出混合自动扩缩算法：结合基于机器学习的主动预测算法（预测资源需求）和反应式扩缩器（基于当前资源利用率和SLA约束进行即时调整），并集成到Kubernetes作为扩展。

Result: 在边缘环境中进行广泛实验评估，结果显示现有解决方案的SLA违规率高达23%，而提出的混合解决方案仅6%，在各种应用中都能确保稳定的SLA合规性。

Conclusion: 提出的混合自动扩缩算法能有效解决边缘计算中的SLA合规问题，显著降低违规率，为边缘计算应用提供可靠的资源管理解决方案。

Abstract: Edge computing decentralizes computing resources, allowing for novel applications in domains such as the Internet of Things (IoT) in healthcare and agriculture by reducing latency and improving performance. This decentralization is achieved through the implementation of microservice architectures, which require low latencies to meet stringent service level agreements (SLA) such as performance, reliability, and availability metrics. While cloud computing offers the large data storage and computation resources necessary to handle peak demands, a hybrid cloud and edge environment is required to ensure SLA compliance. This is achieved by sophisticated orchestration strategies such as Kubernetes, which help facilitate resource management. The orchestration strategies alone do not guarantee SLA adherence due to the inherent delay of scaling resources. Existing auto-scaling algorithms have been proposed to address these challenges, but they suffer from performance issues and configuration complexity. In this paper, a novel auto-scaling algorithm is proposed for SLA-constrained edge computing applications. This approach combines a Machine Learning (ML) based proactive auto-scaling algorithm, capable of predicting incoming resource requests to forecast demand, with a reactive autoscaler which considers current resource utilization and SLA constraints for immediate adjustments. The algorithm is integrated into Kubernetes as an extension, and its performance is evaluated through extensive experiments in an edge environment with real applications. The results demonstrate that existing solutions have an SLA violation rate of up to 23%, whereas the proposed hybrid solution outperforms the baselines with an SLA violation rate of only 6%, ensuring stable SLA compliance across various applications.

</details>


### [3] [Performance and Stability of Barrier Mode Parallel Systems with Heterogeneous and Redundant Jobs](https://arxiv.org/abs/2512.14445)
*Brenton Walker,Markus Fidler*

Main category: cs.DC

TL;DR: 分析屏障（barrier）在并行计算中对系统稳定性和性能的影响，特别针对Apache Spark的屏障执行模式，研究屏障导致的空闲时间、稳定性惩罚和性能开销。


<details>
  <summary>Details</summary>
Motivation: 在并行计算中，许多任务需要同步开始或结束时间（如Apache Spark的屏障执行模式），这会导致部分工作节点出现空闲期，从而降低系统稳定性和性能。需要量化分析屏障带来的负面影响。

Method: 1. 分析(s,k,l)屏障系统的稳定性（允许任务在完成l/k后离开）；2. 推导混合屏障系统的性能界限（同时处理有屏障和无屏障作业）；3. 针对纯1-屏障情况，将理论界限和仿真结果与真实Spark系统基准数据对比；4. 研究真实系统中的开销分布，归因于双重事件和轮询驱动调度机制；5. 为此类开销建立模型并通过仿真验证。

Result: 论文提供了屏障系统稳定性和性能惩罚的理论分析框架，建立了屏障开销的数学模型，并通过与真实Spark系统的对比验证了模型的有效性，揭示了屏障执行模式对并行计算性能的具体影响。

Conclusion: 屏障在并行计算中不可避免地引入空闲时间和性能开销，通过建立数学模型可以量化这些影响。研究结果为优化屏障执行模式（如Apache Spark的屏障执行）提供了理论基础，有助于设计更高效的并行计算系统。

Abstract: In some models of parallel computation, jobs are split into smaller tasks and can be executed completely asynchronously. In other situations the parallel tasks have constraints that require them to synchronize their start and possibly departure times. This is true of many parallelized machine learning workloads, and the popular Apache Spark processing engine has recently added support for Barrier Execution Mode, which allows users to add such barriers to their jobs. These barriers necessarily result in idle periods on some of the workers, which reduces their stability and performance, compared to equivalent workloads with no barriers.
  In this paper we will consider and analyze the stability and performance penalties resulting from barriers. We include an analysis of the stability of $(s,k,l)$ barrier systems that allow jobs to depart after $l$ out of $k$ of their tasks complete. We also derive and evaluate performance bounds for hybrid barrier systems servicing a mix of jobs, both with and without barriers, and with varying degrees of parallelism. For the purely 1-barrier case we compare the bounds and simulation results to benchmark data from a standalone Spark system. We study the overhead in the real system, and based on its distribution we attribute it to the dual event and polling-driven mechanism used to schedule barrier-mode jobs. We develop a model for this type of overhead and validate it against the real system through simulation.

</details>


### [4] [PruneX: A Hierarchical Communication-Efficient System for Distributed CNN Training with Structured Pruning](https://arxiv.org/abs/2512.14628)
*Alireza Olama,Andreas Lundell,Izzat El Hajj,Johan Lilius,Jerker Björkqvist*

Main category: cs.DC

TL;DR: PruneX是一个分布式训练系统，通过层次化结构化剪枝算法减少节点间通信开销，在64个GPU上实现6.75倍强扩展加速


<details>
  <summary>Details</summary>
Motivation: 多节点GPU集群中，节点间通信带宽日益成为分布式训练的主要瓶颈。传统剪枝感知的分布式训练系统通常无法有效减少通信开销，因为非结构化稀疏性无法被高度优化的密集集合原语有效利用。

Method: 提出PruneX系统，采用层次化结构化ADMM算法，在节点间同步前强制执行节点级结构化稀疏性，实现动态缓冲区压缩。系统采用领导者-追随者执行模型，分离节点内和节点间进程组，在带宽受限链路上对压缩张量执行密集集合操作，同时将完全同步限制在高带宽节点内互连上。

Result: 在64个GPU上的ResNet架构评估显示，PruneX减少约60%的节点间通信量，实现6.75倍强扩展加速，优于密集基线（5.81倍）和Top-K梯度压缩（3.71倍）。

Conclusion: PruneX通过剪枝算法与集群层次结构的协同设计，有效减少分布式训练中的节点间通信开销，显著提升扩展性能，为解决大规模分布式训练通信瓶颈提供了新思路。

Abstract: Inter-node communication bandwidth increasingly constrains distributed training at scale on multi-node GPU clusters. While compact models are the ultimate deployment target, conventional pruning-aware distributed training systems typically fail to reduce communication overhead because unstructured sparsity cannot be efficiently exploited by highly optimized dense collective primitives. We present PruneX, a distributed data-parallel training system that co-designs pruning algorithms with cluster hierarchy to reduce inter-node bandwidth usage. PruneX introduces the Hierarchical Structured ADMM (H-SADMM) algorithm, which enforces node-level structured sparsity before inter-node synchronization, enabling dynamic buffer compaction that eliminates both zero-valued transmissions and indexing overhead. The system adopts a leader-follower execution model with separated intra-node and inter-node process groups, performing dense collectives on compacted tensors over bandwidth-limited links while confining full synchronization to high-bandwidth intra-node interconnects. Evaluation on ResNet architectures across 64 GPUs demonstrates that PruneX reduces inter-node communication volume by approximately 60% and achieves 6.75x strong scaling speedup, outperforming the dense baseline (5.81x) and Top-K gradient compression (3.71x) on the Puhti supercomputer at CSC - IT Center for Science (Finland).

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [5] [Adaptive Cache Pollution Control for Large Language Model Inference Workloads Using Temporal CNN-Based Prediction and Priority-Aware Replacement](https://arxiv.org/abs/2512.14151)
*Songze Liu,Hongkun Du,Shaowen Wang*

Main category: cs.AR

TL;DR: 提出针对LLM推理的自适应缓存污染控制机制ACPC，结合TCN访问预测和优先级感知替换策略，显著减少缓存污染并提升性能


<details>
  <summary>Details</summary>
Motivation: LLM推理时频繁的token序列查找和嵌入向量检索产生高度不规则和突发性访问模式，导致传统预取和替换策略误判，引发严重缓存污染，降低系统性能

Method: 提出自适应缓存污染控制机制ACPC，集成基于TCN的访问预测和优先级感知替换策略。TCN模块学习token访问序列的时间依赖性以识别高重用缓存行，替换策略根据预测重用可能性和缓存占用动态调整驱逐优先级

Result: ACPC减少缓存污染41.7%，提升缓存命中率8.9%，降低L2缺失惩罚60.0%。相比最先进的机器学习替换基线，token生成吞吐量提升15.9%，最终损失最低为0.21

Conclusion: ACPC能有效识别有用缓存行并减少冗余预取，为大规模LLM服务系统提供可扩展的学习驱动解决方案，优化内存效率和延迟

Abstract: Large Language Models (LLMs), such as GPT and LLaMA, introduce unique memory access characteristics during inference due to frequent token sequence lookups and embedding vector retrievals. These workloads generate highly irregular and bursty access patterns, causing traditional prefetching and replacement policies to mispredict and trigger severe cache pollution, thereby degrading system performance. To address this challenge, this paper proposes an Adaptive Cache Pollution Control (ACPC) mechanism tailored for LLM inference workloads, integrating Temporal Convolutional Network (TCN)-based access prediction with a priority-aware replacement strategy. The TCN module learns temporal dependencies in token access sequences to identify potential high-reuse cache lines, while the replacement policy dynamically adjusts eviction priorities based on predicted reuse likelihood and cache occupancy. The proposed framework is implemented and evaluated on representative transformer-based inference traces, including GPT-style autoregressive decoding and embedding retrieval workloads. Experimental results demonstrate that ACPC reduces cache pollution by 41.7 percent, improves cache hit rate by 8.9 percent, and achieves a 60.0 percent reduction in L2 miss penalty, compared with state-of-the-art machine-learning-based replacement baselines. Additionally, the proposed Temporal CNN-based ACPC framework increases token generation throughput by 15.9 percent and achieves the lowest final loss of 0.21, confirming its superior efficiency and stability under complex LLM inference workloads. These results highlight ACPC's effectiveness in recognizing useful cache lines and mitigating redundant prefetches under dynamic LLM access behaviors. The proposed approach provides a scalable, learning-driven solution for optimizing memory efficiency and latency in large-scale LLM serving and inference systems.

</details>


### [6] [ReadyPower: A Reliable, Interpretable, and Handy Architectural Power Model Based on Analytical Framework](https://arxiv.org/abs/2512.14172)
*Qijun Zhang,Shang Liu,Yao Lu,Mengming Li,Zhiyao Xie*

Main category: cs.AR

TL;DR: ReadyPower是一个新的分析性功耗建模框架，通过引入架构、实现和技术三个层面的参数来改进传统McPAT模型，解决了ML模型不可靠、难解释、难使用的问题，在准确性和相关性上都优于ML基线模型。


<details>
  <summary>Details</summary>
Motivation: 现代处理器设计中功耗是主要目标，需要准确高效的功耗建模技术。传统分析性架构级功耗模型（如McPAT）存在显著不准确问题，而基于机器学习的方法虽然精度更高但存在不可靠、解释性差、使用困难三大局限，难以在工业界广泛应用。

Method: 提出ReadyPower分析性功耗建模框架，通过向广泛采用的McPAT分析模型中引入架构级、实现级和技术级参数来弥补实际处理器实现与分析模型之间的差异。三个不同层次的参数采用不同方式确定。

Result: 在不同训练场景下平均，ReadyPower在BOOM和XiangShan两种CPU架构上都实现了比ML基线模型更低的平均绝对百分比误差（MAPE降低>20%）和更高的相关系数R（提高>0.2）。

Conclusion: ReadyPower框架通过改进传统分析模型，解决了ML功耗模型的局限性，提供了一个可靠、可解释且易于使用的功耗建模解决方案，在准确性和相关性方面都优于现有ML方法。

Abstract: Power is a primary objective in modern processor design, requiring accurate yet efficient power modeling techniques. Architecture-level power models are necessary for early power optimization and design space exploration. However, classical analytical architecture-level power models (e.g., McPAT) suffer from significant inaccuracies. Emerging machine learning (ML)-based power models, despite their superior accuracy in research papers, are not widely adopted in the industry. In this work, we point out three inherent limitations of ML-based power models: unreliability, limited interpretability, and difficulty in usage. This work proposes a new analytical power modeling framework named ReadyPower, which is ready-for-use by being reliable, interpretable, and handy. We observe that the root cause of the low accuracy of classical analytical power models is the discrepancies between the real processor implementation and the processor's analytical model. To bridge the discrepancies, we introduce architecture-level, implementation-level, and technology-level parameters into the widely adopted McPAT analytical model to build ReadyPower. The parameters at three different levels are decided in different ways. In our experiment, averaged across different training scenarios, ReadyPower achieves >20% lower mean absolute percentage error (MAPE) and >0.2 higher correlation coefficient R compared with the ML-based baselines, on both BOOM and XiangShan CPU architectures.baselines, on both BOOM and XiangShan CPU architectures.

</details>


### [7] [TEMP: A Memory Efficient Physical-aware Tensor Partition-Mapping Framework on Wafer-scale Chips](https://arxiv.org/abs/2512.14256)
*Huizheng Wang,Taiquan Wei,Zichuan Wang,Dingcheng Jiang,Qize Yang,Jiaxin Liu,Jingxiang Hou,Chao Li,Jinyi Deng,Yang Hu,Shouyi Yin*

Main category: cs.AR

TL;DR: TEMP框架通过拓扑感知的张量流分区、流量感知映射和双层晶圆求解，优化了晶圆级芯片上大语言模型训练的内存效率和吞吐量，相比现有系统实现了1.7倍的平均吞吐量提升。


<details>
  <summary>Details</summary>
Motivation: 晶圆级芯片虽然提供高计算能力和芯片间带宽，但面临片上内存与计算资源的独特权衡。现有张量并行策略未能充分利用晶圆级芯片的通信优势同时保持内存效率，限制了其性能发挥。

Method: 提出张量流分区范式，并开发TEMP框架，包含：1) 拓扑感知的张量流分区，2) 流量感知映射，3) 双层晶圆求解，以克服晶圆级芯片2D网格拓扑的通信限制和并行化挑战。

Result: TEMP在各种大语言模型上相比最先进的训练系统实现了1.7倍的平均吞吐量提升，有效优化了内存效率和计算吞吐量。

Conclusion: TEMP框架通过集成拓扑感知分区、流量映射和双层求解，成功克服了晶圆级芯片的硬件约束，释放了张量流分区范式在晶圆级芯片上的全部潜力。

Abstract: Large language models (LLMs) demand significant memory and computation resources. Wafer-scale chips (WSCs) provide high computation power and die-to-die (D2D) bandwidth but face a unique trade-off between on-chip memory and compute resources due to limited wafer area. Therefore, tensor parallelism strategies for wafer should leverage communication advantages while maintaining memory efficiency to maximize WSC performance. However, existing approaches fail to address these challenges.
  To address these challenges, we propose the tensor stream partition paradigm (TSPP), which reveals an opportunity to leverage WSCs' abundant communication bandwidth to alleviate stringent on-chip memory constraints. However, the 2D mesh topology of WSCs lacks long-distance and flexible interconnects, leading to three challenges: 1) severe tail latency, 2) prohibitive D2D traffic contention, and 3) intractable search time for optimal design.
  We present TEMP, a framework for LLM training on WSCs that leverages topology-aware tensor-stream partition, traffic-conscious mapping, and dual-level wafer solving to overcome hardware constraints and parallelism challenges. These integrated approaches optimize memory efficiency and throughput, unlocking TSPP's full potential on WSCs. Evaluations show TEMP achieves 1.7x average throughput improvement over state-of-the-art LLM training systems across various models.

</details>


### [8] [PADE: A Predictor-Free Sparse Attention Accelerator via Unified Execution and Stage Fusion](https://arxiv.org/abs/2512.14322)
*Huizheng Wang,Hongbin Wang,Zichuan Wang,Zhiheng Yue,Yang Wang,Chao Li,Yang Hu,Shouyi Yin*

Main category: cs.AR

TL;DR: PADE是一个无需预测器的算法-硬件协同设计，通过位级不确定性区间保护过滤、双向稀疏性乱序执行和交错稀疏分块注意力等技术，实现了动态稀疏注意力加速，相比H100 GPU获得7.43倍加速和31.1倍能效提升。


<details>
  <summary>Details</summary>
Motivation: 基于注意力的模型虽然革命性，但自注意力的二次计算成本带来严重的计算和内存开销。现有稀疏注意力方法因需要额外的稀疏性预测器而硬件效率低下，缺乏实用性。

Method: 提出PADE算法-硬件协同设计：1) 位级不确定性区间保护过滤策略，在每个比特轮次准确识别无关紧要的token；2) 双向稀疏性乱序执行，提高硬件利用率；3) 交错稀疏分块注意力，降低I/O和计算复杂度。结合定制加速器设计，实现无需预测器的稀疏加速。

Result: 在22个基准测试中，PADE相比Nvidia H100 GPU实现7.43倍加速和31.1倍能效提升。相比SOTA加速器Sanger、DOTA和SOFA，分别实现5.1倍、4.3倍和3.4倍节能。

Conclusion: PADE通过创新的算法-硬件协同设计，解决了现有稀疏注意力方法的预测器依赖问题，实现了高效实用的动态稀疏注意力加速，显著提升了计算效率和能效比。

Abstract: Attention-based models have revolutionized AI, but the quadratic cost of self-attention incurs severe computational and memory overhead. Sparse attention methods alleviate this by skipping low-relevance token pairs. However, current approaches lack practicality due to the heavy expense of added sparsity predictor, which severely drops their hardware efficiency.
  This paper advances the state-of-the-art (SOTA) by proposing a bit-serial enable stage-fusion (BSF) mechanism, which eliminates the need for a separate predictor. However, it faces key challenges: 1) Inaccurate bit-sliced sparsity speculation leads to incorrect pruning; 2) Hardware under-utilization due to fine-grained and imbalanced bit-level workloads. 3) Tiling difficulty caused by the row-wise dependency in sparsity pruning criteria.
  We propose PADE, a predictor-free algorithm-hardware co-design for dynamic sparse attention acceleration. PADE features three key innovations: 1) Bit-wise uncertainty interval-enabled guard filtering (BUI-GF) strategy to accurately identify trivial tokens during each bit round; 2) Bidirectional sparsity-based out-of-order execution (BS-OOE) to improve hardware utilization; 3) Interleaving-based sparsity-tiled attention (ISTA) to reduce both I/O and computational complexity. These techniques, combined with custom accelerator designs, enable practical sparsity acceleration without relying on an added sparsity predictor. Extensive experiments on 22 benchmarks show that PADE achieves 7.43x speed up and 31.1x higher energy efficiency than Nvidia H100 GPU. Compared to SOTA accelerators, PADE achieves 5.1x, 4.3x and 3.4x energy saving than Sanger, DOTA and SOFA.

</details>


### [9] [Focus: A Streaming Concentration Architecture for Efficient Vision-Language Models](https://arxiv.org/abs/2512.14661)
*Chiyue Wei,Cong Guo,Junyao Zhang,Haoxuan Shan,Yifan Xu,Ziyue Zhang,Yudong Liu,Qinsi Wang,Changchun Zhou,Hai "Helen" Li,Yiran Chen*

Main category: cs.AR

TL;DR: Focus提出了一种流式聚焦架构，通过多层次细粒度冗余消除来加速视觉语言模型推理，实现2.4倍加速和3.3倍能耗降低。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型规模增大和视频级输入导致计算和内存开销显著，给硬件加速器实时部署带来挑战。现有方法通常采用粗粒度令牌剪枝或合并，存在运行时开销高的问题。

Method: 提出多层次聚焦范式：1）基于文本提示的语义引导令牌剪枝；2）使用局部比较的空间-时间块级聚焦；3）通过运动感知匹配的向量级冗余消除。所有步骤与架构协同设计，支持流式友好的片上执行。

Result: 在脉动阵列加速器中实现，相比现有最先进加速器，Focus实现了2.4倍加速和3.3倍能耗降低，在性能和能效方面均有显著优势。

Conclusion: Focus通过渐进式细粒度冗余消除有效加速VLM推理，为硬件加速器上的实时部署提供了高效解决方案，代码已开源。

Abstract: Vision-Language Models (VLMs) have demonstrated strong performance on tasks such as video captioning and visual question answering. However, their growing scale and video-level inputs lead to significant computational and memory overhead, posing challenges for real-time deployment on hardware accelerators. While prior work attempts to reduce redundancy via token pruning or merging, these methods typically operate at coarse granularity and incur high runtime overhead due to global token-level operations. In this study, we propose Focus, a Streaming Concentration Architecture that efficiently accelerates VLM inference through progressive, fine-grained redundancy elimination. Focus introduces a multilevel concentration paradigm that hierarchically compresses vision-language inputs at three levels: (1) semantic-guided token pruning based on textual prompts, (2) spatial-temporal block-level concentration using localized comparisons, and (3) vector-level redundancy removal via motion-aware matching. All concentration steps are tightly co-designed with the architecture to support streaming-friendly, on-chip execution. Focus leverages GEMM tiling, convolution-style layout, and cross-modal attention to minimize off-chip access while enabling high throughput. Implemented as a modular unit within a systolic-array accelerator, Focus achieves a 2.4x speedup and 3.3x reduction in energy, significantly outperforming state-of-the-art accelerators in both performance and energy efficiency. Full-stack implementation of Focus is open-sourced at https://github.com/dubcyfor3/Focus.

</details>
